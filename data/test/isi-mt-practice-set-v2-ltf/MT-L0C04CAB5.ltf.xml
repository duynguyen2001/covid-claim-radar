<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAB5" lang="spa" raw_text_char_length="16112" raw_text_md5="8ea504c2dc34cd3a1472631173bb92e9" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="75" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Satellite data suggests coronavirus may have hit China earlier: Researchers</ORIGINAL_TEXT>
<TOKEN end_char="9" id="token-0-0" morph="none" pos="word" start_char="1">Satellite</TOKEN>
<TOKEN end_char="14" id="token-0-1" morph="none" pos="word" start_char="11">data</TOKEN>
<TOKEN end_char="23" id="token-0-2" morph="none" pos="word" start_char="16">suggests</TOKEN>
<TOKEN end_char="35" id="token-0-3" morph="none" pos="word" start_char="25">coronavirus</TOKEN>
<TOKEN end_char="39" id="token-0-4" morph="none" pos="word" start_char="37">may</TOKEN>
<TOKEN end_char="44" id="token-0-5" morph="none" pos="word" start_char="41">have</TOKEN>
<TOKEN end_char="48" id="token-0-6" morph="none" pos="word" start_char="46">hit</TOKEN>
<TOKEN end_char="54" id="token-0-7" morph="none" pos="word" start_char="50">China</TOKEN>
<TOKEN end_char="62" id="token-0-8" morph="none" pos="word" start_char="56">earlier</TOKEN>
<TOKEN end_char="63" id="token-0-9" morph="none" pos="punct" start_char="63">:</TOKEN>
<TOKEN end_char="75" id="token-0-10" morph="none" pos="word" start_char="65">Researchers</TOKEN>
</SEG>
<SEG end_char="347" id="segment-1" start_char="80">
<ORIGINAL_TEXT>Dramatic spikes in auto traffic around major hospitals in Wuhan last fall suggest the novel coronavirus may have been present and spreading through central China long before the outbreak was first reported to the world, according to a new Harvard Medical School study.</ORIGINAL_TEXT>
<TOKEN end_char="87" id="token-1-0" morph="none" pos="word" start_char="80">Dramatic</TOKEN>
<TOKEN end_char="94" id="token-1-1" morph="none" pos="word" start_char="89">spikes</TOKEN>
<TOKEN end_char="97" id="token-1-2" morph="none" pos="word" start_char="96">in</TOKEN>
<TOKEN end_char="102" id="token-1-3" morph="none" pos="word" start_char="99">auto</TOKEN>
<TOKEN end_char="110" id="token-1-4" morph="none" pos="word" start_char="104">traffic</TOKEN>
<TOKEN end_char="117" id="token-1-5" morph="none" pos="word" start_char="112">around</TOKEN>
<TOKEN end_char="123" id="token-1-6" morph="none" pos="word" start_char="119">major</TOKEN>
<TOKEN end_char="133" id="token-1-7" morph="none" pos="word" start_char="125">hospitals</TOKEN>
<TOKEN end_char="136" id="token-1-8" morph="none" pos="word" start_char="135">in</TOKEN>
<TOKEN end_char="142" id="token-1-9" morph="none" pos="word" start_char="138">Wuhan</TOKEN>
<TOKEN end_char="147" id="token-1-10" morph="none" pos="word" start_char="144">last</TOKEN>
<TOKEN end_char="152" id="token-1-11" morph="none" pos="word" start_char="149">fall</TOKEN>
<TOKEN end_char="160" id="token-1-12" morph="none" pos="word" start_char="154">suggest</TOKEN>
<TOKEN end_char="164" id="token-1-13" morph="none" pos="word" start_char="162">the</TOKEN>
<TOKEN end_char="170" id="token-1-14" morph="none" pos="word" start_char="166">novel</TOKEN>
<TOKEN end_char="182" id="token-1-15" morph="none" pos="word" start_char="172">coronavirus</TOKEN>
<TOKEN end_char="186" id="token-1-16" morph="none" pos="word" start_char="184">may</TOKEN>
<TOKEN end_char="191" id="token-1-17" morph="none" pos="word" start_char="188">have</TOKEN>
<TOKEN end_char="196" id="token-1-18" morph="none" pos="word" start_char="193">been</TOKEN>
<TOKEN end_char="204" id="token-1-19" morph="none" pos="word" start_char="198">present</TOKEN>
<TOKEN end_char="208" id="token-1-20" morph="none" pos="word" start_char="206">and</TOKEN>
<TOKEN end_char="218" id="token-1-21" morph="none" pos="word" start_char="210">spreading</TOKEN>
<TOKEN end_char="226" id="token-1-22" morph="none" pos="word" start_char="220">through</TOKEN>
<TOKEN end_char="234" id="token-1-23" morph="none" pos="word" start_char="228">central</TOKEN>
<TOKEN end_char="240" id="token-1-24" morph="none" pos="word" start_char="236">China</TOKEN>
<TOKEN end_char="245" id="token-1-25" morph="none" pos="word" start_char="242">long</TOKEN>
<TOKEN end_char="252" id="token-1-26" morph="none" pos="word" start_char="247">before</TOKEN>
<TOKEN end_char="256" id="token-1-27" morph="none" pos="word" start_char="254">the</TOKEN>
<TOKEN end_char="265" id="token-1-28" morph="none" pos="word" start_char="258">outbreak</TOKEN>
<TOKEN end_char="269" id="token-1-29" morph="none" pos="word" start_char="267">was</TOKEN>
<TOKEN end_char="275" id="token-1-30" morph="none" pos="word" start_char="271">first</TOKEN>
<TOKEN end_char="284" id="token-1-31" morph="none" pos="word" start_char="277">reported</TOKEN>
<TOKEN end_char="287" id="token-1-32" morph="none" pos="word" start_char="286">to</TOKEN>
<TOKEN end_char="291" id="token-1-33" morph="none" pos="word" start_char="289">the</TOKEN>
<TOKEN end_char="297" id="token-1-34" morph="none" pos="word" start_char="293">world</TOKEN>
<TOKEN end_char="298" id="token-1-35" morph="none" pos="punct" start_char="298">,</TOKEN>
<TOKEN end_char="308" id="token-1-36" morph="none" pos="word" start_char="300">according</TOKEN>
<TOKEN end_char="311" id="token-1-37" morph="none" pos="word" start_char="310">to</TOKEN>
<TOKEN end_char="313" id="token-1-38" morph="none" pos="word" start_char="313">a</TOKEN>
<TOKEN end_char="317" id="token-1-39" morph="none" pos="word" start_char="315">new</TOKEN>
<TOKEN end_char="325" id="token-1-40" morph="none" pos="word" start_char="319">Harvard</TOKEN>
<TOKEN end_char="333" id="token-1-41" morph="none" pos="word" start_char="327">Medical</TOKEN>
<TOKEN end_char="340" id="token-1-42" morph="none" pos="word" start_char="335">School</TOKEN>
<TOKEN end_char="346" id="token-1-43" morph="none" pos="word" start_char="342">study</TOKEN>
<TOKEN end_char="347" id="token-1-44" morph="none" pos="punct" start_char="347">.</TOKEN>
</SEG>
<SEG end_char="640" id="segment-2" start_char="350">
<ORIGINAL_TEXT>Using techniques similar to those employed by intelligence agencies, the research team behind the study analyzed commercial satellite imagery and "observed a dramatic increase in hospital traffic outside five major Wuhan hospitals beginning late summer and early fall 2019," according to Dr.</ORIGINAL_TEXT>
<TOKEN end_char="354" id="token-2-0" morph="none" pos="word" start_char="350">Using</TOKEN>
<TOKEN end_char="365" id="token-2-1" morph="none" pos="word" start_char="356">techniques</TOKEN>
<TOKEN end_char="373" id="token-2-2" morph="none" pos="word" start_char="367">similar</TOKEN>
<TOKEN end_char="376" id="token-2-3" morph="none" pos="word" start_char="375">to</TOKEN>
<TOKEN end_char="382" id="token-2-4" morph="none" pos="word" start_char="378">those</TOKEN>
<TOKEN end_char="391" id="token-2-5" morph="none" pos="word" start_char="384">employed</TOKEN>
<TOKEN end_char="394" id="token-2-6" morph="none" pos="word" start_char="393">by</TOKEN>
<TOKEN end_char="407" id="token-2-7" morph="none" pos="word" start_char="396">intelligence</TOKEN>
<TOKEN end_char="416" id="token-2-8" morph="none" pos="word" start_char="409">agencies</TOKEN>
<TOKEN end_char="417" id="token-2-9" morph="none" pos="punct" start_char="417">,</TOKEN>
<TOKEN end_char="421" id="token-2-10" morph="none" pos="word" start_char="419">the</TOKEN>
<TOKEN end_char="430" id="token-2-11" morph="none" pos="word" start_char="423">research</TOKEN>
<TOKEN end_char="435" id="token-2-12" morph="none" pos="word" start_char="432">team</TOKEN>
<TOKEN end_char="442" id="token-2-13" morph="none" pos="word" start_char="437">behind</TOKEN>
<TOKEN end_char="446" id="token-2-14" morph="none" pos="word" start_char="444">the</TOKEN>
<TOKEN end_char="452" id="token-2-15" morph="none" pos="word" start_char="448">study</TOKEN>
<TOKEN end_char="461" id="token-2-16" morph="none" pos="word" start_char="454">analyzed</TOKEN>
<TOKEN end_char="472" id="token-2-17" morph="none" pos="word" start_char="463">commercial</TOKEN>
<TOKEN end_char="482" id="token-2-18" morph="none" pos="word" start_char="474">satellite</TOKEN>
<TOKEN end_char="490" id="token-2-19" morph="none" pos="word" start_char="484">imagery</TOKEN>
<TOKEN end_char="494" id="token-2-20" morph="none" pos="word" start_char="492">and</TOKEN>
<TOKEN end_char="496" id="token-2-21" morph="none" pos="punct" start_char="496">"</TOKEN>
<TOKEN end_char="504" id="token-2-22" morph="none" pos="word" start_char="497">observed</TOKEN>
<TOKEN end_char="506" id="token-2-23" morph="none" pos="word" start_char="506">a</TOKEN>
<TOKEN end_char="515" id="token-2-24" morph="none" pos="word" start_char="508">dramatic</TOKEN>
<TOKEN end_char="524" id="token-2-25" morph="none" pos="word" start_char="517">increase</TOKEN>
<TOKEN end_char="527" id="token-2-26" morph="none" pos="word" start_char="526">in</TOKEN>
<TOKEN end_char="536" id="token-2-27" morph="none" pos="word" start_char="529">hospital</TOKEN>
<TOKEN end_char="544" id="token-2-28" morph="none" pos="word" start_char="538">traffic</TOKEN>
<TOKEN end_char="552" id="token-2-29" morph="none" pos="word" start_char="546">outside</TOKEN>
<TOKEN end_char="557" id="token-2-30" morph="none" pos="word" start_char="554">five</TOKEN>
<TOKEN end_char="563" id="token-2-31" morph="none" pos="word" start_char="559">major</TOKEN>
<TOKEN end_char="569" id="token-2-32" morph="none" pos="word" start_char="565">Wuhan</TOKEN>
<TOKEN end_char="579" id="token-2-33" morph="none" pos="word" start_char="571">hospitals</TOKEN>
<TOKEN end_char="589" id="token-2-34" morph="none" pos="word" start_char="581">beginning</TOKEN>
<TOKEN end_char="594" id="token-2-35" morph="none" pos="word" start_char="591">late</TOKEN>
<TOKEN end_char="601" id="token-2-36" morph="none" pos="word" start_char="596">summer</TOKEN>
<TOKEN end_char="605" id="token-2-37" morph="none" pos="word" start_char="603">and</TOKEN>
<TOKEN end_char="611" id="token-2-38" morph="none" pos="word" start_char="607">early</TOKEN>
<TOKEN end_char="616" id="token-2-39" morph="none" pos="word" start_char="613">fall</TOKEN>
<TOKEN end_char="621" id="token-2-40" morph="none" pos="word" start_char="618">2019</TOKEN>
<TOKEN end_char="623" id="token-2-41" morph="none" pos="punct" start_char="622">,"</TOKEN>
<TOKEN end_char="633" id="token-2-42" morph="none" pos="word" start_char="625">according</TOKEN>
<TOKEN end_char="636" id="token-2-43" morph="none" pos="word" start_char="635">to</TOKEN>
<TOKEN end_char="639" id="token-2-44" morph="none" pos="word" start_char="638">Dr</TOKEN>
<TOKEN end_char="640" id="token-2-45" morph="none" pos="punct" start_char="640">.</TOKEN>
</SEG>
<SEG end_char="709" id="segment-3" start_char="642">
<ORIGINAL_TEXT>John Brownstein, the Harvard Medical professor who led the research.</ORIGINAL_TEXT>
<TOKEN end_char="645" id="token-3-0" morph="none" pos="word" start_char="642">John</TOKEN>
<TOKEN end_char="656" id="token-3-1" morph="none" pos="word" start_char="647">Brownstein</TOKEN>
<TOKEN end_char="657" id="token-3-2" morph="none" pos="punct" start_char="657">,</TOKEN>
<TOKEN end_char="661" id="token-3-3" morph="none" pos="word" start_char="659">the</TOKEN>
<TOKEN end_char="669" id="token-3-4" morph="none" pos="word" start_char="663">Harvard</TOKEN>
<TOKEN end_char="677" id="token-3-5" morph="none" pos="word" start_char="671">Medical</TOKEN>
<TOKEN end_char="687" id="token-3-6" morph="none" pos="word" start_char="679">professor</TOKEN>
<TOKEN end_char="691" id="token-3-7" morph="none" pos="word" start_char="689">who</TOKEN>
<TOKEN end_char="695" id="token-3-8" morph="none" pos="word" start_char="693">led</TOKEN>
<TOKEN end_char="699" id="token-3-9" morph="none" pos="word" start_char="697">the</TOKEN>
<TOKEN end_char="708" id="token-3-10" morph="none" pos="word" start_char="701">research</TOKEN>
<TOKEN end_char="709" id="token-3-11" morph="none" pos="punct" start_char="709">.</TOKEN>
</SEG>
<SEG end_char="945" id="segment-4" start_char="712">
<ORIGINAL_TEXT>Brownstein, an ABC News contributor, said the traffic increase also "coincided with" elevated queries on a Chinese internet search for "certain symptoms that would later be determined as closely associated with the novel coronavirus."</ORIGINAL_TEXT>
<TOKEN end_char="721" id="token-4-0" morph="none" pos="word" start_char="712">Brownstein</TOKEN>
<TOKEN end_char="722" id="token-4-1" morph="none" pos="punct" start_char="722">,</TOKEN>
<TOKEN end_char="725" id="token-4-2" morph="none" pos="word" start_char="724">an</TOKEN>
<TOKEN end_char="729" id="token-4-3" morph="none" pos="word" start_char="727">ABC</TOKEN>
<TOKEN end_char="734" id="token-4-4" morph="none" pos="word" start_char="731">News</TOKEN>
<TOKEN end_char="746" id="token-4-5" morph="none" pos="word" start_char="736">contributor</TOKEN>
<TOKEN end_char="747" id="token-4-6" morph="none" pos="punct" start_char="747">,</TOKEN>
<TOKEN end_char="752" id="token-4-7" morph="none" pos="word" start_char="749">said</TOKEN>
<TOKEN end_char="756" id="token-4-8" morph="none" pos="word" start_char="754">the</TOKEN>
<TOKEN end_char="764" id="token-4-9" morph="none" pos="word" start_char="758">traffic</TOKEN>
<TOKEN end_char="773" id="token-4-10" morph="none" pos="word" start_char="766">increase</TOKEN>
<TOKEN end_char="778" id="token-4-11" morph="none" pos="word" start_char="775">also</TOKEN>
<TOKEN end_char="780" id="token-4-12" morph="none" pos="punct" start_char="780">"</TOKEN>
<TOKEN end_char="789" id="token-4-13" morph="none" pos="word" start_char="781">coincided</TOKEN>
<TOKEN end_char="794" id="token-4-14" morph="none" pos="word" start_char="791">with</TOKEN>
<TOKEN end_char="795" id="token-4-15" morph="none" pos="punct" start_char="795">"</TOKEN>
<TOKEN end_char="804" id="token-4-16" morph="none" pos="word" start_char="797">elevated</TOKEN>
<TOKEN end_char="812" id="token-4-17" morph="none" pos="word" start_char="806">queries</TOKEN>
<TOKEN end_char="815" id="token-4-18" morph="none" pos="word" start_char="814">on</TOKEN>
<TOKEN end_char="817" id="token-4-19" morph="none" pos="word" start_char="817">a</TOKEN>
<TOKEN end_char="825" id="token-4-20" morph="none" pos="word" start_char="819">Chinese</TOKEN>
<TOKEN end_char="834" id="token-4-21" morph="none" pos="word" start_char="827">internet</TOKEN>
<TOKEN end_char="841" id="token-4-22" morph="none" pos="word" start_char="836">search</TOKEN>
<TOKEN end_char="845" id="token-4-23" morph="none" pos="word" start_char="843">for</TOKEN>
<TOKEN end_char="847" id="token-4-24" morph="none" pos="punct" start_char="847">"</TOKEN>
<TOKEN end_char="854" id="token-4-25" morph="none" pos="word" start_char="848">certain</TOKEN>
<TOKEN end_char="863" id="token-4-26" morph="none" pos="word" start_char="856">symptoms</TOKEN>
<TOKEN end_char="868" id="token-4-27" morph="none" pos="word" start_char="865">that</TOKEN>
<TOKEN end_char="874" id="token-4-28" morph="none" pos="word" start_char="870">would</TOKEN>
<TOKEN end_char="880" id="token-4-29" morph="none" pos="word" start_char="876">later</TOKEN>
<TOKEN end_char="883" id="token-4-30" morph="none" pos="word" start_char="882">be</TOKEN>
<TOKEN end_char="894" id="token-4-31" morph="none" pos="word" start_char="885">determined</TOKEN>
<TOKEN end_char="897" id="token-4-32" morph="none" pos="word" start_char="896">as</TOKEN>
<TOKEN end_char="905" id="token-4-33" morph="none" pos="word" start_char="899">closely</TOKEN>
<TOKEN end_char="916" id="token-4-34" morph="none" pos="word" start_char="907">associated</TOKEN>
<TOKEN end_char="921" id="token-4-35" morph="none" pos="word" start_char="918">with</TOKEN>
<TOKEN end_char="925" id="token-4-36" morph="none" pos="word" start_char="923">the</TOKEN>
<TOKEN end_char="931" id="token-4-37" morph="none" pos="word" start_char="927">novel</TOKEN>
<TOKEN end_char="943" id="token-4-38" morph="none" pos="word" start_char="933">coronavirus</TOKEN>
<TOKEN end_char="945" id="token-4-39" morph="none" pos="punct" start_char="944">."</TOKEN>
</SEG>
<SEG end_char="1103" id="segment-5" start_char="948">
<ORIGINAL_TEXT>Though Brownstein acknowledged the evidence is circumstantial, he said the study makes for an important new data point in the mystery of COVID-19's origins.</ORIGINAL_TEXT>
<TOKEN end_char="953" id="token-5-0" morph="none" pos="word" start_char="948">Though</TOKEN>
<TOKEN end_char="964" id="token-5-1" morph="none" pos="word" start_char="955">Brownstein</TOKEN>
<TOKEN end_char="977" id="token-5-2" morph="none" pos="word" start_char="966">acknowledged</TOKEN>
<TOKEN end_char="981" id="token-5-3" morph="none" pos="word" start_char="979">the</TOKEN>
<TOKEN end_char="990" id="token-5-4" morph="none" pos="word" start_char="983">evidence</TOKEN>
<TOKEN end_char="993" id="token-5-5" morph="none" pos="word" start_char="992">is</TOKEN>
<TOKEN end_char="1008" id="token-5-6" morph="none" pos="word" start_char="995">circumstantial</TOKEN>
<TOKEN end_char="1009" id="token-5-7" morph="none" pos="punct" start_char="1009">,</TOKEN>
<TOKEN end_char="1012" id="token-5-8" morph="none" pos="word" start_char="1011">he</TOKEN>
<TOKEN end_char="1017" id="token-5-9" morph="none" pos="word" start_char="1014">said</TOKEN>
<TOKEN end_char="1021" id="token-5-10" morph="none" pos="word" start_char="1019">the</TOKEN>
<TOKEN end_char="1027" id="token-5-11" morph="none" pos="word" start_char="1023">study</TOKEN>
<TOKEN end_char="1033" id="token-5-12" morph="none" pos="word" start_char="1029">makes</TOKEN>
<TOKEN end_char="1037" id="token-5-13" morph="none" pos="word" start_char="1035">for</TOKEN>
<TOKEN end_char="1040" id="token-5-14" morph="none" pos="word" start_char="1039">an</TOKEN>
<TOKEN end_char="1050" id="token-5-15" morph="none" pos="word" start_char="1042">important</TOKEN>
<TOKEN end_char="1054" id="token-5-16" morph="none" pos="word" start_char="1052">new</TOKEN>
<TOKEN end_char="1059" id="token-5-17" morph="none" pos="word" start_char="1056">data</TOKEN>
<TOKEN end_char="1065" id="token-5-18" morph="none" pos="word" start_char="1061">point</TOKEN>
<TOKEN end_char="1068" id="token-5-19" morph="none" pos="word" start_char="1067">in</TOKEN>
<TOKEN end_char="1072" id="token-5-20" morph="none" pos="word" start_char="1070">the</TOKEN>
<TOKEN end_char="1080" id="token-5-21" morph="none" pos="word" start_char="1074">mystery</TOKEN>
<TOKEN end_char="1083" id="token-5-22" morph="none" pos="word" start_char="1082">of</TOKEN>
<TOKEN end_char="1094" id="token-5-23" morph="none" pos="unknown" start_char="1085">COVID-19's</TOKEN>
<TOKEN end_char="1102" id="token-5-24" morph="none" pos="word" start_char="1096">origins</TOKEN>
<TOKEN end_char="1103" id="token-5-25" morph="none" pos="punct" start_char="1103">.</TOKEN>
</SEG>
<SEG end_char="1287" id="segment-6" start_char="1106">
<ORIGINAL_TEXT>"Something was happening in October," said Brownstein, the chief innovation officer at Boston Children’s Hospital and director of the medical center’s Computational Epidemiology Lab.</ORIGINAL_TEXT>
<TOKEN end_char="1106" id="token-6-0" morph="none" pos="punct" start_char="1106">"</TOKEN>
<TOKEN end_char="1115" id="token-6-1" morph="none" pos="word" start_char="1107">Something</TOKEN>
<TOKEN end_char="1119" id="token-6-2" morph="none" pos="word" start_char="1117">was</TOKEN>
<TOKEN end_char="1129" id="token-6-3" morph="none" pos="word" start_char="1121">happening</TOKEN>
<TOKEN end_char="1132" id="token-6-4" morph="none" pos="word" start_char="1131">in</TOKEN>
<TOKEN end_char="1140" id="token-6-5" morph="none" pos="word" start_char="1134">October</TOKEN>
<TOKEN end_char="1142" id="token-6-6" morph="none" pos="punct" start_char="1141">,"</TOKEN>
<TOKEN end_char="1147" id="token-6-7" morph="none" pos="word" start_char="1144">said</TOKEN>
<TOKEN end_char="1158" id="token-6-8" morph="none" pos="word" start_char="1149">Brownstein</TOKEN>
<TOKEN end_char="1159" id="token-6-9" morph="none" pos="punct" start_char="1159">,</TOKEN>
<TOKEN end_char="1163" id="token-6-10" morph="none" pos="word" start_char="1161">the</TOKEN>
<TOKEN end_char="1169" id="token-6-11" morph="none" pos="word" start_char="1165">chief</TOKEN>
<TOKEN end_char="1180" id="token-6-12" morph="none" pos="word" start_char="1171">innovation</TOKEN>
<TOKEN end_char="1188" id="token-6-13" morph="none" pos="word" start_char="1182">officer</TOKEN>
<TOKEN end_char="1191" id="token-6-14" morph="none" pos="word" start_char="1190">at</TOKEN>
<TOKEN end_char="1198" id="token-6-15" morph="none" pos="word" start_char="1193">Boston</TOKEN>
<TOKEN end_char="1209" id="token-6-16" morph="none" pos="word" start_char="1200">Children’s</TOKEN>
<TOKEN end_char="1218" id="token-6-17" morph="none" pos="word" start_char="1211">Hospital</TOKEN>
<TOKEN end_char="1222" id="token-6-18" morph="none" pos="word" start_char="1220">and</TOKEN>
<TOKEN end_char="1231" id="token-6-19" morph="none" pos="word" start_char="1224">director</TOKEN>
<TOKEN end_char="1234" id="token-6-20" morph="none" pos="word" start_char="1233">of</TOKEN>
<TOKEN end_char="1238" id="token-6-21" morph="none" pos="word" start_char="1236">the</TOKEN>
<TOKEN end_char="1246" id="token-6-22" morph="none" pos="word" start_char="1240">medical</TOKEN>
<TOKEN end_char="1255" id="token-6-23" morph="none" pos="word" start_char="1248">center’s</TOKEN>
<TOKEN end_char="1269" id="token-6-24" morph="none" pos="word" start_char="1257">Computational</TOKEN>
<TOKEN end_char="1282" id="token-6-25" morph="none" pos="word" start_char="1271">Epidemiology</TOKEN>
<TOKEN end_char="1286" id="token-6-26" morph="none" pos="word" start_char="1284">Lab</TOKEN>
<TOKEN end_char="1287" id="token-6-27" morph="none" pos="punct" start_char="1287">.</TOKEN>
</SEG>
<SEG end_char="1444" id="segment-7" start_char="1289">
<ORIGINAL_TEXT>"Clearly, there was some level of social disruption taking place well before what was previously identified as the start of the novel coronavirus pandemic."</ORIGINAL_TEXT>
<TOKEN end_char="1289" id="token-7-0" morph="none" pos="punct" start_char="1289">"</TOKEN>
<TOKEN end_char="1296" id="token-7-1" morph="none" pos="word" start_char="1290">Clearly</TOKEN>
<TOKEN end_char="1297" id="token-7-2" morph="none" pos="punct" start_char="1297">,</TOKEN>
<TOKEN end_char="1303" id="token-7-3" morph="none" pos="word" start_char="1299">there</TOKEN>
<TOKEN end_char="1307" id="token-7-4" morph="none" pos="word" start_char="1305">was</TOKEN>
<TOKEN end_char="1312" id="token-7-5" morph="none" pos="word" start_char="1309">some</TOKEN>
<TOKEN end_char="1318" id="token-7-6" morph="none" pos="word" start_char="1314">level</TOKEN>
<TOKEN end_char="1321" id="token-7-7" morph="none" pos="word" start_char="1320">of</TOKEN>
<TOKEN end_char="1328" id="token-7-8" morph="none" pos="word" start_char="1323">social</TOKEN>
<TOKEN end_char="1339" id="token-7-9" morph="none" pos="word" start_char="1330">disruption</TOKEN>
<TOKEN end_char="1346" id="token-7-10" morph="none" pos="word" start_char="1341">taking</TOKEN>
<TOKEN end_char="1352" id="token-7-11" morph="none" pos="word" start_char="1348">place</TOKEN>
<TOKEN end_char="1357" id="token-7-12" morph="none" pos="word" start_char="1354">well</TOKEN>
<TOKEN end_char="1364" id="token-7-13" morph="none" pos="word" start_char="1359">before</TOKEN>
<TOKEN end_char="1369" id="token-7-14" morph="none" pos="word" start_char="1366">what</TOKEN>
<TOKEN end_char="1373" id="token-7-15" morph="none" pos="word" start_char="1371">was</TOKEN>
<TOKEN end_char="1384" id="token-7-16" morph="none" pos="word" start_char="1375">previously</TOKEN>
<TOKEN end_char="1395" id="token-7-17" morph="none" pos="word" start_char="1386">identified</TOKEN>
<TOKEN end_char="1398" id="token-7-18" morph="none" pos="word" start_char="1397">as</TOKEN>
<TOKEN end_char="1402" id="token-7-19" morph="none" pos="word" start_char="1400">the</TOKEN>
<TOKEN end_char="1408" id="token-7-20" morph="none" pos="word" start_char="1404">start</TOKEN>
<TOKEN end_char="1411" id="token-7-21" morph="none" pos="word" start_char="1410">of</TOKEN>
<TOKEN end_char="1415" id="token-7-22" morph="none" pos="word" start_char="1413">the</TOKEN>
<TOKEN end_char="1421" id="token-7-23" morph="none" pos="word" start_char="1417">novel</TOKEN>
<TOKEN end_char="1433" id="token-7-24" morph="none" pos="word" start_char="1423">coronavirus</TOKEN>
<TOKEN end_char="1442" id="token-7-25" morph="none" pos="word" start_char="1435">pandemic</TOKEN>
<TOKEN end_char="1444" id="token-7-26" morph="none" pos="punct" start_char="1443">."</TOKEN>
</SEG>
<SEG end_char="1645" id="segment-8" start_char="1447">
<ORIGINAL_TEXT>Since the outbreak in China last year, the coronavirus has swept across the globe infecting nearly 7 million and killing more than 400,000 worldwide, according to a count by Johns Hopkins University.</ORIGINAL_TEXT>
<TOKEN end_char="1451" id="token-8-0" morph="none" pos="word" start_char="1447">Since</TOKEN>
<TOKEN end_char="1455" id="token-8-1" morph="none" pos="word" start_char="1453">the</TOKEN>
<TOKEN end_char="1464" id="token-8-2" morph="none" pos="word" start_char="1457">outbreak</TOKEN>
<TOKEN end_char="1467" id="token-8-3" morph="none" pos="word" start_char="1466">in</TOKEN>
<TOKEN end_char="1473" id="token-8-4" morph="none" pos="word" start_char="1469">China</TOKEN>
<TOKEN end_char="1478" id="token-8-5" morph="none" pos="word" start_char="1475">last</TOKEN>
<TOKEN end_char="1483" id="token-8-6" morph="none" pos="word" start_char="1480">year</TOKEN>
<TOKEN end_char="1484" id="token-8-7" morph="none" pos="punct" start_char="1484">,</TOKEN>
<TOKEN end_char="1488" id="token-8-8" morph="none" pos="word" start_char="1486">the</TOKEN>
<TOKEN end_char="1500" id="token-8-9" morph="none" pos="word" start_char="1490">coronavirus</TOKEN>
<TOKEN end_char="1504" id="token-8-10" morph="none" pos="word" start_char="1502">has</TOKEN>
<TOKEN end_char="1510" id="token-8-11" morph="none" pos="word" start_char="1506">swept</TOKEN>
<TOKEN end_char="1517" id="token-8-12" morph="none" pos="word" start_char="1512">across</TOKEN>
<TOKEN end_char="1521" id="token-8-13" morph="none" pos="word" start_char="1519">the</TOKEN>
<TOKEN end_char="1527" id="token-8-14" morph="none" pos="word" start_char="1523">globe</TOKEN>
<TOKEN end_char="1537" id="token-8-15" morph="none" pos="word" start_char="1529">infecting</TOKEN>
<TOKEN end_char="1544" id="token-8-16" morph="none" pos="word" start_char="1539">nearly</TOKEN>
<TOKEN end_char="1546" id="token-8-17" morph="none" pos="word" start_char="1546">7</TOKEN>
<TOKEN end_char="1554" id="token-8-18" morph="none" pos="word" start_char="1548">million</TOKEN>
<TOKEN end_char="1558" id="token-8-19" morph="none" pos="word" start_char="1556">and</TOKEN>
<TOKEN end_char="1566" id="token-8-20" morph="none" pos="word" start_char="1560">killing</TOKEN>
<TOKEN end_char="1571" id="token-8-21" morph="none" pos="word" start_char="1568">more</TOKEN>
<TOKEN end_char="1576" id="token-8-22" morph="none" pos="word" start_char="1573">than</TOKEN>
<TOKEN end_char="1584" id="token-8-23" morph="none" pos="unknown" start_char="1578">400,000</TOKEN>
<TOKEN end_char="1594" id="token-8-24" morph="none" pos="word" start_char="1586">worldwide</TOKEN>
<TOKEN end_char="1595" id="token-8-25" morph="none" pos="punct" start_char="1595">,</TOKEN>
<TOKEN end_char="1605" id="token-8-26" morph="none" pos="word" start_char="1597">according</TOKEN>
<TOKEN end_char="1608" id="token-8-27" morph="none" pos="word" start_char="1607">to</TOKEN>
<TOKEN end_char="1610" id="token-8-28" morph="none" pos="word" start_char="1610">a</TOKEN>
<TOKEN end_char="1616" id="token-8-29" morph="none" pos="word" start_char="1612">count</TOKEN>
<TOKEN end_char="1619" id="token-8-30" morph="none" pos="word" start_char="1618">by</TOKEN>
<TOKEN end_char="1625" id="token-8-31" morph="none" pos="word" start_char="1621">Johns</TOKEN>
<TOKEN end_char="1633" id="token-8-32" morph="none" pos="word" start_char="1627">Hopkins</TOKEN>
<TOKEN end_char="1644" id="token-8-33" morph="none" pos="word" start_char="1635">University</TOKEN>
<TOKEN end_char="1645" id="token-8-34" morph="none" pos="punct" start_char="1645">.</TOKEN>
</SEG>
<SEG end_char="1839" id="segment-9" start_char="1647">
<ORIGINAL_TEXT>It is believed that the virus jumped from animal species, where it had little effect, to humans, where it has become the most potent natural killer since the Spanish flu pandemic a century ago.</ORIGINAL_TEXT>
<TOKEN end_char="1648" id="token-9-0" morph="none" pos="word" start_char="1647">It</TOKEN>
<TOKEN end_char="1651" id="token-9-1" morph="none" pos="word" start_char="1650">is</TOKEN>
<TOKEN end_char="1660" id="token-9-2" morph="none" pos="word" start_char="1653">believed</TOKEN>
<TOKEN end_char="1665" id="token-9-3" morph="none" pos="word" start_char="1662">that</TOKEN>
<TOKEN end_char="1669" id="token-9-4" morph="none" pos="word" start_char="1667">the</TOKEN>
<TOKEN end_char="1675" id="token-9-5" morph="none" pos="word" start_char="1671">virus</TOKEN>
<TOKEN end_char="1682" id="token-9-6" morph="none" pos="word" start_char="1677">jumped</TOKEN>
<TOKEN end_char="1687" id="token-9-7" morph="none" pos="word" start_char="1684">from</TOKEN>
<TOKEN end_char="1694" id="token-9-8" morph="none" pos="word" start_char="1689">animal</TOKEN>
<TOKEN end_char="1702" id="token-9-9" morph="none" pos="word" start_char="1696">species</TOKEN>
<TOKEN end_char="1703" id="token-9-10" morph="none" pos="punct" start_char="1703">,</TOKEN>
<TOKEN end_char="1709" id="token-9-11" morph="none" pos="word" start_char="1705">where</TOKEN>
<TOKEN end_char="1712" id="token-9-12" morph="none" pos="word" start_char="1711">it</TOKEN>
<TOKEN end_char="1716" id="token-9-13" morph="none" pos="word" start_char="1714">had</TOKEN>
<TOKEN end_char="1723" id="token-9-14" morph="none" pos="word" start_char="1718">little</TOKEN>
<TOKEN end_char="1730" id="token-9-15" morph="none" pos="word" start_char="1725">effect</TOKEN>
<TOKEN end_char="1731" id="token-9-16" morph="none" pos="punct" start_char="1731">,</TOKEN>
<TOKEN end_char="1734" id="token-9-17" morph="none" pos="word" start_char="1733">to</TOKEN>
<TOKEN end_char="1741" id="token-9-18" morph="none" pos="word" start_char="1736">humans</TOKEN>
<TOKEN end_char="1742" id="token-9-19" morph="none" pos="punct" start_char="1742">,</TOKEN>
<TOKEN end_char="1748" id="token-9-20" morph="none" pos="word" start_char="1744">where</TOKEN>
<TOKEN end_char="1751" id="token-9-21" morph="none" pos="word" start_char="1750">it</TOKEN>
<TOKEN end_char="1755" id="token-9-22" morph="none" pos="word" start_char="1753">has</TOKEN>
<TOKEN end_char="1762" id="token-9-23" morph="none" pos="word" start_char="1757">become</TOKEN>
<TOKEN end_char="1766" id="token-9-24" morph="none" pos="word" start_char="1764">the</TOKEN>
<TOKEN end_char="1771" id="token-9-25" morph="none" pos="word" start_char="1768">most</TOKEN>
<TOKEN end_char="1778" id="token-9-26" morph="none" pos="word" start_char="1773">potent</TOKEN>
<TOKEN end_char="1786" id="token-9-27" morph="none" pos="word" start_char="1780">natural</TOKEN>
<TOKEN end_char="1793" id="token-9-28" morph="none" pos="word" start_char="1788">killer</TOKEN>
<TOKEN end_char="1799" id="token-9-29" morph="none" pos="word" start_char="1795">since</TOKEN>
<TOKEN end_char="1803" id="token-9-30" morph="none" pos="word" start_char="1801">the</TOKEN>
<TOKEN end_char="1811" id="token-9-31" morph="none" pos="word" start_char="1805">Spanish</TOKEN>
<TOKEN end_char="1815" id="token-9-32" morph="none" pos="word" start_char="1813">flu</TOKEN>
<TOKEN end_char="1824" id="token-9-33" morph="none" pos="word" start_char="1817">pandemic</TOKEN>
<TOKEN end_char="1826" id="token-9-34" morph="none" pos="word" start_char="1826">a</TOKEN>
<TOKEN end_char="1834" id="token-9-35" morph="none" pos="word" start_char="1828">century</TOKEN>
<TOKEN end_char="1838" id="token-9-36" morph="none" pos="word" start_char="1836">ago</TOKEN>
<TOKEN end_char="1839" id="token-9-37" morph="none" pos="punct" start_char="1839">.</TOKEN>
</SEG>
<SEG end_char="1932" id="segment-10" start_char="1842">
<ORIGINAL_TEXT>Though Chinese officials would not formally notify the World Health Organization until Dec.</ORIGINAL_TEXT>
<TOKEN end_char="1847" id="token-10-0" morph="none" pos="word" start_char="1842">Though</TOKEN>
<TOKEN end_char="1855" id="token-10-1" morph="none" pos="word" start_char="1849">Chinese</TOKEN>
<TOKEN end_char="1865" id="token-10-2" morph="none" pos="word" start_char="1857">officials</TOKEN>
<TOKEN end_char="1871" id="token-10-3" morph="none" pos="word" start_char="1867">would</TOKEN>
<TOKEN end_char="1875" id="token-10-4" morph="none" pos="word" start_char="1873">not</TOKEN>
<TOKEN end_char="1884" id="token-10-5" morph="none" pos="word" start_char="1877">formally</TOKEN>
<TOKEN end_char="1891" id="token-10-6" morph="none" pos="word" start_char="1886">notify</TOKEN>
<TOKEN end_char="1895" id="token-10-7" morph="none" pos="word" start_char="1893">the</TOKEN>
<TOKEN end_char="1901" id="token-10-8" morph="none" pos="word" start_char="1897">World</TOKEN>
<TOKEN end_char="1908" id="token-10-9" morph="none" pos="word" start_char="1903">Health</TOKEN>
<TOKEN end_char="1921" id="token-10-10" morph="none" pos="word" start_char="1910">Organization</TOKEN>
<TOKEN end_char="1927" id="token-10-11" morph="none" pos="word" start_char="1923">until</TOKEN>
<TOKEN end_char="1931" id="token-10-12" morph="none" pos="word" start_char="1929">Dec</TOKEN>
<TOKEN end_char="1932" id="token-10-13" morph="none" pos="punct" start_char="1932">.</TOKEN>
</SEG>
<SEG end_char="2158" id="segment-11" start_char="1934">
<ORIGINAL_TEXT>31 that a new respiratory pathogen was coursing through Wuhan, U.S. intelligence caught wind of a problem as early as late November and notified the Pentagon, according to four sources briefed on the confidential information.</ORIGINAL_TEXT>
<TOKEN end_char="1935" id="token-11-0" morph="none" pos="word" start_char="1934">31</TOKEN>
<TOKEN end_char="1940" id="token-11-1" morph="none" pos="word" start_char="1937">that</TOKEN>
<TOKEN end_char="1942" id="token-11-2" morph="none" pos="word" start_char="1942">a</TOKEN>
<TOKEN end_char="1946" id="token-11-3" morph="none" pos="word" start_char="1944">new</TOKEN>
<TOKEN end_char="1958" id="token-11-4" morph="none" pos="word" start_char="1948">respiratory</TOKEN>
<TOKEN end_char="1967" id="token-11-5" morph="none" pos="word" start_char="1960">pathogen</TOKEN>
<TOKEN end_char="1971" id="token-11-6" morph="none" pos="word" start_char="1969">was</TOKEN>
<TOKEN end_char="1980" id="token-11-7" morph="none" pos="word" start_char="1973">coursing</TOKEN>
<TOKEN end_char="1988" id="token-11-8" morph="none" pos="word" start_char="1982">through</TOKEN>
<TOKEN end_char="1994" id="token-11-9" morph="none" pos="word" start_char="1990">Wuhan</TOKEN>
<TOKEN end_char="1995" id="token-11-10" morph="none" pos="punct" start_char="1995">,</TOKEN>
<TOKEN end_char="1999" id="token-11-11" morph="none" pos="unknown" start_char="1997">U.S</TOKEN>
<TOKEN end_char="2000" id="token-11-12" morph="none" pos="punct" start_char="2000">.</TOKEN>
<TOKEN end_char="2013" id="token-11-13" morph="none" pos="word" start_char="2002">intelligence</TOKEN>
<TOKEN end_char="2020" id="token-11-14" morph="none" pos="word" start_char="2015">caught</TOKEN>
<TOKEN end_char="2025" id="token-11-15" morph="none" pos="word" start_char="2022">wind</TOKEN>
<TOKEN end_char="2028" id="token-11-16" morph="none" pos="word" start_char="2027">of</TOKEN>
<TOKEN end_char="2030" id="token-11-17" morph="none" pos="word" start_char="2030">a</TOKEN>
<TOKEN end_char="2038" id="token-11-18" morph="none" pos="word" start_char="2032">problem</TOKEN>
<TOKEN end_char="2041" id="token-11-19" morph="none" pos="word" start_char="2040">as</TOKEN>
<TOKEN end_char="2047" id="token-11-20" morph="none" pos="word" start_char="2043">early</TOKEN>
<TOKEN end_char="2050" id="token-11-21" morph="none" pos="word" start_char="2049">as</TOKEN>
<TOKEN end_char="2055" id="token-11-22" morph="none" pos="word" start_char="2052">late</TOKEN>
<TOKEN end_char="2064" id="token-11-23" morph="none" pos="word" start_char="2057">November</TOKEN>
<TOKEN end_char="2068" id="token-11-24" morph="none" pos="word" start_char="2066">and</TOKEN>
<TOKEN end_char="2077" id="token-11-25" morph="none" pos="word" start_char="2070">notified</TOKEN>
<TOKEN end_char="2081" id="token-11-26" morph="none" pos="word" start_char="2079">the</TOKEN>
<TOKEN end_char="2090" id="token-11-27" morph="none" pos="word" start_char="2083">Pentagon</TOKEN>
<TOKEN end_char="2091" id="token-11-28" morph="none" pos="punct" start_char="2091">,</TOKEN>
<TOKEN end_char="2101" id="token-11-29" morph="none" pos="word" start_char="2093">according</TOKEN>
<TOKEN end_char="2104" id="token-11-30" morph="none" pos="word" start_char="2103">to</TOKEN>
<TOKEN end_char="2109" id="token-11-31" morph="none" pos="word" start_char="2106">four</TOKEN>
<TOKEN end_char="2117" id="token-11-32" morph="none" pos="word" start_char="2111">sources</TOKEN>
<TOKEN end_char="2125" id="token-11-33" morph="none" pos="word" start_char="2119">briefed</TOKEN>
<TOKEN end_char="2128" id="token-11-34" morph="none" pos="word" start_char="2127">on</TOKEN>
<TOKEN end_char="2132" id="token-11-35" morph="none" pos="word" start_char="2130">the</TOKEN>
<TOKEN end_char="2145" id="token-11-36" morph="none" pos="word" start_char="2134">confidential</TOKEN>
<TOKEN end_char="2157" id="token-11-37" morph="none" pos="word" start_char="2147">information</TOKEN>
<TOKEN end_char="2158" id="token-11-38" morph="none" pos="punct" start_char="2158">.</TOKEN>
</SEG>
<SEG end_char="2380" id="segment-12" start_char="2161">
<ORIGINAL_TEXT>Because the origin of a novel virus is so hard to pin down but so critically important for scientists to understand, experts around the world are racing to uncover the secrets of the pathogen formally known as SARS-CoV2.</ORIGINAL_TEXT>
<TOKEN end_char="2167" id="token-12-0" morph="none" pos="word" start_char="2161">Because</TOKEN>
<TOKEN end_char="2171" id="token-12-1" morph="none" pos="word" start_char="2169">the</TOKEN>
<TOKEN end_char="2178" id="token-12-2" morph="none" pos="word" start_char="2173">origin</TOKEN>
<TOKEN end_char="2181" id="token-12-3" morph="none" pos="word" start_char="2180">of</TOKEN>
<TOKEN end_char="2183" id="token-12-4" morph="none" pos="word" start_char="2183">a</TOKEN>
<TOKEN end_char="2189" id="token-12-5" morph="none" pos="word" start_char="2185">novel</TOKEN>
<TOKEN end_char="2195" id="token-12-6" morph="none" pos="word" start_char="2191">virus</TOKEN>
<TOKEN end_char="2198" id="token-12-7" morph="none" pos="word" start_char="2197">is</TOKEN>
<TOKEN end_char="2201" id="token-12-8" morph="none" pos="word" start_char="2200">so</TOKEN>
<TOKEN end_char="2206" id="token-12-9" morph="none" pos="word" start_char="2203">hard</TOKEN>
<TOKEN end_char="2209" id="token-12-10" morph="none" pos="word" start_char="2208">to</TOKEN>
<TOKEN end_char="2213" id="token-12-11" morph="none" pos="word" start_char="2211">pin</TOKEN>
<TOKEN end_char="2218" id="token-12-12" morph="none" pos="word" start_char="2215">down</TOKEN>
<TOKEN end_char="2222" id="token-12-13" morph="none" pos="word" start_char="2220">but</TOKEN>
<TOKEN end_char="2225" id="token-12-14" morph="none" pos="word" start_char="2224">so</TOKEN>
<TOKEN end_char="2236" id="token-12-15" morph="none" pos="word" start_char="2227">critically</TOKEN>
<TOKEN end_char="2246" id="token-12-16" morph="none" pos="word" start_char="2238">important</TOKEN>
<TOKEN end_char="2250" id="token-12-17" morph="none" pos="word" start_char="2248">for</TOKEN>
<TOKEN end_char="2261" id="token-12-18" morph="none" pos="word" start_char="2252">scientists</TOKEN>
<TOKEN end_char="2264" id="token-12-19" morph="none" pos="word" start_char="2263">to</TOKEN>
<TOKEN end_char="2275" id="token-12-20" morph="none" pos="word" start_char="2266">understand</TOKEN>
<TOKEN end_char="2276" id="token-12-21" morph="none" pos="punct" start_char="2276">,</TOKEN>
<TOKEN end_char="2284" id="token-12-22" morph="none" pos="word" start_char="2278">experts</TOKEN>
<TOKEN end_char="2291" id="token-12-23" morph="none" pos="word" start_char="2286">around</TOKEN>
<TOKEN end_char="2295" id="token-12-24" morph="none" pos="word" start_char="2293">the</TOKEN>
<TOKEN end_char="2301" id="token-12-25" morph="none" pos="word" start_char="2297">world</TOKEN>
<TOKEN end_char="2305" id="token-12-26" morph="none" pos="word" start_char="2303">are</TOKEN>
<TOKEN end_char="2312" id="token-12-27" morph="none" pos="word" start_char="2307">racing</TOKEN>
<TOKEN end_char="2315" id="token-12-28" morph="none" pos="word" start_char="2314">to</TOKEN>
<TOKEN end_char="2323" id="token-12-29" morph="none" pos="word" start_char="2317">uncover</TOKEN>
<TOKEN end_char="2327" id="token-12-30" morph="none" pos="word" start_char="2325">the</TOKEN>
<TOKEN end_char="2335" id="token-12-31" morph="none" pos="word" start_char="2329">secrets</TOKEN>
<TOKEN end_char="2338" id="token-12-32" morph="none" pos="word" start_char="2337">of</TOKEN>
<TOKEN end_char="2342" id="token-12-33" morph="none" pos="word" start_char="2340">the</TOKEN>
<TOKEN end_char="2351" id="token-12-34" morph="none" pos="word" start_char="2344">pathogen</TOKEN>
<TOKEN end_char="2360" id="token-12-35" morph="none" pos="word" start_char="2353">formally</TOKEN>
<TOKEN end_char="2366" id="token-12-36" morph="none" pos="word" start_char="2362">known</TOKEN>
<TOKEN end_char="2369" id="token-12-37" morph="none" pos="word" start_char="2368">as</TOKEN>
<TOKEN end_char="2379" id="token-12-38" morph="none" pos="unknown" start_char="2371">SARS-CoV2</TOKEN>
<TOKEN end_char="2380" id="token-12-39" morph="none" pos="punct" start_char="2380">.</TOKEN>
</SEG>
<SEG end_char="2578" id="segment-13" start_char="2382">
<ORIGINAL_TEXT>The task for researchers is made far more complicated by the Chinese government’s refusal to fully cooperate with Western and international health authorities, American and WHO officials have said.</ORIGINAL_TEXT>
<TOKEN end_char="2384" id="token-13-0" morph="none" pos="word" start_char="2382">The</TOKEN>
<TOKEN end_char="2389" id="token-13-1" morph="none" pos="word" start_char="2386">task</TOKEN>
<TOKEN end_char="2393" id="token-13-2" morph="none" pos="word" start_char="2391">for</TOKEN>
<TOKEN end_char="2405" id="token-13-3" morph="none" pos="word" start_char="2395">researchers</TOKEN>
<TOKEN end_char="2408" id="token-13-4" morph="none" pos="word" start_char="2407">is</TOKEN>
<TOKEN end_char="2413" id="token-13-5" morph="none" pos="word" start_char="2410">made</TOKEN>
<TOKEN end_char="2417" id="token-13-6" morph="none" pos="word" start_char="2415">far</TOKEN>
<TOKEN end_char="2422" id="token-13-7" morph="none" pos="word" start_char="2419">more</TOKEN>
<TOKEN end_char="2434" id="token-13-8" morph="none" pos="word" start_char="2424">complicated</TOKEN>
<TOKEN end_char="2437" id="token-13-9" morph="none" pos="word" start_char="2436">by</TOKEN>
<TOKEN end_char="2441" id="token-13-10" morph="none" pos="word" start_char="2439">the</TOKEN>
<TOKEN end_char="2449" id="token-13-11" morph="none" pos="word" start_char="2443">Chinese</TOKEN>
<TOKEN end_char="2462" id="token-13-12" morph="none" pos="word" start_char="2451">government’s</TOKEN>
<TOKEN end_char="2470" id="token-13-13" morph="none" pos="word" start_char="2464">refusal</TOKEN>
<TOKEN end_char="2473" id="token-13-14" morph="none" pos="word" start_char="2472">to</TOKEN>
<TOKEN end_char="2479" id="token-13-15" morph="none" pos="word" start_char="2475">fully</TOKEN>
<TOKEN end_char="2489" id="token-13-16" morph="none" pos="word" start_char="2481">cooperate</TOKEN>
<TOKEN end_char="2494" id="token-13-17" morph="none" pos="word" start_char="2491">with</TOKEN>
<TOKEN end_char="2502" id="token-13-18" morph="none" pos="word" start_char="2496">Western</TOKEN>
<TOKEN end_char="2506" id="token-13-19" morph="none" pos="word" start_char="2504">and</TOKEN>
<TOKEN end_char="2520" id="token-13-20" morph="none" pos="word" start_char="2508">international</TOKEN>
<TOKEN end_char="2527" id="token-13-21" morph="none" pos="word" start_char="2522">health</TOKEN>
<TOKEN end_char="2539" id="token-13-22" morph="none" pos="word" start_char="2529">authorities</TOKEN>
<TOKEN end_char="2540" id="token-13-23" morph="none" pos="punct" start_char="2540">,</TOKEN>
<TOKEN end_char="2549" id="token-13-24" morph="none" pos="word" start_char="2542">American</TOKEN>
<TOKEN end_char="2553" id="token-13-25" morph="none" pos="word" start_char="2551">and</TOKEN>
<TOKEN end_char="2557" id="token-13-26" morph="none" pos="word" start_char="2555">WHO</TOKEN>
<TOKEN end_char="2567" id="token-13-27" morph="none" pos="word" start_char="2559">officials</TOKEN>
<TOKEN end_char="2572" id="token-13-28" morph="none" pos="word" start_char="2569">have</TOKEN>
<TOKEN end_char="2577" id="token-13-29" morph="none" pos="word" start_char="2574">said</TOKEN>
<TOKEN end_char="2578" id="token-13-30" morph="none" pos="punct" start_char="2578">.</TOKEN>
</SEG>
<SEG end_char="2825" id="segment-14" start_char="2581">
<ORIGINAL_TEXT>Brownstein and his team, which included researchers from Boston University and Boston Children’s Hospital, have spent more than a month trying to pin down the signs for when the population of Hubei province in China first started to be stricken.</ORIGINAL_TEXT>
<TOKEN end_char="2590" id="token-14-0" morph="none" pos="word" start_char="2581">Brownstein</TOKEN>
<TOKEN end_char="2594" id="token-14-1" morph="none" pos="word" start_char="2592">and</TOKEN>
<TOKEN end_char="2598" id="token-14-2" morph="none" pos="word" start_char="2596">his</TOKEN>
<TOKEN end_char="2603" id="token-14-3" morph="none" pos="word" start_char="2600">team</TOKEN>
<TOKEN end_char="2604" id="token-14-4" morph="none" pos="punct" start_char="2604">,</TOKEN>
<TOKEN end_char="2610" id="token-14-5" morph="none" pos="word" start_char="2606">which</TOKEN>
<TOKEN end_char="2619" id="token-14-6" morph="none" pos="word" start_char="2612">included</TOKEN>
<TOKEN end_char="2631" id="token-14-7" morph="none" pos="word" start_char="2621">researchers</TOKEN>
<TOKEN end_char="2636" id="token-14-8" morph="none" pos="word" start_char="2633">from</TOKEN>
<TOKEN end_char="2643" id="token-14-9" morph="none" pos="word" start_char="2638">Boston</TOKEN>
<TOKEN end_char="2654" id="token-14-10" morph="none" pos="word" start_char="2645">University</TOKEN>
<TOKEN end_char="2658" id="token-14-11" morph="none" pos="word" start_char="2656">and</TOKEN>
<TOKEN end_char="2665" id="token-14-12" morph="none" pos="word" start_char="2660">Boston</TOKEN>
<TOKEN end_char="2676" id="token-14-13" morph="none" pos="word" start_char="2667">Children’s</TOKEN>
<TOKEN end_char="2685" id="token-14-14" morph="none" pos="word" start_char="2678">Hospital</TOKEN>
<TOKEN end_char="2686" id="token-14-15" morph="none" pos="punct" start_char="2686">,</TOKEN>
<TOKEN end_char="2691" id="token-14-16" morph="none" pos="word" start_char="2688">have</TOKEN>
<TOKEN end_char="2697" id="token-14-17" morph="none" pos="word" start_char="2693">spent</TOKEN>
<TOKEN end_char="2702" id="token-14-18" morph="none" pos="word" start_char="2699">more</TOKEN>
<TOKEN end_char="2707" id="token-14-19" morph="none" pos="word" start_char="2704">than</TOKEN>
<TOKEN end_char="2709" id="token-14-20" morph="none" pos="word" start_char="2709">a</TOKEN>
<TOKEN end_char="2715" id="token-14-21" morph="none" pos="word" start_char="2711">month</TOKEN>
<TOKEN end_char="2722" id="token-14-22" morph="none" pos="word" start_char="2717">trying</TOKEN>
<TOKEN end_char="2725" id="token-14-23" morph="none" pos="word" start_char="2724">to</TOKEN>
<TOKEN end_char="2729" id="token-14-24" morph="none" pos="word" start_char="2727">pin</TOKEN>
<TOKEN end_char="2734" id="token-14-25" morph="none" pos="word" start_char="2731">down</TOKEN>
<TOKEN end_char="2738" id="token-14-26" morph="none" pos="word" start_char="2736">the</TOKEN>
<TOKEN end_char="2744" id="token-14-27" morph="none" pos="word" start_char="2740">signs</TOKEN>
<TOKEN end_char="2748" id="token-14-28" morph="none" pos="word" start_char="2746">for</TOKEN>
<TOKEN end_char="2753" id="token-14-29" morph="none" pos="word" start_char="2750">when</TOKEN>
<TOKEN end_char="2757" id="token-14-30" morph="none" pos="word" start_char="2755">the</TOKEN>
<TOKEN end_char="2768" id="token-14-31" morph="none" pos="word" start_char="2759">population</TOKEN>
<TOKEN end_char="2771" id="token-14-32" morph="none" pos="word" start_char="2770">of</TOKEN>
<TOKEN end_char="2777" id="token-14-33" morph="none" pos="word" start_char="2773">Hubei</TOKEN>
<TOKEN end_char="2786" id="token-14-34" morph="none" pos="word" start_char="2779">province</TOKEN>
<TOKEN end_char="2789" id="token-14-35" morph="none" pos="word" start_char="2788">in</TOKEN>
<TOKEN end_char="2795" id="token-14-36" morph="none" pos="word" start_char="2791">China</TOKEN>
<TOKEN end_char="2801" id="token-14-37" morph="none" pos="word" start_char="2797">first</TOKEN>
<TOKEN end_char="2809" id="token-14-38" morph="none" pos="word" start_char="2803">started</TOKEN>
<TOKEN end_char="2812" id="token-14-39" morph="none" pos="word" start_char="2811">to</TOKEN>
<TOKEN end_char="2815" id="token-14-40" morph="none" pos="word" start_char="2814">be</TOKEN>
<TOKEN end_char="2824" id="token-14-41" morph="none" pos="word" start_char="2817">stricken</TOKEN>
<TOKEN end_char="2825" id="token-14-42" morph="none" pos="punct" start_char="2825">.</TOKEN>
</SEG>
<SEG end_char="2991" id="segment-15" start_char="2828">
<ORIGINAL_TEXT>The logic of Brownstein’s research project was straightforward: respiratory diseases lead to very specific types of behavior in communities where they’re spreading.</ORIGINAL_TEXT>
<TOKEN end_char="2830" id="token-15-0" morph="none" pos="word" start_char="2828">The</TOKEN>
<TOKEN end_char="2836" id="token-15-1" morph="none" pos="word" start_char="2832">logic</TOKEN>
<TOKEN end_char="2839" id="token-15-2" morph="none" pos="word" start_char="2838">of</TOKEN>
<TOKEN end_char="2852" id="token-15-3" morph="none" pos="word" start_char="2841">Brownstein’s</TOKEN>
<TOKEN end_char="2861" id="token-15-4" morph="none" pos="word" start_char="2854">research</TOKEN>
<TOKEN end_char="2869" id="token-15-5" morph="none" pos="word" start_char="2863">project</TOKEN>
<TOKEN end_char="2873" id="token-15-6" morph="none" pos="word" start_char="2871">was</TOKEN>
<TOKEN end_char="2889" id="token-15-7" morph="none" pos="word" start_char="2875">straightforward</TOKEN>
<TOKEN end_char="2890" id="token-15-8" morph="none" pos="punct" start_char="2890">:</TOKEN>
<TOKEN end_char="2902" id="token-15-9" morph="none" pos="word" start_char="2892">respiratory</TOKEN>
<TOKEN end_char="2911" id="token-15-10" morph="none" pos="word" start_char="2904">diseases</TOKEN>
<TOKEN end_char="2916" id="token-15-11" morph="none" pos="word" start_char="2913">lead</TOKEN>
<TOKEN end_char="2919" id="token-15-12" morph="none" pos="word" start_char="2918">to</TOKEN>
<TOKEN end_char="2924" id="token-15-13" morph="none" pos="word" start_char="2921">very</TOKEN>
<TOKEN end_char="2933" id="token-15-14" morph="none" pos="word" start_char="2926">specific</TOKEN>
<TOKEN end_char="2939" id="token-15-15" morph="none" pos="word" start_char="2935">types</TOKEN>
<TOKEN end_char="2942" id="token-15-16" morph="none" pos="word" start_char="2941">of</TOKEN>
<TOKEN end_char="2951" id="token-15-17" morph="none" pos="word" start_char="2944">behavior</TOKEN>
<TOKEN end_char="2954" id="token-15-18" morph="none" pos="word" start_char="2953">in</TOKEN>
<TOKEN end_char="2966" id="token-15-19" morph="none" pos="word" start_char="2956">communities</TOKEN>
<TOKEN end_char="2972" id="token-15-20" morph="none" pos="word" start_char="2968">where</TOKEN>
<TOKEN end_char="2980" id="token-15-21" morph="none" pos="word" start_char="2974">they’re</TOKEN>
<TOKEN end_char="2990" id="token-15-22" morph="none" pos="word" start_char="2982">spreading</TOKEN>
<TOKEN end_char="2991" id="token-15-23" morph="none" pos="punct" start_char="2991">.</TOKEN>
</SEG>
<SEG end_char="3165" id="segment-16" start_char="2993">
<ORIGINAL_TEXT>So, pictures that show those patterns of behavior could help explain what was happening even if the people who were sickened did not realize the broader problem at the time.</ORIGINAL_TEXT>
<TOKEN end_char="2994" id="token-16-0" morph="none" pos="word" start_char="2993">So</TOKEN>
<TOKEN end_char="2995" id="token-16-1" morph="none" pos="punct" start_char="2995">,</TOKEN>
<TOKEN end_char="3004" id="token-16-2" morph="none" pos="word" start_char="2997">pictures</TOKEN>
<TOKEN end_char="3009" id="token-16-3" morph="none" pos="word" start_char="3006">that</TOKEN>
<TOKEN end_char="3014" id="token-16-4" morph="none" pos="word" start_char="3011">show</TOKEN>
<TOKEN end_char="3020" id="token-16-5" morph="none" pos="word" start_char="3016">those</TOKEN>
<TOKEN end_char="3029" id="token-16-6" morph="none" pos="word" start_char="3022">patterns</TOKEN>
<TOKEN end_char="3032" id="token-16-7" morph="none" pos="word" start_char="3031">of</TOKEN>
<TOKEN end_char="3041" id="token-16-8" morph="none" pos="word" start_char="3034">behavior</TOKEN>
<TOKEN end_char="3047" id="token-16-9" morph="none" pos="word" start_char="3043">could</TOKEN>
<TOKEN end_char="3052" id="token-16-10" morph="none" pos="word" start_char="3049">help</TOKEN>
<TOKEN end_char="3060" id="token-16-11" morph="none" pos="word" start_char="3054">explain</TOKEN>
<TOKEN end_char="3065" id="token-16-12" morph="none" pos="word" start_char="3062">what</TOKEN>
<TOKEN end_char="3069" id="token-16-13" morph="none" pos="word" start_char="3067">was</TOKEN>
<TOKEN end_char="3079" id="token-16-14" morph="none" pos="word" start_char="3071">happening</TOKEN>
<TOKEN end_char="3084" id="token-16-15" morph="none" pos="word" start_char="3081">even</TOKEN>
<TOKEN end_char="3087" id="token-16-16" morph="none" pos="word" start_char="3086">if</TOKEN>
<TOKEN end_char="3091" id="token-16-17" morph="none" pos="word" start_char="3089">the</TOKEN>
<TOKEN end_char="3098" id="token-16-18" morph="none" pos="word" start_char="3093">people</TOKEN>
<TOKEN end_char="3102" id="token-16-19" morph="none" pos="word" start_char="3100">who</TOKEN>
<TOKEN end_char="3107" id="token-16-20" morph="none" pos="word" start_char="3104">were</TOKEN>
<TOKEN end_char="3116" id="token-16-21" morph="none" pos="word" start_char="3109">sickened</TOKEN>
<TOKEN end_char="3120" id="token-16-22" morph="none" pos="word" start_char="3118">did</TOKEN>
<TOKEN end_char="3124" id="token-16-23" morph="none" pos="word" start_char="3122">not</TOKEN>
<TOKEN end_char="3132" id="token-16-24" morph="none" pos="word" start_char="3126">realize</TOKEN>
<TOKEN end_char="3136" id="token-16-25" morph="none" pos="word" start_char="3134">the</TOKEN>
<TOKEN end_char="3144" id="token-16-26" morph="none" pos="word" start_char="3138">broader</TOKEN>
<TOKEN end_char="3152" id="token-16-27" morph="none" pos="word" start_char="3146">problem</TOKEN>
<TOKEN end_char="3155" id="token-16-28" morph="none" pos="word" start_char="3154">at</TOKEN>
<TOKEN end_char="3159" id="token-16-29" morph="none" pos="word" start_char="3157">the</TOKEN>
<TOKEN end_char="3164" id="token-16-30" morph="none" pos="word" start_char="3161">time</TOKEN>
<TOKEN end_char="3165" id="token-16-31" morph="none" pos="punct" start_char="3165">.</TOKEN>
</SEG>
<SEG end_char="3258" id="segment-17" start_char="3168">
<ORIGINAL_TEXT>"What we're trying to do is look at the activity, how busy a hospital is," Brownstein said.</ORIGINAL_TEXT>
<TOKEN end_char="3168" id="token-17-0" morph="none" pos="punct" start_char="3168">"</TOKEN>
<TOKEN end_char="3172" id="token-17-1" morph="none" pos="word" start_char="3169">What</TOKEN>
<TOKEN end_char="3178" id="token-17-2" morph="none" pos="word" start_char="3174">we're</TOKEN>
<TOKEN end_char="3185" id="token-17-3" morph="none" pos="word" start_char="3180">trying</TOKEN>
<TOKEN end_char="3188" id="token-17-4" morph="none" pos="word" start_char="3187">to</TOKEN>
<TOKEN end_char="3191" id="token-17-5" morph="none" pos="word" start_char="3190">do</TOKEN>
<TOKEN end_char="3194" id="token-17-6" morph="none" pos="word" start_char="3193">is</TOKEN>
<TOKEN end_char="3199" id="token-17-7" morph="none" pos="word" start_char="3196">look</TOKEN>
<TOKEN end_char="3202" id="token-17-8" morph="none" pos="word" start_char="3201">at</TOKEN>
<TOKEN end_char="3206" id="token-17-9" morph="none" pos="word" start_char="3204">the</TOKEN>
<TOKEN end_char="3215" id="token-17-10" morph="none" pos="word" start_char="3208">activity</TOKEN>
<TOKEN end_char="3216" id="token-17-11" morph="none" pos="punct" start_char="3216">,</TOKEN>
<TOKEN end_char="3220" id="token-17-12" morph="none" pos="word" start_char="3218">how</TOKEN>
<TOKEN end_char="3225" id="token-17-13" morph="none" pos="word" start_char="3222">busy</TOKEN>
<TOKEN end_char="3227" id="token-17-14" morph="none" pos="word" start_char="3227">a</TOKEN>
<TOKEN end_char="3236" id="token-17-15" morph="none" pos="word" start_char="3229">hospital</TOKEN>
<TOKEN end_char="3239" id="token-17-16" morph="none" pos="word" start_char="3238">is</TOKEN>
<TOKEN end_char="3241" id="token-17-17" morph="none" pos="punct" start_char="3240">,"</TOKEN>
<TOKEN end_char="3252" id="token-17-18" morph="none" pos="word" start_char="3243">Brownstein</TOKEN>
<TOKEN end_char="3257" id="token-17-19" morph="none" pos="word" start_char="3254">said</TOKEN>
<TOKEN end_char="3258" id="token-17-20" morph="none" pos="punct" start_char="3258">.</TOKEN>
</SEG>
<SEG end_char="3333" id="segment-18" start_char="3260">
<ORIGINAL_TEXT>"And the way we do that is by counting the cars that are at that hospital.</ORIGINAL_TEXT>
<TOKEN end_char="3260" id="token-18-0" morph="none" pos="punct" start_char="3260">"</TOKEN>
<TOKEN end_char="3263" id="token-18-1" morph="none" pos="word" start_char="3261">And</TOKEN>
<TOKEN end_char="3267" id="token-18-2" morph="none" pos="word" start_char="3265">the</TOKEN>
<TOKEN end_char="3271" id="token-18-3" morph="none" pos="word" start_char="3269">way</TOKEN>
<TOKEN end_char="3274" id="token-18-4" morph="none" pos="word" start_char="3273">we</TOKEN>
<TOKEN end_char="3277" id="token-18-5" morph="none" pos="word" start_char="3276">do</TOKEN>
<TOKEN end_char="3282" id="token-18-6" morph="none" pos="word" start_char="3279">that</TOKEN>
<TOKEN end_char="3285" id="token-18-7" morph="none" pos="word" start_char="3284">is</TOKEN>
<TOKEN end_char="3288" id="token-18-8" morph="none" pos="word" start_char="3287">by</TOKEN>
<TOKEN end_char="3297" id="token-18-9" morph="none" pos="word" start_char="3290">counting</TOKEN>
<TOKEN end_char="3301" id="token-18-10" morph="none" pos="word" start_char="3299">the</TOKEN>
<TOKEN end_char="3306" id="token-18-11" morph="none" pos="word" start_char="3303">cars</TOKEN>
<TOKEN end_char="3311" id="token-18-12" morph="none" pos="word" start_char="3308">that</TOKEN>
<TOKEN end_char="3315" id="token-18-13" morph="none" pos="word" start_char="3313">are</TOKEN>
<TOKEN end_char="3318" id="token-18-14" morph="none" pos="word" start_char="3317">at</TOKEN>
<TOKEN end_char="3323" id="token-18-15" morph="none" pos="word" start_char="3320">that</TOKEN>
<TOKEN end_char="3332" id="token-18-16" morph="none" pos="word" start_char="3325">hospital</TOKEN>
<TOKEN end_char="3333" id="token-18-17" morph="none" pos="punct" start_char="3333">.</TOKEN>
</SEG>
<SEG end_char="3385" id="segment-19" start_char="3335">
<ORIGINAL_TEXT>Parking lots will get full as a hospital gets busy.</ORIGINAL_TEXT>
<TOKEN end_char="3341" id="token-19-0" morph="none" pos="word" start_char="3335">Parking</TOKEN>
<TOKEN end_char="3346" id="token-19-1" morph="none" pos="word" start_char="3343">lots</TOKEN>
<TOKEN end_char="3351" id="token-19-2" morph="none" pos="word" start_char="3348">will</TOKEN>
<TOKEN end_char="3355" id="token-19-3" morph="none" pos="word" start_char="3353">get</TOKEN>
<TOKEN end_char="3360" id="token-19-4" morph="none" pos="word" start_char="3357">full</TOKEN>
<TOKEN end_char="3363" id="token-19-5" morph="none" pos="word" start_char="3362">as</TOKEN>
<TOKEN end_char="3365" id="token-19-6" morph="none" pos="word" start_char="3365">a</TOKEN>
<TOKEN end_char="3374" id="token-19-7" morph="none" pos="word" start_char="3367">hospital</TOKEN>
<TOKEN end_char="3379" id="token-19-8" morph="none" pos="word" start_char="3376">gets</TOKEN>
<TOKEN end_char="3384" id="token-19-9" morph="none" pos="word" start_char="3381">busy</TOKEN>
<TOKEN end_char="3385" id="token-19-10" morph="none" pos="punct" start_char="3385">.</TOKEN>
</SEG>
<SEG end_char="3548" id="segment-20" start_char="3387">
<ORIGINAL_TEXT>So more cars in a hospital, the hospital's busier, likely because something's happening in the community, an infection is growing and people have to see a doctor.</ORIGINAL_TEXT>
<TOKEN end_char="3388" id="token-20-0" morph="none" pos="word" start_char="3387">So</TOKEN>
<TOKEN end_char="3393" id="token-20-1" morph="none" pos="word" start_char="3390">more</TOKEN>
<TOKEN end_char="3398" id="token-20-2" morph="none" pos="word" start_char="3395">cars</TOKEN>
<TOKEN end_char="3401" id="token-20-3" morph="none" pos="word" start_char="3400">in</TOKEN>
<TOKEN end_char="3403" id="token-20-4" morph="none" pos="word" start_char="3403">a</TOKEN>
<TOKEN end_char="3412" id="token-20-5" morph="none" pos="word" start_char="3405">hospital</TOKEN>
<TOKEN end_char="3413" id="token-20-6" morph="none" pos="punct" start_char="3413">,</TOKEN>
<TOKEN end_char="3417" id="token-20-7" morph="none" pos="word" start_char="3415">the</TOKEN>
<TOKEN end_char="3428" id="token-20-8" morph="none" pos="word" start_char="3419">hospital's</TOKEN>
<TOKEN end_char="3435" id="token-20-9" morph="none" pos="word" start_char="3430">busier</TOKEN>
<TOKEN end_char="3436" id="token-20-10" morph="none" pos="punct" start_char="3436">,</TOKEN>
<TOKEN end_char="3443" id="token-20-11" morph="none" pos="word" start_char="3438">likely</TOKEN>
<TOKEN end_char="3451" id="token-20-12" morph="none" pos="word" start_char="3445">because</TOKEN>
<TOKEN end_char="3463" id="token-20-13" morph="none" pos="word" start_char="3453">something's</TOKEN>
<TOKEN end_char="3473" id="token-20-14" morph="none" pos="word" start_char="3465">happening</TOKEN>
<TOKEN end_char="3476" id="token-20-15" morph="none" pos="word" start_char="3475">in</TOKEN>
<TOKEN end_char="3480" id="token-20-16" morph="none" pos="word" start_char="3478">the</TOKEN>
<TOKEN end_char="3490" id="token-20-17" morph="none" pos="word" start_char="3482">community</TOKEN>
<TOKEN end_char="3491" id="token-20-18" morph="none" pos="punct" start_char="3491">,</TOKEN>
<TOKEN end_char="3494" id="token-20-19" morph="none" pos="word" start_char="3493">an</TOKEN>
<TOKEN end_char="3504" id="token-20-20" morph="none" pos="word" start_char="3496">infection</TOKEN>
<TOKEN end_char="3507" id="token-20-21" morph="none" pos="word" start_char="3506">is</TOKEN>
<TOKEN end_char="3515" id="token-20-22" morph="none" pos="word" start_char="3509">growing</TOKEN>
<TOKEN end_char="3519" id="token-20-23" morph="none" pos="word" start_char="3517">and</TOKEN>
<TOKEN end_char="3526" id="token-20-24" morph="none" pos="word" start_char="3521">people</TOKEN>
<TOKEN end_char="3531" id="token-20-25" morph="none" pos="word" start_char="3528">have</TOKEN>
<TOKEN end_char="3534" id="token-20-26" morph="none" pos="word" start_char="3533">to</TOKEN>
<TOKEN end_char="3538" id="token-20-27" morph="none" pos="word" start_char="3536">see</TOKEN>
<TOKEN end_char="3540" id="token-20-28" morph="none" pos="word" start_char="3540">a</TOKEN>
<TOKEN end_char="3547" id="token-20-29" morph="none" pos="word" start_char="3542">doctor</TOKEN>
<TOKEN end_char="3548" id="token-20-30" morph="none" pos="punct" start_char="3548">.</TOKEN>
</SEG>
<SEG end_char="3659" id="segment-21" start_char="3550">
<ORIGINAL_TEXT>So you see the increases in the hospital business through the cars… We saw this across multiple institutions."</ORIGINAL_TEXT>
<TOKEN end_char="3551" id="token-21-0" morph="none" pos="word" start_char="3550">So</TOKEN>
<TOKEN end_char="3555" id="token-21-1" morph="none" pos="word" start_char="3553">you</TOKEN>
<TOKEN end_char="3559" id="token-21-2" morph="none" pos="word" start_char="3557">see</TOKEN>
<TOKEN end_char="3563" id="token-21-3" morph="none" pos="word" start_char="3561">the</TOKEN>
<TOKEN end_char="3573" id="token-21-4" morph="none" pos="word" start_char="3565">increases</TOKEN>
<TOKEN end_char="3576" id="token-21-5" morph="none" pos="word" start_char="3575">in</TOKEN>
<TOKEN end_char="3580" id="token-21-6" morph="none" pos="word" start_char="3578">the</TOKEN>
<TOKEN end_char="3589" id="token-21-7" morph="none" pos="word" start_char="3582">hospital</TOKEN>
<TOKEN end_char="3598" id="token-21-8" morph="none" pos="word" start_char="3591">business</TOKEN>
<TOKEN end_char="3606" id="token-21-9" morph="none" pos="word" start_char="3600">through</TOKEN>
<TOKEN end_char="3610" id="token-21-10" morph="none" pos="word" start_char="3608">the</TOKEN>
<TOKEN end_char="3615" id="token-21-11" morph="none" pos="word" start_char="3612">cars</TOKEN>
<TOKEN end_char="3616" id="token-21-12" morph="none" pos="punct" start_char="3616">…</TOKEN>
<TOKEN end_char="3619" id="token-21-13" morph="none" pos="word" start_char="3618">We</TOKEN>
<TOKEN end_char="3623" id="token-21-14" morph="none" pos="word" start_char="3621">saw</TOKEN>
<TOKEN end_char="3628" id="token-21-15" morph="none" pos="word" start_char="3625">this</TOKEN>
<TOKEN end_char="3635" id="token-21-16" morph="none" pos="word" start_char="3630">across</TOKEN>
<TOKEN end_char="3644" id="token-21-17" morph="none" pos="word" start_char="3637">multiple</TOKEN>
<TOKEN end_char="3657" id="token-21-18" morph="none" pos="word" start_char="3646">institutions</TOKEN>
<TOKEN end_char="3659" id="token-21-19" morph="none" pos="punct" start_char="3658">."</TOKEN>
</SEG>
<SEG end_char="3783" id="segment-22" start_char="3662">
<ORIGINAL_TEXT>The picture painted by the data is not in itself conclusive, Brownstein acknowledged, but he said the numbers are telling.</ORIGINAL_TEXT>
<TOKEN end_char="3664" id="token-22-0" morph="none" pos="word" start_char="3662">The</TOKEN>
<TOKEN end_char="3672" id="token-22-1" morph="none" pos="word" start_char="3666">picture</TOKEN>
<TOKEN end_char="3680" id="token-22-2" morph="none" pos="word" start_char="3674">painted</TOKEN>
<TOKEN end_char="3683" id="token-22-3" morph="none" pos="word" start_char="3682">by</TOKEN>
<TOKEN end_char="3687" id="token-22-4" morph="none" pos="word" start_char="3685">the</TOKEN>
<TOKEN end_char="3692" id="token-22-5" morph="none" pos="word" start_char="3689">data</TOKEN>
<TOKEN end_char="3695" id="token-22-6" morph="none" pos="word" start_char="3694">is</TOKEN>
<TOKEN end_char="3699" id="token-22-7" morph="none" pos="word" start_char="3697">not</TOKEN>
<TOKEN end_char="3702" id="token-22-8" morph="none" pos="word" start_char="3701">in</TOKEN>
<TOKEN end_char="3709" id="token-22-9" morph="none" pos="word" start_char="3704">itself</TOKEN>
<TOKEN end_char="3720" id="token-22-10" morph="none" pos="word" start_char="3711">conclusive</TOKEN>
<TOKEN end_char="3721" id="token-22-11" morph="none" pos="punct" start_char="3721">,</TOKEN>
<TOKEN end_char="3732" id="token-22-12" morph="none" pos="word" start_char="3723">Brownstein</TOKEN>
<TOKEN end_char="3745" id="token-22-13" morph="none" pos="word" start_char="3734">acknowledged</TOKEN>
<TOKEN end_char="3746" id="token-22-14" morph="none" pos="punct" start_char="3746">,</TOKEN>
<TOKEN end_char="3750" id="token-22-15" morph="none" pos="word" start_char="3748">but</TOKEN>
<TOKEN end_char="3753" id="token-22-16" morph="none" pos="word" start_char="3752">he</TOKEN>
<TOKEN end_char="3758" id="token-22-17" morph="none" pos="word" start_char="3755">said</TOKEN>
<TOKEN end_char="3762" id="token-22-18" morph="none" pos="word" start_char="3760">the</TOKEN>
<TOKEN end_char="3770" id="token-22-19" morph="none" pos="word" start_char="3764">numbers</TOKEN>
<TOKEN end_char="3774" id="token-22-20" morph="none" pos="word" start_char="3772">are</TOKEN>
<TOKEN end_char="3782" id="token-22-21" morph="none" pos="word" start_char="3776">telling</TOKEN>
<TOKEN end_char="3783" id="token-22-22" morph="none" pos="punct" start_char="3783">.</TOKEN>
</SEG>
<SEG end_char="3908" id="segment-23" start_char="3786">
<ORIGINAL_TEXT>"This is all about a growing body of information pointing to something taking place in Wuhan at the time," Brownstein said.</ORIGINAL_TEXT>
<TOKEN end_char="3786" id="token-23-0" morph="none" pos="punct" start_char="3786">"</TOKEN>
<TOKEN end_char="3790" id="token-23-1" morph="none" pos="word" start_char="3787">This</TOKEN>
<TOKEN end_char="3793" id="token-23-2" morph="none" pos="word" start_char="3792">is</TOKEN>
<TOKEN end_char="3797" id="token-23-3" morph="none" pos="word" start_char="3795">all</TOKEN>
<TOKEN end_char="3803" id="token-23-4" morph="none" pos="word" start_char="3799">about</TOKEN>
<TOKEN end_char="3805" id="token-23-5" morph="none" pos="word" start_char="3805">a</TOKEN>
<TOKEN end_char="3813" id="token-23-6" morph="none" pos="word" start_char="3807">growing</TOKEN>
<TOKEN end_char="3818" id="token-23-7" morph="none" pos="word" start_char="3815">body</TOKEN>
<TOKEN end_char="3821" id="token-23-8" morph="none" pos="word" start_char="3820">of</TOKEN>
<TOKEN end_char="3833" id="token-23-9" morph="none" pos="word" start_char="3823">information</TOKEN>
<TOKEN end_char="3842" id="token-23-10" morph="none" pos="word" start_char="3835">pointing</TOKEN>
<TOKEN end_char="3845" id="token-23-11" morph="none" pos="word" start_char="3844">to</TOKEN>
<TOKEN end_char="3855" id="token-23-12" morph="none" pos="word" start_char="3847">something</TOKEN>
<TOKEN end_char="3862" id="token-23-13" morph="none" pos="word" start_char="3857">taking</TOKEN>
<TOKEN end_char="3868" id="token-23-14" morph="none" pos="word" start_char="3864">place</TOKEN>
<TOKEN end_char="3871" id="token-23-15" morph="none" pos="word" start_char="3870">in</TOKEN>
<TOKEN end_char="3877" id="token-23-16" morph="none" pos="word" start_char="3873">Wuhan</TOKEN>
<TOKEN end_char="3880" id="token-23-17" morph="none" pos="word" start_char="3879">at</TOKEN>
<TOKEN end_char="3884" id="token-23-18" morph="none" pos="word" start_char="3882">the</TOKEN>
<TOKEN end_char="3889" id="token-23-19" morph="none" pos="word" start_char="3886">time</TOKEN>
<TOKEN end_char="3891" id="token-23-20" morph="none" pos="punct" start_char="3890">,"</TOKEN>
<TOKEN end_char="3902" id="token-23-21" morph="none" pos="word" start_char="3893">Brownstein</TOKEN>
<TOKEN end_char="3907" id="token-23-22" morph="none" pos="word" start_char="3904">said</TOKEN>
<TOKEN end_char="3908" id="token-23-23" morph="none" pos="punct" start_char="3908">.</TOKEN>
</SEG>
<SEG end_char="4071" id="segment-24" start_char="3910">
<ORIGINAL_TEXT>"Many studies are still needed to fully uncover what took place and for people to really learn about how these disease outbreaks unfold and emerge in populations.</ORIGINAL_TEXT>
<TOKEN end_char="3910" id="token-24-0" morph="none" pos="punct" start_char="3910">"</TOKEN>
<TOKEN end_char="3914" id="token-24-1" morph="none" pos="word" start_char="3911">Many</TOKEN>
<TOKEN end_char="3922" id="token-24-2" morph="none" pos="word" start_char="3916">studies</TOKEN>
<TOKEN end_char="3926" id="token-24-3" morph="none" pos="word" start_char="3924">are</TOKEN>
<TOKEN end_char="3932" id="token-24-4" morph="none" pos="word" start_char="3928">still</TOKEN>
<TOKEN end_char="3939" id="token-24-5" morph="none" pos="word" start_char="3934">needed</TOKEN>
<TOKEN end_char="3942" id="token-24-6" morph="none" pos="word" start_char="3941">to</TOKEN>
<TOKEN end_char="3948" id="token-24-7" morph="none" pos="word" start_char="3944">fully</TOKEN>
<TOKEN end_char="3956" id="token-24-8" morph="none" pos="word" start_char="3950">uncover</TOKEN>
<TOKEN end_char="3961" id="token-24-9" morph="none" pos="word" start_char="3958">what</TOKEN>
<TOKEN end_char="3966" id="token-24-10" morph="none" pos="word" start_char="3963">took</TOKEN>
<TOKEN end_char="3972" id="token-24-11" morph="none" pos="word" start_char="3968">place</TOKEN>
<TOKEN end_char="3976" id="token-24-12" morph="none" pos="word" start_char="3974">and</TOKEN>
<TOKEN end_char="3980" id="token-24-13" morph="none" pos="word" start_char="3978">for</TOKEN>
<TOKEN end_char="3987" id="token-24-14" morph="none" pos="word" start_char="3982">people</TOKEN>
<TOKEN end_char="3990" id="token-24-15" morph="none" pos="word" start_char="3989">to</TOKEN>
<TOKEN end_char="3997" id="token-24-16" morph="none" pos="word" start_char="3992">really</TOKEN>
<TOKEN end_char="4003" id="token-24-17" morph="none" pos="word" start_char="3999">learn</TOKEN>
<TOKEN end_char="4009" id="token-24-18" morph="none" pos="word" start_char="4005">about</TOKEN>
<TOKEN end_char="4013" id="token-24-19" morph="none" pos="word" start_char="4011">how</TOKEN>
<TOKEN end_char="4019" id="token-24-20" morph="none" pos="word" start_char="4015">these</TOKEN>
<TOKEN end_char="4027" id="token-24-21" morph="none" pos="word" start_char="4021">disease</TOKEN>
<TOKEN end_char="4037" id="token-24-22" morph="none" pos="word" start_char="4029">outbreaks</TOKEN>
<TOKEN end_char="4044" id="token-24-23" morph="none" pos="word" start_char="4039">unfold</TOKEN>
<TOKEN end_char="4048" id="token-24-24" morph="none" pos="word" start_char="4046">and</TOKEN>
<TOKEN end_char="4055" id="token-24-25" morph="none" pos="word" start_char="4050">emerge</TOKEN>
<TOKEN end_char="4058" id="token-24-26" morph="none" pos="word" start_char="4057">in</TOKEN>
<TOKEN end_char="4070" id="token-24-27" morph="none" pos="word" start_char="4060">populations</TOKEN>
<TOKEN end_char="4071" id="token-24-28" morph="none" pos="punct" start_char="4071">.</TOKEN>
</SEG>
<SEG end_char="4115" id="segment-25" start_char="4073">
<ORIGINAL_TEXT>So this is just another point of evidence."</ORIGINAL_TEXT>
<TOKEN end_char="4074" id="token-25-0" morph="none" pos="word" start_char="4073">So</TOKEN>
<TOKEN end_char="4079" id="token-25-1" morph="none" pos="word" start_char="4076">this</TOKEN>
<TOKEN end_char="4082" id="token-25-2" morph="none" pos="word" start_char="4081">is</TOKEN>
<TOKEN end_char="4087" id="token-25-3" morph="none" pos="word" start_char="4084">just</TOKEN>
<TOKEN end_char="4095" id="token-25-4" morph="none" pos="word" start_char="4089">another</TOKEN>
<TOKEN end_char="4101" id="token-25-5" morph="none" pos="word" start_char="4097">point</TOKEN>
<TOKEN end_char="4104" id="token-25-6" morph="none" pos="word" start_char="4103">of</TOKEN>
<TOKEN end_char="4113" id="token-25-7" morph="none" pos="word" start_char="4106">evidence</TOKEN>
<TOKEN end_char="4115" id="token-25-8" morph="none" pos="punct" start_char="4114">."</TOKEN>
</SEG>
<SEG end_char="4260" id="segment-26" start_char="4118">
<ORIGINAL_TEXT>Disease ecologist Peter Daszak, president of the nonprofit EcoHealth Alliance in Manhattan, said the Harvard study "is absolutely fascinating."</ORIGINAL_TEXT>
<TOKEN end_char="4124" id="token-26-0" morph="none" pos="word" start_char="4118">Disease</TOKEN>
<TOKEN end_char="4134" id="token-26-1" morph="none" pos="word" start_char="4126">ecologist</TOKEN>
<TOKEN end_char="4140" id="token-26-2" morph="none" pos="word" start_char="4136">Peter</TOKEN>
<TOKEN end_char="4147" id="token-26-3" morph="none" pos="word" start_char="4142">Daszak</TOKEN>
<TOKEN end_char="4148" id="token-26-4" morph="none" pos="punct" start_char="4148">,</TOKEN>
<TOKEN end_char="4158" id="token-26-5" morph="none" pos="word" start_char="4150">president</TOKEN>
<TOKEN end_char="4161" id="token-26-6" morph="none" pos="word" start_char="4160">of</TOKEN>
<TOKEN end_char="4165" id="token-26-7" morph="none" pos="word" start_char="4163">the</TOKEN>
<TOKEN end_char="4175" id="token-26-8" morph="none" pos="word" start_char="4167">nonprofit</TOKEN>
<TOKEN end_char="4185" id="token-26-9" morph="none" pos="word" start_char="4177">EcoHealth</TOKEN>
<TOKEN end_char="4194" id="token-26-10" morph="none" pos="word" start_char="4187">Alliance</TOKEN>
<TOKEN end_char="4197" id="token-26-11" morph="none" pos="word" start_char="4196">in</TOKEN>
<TOKEN end_char="4207" id="token-26-12" morph="none" pos="word" start_char="4199">Manhattan</TOKEN>
<TOKEN end_char="4208" id="token-26-13" morph="none" pos="punct" start_char="4208">,</TOKEN>
<TOKEN end_char="4213" id="token-26-14" morph="none" pos="word" start_char="4210">said</TOKEN>
<TOKEN end_char="4217" id="token-26-15" morph="none" pos="word" start_char="4215">the</TOKEN>
<TOKEN end_char="4225" id="token-26-16" morph="none" pos="word" start_char="4219">Harvard</TOKEN>
<TOKEN end_char="4231" id="token-26-17" morph="none" pos="word" start_char="4227">study</TOKEN>
<TOKEN end_char="4233" id="token-26-18" morph="none" pos="punct" start_char="4233">"</TOKEN>
<TOKEN end_char="4235" id="token-26-19" morph="none" pos="word" start_char="4234">is</TOKEN>
<TOKEN end_char="4246" id="token-26-20" morph="none" pos="word" start_char="4237">absolutely</TOKEN>
<TOKEN end_char="4258" id="token-26-21" morph="none" pos="word" start_char="4248">fascinating</TOKEN>
<TOKEN end_char="4260" id="token-26-22" morph="none" pos="punct" start_char="4259">."</TOKEN>
</SEG>
<SEG end_char="4440" id="segment-27" start_char="4263">
<ORIGINAL_TEXT>"You need to look at every possible bit of evidence, where it came from and when it emerged," said Daszak, whose organization works to understand the origin of emerging diseases.</ORIGINAL_TEXT>
<TOKEN end_char="4263" id="token-27-0" morph="none" pos="punct" start_char="4263">"</TOKEN>
<TOKEN end_char="4266" id="token-27-1" morph="none" pos="word" start_char="4264">You</TOKEN>
<TOKEN end_char="4271" id="token-27-2" morph="none" pos="word" start_char="4268">need</TOKEN>
<TOKEN end_char="4274" id="token-27-3" morph="none" pos="word" start_char="4273">to</TOKEN>
<TOKEN end_char="4279" id="token-27-4" morph="none" pos="word" start_char="4276">look</TOKEN>
<TOKEN end_char="4282" id="token-27-5" morph="none" pos="word" start_char="4281">at</TOKEN>
<TOKEN end_char="4288" id="token-27-6" morph="none" pos="word" start_char="4284">every</TOKEN>
<TOKEN end_char="4297" id="token-27-7" morph="none" pos="word" start_char="4290">possible</TOKEN>
<TOKEN end_char="4301" id="token-27-8" morph="none" pos="word" start_char="4299">bit</TOKEN>
<TOKEN end_char="4304" id="token-27-9" morph="none" pos="word" start_char="4303">of</TOKEN>
<TOKEN end_char="4313" id="token-27-10" morph="none" pos="word" start_char="4306">evidence</TOKEN>
<TOKEN end_char="4314" id="token-27-11" morph="none" pos="punct" start_char="4314">,</TOKEN>
<TOKEN end_char="4320" id="token-27-12" morph="none" pos="word" start_char="4316">where</TOKEN>
<TOKEN end_char="4323" id="token-27-13" morph="none" pos="word" start_char="4322">it</TOKEN>
<TOKEN end_char="4328" id="token-27-14" morph="none" pos="word" start_char="4325">came</TOKEN>
<TOKEN end_char="4333" id="token-27-15" morph="none" pos="word" start_char="4330">from</TOKEN>
<TOKEN end_char="4337" id="token-27-16" morph="none" pos="word" start_char="4335">and</TOKEN>
<TOKEN end_char="4342" id="token-27-17" morph="none" pos="word" start_char="4339">when</TOKEN>
<TOKEN end_char="4345" id="token-27-18" morph="none" pos="word" start_char="4344">it</TOKEN>
<TOKEN end_char="4353" id="token-27-19" morph="none" pos="word" start_char="4347">emerged</TOKEN>
<TOKEN end_char="4355" id="token-27-20" morph="none" pos="punct" start_char="4354">,"</TOKEN>
<TOKEN end_char="4360" id="token-27-21" morph="none" pos="word" start_char="4357">said</TOKEN>
<TOKEN end_char="4367" id="token-27-22" morph="none" pos="word" start_char="4362">Daszak</TOKEN>
<TOKEN end_char="4368" id="token-27-23" morph="none" pos="punct" start_char="4368">,</TOKEN>
<TOKEN end_char="4374" id="token-27-24" morph="none" pos="word" start_char="4370">whose</TOKEN>
<TOKEN end_char="4387" id="token-27-25" morph="none" pos="word" start_char="4376">organization</TOKEN>
<TOKEN end_char="4393" id="token-27-26" morph="none" pos="word" start_char="4389">works</TOKEN>
<TOKEN end_char="4396" id="token-27-27" morph="none" pos="word" start_char="4395">to</TOKEN>
<TOKEN end_char="4407" id="token-27-28" morph="none" pos="word" start_char="4398">understand</TOKEN>
<TOKEN end_char="4411" id="token-27-29" morph="none" pos="word" start_char="4409">the</TOKEN>
<TOKEN end_char="4418" id="token-27-30" morph="none" pos="word" start_char="4413">origin</TOKEN>
<TOKEN end_char="4421" id="token-27-31" morph="none" pos="word" start_char="4420">of</TOKEN>
<TOKEN end_char="4430" id="token-27-32" morph="none" pos="word" start_char="4423">emerging</TOKEN>
<TOKEN end_char="4439" id="token-27-33" morph="none" pos="word" start_char="4432">diseases</TOKEN>
<TOKEN end_char="4440" id="token-27-34" morph="none" pos="punct" start_char="4440">.</TOKEN>
</SEG>
<SEG end_char="4563" id="segment-28" start_char="4442">
<ORIGINAL_TEXT>"When we do analysis after outbreaks, we find that the diseases had been in circulation days, weeks, months, years before.</ORIGINAL_TEXT>
<TOKEN end_char="4442" id="token-28-0" morph="none" pos="punct" start_char="4442">"</TOKEN>
<TOKEN end_char="4446" id="token-28-1" morph="none" pos="word" start_char="4443">When</TOKEN>
<TOKEN end_char="4449" id="token-28-2" morph="none" pos="word" start_char="4448">we</TOKEN>
<TOKEN end_char="4452" id="token-28-3" morph="none" pos="word" start_char="4451">do</TOKEN>
<TOKEN end_char="4461" id="token-28-4" morph="none" pos="word" start_char="4454">analysis</TOKEN>
<TOKEN end_char="4467" id="token-28-5" morph="none" pos="word" start_char="4463">after</TOKEN>
<TOKEN end_char="4477" id="token-28-6" morph="none" pos="word" start_char="4469">outbreaks</TOKEN>
<TOKEN end_char="4478" id="token-28-7" morph="none" pos="punct" start_char="4478">,</TOKEN>
<TOKEN end_char="4481" id="token-28-8" morph="none" pos="word" start_char="4480">we</TOKEN>
<TOKEN end_char="4486" id="token-28-9" morph="none" pos="word" start_char="4483">find</TOKEN>
<TOKEN end_char="4491" id="token-28-10" morph="none" pos="word" start_char="4488">that</TOKEN>
<TOKEN end_char="4495" id="token-28-11" morph="none" pos="word" start_char="4493">the</TOKEN>
<TOKEN end_char="4504" id="token-28-12" morph="none" pos="word" start_char="4497">diseases</TOKEN>
<TOKEN end_char="4508" id="token-28-13" morph="none" pos="word" start_char="4506">had</TOKEN>
<TOKEN end_char="4513" id="token-28-14" morph="none" pos="word" start_char="4510">been</TOKEN>
<TOKEN end_char="4516" id="token-28-15" morph="none" pos="word" start_char="4515">in</TOKEN>
<TOKEN end_char="4528" id="token-28-16" morph="none" pos="word" start_char="4518">circulation</TOKEN>
<TOKEN end_char="4533" id="token-28-17" morph="none" pos="word" start_char="4530">days</TOKEN>
<TOKEN end_char="4534" id="token-28-18" morph="none" pos="punct" start_char="4534">,</TOKEN>
<TOKEN end_char="4540" id="token-28-19" morph="none" pos="word" start_char="4536">weeks</TOKEN>
<TOKEN end_char="4541" id="token-28-20" morph="none" pos="punct" start_char="4541">,</TOKEN>
<TOKEN end_char="4548" id="token-28-21" morph="none" pos="word" start_char="4543">months</TOKEN>
<TOKEN end_char="4549" id="token-28-22" morph="none" pos="punct" start_char="4549">,</TOKEN>
<TOKEN end_char="4555" id="token-28-23" morph="none" pos="word" start_char="4551">years</TOKEN>
<TOKEN end_char="4562" id="token-28-24" morph="none" pos="word" start_char="4557">before</TOKEN>
<TOKEN end_char="4563" id="token-28-25" morph="none" pos="punct" start_char="4563">.</TOKEN>
</SEG>
<SEG end_char="4628" id="segment-29" start_char="4565">
<ORIGINAL_TEXT>I really believe that’s what we’re going to find with COVID-19."</ORIGINAL_TEXT>
<TOKEN end_char="4565" id="token-29-0" morph="none" pos="word" start_char="4565">I</TOKEN>
<TOKEN end_char="4572" id="token-29-1" morph="none" pos="word" start_char="4567">really</TOKEN>
<TOKEN end_char="4580" id="token-29-2" morph="none" pos="word" start_char="4574">believe</TOKEN>
<TOKEN end_char="4587" id="token-29-3" morph="none" pos="word" start_char="4582">that’s</TOKEN>
<TOKEN end_char="4592" id="token-29-4" morph="none" pos="word" start_char="4589">what</TOKEN>
<TOKEN end_char="4598" id="token-29-5" morph="none" pos="word" start_char="4594">we’re</TOKEN>
<TOKEN end_char="4604" id="token-29-6" morph="none" pos="word" start_char="4600">going</TOKEN>
<TOKEN end_char="4607" id="token-29-7" morph="none" pos="word" start_char="4606">to</TOKEN>
<TOKEN end_char="4612" id="token-29-8" morph="none" pos="word" start_char="4609">find</TOKEN>
<TOKEN end_char="4617" id="token-29-9" morph="none" pos="word" start_char="4614">with</TOKEN>
<TOKEN end_char="4626" id="token-29-10" morph="none" pos="unknown" start_char="4619">COVID-19</TOKEN>
<TOKEN end_char="4628" id="token-29-11" morph="none" pos="punct" start_char="4627">."</TOKEN>
</SEG>
<SEG end_char="4807" id="segment-30" start_char="4631">
<ORIGINAL_TEXT>David Perlin, chief science officer at the Center for Discovery and Innovation in New Jersey, said he was intrigued by Brownstein’s research, though he wasn’t totally convinced.</ORIGINAL_TEXT>
<TOKEN end_char="4635" id="token-30-0" morph="none" pos="word" start_char="4631">David</TOKEN>
<TOKEN end_char="4642" id="token-30-1" morph="none" pos="word" start_char="4637">Perlin</TOKEN>
<TOKEN end_char="4643" id="token-30-2" morph="none" pos="punct" start_char="4643">,</TOKEN>
<TOKEN end_char="4649" id="token-30-3" morph="none" pos="word" start_char="4645">chief</TOKEN>
<TOKEN end_char="4657" id="token-30-4" morph="none" pos="word" start_char="4651">science</TOKEN>
<TOKEN end_char="4665" id="token-30-5" morph="none" pos="word" start_char="4659">officer</TOKEN>
<TOKEN end_char="4668" id="token-30-6" morph="none" pos="word" start_char="4667">at</TOKEN>
<TOKEN end_char="4672" id="token-30-7" morph="none" pos="word" start_char="4670">the</TOKEN>
<TOKEN end_char="4679" id="token-30-8" morph="none" pos="word" start_char="4674">Center</TOKEN>
<TOKEN end_char="4683" id="token-30-9" morph="none" pos="word" start_char="4681">for</TOKEN>
<TOKEN end_char="4693" id="token-30-10" morph="none" pos="word" start_char="4685">Discovery</TOKEN>
<TOKEN end_char="4697" id="token-30-11" morph="none" pos="word" start_char="4695">and</TOKEN>
<TOKEN end_char="4708" id="token-30-12" morph="none" pos="word" start_char="4699">Innovation</TOKEN>
<TOKEN end_char="4711" id="token-30-13" morph="none" pos="word" start_char="4710">in</TOKEN>
<TOKEN end_char="4715" id="token-30-14" morph="none" pos="word" start_char="4713">New</TOKEN>
<TOKEN end_char="4722" id="token-30-15" morph="none" pos="word" start_char="4717">Jersey</TOKEN>
<TOKEN end_char="4723" id="token-30-16" morph="none" pos="punct" start_char="4723">,</TOKEN>
<TOKEN end_char="4728" id="token-30-17" morph="none" pos="word" start_char="4725">said</TOKEN>
<TOKEN end_char="4731" id="token-30-18" morph="none" pos="word" start_char="4730">he</TOKEN>
<TOKEN end_char="4735" id="token-30-19" morph="none" pos="word" start_char="4733">was</TOKEN>
<TOKEN end_char="4745" id="token-30-20" morph="none" pos="word" start_char="4737">intrigued</TOKEN>
<TOKEN end_char="4748" id="token-30-21" morph="none" pos="word" start_char="4747">by</TOKEN>
<TOKEN end_char="4761" id="token-30-22" morph="none" pos="word" start_char="4750">Brownstein’s</TOKEN>
<TOKEN end_char="4770" id="token-30-23" morph="none" pos="word" start_char="4763">research</TOKEN>
<TOKEN end_char="4771" id="token-30-24" morph="none" pos="punct" start_char="4771">,</TOKEN>
<TOKEN end_char="4778" id="token-30-25" morph="none" pos="word" start_char="4773">though</TOKEN>
<TOKEN end_char="4781" id="token-30-26" morph="none" pos="word" start_char="4780">he</TOKEN>
<TOKEN end_char="4788" id="token-30-27" morph="none" pos="word" start_char="4783">wasn’t</TOKEN>
<TOKEN end_char="4796" id="token-30-28" morph="none" pos="word" start_char="4790">totally</TOKEN>
<TOKEN end_char="4806" id="token-30-29" morph="none" pos="word" start_char="4798">convinced</TOKEN>
<TOKEN end_char="4807" id="token-30-30" morph="none" pos="punct" start_char="4807">.</TOKEN>
</SEG>
<SEG end_char="4923" id="segment-31" start_char="4810">
<ORIGINAL_TEXT>"I think some of the methods are questionable and their interpretation is slightly over-interpreted," Perlin said.</ORIGINAL_TEXT>
<TOKEN end_char="4810" id="token-31-0" morph="none" pos="punct" start_char="4810">"</TOKEN>
<TOKEN end_char="4811" id="token-31-1" morph="none" pos="word" start_char="4811">I</TOKEN>
<TOKEN end_char="4817" id="token-31-2" morph="none" pos="word" start_char="4813">think</TOKEN>
<TOKEN end_char="4822" id="token-31-3" morph="none" pos="word" start_char="4819">some</TOKEN>
<TOKEN end_char="4825" id="token-31-4" morph="none" pos="word" start_char="4824">of</TOKEN>
<TOKEN end_char="4829" id="token-31-5" morph="none" pos="word" start_char="4827">the</TOKEN>
<TOKEN end_char="4837" id="token-31-6" morph="none" pos="word" start_char="4831">methods</TOKEN>
<TOKEN end_char="4841" id="token-31-7" morph="none" pos="word" start_char="4839">are</TOKEN>
<TOKEN end_char="4854" id="token-31-8" morph="none" pos="word" start_char="4843">questionable</TOKEN>
<TOKEN end_char="4858" id="token-31-9" morph="none" pos="word" start_char="4856">and</TOKEN>
<TOKEN end_char="4864" id="token-31-10" morph="none" pos="word" start_char="4860">their</TOKEN>
<TOKEN end_char="4879" id="token-31-11" morph="none" pos="word" start_char="4866">interpretation</TOKEN>
<TOKEN end_char="4882" id="token-31-12" morph="none" pos="word" start_char="4881">is</TOKEN>
<TOKEN end_char="4891" id="token-31-13" morph="none" pos="word" start_char="4884">slightly</TOKEN>
<TOKEN end_char="4908" id="token-31-14" morph="none" pos="unknown" start_char="4893">over-interpreted</TOKEN>
<TOKEN end_char="4910" id="token-31-15" morph="none" pos="punct" start_char="4909">,"</TOKEN>
<TOKEN end_char="4917" id="token-31-16" morph="none" pos="word" start_char="4912">Perlin</TOKEN>
<TOKEN end_char="4922" id="token-31-17" morph="none" pos="word" start_char="4919">said</TOKEN>
<TOKEN end_char="4923" id="token-31-18" morph="none" pos="punct" start_char="4923">.</TOKEN>
</SEG>
<SEG end_char="4975" id="segment-32" start_char="4925">
<ORIGINAL_TEXT>"The problem is we only have a subset of data here.</ORIGINAL_TEXT>
<TOKEN end_char="4925" id="token-32-0" morph="none" pos="punct" start_char="4925">"</TOKEN>
<TOKEN end_char="4928" id="token-32-1" morph="none" pos="word" start_char="4926">The</TOKEN>
<TOKEN end_char="4936" id="token-32-2" morph="none" pos="word" start_char="4930">problem</TOKEN>
<TOKEN end_char="4939" id="token-32-3" morph="none" pos="word" start_char="4938">is</TOKEN>
<TOKEN end_char="4942" id="token-32-4" morph="none" pos="word" start_char="4941">we</TOKEN>
<TOKEN end_char="4947" id="token-32-5" morph="none" pos="word" start_char="4944">only</TOKEN>
<TOKEN end_char="4952" id="token-32-6" morph="none" pos="word" start_char="4949">have</TOKEN>
<TOKEN end_char="4954" id="token-32-7" morph="none" pos="word" start_char="4954">a</TOKEN>
<TOKEN end_char="4961" id="token-32-8" morph="none" pos="word" start_char="4956">subset</TOKEN>
<TOKEN end_char="4964" id="token-32-9" morph="none" pos="word" start_char="4963">of</TOKEN>
<TOKEN end_char="4969" id="token-32-10" morph="none" pos="word" start_char="4966">data</TOKEN>
<TOKEN end_char="4974" id="token-32-11" morph="none" pos="word" start_char="4971">here</TOKEN>
<TOKEN end_char="4975" id="token-32-12" morph="none" pos="punct" start_char="4975">.</TOKEN>
</SEG>
<SEG end_char="5096" id="segment-33" start_char="4977">
<ORIGINAL_TEXT>I always worry when people start drawing inferences from data subsets, cherry-picking data [like the internet searches].</ORIGINAL_TEXT>
<TOKEN end_char="4977" id="token-33-0" morph="none" pos="word" start_char="4977">I</TOKEN>
<TOKEN end_char="4984" id="token-33-1" morph="none" pos="word" start_char="4979">always</TOKEN>
<TOKEN end_char="4990" id="token-33-2" morph="none" pos="word" start_char="4986">worry</TOKEN>
<TOKEN end_char="4995" id="token-33-3" morph="none" pos="word" start_char="4992">when</TOKEN>
<TOKEN end_char="5002" id="token-33-4" morph="none" pos="word" start_char="4997">people</TOKEN>
<TOKEN end_char="5008" id="token-33-5" morph="none" pos="word" start_char="5004">start</TOKEN>
<TOKEN end_char="5016" id="token-33-6" morph="none" pos="word" start_char="5010">drawing</TOKEN>
<TOKEN end_char="5027" id="token-33-7" morph="none" pos="word" start_char="5018">inferences</TOKEN>
<TOKEN end_char="5032" id="token-33-8" morph="none" pos="word" start_char="5029">from</TOKEN>
<TOKEN end_char="5037" id="token-33-9" morph="none" pos="word" start_char="5034">data</TOKEN>
<TOKEN end_char="5045" id="token-33-10" morph="none" pos="word" start_char="5039">subsets</TOKEN>
<TOKEN end_char="5046" id="token-33-11" morph="none" pos="punct" start_char="5046">,</TOKEN>
<TOKEN end_char="5061" id="token-33-12" morph="none" pos="unknown" start_char="5048">cherry-picking</TOKEN>
<TOKEN end_char="5066" id="token-33-13" morph="none" pos="word" start_char="5063">data</TOKEN>
<TOKEN end_char="5068" id="token-33-14" morph="none" pos="punct" start_char="5068">[</TOKEN>
<TOKEN end_char="5072" id="token-33-15" morph="none" pos="word" start_char="5069">like</TOKEN>
<TOKEN end_char="5076" id="token-33-16" morph="none" pos="word" start_char="5074">the</TOKEN>
<TOKEN end_char="5085" id="token-33-17" morph="none" pos="word" start_char="5078">internet</TOKEN>
<TOKEN end_char="5094" id="token-33-18" morph="none" pos="word" start_char="5087">searches</TOKEN>
<TOKEN end_char="5096" id="token-33-19" morph="none" pos="punct" start_char="5095">].</TOKEN>
</SEG>
<SEG end_char="5114" id="segment-34" start_char="5098">
<ORIGINAL_TEXT>It’s suggestive."</ORIGINAL_TEXT>
<TOKEN end_char="5101" id="token-34-0" morph="none" pos="word" start_char="5098">It’s</TOKEN>
<TOKEN end_char="5112" id="token-34-1" morph="none" pos="word" start_char="5103">suggestive</TOKEN>
<TOKEN end_char="5114" id="token-34-2" morph="none" pos="punct" start_char="5113">."</TOKEN>
</SEG>
<SEG end_char="5168" id="segment-35" start_char="5117">
<ORIGINAL_TEXT>Photographs taken from space suggests a crisis below</ORIGINAL_TEXT>
<TOKEN end_char="5127" id="token-35-0" morph="none" pos="word" start_char="5117">Photographs</TOKEN>
<TOKEN end_char="5133" id="token-35-1" morph="none" pos="word" start_char="5129">taken</TOKEN>
<TOKEN end_char="5138" id="token-35-2" morph="none" pos="word" start_char="5135">from</TOKEN>
<TOKEN end_char="5144" id="token-35-3" morph="none" pos="word" start_char="5140">space</TOKEN>
<TOKEN end_char="5153" id="token-35-4" morph="none" pos="word" start_char="5146">suggests</TOKEN>
<TOKEN end_char="5155" id="token-35-5" morph="none" pos="word" start_char="5155">a</TOKEN>
<TOKEN end_char="5162" id="token-35-6" morph="none" pos="word" start_char="5157">crisis</TOKEN>
<TOKEN end_char="5168" id="token-35-7" morph="none" pos="word" start_char="5164">below</TOKEN>
</SEG>
<SEG end_char="5363" id="segment-36" start_char="5171">
<ORIGINAL_TEXT>Starting with nearly 350 images captured by private satellites circling the globe, Brownstein’s study first examined traffic and parking outside major hospitals in Wuhan for the past two years.</ORIGINAL_TEXT>
<TOKEN end_char="5178" id="token-36-0" morph="none" pos="word" start_char="5171">Starting</TOKEN>
<TOKEN end_char="5183" id="token-36-1" morph="none" pos="word" start_char="5180">with</TOKEN>
<TOKEN end_char="5190" id="token-36-2" morph="none" pos="word" start_char="5185">nearly</TOKEN>
<TOKEN end_char="5194" id="token-36-3" morph="none" pos="word" start_char="5192">350</TOKEN>
<TOKEN end_char="5201" id="token-36-4" morph="none" pos="word" start_char="5196">images</TOKEN>
<TOKEN end_char="5210" id="token-36-5" morph="none" pos="word" start_char="5203">captured</TOKEN>
<TOKEN end_char="5213" id="token-36-6" morph="none" pos="word" start_char="5212">by</TOKEN>
<TOKEN end_char="5221" id="token-36-7" morph="none" pos="word" start_char="5215">private</TOKEN>
<TOKEN end_char="5232" id="token-36-8" morph="none" pos="word" start_char="5223">satellites</TOKEN>
<TOKEN end_char="5241" id="token-36-9" morph="none" pos="word" start_char="5234">circling</TOKEN>
<TOKEN end_char="5245" id="token-36-10" morph="none" pos="word" start_char="5243">the</TOKEN>
<TOKEN end_char="5251" id="token-36-11" morph="none" pos="word" start_char="5247">globe</TOKEN>
<TOKEN end_char="5252" id="token-36-12" morph="none" pos="punct" start_char="5252">,</TOKEN>
<TOKEN end_char="5265" id="token-36-13" morph="none" pos="word" start_char="5254">Brownstein’s</TOKEN>
<TOKEN end_char="5271" id="token-36-14" morph="none" pos="word" start_char="5267">study</TOKEN>
<TOKEN end_char="5277" id="token-36-15" morph="none" pos="word" start_char="5273">first</TOKEN>
<TOKEN end_char="5286" id="token-36-16" morph="none" pos="word" start_char="5279">examined</TOKEN>
<TOKEN end_char="5294" id="token-36-17" morph="none" pos="word" start_char="5288">traffic</TOKEN>
<TOKEN end_char="5298" id="token-36-18" morph="none" pos="word" start_char="5296">and</TOKEN>
<TOKEN end_char="5306" id="token-36-19" morph="none" pos="word" start_char="5300">parking</TOKEN>
<TOKEN end_char="5314" id="token-36-20" morph="none" pos="word" start_char="5308">outside</TOKEN>
<TOKEN end_char="5320" id="token-36-21" morph="none" pos="word" start_char="5316">major</TOKEN>
<TOKEN end_char="5330" id="token-36-22" morph="none" pos="word" start_char="5322">hospitals</TOKEN>
<TOKEN end_char="5333" id="token-36-23" morph="none" pos="word" start_char="5332">in</TOKEN>
<TOKEN end_char="5339" id="token-36-24" morph="none" pos="word" start_char="5335">Wuhan</TOKEN>
<TOKEN end_char="5343" id="token-36-25" morph="none" pos="word" start_char="5341">for</TOKEN>
<TOKEN end_char="5347" id="token-36-26" morph="none" pos="word" start_char="5345">the</TOKEN>
<TOKEN end_char="5352" id="token-36-27" morph="none" pos="word" start_char="5349">past</TOKEN>
<TOKEN end_char="5356" id="token-36-28" morph="none" pos="word" start_char="5354">two</TOKEN>
<TOKEN end_char="5362" id="token-36-29" morph="none" pos="word" start_char="5358">years</TOKEN>
<TOKEN end_char="5363" id="token-36-30" morph="none" pos="punct" start_char="5363">.</TOKEN>
</SEG>
<SEG end_char="5481" id="segment-37" start_char="5365">
<ORIGINAL_TEXT>Among them were photographs snapped from space approximately every week or every other week through the fall of 2019.</ORIGINAL_TEXT>
<TOKEN end_char="5369" id="token-37-0" morph="none" pos="word" start_char="5365">Among</TOKEN>
<TOKEN end_char="5374" id="token-37-1" morph="none" pos="word" start_char="5371">them</TOKEN>
<TOKEN end_char="5379" id="token-37-2" morph="none" pos="word" start_char="5376">were</TOKEN>
<TOKEN end_char="5391" id="token-37-3" morph="none" pos="word" start_char="5381">photographs</TOKEN>
<TOKEN end_char="5399" id="token-37-4" morph="none" pos="word" start_char="5393">snapped</TOKEN>
<TOKEN end_char="5404" id="token-37-5" morph="none" pos="word" start_char="5401">from</TOKEN>
<TOKEN end_char="5410" id="token-37-6" morph="none" pos="word" start_char="5406">space</TOKEN>
<TOKEN end_char="5424" id="token-37-7" morph="none" pos="word" start_char="5412">approximately</TOKEN>
<TOKEN end_char="5430" id="token-37-8" morph="none" pos="word" start_char="5426">every</TOKEN>
<TOKEN end_char="5435" id="token-37-9" morph="none" pos="word" start_char="5432">week</TOKEN>
<TOKEN end_char="5438" id="token-37-10" morph="none" pos="word" start_char="5437">or</TOKEN>
<TOKEN end_char="5444" id="token-37-11" morph="none" pos="word" start_char="5440">every</TOKEN>
<TOKEN end_char="5450" id="token-37-12" morph="none" pos="word" start_char="5446">other</TOKEN>
<TOKEN end_char="5455" id="token-37-13" morph="none" pos="word" start_char="5452">week</TOKEN>
<TOKEN end_char="5463" id="token-37-14" morph="none" pos="word" start_char="5457">through</TOKEN>
<TOKEN end_char="5467" id="token-37-15" morph="none" pos="word" start_char="5465">the</TOKEN>
<TOKEN end_char="5472" id="token-37-16" morph="none" pos="word" start_char="5469">fall</TOKEN>
<TOKEN end_char="5475" id="token-37-17" morph="none" pos="word" start_char="5474">of</TOKEN>
<TOKEN end_char="5480" id="token-37-18" morph="none" pos="word" start_char="5477">2019</TOKEN>
<TOKEN end_char="5481" id="token-37-19" morph="none" pos="punct" start_char="5481">.</TOKEN>
</SEG>
<SEG end_char="5685" id="segment-38" start_char="5483">
<ORIGINAL_TEXT>From the approximately 350 frames, researchers found 108 usable images, showing locations without obstruction from smog, tall buildings, clouds or other features that could complicate satellite analysis.</ORIGINAL_TEXT>
<TOKEN end_char="5486" id="token-38-0" morph="none" pos="word" start_char="5483">From</TOKEN>
<TOKEN end_char="5490" id="token-38-1" morph="none" pos="word" start_char="5488">the</TOKEN>
<TOKEN end_char="5504" id="token-38-2" morph="none" pos="word" start_char="5492">approximately</TOKEN>
<TOKEN end_char="5508" id="token-38-3" morph="none" pos="word" start_char="5506">350</TOKEN>
<TOKEN end_char="5515" id="token-38-4" morph="none" pos="word" start_char="5510">frames</TOKEN>
<TOKEN end_char="5516" id="token-38-5" morph="none" pos="punct" start_char="5516">,</TOKEN>
<TOKEN end_char="5528" id="token-38-6" morph="none" pos="word" start_char="5518">researchers</TOKEN>
<TOKEN end_char="5534" id="token-38-7" morph="none" pos="word" start_char="5530">found</TOKEN>
<TOKEN end_char="5538" id="token-38-8" morph="none" pos="word" start_char="5536">108</TOKEN>
<TOKEN end_char="5545" id="token-38-9" morph="none" pos="word" start_char="5540">usable</TOKEN>
<TOKEN end_char="5552" id="token-38-10" morph="none" pos="word" start_char="5547">images</TOKEN>
<TOKEN end_char="5553" id="token-38-11" morph="none" pos="punct" start_char="5553">,</TOKEN>
<TOKEN end_char="5561" id="token-38-12" morph="none" pos="word" start_char="5555">showing</TOKEN>
<TOKEN end_char="5571" id="token-38-13" morph="none" pos="word" start_char="5563">locations</TOKEN>
<TOKEN end_char="5579" id="token-38-14" morph="none" pos="word" start_char="5573">without</TOKEN>
<TOKEN end_char="5591" id="token-38-15" morph="none" pos="word" start_char="5581">obstruction</TOKEN>
<TOKEN end_char="5596" id="token-38-16" morph="none" pos="word" start_char="5593">from</TOKEN>
<TOKEN end_char="5601" id="token-38-17" morph="none" pos="word" start_char="5598">smog</TOKEN>
<TOKEN end_char="5602" id="token-38-18" morph="none" pos="punct" start_char="5602">,</TOKEN>
<TOKEN end_char="5607" id="token-38-19" morph="none" pos="word" start_char="5604">tall</TOKEN>
<TOKEN end_char="5617" id="token-38-20" morph="none" pos="word" start_char="5609">buildings</TOKEN>
<TOKEN end_char="5618" id="token-38-21" morph="none" pos="punct" start_char="5618">,</TOKEN>
<TOKEN end_char="5625" id="token-38-22" morph="none" pos="word" start_char="5620">clouds</TOKEN>
<TOKEN end_char="5628" id="token-38-23" morph="none" pos="word" start_char="5627">or</TOKEN>
<TOKEN end_char="5634" id="token-38-24" morph="none" pos="word" start_char="5630">other</TOKEN>
<TOKEN end_char="5643" id="token-38-25" morph="none" pos="word" start_char="5636">features</TOKEN>
<TOKEN end_char="5648" id="token-38-26" morph="none" pos="word" start_char="5645">that</TOKEN>
<TOKEN end_char="5654" id="token-38-27" morph="none" pos="word" start_char="5650">could</TOKEN>
<TOKEN end_char="5665" id="token-38-28" morph="none" pos="word" start_char="5656">complicate</TOKEN>
<TOKEN end_char="5675" id="token-38-29" morph="none" pos="word" start_char="5667">satellite</TOKEN>
<TOKEN end_char="5684" id="token-38-30" morph="none" pos="word" start_char="5677">analysis</TOKEN>
<TOKEN end_char="5685" id="token-38-31" morph="none" pos="punct" start_char="5685">.</TOKEN>
</SEG>
<SEG end_char="5778" id="segment-39" start_char="5688">
<ORIGINAL_TEXT>"It has to be right at noon," Brownstein said, "because you basically want direct sunlight.</ORIGINAL_TEXT>
<TOKEN end_char="5688" id="token-39-0" morph="none" pos="punct" start_char="5688">"</TOKEN>
<TOKEN end_char="5690" id="token-39-1" morph="none" pos="word" start_char="5689">It</TOKEN>
<TOKEN end_char="5694" id="token-39-2" morph="none" pos="word" start_char="5692">has</TOKEN>
<TOKEN end_char="5697" id="token-39-3" morph="none" pos="word" start_char="5696">to</TOKEN>
<TOKEN end_char="5700" id="token-39-4" morph="none" pos="word" start_char="5699">be</TOKEN>
<TOKEN end_char="5706" id="token-39-5" morph="none" pos="word" start_char="5702">right</TOKEN>
<TOKEN end_char="5709" id="token-39-6" morph="none" pos="word" start_char="5708">at</TOKEN>
<TOKEN end_char="5714" id="token-39-7" morph="none" pos="word" start_char="5711">noon</TOKEN>
<TOKEN end_char="5716" id="token-39-8" morph="none" pos="punct" start_char="5715">,"</TOKEN>
<TOKEN end_char="5727" id="token-39-9" morph="none" pos="word" start_char="5718">Brownstein</TOKEN>
<TOKEN end_char="5732" id="token-39-10" morph="none" pos="word" start_char="5729">said</TOKEN>
<TOKEN end_char="5733" id="token-39-11" morph="none" pos="punct" start_char="5733">,</TOKEN>
<TOKEN end_char="5735" id="token-39-12" morph="none" pos="punct" start_char="5735">"</TOKEN>
<TOKEN end_char="5742" id="token-39-13" morph="none" pos="word" start_char="5736">because</TOKEN>
<TOKEN end_char="5746" id="token-39-14" morph="none" pos="word" start_char="5744">you</TOKEN>
<TOKEN end_char="5756" id="token-39-15" morph="none" pos="word" start_char="5748">basically</TOKEN>
<TOKEN end_char="5761" id="token-39-16" morph="none" pos="word" start_char="5758">want</TOKEN>
<TOKEN end_char="5768" id="token-39-17" morph="none" pos="word" start_char="5763">direct</TOKEN>
<TOKEN end_char="5777" id="token-39-18" morph="none" pos="word" start_char="5770">sunlight</TOKEN>
<TOKEN end_char="5778" id="token-39-19" morph="none" pos="punct" start_char="5778">.</TOKEN>
</SEG>
<SEG end_char="5844" id="segment-40" start_char="5780">
<ORIGINAL_TEXT>You don’t want shadows to prevent our ability to count the cars."</ORIGINAL_TEXT>
<TOKEN end_char="5782" id="token-40-0" morph="none" pos="word" start_char="5780">You</TOKEN>
<TOKEN end_char="5788" id="token-40-1" morph="none" pos="word" start_char="5784">don’t</TOKEN>
<TOKEN end_char="5793" id="token-40-2" morph="none" pos="word" start_char="5790">want</TOKEN>
<TOKEN end_char="5801" id="token-40-3" morph="none" pos="word" start_char="5795">shadows</TOKEN>
<TOKEN end_char="5804" id="token-40-4" morph="none" pos="word" start_char="5803">to</TOKEN>
<TOKEN end_char="5812" id="token-40-5" morph="none" pos="word" start_char="5806">prevent</TOKEN>
<TOKEN end_char="5816" id="token-40-6" morph="none" pos="word" start_char="5814">our</TOKEN>
<TOKEN end_char="5824" id="token-40-7" morph="none" pos="word" start_char="5818">ability</TOKEN>
<TOKEN end_char="5827" id="token-40-8" morph="none" pos="word" start_char="5826">to</TOKEN>
<TOKEN end_char="5833" id="token-40-9" morph="none" pos="word" start_char="5829">count</TOKEN>
<TOKEN end_char="5837" id="token-40-10" morph="none" pos="word" start_char="5835">the</TOKEN>
<TOKEN end_char="5842" id="token-40-11" morph="none" pos="word" start_char="5839">cars</TOKEN>
<TOKEN end_char="5844" id="token-40-12" morph="none" pos="punct" start_char="5843">."</TOKEN>
</SEG>
<SEG end_char="5853" id="segment-41" start_char="5847">
<ORIGINAL_TEXT>On Oct.</ORIGINAL_TEXT>
<TOKEN end_char="5848" id="token-41-0" morph="none" pos="word" start_char="5847">On</TOKEN>
<TOKEN end_char="5852" id="token-41-1" morph="none" pos="word" start_char="5850">Oct</TOKEN>
<TOKEN end_char="5853" id="token-41-2" morph="none" pos="punct" start_char="5853">.</TOKEN>
</SEG>
<SEG end_char="5958" id="segment-42" start_char="5855">
<ORIGINAL_TEXT>10, 2018, there were 171 cars in the parking lot of Wuhan’s Tianyou Hospital, one of the city’s largest.</ORIGINAL_TEXT>
<TOKEN end_char="5856" id="token-42-0" morph="none" pos="word" start_char="5855">10</TOKEN>
<TOKEN end_char="5857" id="token-42-1" morph="none" pos="punct" start_char="5857">,</TOKEN>
<TOKEN end_char="5862" id="token-42-2" morph="none" pos="word" start_char="5859">2018</TOKEN>
<TOKEN end_char="5863" id="token-42-3" morph="none" pos="punct" start_char="5863">,</TOKEN>
<TOKEN end_char="5869" id="token-42-4" morph="none" pos="word" start_char="5865">there</TOKEN>
<TOKEN end_char="5874" id="token-42-5" morph="none" pos="word" start_char="5871">were</TOKEN>
<TOKEN end_char="5878" id="token-42-6" morph="none" pos="word" start_char="5876">171</TOKEN>
<TOKEN end_char="5883" id="token-42-7" morph="none" pos="word" start_char="5880">cars</TOKEN>
<TOKEN end_char="5886" id="token-42-8" morph="none" pos="word" start_char="5885">in</TOKEN>
<TOKEN end_char="5890" id="token-42-9" morph="none" pos="word" start_char="5888">the</TOKEN>
<TOKEN end_char="5898" id="token-42-10" morph="none" pos="word" start_char="5892">parking</TOKEN>
<TOKEN end_char="5902" id="token-42-11" morph="none" pos="word" start_char="5900">lot</TOKEN>
<TOKEN end_char="5905" id="token-42-12" morph="none" pos="word" start_char="5904">of</TOKEN>
<TOKEN end_char="5913" id="token-42-13" morph="none" pos="word" start_char="5907">Wuhan’s</TOKEN>
<TOKEN end_char="5921" id="token-42-14" morph="none" pos="word" start_char="5915">Tianyou</TOKEN>
<TOKEN end_char="5930" id="token-42-15" morph="none" pos="word" start_char="5923">Hospital</TOKEN>
<TOKEN end_char="5931" id="token-42-16" morph="none" pos="punct" start_char="5931">,</TOKEN>
<TOKEN end_char="5935" id="token-42-17" morph="none" pos="word" start_char="5933">one</TOKEN>
<TOKEN end_char="5938" id="token-42-18" morph="none" pos="word" start_char="5937">of</TOKEN>
<TOKEN end_char="5942" id="token-42-19" morph="none" pos="word" start_char="5940">the</TOKEN>
<TOKEN end_char="5949" id="token-42-20" morph="none" pos="word" start_char="5944">city’s</TOKEN>
<TOKEN end_char="5957" id="token-42-21" morph="none" pos="word" start_char="5951">largest</TOKEN>
<TOKEN end_char="5958" id="token-42-22" morph="none" pos="punct" start_char="5958">.</TOKEN>
</SEG>
<SEG end_char="6096" id="segment-43" start_char="5960">
<ORIGINAL_TEXT>A year later, satellites recorded 285 cars -- a 67% increase, according to the data reviewed by the researchers and shared with ABC News.</ORIGINAL_TEXT>
<TOKEN end_char="5960" id="token-43-0" morph="none" pos="word" start_char="5960">A</TOKEN>
<TOKEN end_char="5965" id="token-43-1" morph="none" pos="word" start_char="5962">year</TOKEN>
<TOKEN end_char="5971" id="token-43-2" morph="none" pos="word" start_char="5967">later</TOKEN>
<TOKEN end_char="5972" id="token-43-3" morph="none" pos="punct" start_char="5972">,</TOKEN>
<TOKEN end_char="5983" id="token-43-4" morph="none" pos="word" start_char="5974">satellites</TOKEN>
<TOKEN end_char="5992" id="token-43-5" morph="none" pos="word" start_char="5985">recorded</TOKEN>
<TOKEN end_char="5996" id="token-43-6" morph="none" pos="word" start_char="5994">285</TOKEN>
<TOKEN end_char="6001" id="token-43-7" morph="none" pos="word" start_char="5998">cars</TOKEN>
<TOKEN end_char="6004" id="token-43-8" morph="none" pos="punct" start_char="6003">--</TOKEN>
<TOKEN end_char="6006" id="token-43-9" morph="none" pos="word" start_char="6006">a</TOKEN>
<TOKEN end_char="6009" id="token-43-10" morph="none" pos="word" start_char="6008">67</TOKEN>
<TOKEN end_char="6010" id="token-43-11" morph="none" pos="punct" start_char="6010">%</TOKEN>
<TOKEN end_char="6019" id="token-43-12" morph="none" pos="word" start_char="6012">increase</TOKEN>
<TOKEN end_char="6020" id="token-43-13" morph="none" pos="punct" start_char="6020">,</TOKEN>
<TOKEN end_char="6030" id="token-43-14" morph="none" pos="word" start_char="6022">according</TOKEN>
<TOKEN end_char="6033" id="token-43-15" morph="none" pos="word" start_char="6032">to</TOKEN>
<TOKEN end_char="6037" id="token-43-16" morph="none" pos="word" start_char="6035">the</TOKEN>
<TOKEN end_char="6042" id="token-43-17" morph="none" pos="word" start_char="6039">data</TOKEN>
<TOKEN end_char="6051" id="token-43-18" morph="none" pos="word" start_char="6044">reviewed</TOKEN>
<TOKEN end_char="6054" id="token-43-19" morph="none" pos="word" start_char="6053">by</TOKEN>
<TOKEN end_char="6058" id="token-43-20" morph="none" pos="word" start_char="6056">the</TOKEN>
<TOKEN end_char="6070" id="token-43-21" morph="none" pos="word" start_char="6060">researchers</TOKEN>
<TOKEN end_char="6074" id="token-43-22" morph="none" pos="word" start_char="6072">and</TOKEN>
<TOKEN end_char="6081" id="token-43-23" morph="none" pos="word" start_char="6076">shared</TOKEN>
<TOKEN end_char="6086" id="token-43-24" morph="none" pos="word" start_char="6083">with</TOKEN>
<TOKEN end_char="6090" id="token-43-25" morph="none" pos="word" start_char="6088">ABC</TOKEN>
<TOKEN end_char="6095" id="token-43-26" morph="none" pos="word" start_char="6092">News</TOKEN>
<TOKEN end_char="6096" id="token-43-27" morph="none" pos="punct" start_char="6096">.</TOKEN>
</SEG>
<SEG end_char="6219" id="segment-44" start_char="6099">
<ORIGINAL_TEXT>Other hospitals showed up to a 90% increase when comparing traffic between fall of 2018 and 2019, according to the study.</ORIGINAL_TEXT>
<TOKEN end_char="6103" id="token-44-0" morph="none" pos="word" start_char="6099">Other</TOKEN>
<TOKEN end_char="6113" id="token-44-1" morph="none" pos="word" start_char="6105">hospitals</TOKEN>
<TOKEN end_char="6120" id="token-44-2" morph="none" pos="word" start_char="6115">showed</TOKEN>
<TOKEN end_char="6123" id="token-44-3" morph="none" pos="word" start_char="6122">up</TOKEN>
<TOKEN end_char="6126" id="token-44-4" morph="none" pos="word" start_char="6125">to</TOKEN>
<TOKEN end_char="6128" id="token-44-5" morph="none" pos="word" start_char="6128">a</TOKEN>
<TOKEN end_char="6131" id="token-44-6" morph="none" pos="word" start_char="6130">90</TOKEN>
<TOKEN end_char="6132" id="token-44-7" morph="none" pos="punct" start_char="6132">%</TOKEN>
<TOKEN end_char="6141" id="token-44-8" morph="none" pos="word" start_char="6134">increase</TOKEN>
<TOKEN end_char="6146" id="token-44-9" morph="none" pos="word" start_char="6143">when</TOKEN>
<TOKEN end_char="6156" id="token-44-10" morph="none" pos="word" start_char="6148">comparing</TOKEN>
<TOKEN end_char="6164" id="token-44-11" morph="none" pos="word" start_char="6158">traffic</TOKEN>
<TOKEN end_char="6172" id="token-44-12" morph="none" pos="word" start_char="6166">between</TOKEN>
<TOKEN end_char="6177" id="token-44-13" morph="none" pos="word" start_char="6174">fall</TOKEN>
<TOKEN end_char="6180" id="token-44-14" morph="none" pos="word" start_char="6179">of</TOKEN>
<TOKEN end_char="6185" id="token-44-15" morph="none" pos="word" start_char="6182">2018</TOKEN>
<TOKEN end_char="6189" id="token-44-16" morph="none" pos="word" start_char="6187">and</TOKEN>
<TOKEN end_char="6194" id="token-44-17" morph="none" pos="word" start_char="6191">2019</TOKEN>
<TOKEN end_char="6195" id="token-44-18" morph="none" pos="punct" start_char="6195">,</TOKEN>
<TOKEN end_char="6205" id="token-44-19" morph="none" pos="word" start_char="6197">according</TOKEN>
<TOKEN end_char="6208" id="token-44-20" morph="none" pos="word" start_char="6207">to</TOKEN>
<TOKEN end_char="6212" id="token-44-21" morph="none" pos="word" start_char="6210">the</TOKEN>
<TOKEN end_char="6218" id="token-44-22" morph="none" pos="word" start_char="6214">study</TOKEN>
<TOKEN end_char="6219" id="token-44-23" morph="none" pos="punct" start_char="6219">.</TOKEN>
</SEG>
<SEG end_char="6330" id="segment-45" start_char="6221">
<ORIGINAL_TEXT>At Wuhan Tongji Medical University, the spike in car traffic was found to have occurred in mid-September 2019.</ORIGINAL_TEXT>
<TOKEN end_char="6222" id="token-45-0" morph="none" pos="word" start_char="6221">At</TOKEN>
<TOKEN end_char="6228" id="token-45-1" morph="none" pos="word" start_char="6224">Wuhan</TOKEN>
<TOKEN end_char="6235" id="token-45-2" morph="none" pos="word" start_char="6230">Tongji</TOKEN>
<TOKEN end_char="6243" id="token-45-3" morph="none" pos="word" start_char="6237">Medical</TOKEN>
<TOKEN end_char="6254" id="token-45-4" morph="none" pos="word" start_char="6245">University</TOKEN>
<TOKEN end_char="6255" id="token-45-5" morph="none" pos="punct" start_char="6255">,</TOKEN>
<TOKEN end_char="6259" id="token-45-6" morph="none" pos="word" start_char="6257">the</TOKEN>
<TOKEN end_char="6265" id="token-45-7" morph="none" pos="word" start_char="6261">spike</TOKEN>
<TOKEN end_char="6268" id="token-45-8" morph="none" pos="word" start_char="6267">in</TOKEN>
<TOKEN end_char="6272" id="token-45-9" morph="none" pos="word" start_char="6270">car</TOKEN>
<TOKEN end_char="6280" id="token-45-10" morph="none" pos="word" start_char="6274">traffic</TOKEN>
<TOKEN end_char="6284" id="token-45-11" morph="none" pos="word" start_char="6282">was</TOKEN>
<TOKEN end_char="6290" id="token-45-12" morph="none" pos="word" start_char="6286">found</TOKEN>
<TOKEN end_char="6293" id="token-45-13" morph="none" pos="word" start_char="6292">to</TOKEN>
<TOKEN end_char="6298" id="token-45-14" morph="none" pos="word" start_char="6295">have</TOKEN>
<TOKEN end_char="6307" id="token-45-15" morph="none" pos="word" start_char="6300">occurred</TOKEN>
<TOKEN end_char="6310" id="token-45-16" morph="none" pos="word" start_char="6309">in</TOKEN>
<TOKEN end_char="6324" id="token-45-17" morph="none" pos="unknown" start_char="6312">mid-September</TOKEN>
<TOKEN end_char="6329" id="token-45-18" morph="none" pos="word" start_char="6326">2019</TOKEN>
<TOKEN end_char="6330" id="token-45-19" morph="none" pos="punct" start_char="6330">.</TOKEN>
</SEG>
<SEG end_char="6563" id="segment-46" start_char="6333">
<ORIGINAL_TEXT>To ensure they were not reaching faulty conclusions, researchers said they took into account everything that could explain away traffic surges -- from large public gatherings to the possibility of new construction at the hospitals.</ORIGINAL_TEXT>
<TOKEN end_char="6334" id="token-46-0" morph="none" pos="word" start_char="6333">To</TOKEN>
<TOKEN end_char="6341" id="token-46-1" morph="none" pos="word" start_char="6336">ensure</TOKEN>
<TOKEN end_char="6346" id="token-46-2" morph="none" pos="word" start_char="6343">they</TOKEN>
<TOKEN end_char="6351" id="token-46-3" morph="none" pos="word" start_char="6348">were</TOKEN>
<TOKEN end_char="6355" id="token-46-4" morph="none" pos="word" start_char="6353">not</TOKEN>
<TOKEN end_char="6364" id="token-46-5" morph="none" pos="word" start_char="6357">reaching</TOKEN>
<TOKEN end_char="6371" id="token-46-6" morph="none" pos="word" start_char="6366">faulty</TOKEN>
<TOKEN end_char="6383" id="token-46-7" morph="none" pos="word" start_char="6373">conclusions</TOKEN>
<TOKEN end_char="6384" id="token-46-8" morph="none" pos="punct" start_char="6384">,</TOKEN>
<TOKEN end_char="6396" id="token-46-9" morph="none" pos="word" start_char="6386">researchers</TOKEN>
<TOKEN end_char="6401" id="token-46-10" morph="none" pos="word" start_char="6398">said</TOKEN>
<TOKEN end_char="6406" id="token-46-11" morph="none" pos="word" start_char="6403">they</TOKEN>
<TOKEN end_char="6411" id="token-46-12" morph="none" pos="word" start_char="6408">took</TOKEN>
<TOKEN end_char="6416" id="token-46-13" morph="none" pos="word" start_char="6413">into</TOKEN>
<TOKEN end_char="6424" id="token-46-14" morph="none" pos="word" start_char="6418">account</TOKEN>
<TOKEN end_char="6435" id="token-46-15" morph="none" pos="word" start_char="6426">everything</TOKEN>
<TOKEN end_char="6440" id="token-46-16" morph="none" pos="word" start_char="6437">that</TOKEN>
<TOKEN end_char="6446" id="token-46-17" morph="none" pos="word" start_char="6442">could</TOKEN>
<TOKEN end_char="6454" id="token-46-18" morph="none" pos="word" start_char="6448">explain</TOKEN>
<TOKEN end_char="6459" id="token-46-19" morph="none" pos="word" start_char="6456">away</TOKEN>
<TOKEN end_char="6467" id="token-46-20" morph="none" pos="word" start_char="6461">traffic</TOKEN>
<TOKEN end_char="6474" id="token-46-21" morph="none" pos="word" start_char="6469">surges</TOKEN>
<TOKEN end_char="6477" id="token-46-22" morph="none" pos="punct" start_char="6476">--</TOKEN>
<TOKEN end_char="6482" id="token-46-23" morph="none" pos="word" start_char="6479">from</TOKEN>
<TOKEN end_char="6488" id="token-46-24" morph="none" pos="word" start_char="6484">large</TOKEN>
<TOKEN end_char="6495" id="token-46-25" morph="none" pos="word" start_char="6490">public</TOKEN>
<TOKEN end_char="6506" id="token-46-26" morph="none" pos="word" start_char="6497">gatherings</TOKEN>
<TOKEN end_char="6509" id="token-46-27" morph="none" pos="word" start_char="6508">to</TOKEN>
<TOKEN end_char="6513" id="token-46-28" morph="none" pos="word" start_char="6511">the</TOKEN>
<TOKEN end_char="6525" id="token-46-29" morph="none" pos="word" start_char="6515">possibility</TOKEN>
<TOKEN end_char="6528" id="token-46-30" morph="none" pos="word" start_char="6527">of</TOKEN>
<TOKEN end_char="6532" id="token-46-31" morph="none" pos="word" start_char="6530">new</TOKEN>
<TOKEN end_char="6545" id="token-46-32" morph="none" pos="word" start_char="6534">construction</TOKEN>
<TOKEN end_char="6548" id="token-46-33" morph="none" pos="word" start_char="6547">at</TOKEN>
<TOKEN end_char="6552" id="token-46-34" morph="none" pos="word" start_char="6550">the</TOKEN>
<TOKEN end_char="6562" id="token-46-35" morph="none" pos="word" start_char="6554">hospitals</TOKEN>
<TOKEN end_char="6563" id="token-46-36" morph="none" pos="punct" start_char="6563">.</TOKEN>
</SEG>
<SEG end_char="6659" id="segment-47" start_char="6565">
<ORIGINAL_TEXT>Still, they said they found statistically significant increases in the numbers of cars present.</ORIGINAL_TEXT>
<TOKEN end_char="6569" id="token-47-0" morph="none" pos="word" start_char="6565">Still</TOKEN>
<TOKEN end_char="6570" id="token-47-1" morph="none" pos="punct" start_char="6570">,</TOKEN>
<TOKEN end_char="6575" id="token-47-2" morph="none" pos="word" start_char="6572">they</TOKEN>
<TOKEN end_char="6580" id="token-47-3" morph="none" pos="word" start_char="6577">said</TOKEN>
<TOKEN end_char="6585" id="token-47-4" morph="none" pos="word" start_char="6582">they</TOKEN>
<TOKEN end_char="6591" id="token-47-5" morph="none" pos="word" start_char="6587">found</TOKEN>
<TOKEN end_char="6605" id="token-47-6" morph="none" pos="word" start_char="6593">statistically</TOKEN>
<TOKEN end_char="6617" id="token-47-7" morph="none" pos="word" start_char="6607">significant</TOKEN>
<TOKEN end_char="6627" id="token-47-8" morph="none" pos="word" start_char="6619">increases</TOKEN>
<TOKEN end_char="6630" id="token-47-9" morph="none" pos="word" start_char="6629">in</TOKEN>
<TOKEN end_char="6634" id="token-47-10" morph="none" pos="word" start_char="6632">the</TOKEN>
<TOKEN end_char="6642" id="token-47-11" morph="none" pos="word" start_char="6636">numbers</TOKEN>
<TOKEN end_char="6645" id="token-47-12" morph="none" pos="word" start_char="6644">of</TOKEN>
<TOKEN end_char="6650" id="token-47-13" morph="none" pos="word" start_char="6647">cars</TOKEN>
<TOKEN end_char="6658" id="token-47-14" morph="none" pos="word" start_char="6652">present</TOKEN>
<TOKEN end_char="6659" id="token-47-15" morph="none" pos="punct" start_char="6659">.</TOKEN>
</SEG>
<SEG end_char="6949" id="segment-48" start_char="6662">
<ORIGINAL_TEXT>"If you look at all of the images, observations we've ever had of all of these locations since 2018, almost all of the highest car counts are all in the September through December 2019 time frame," said Tom Diamond, president of RS Metrics, which worked with the Brownstein research team.</ORIGINAL_TEXT>
<TOKEN end_char="6662" id="token-48-0" morph="none" pos="punct" start_char="6662">"</TOKEN>
<TOKEN end_char="6664" id="token-48-1" morph="none" pos="word" start_char="6663">If</TOKEN>
<TOKEN end_char="6668" id="token-48-2" morph="none" pos="word" start_char="6666">you</TOKEN>
<TOKEN end_char="6673" id="token-48-3" morph="none" pos="word" start_char="6670">look</TOKEN>
<TOKEN end_char="6676" id="token-48-4" morph="none" pos="word" start_char="6675">at</TOKEN>
<TOKEN end_char="6680" id="token-48-5" morph="none" pos="word" start_char="6678">all</TOKEN>
<TOKEN end_char="6683" id="token-48-6" morph="none" pos="word" start_char="6682">of</TOKEN>
<TOKEN end_char="6687" id="token-48-7" morph="none" pos="word" start_char="6685">the</TOKEN>
<TOKEN end_char="6694" id="token-48-8" morph="none" pos="word" start_char="6689">images</TOKEN>
<TOKEN end_char="6695" id="token-48-9" morph="none" pos="punct" start_char="6695">,</TOKEN>
<TOKEN end_char="6708" id="token-48-10" morph="none" pos="word" start_char="6697">observations</TOKEN>
<TOKEN end_char="6714" id="token-48-11" morph="none" pos="word" start_char="6710">we've</TOKEN>
<TOKEN end_char="6719" id="token-48-12" morph="none" pos="word" start_char="6716">ever</TOKEN>
<TOKEN end_char="6723" id="token-48-13" morph="none" pos="word" start_char="6721">had</TOKEN>
<TOKEN end_char="6726" id="token-48-14" morph="none" pos="word" start_char="6725">of</TOKEN>
<TOKEN end_char="6730" id="token-48-15" morph="none" pos="word" start_char="6728">all</TOKEN>
<TOKEN end_char="6733" id="token-48-16" morph="none" pos="word" start_char="6732">of</TOKEN>
<TOKEN end_char="6739" id="token-48-17" morph="none" pos="word" start_char="6735">these</TOKEN>
<TOKEN end_char="6749" id="token-48-18" morph="none" pos="word" start_char="6741">locations</TOKEN>
<TOKEN end_char="6755" id="token-48-19" morph="none" pos="word" start_char="6751">since</TOKEN>
<TOKEN end_char="6760" id="token-48-20" morph="none" pos="word" start_char="6757">2018</TOKEN>
<TOKEN end_char="6761" id="token-48-21" morph="none" pos="punct" start_char="6761">,</TOKEN>
<TOKEN end_char="6768" id="token-48-22" morph="none" pos="word" start_char="6763">almost</TOKEN>
<TOKEN end_char="6772" id="token-48-23" morph="none" pos="word" start_char="6770">all</TOKEN>
<TOKEN end_char="6775" id="token-48-24" morph="none" pos="word" start_char="6774">of</TOKEN>
<TOKEN end_char="6779" id="token-48-25" morph="none" pos="word" start_char="6777">the</TOKEN>
<TOKEN end_char="6787" id="token-48-26" morph="none" pos="word" start_char="6781">highest</TOKEN>
<TOKEN end_char="6791" id="token-48-27" morph="none" pos="word" start_char="6789">car</TOKEN>
<TOKEN end_char="6798" id="token-48-28" morph="none" pos="word" start_char="6793">counts</TOKEN>
<TOKEN end_char="6802" id="token-48-29" morph="none" pos="word" start_char="6800">are</TOKEN>
<TOKEN end_char="6806" id="token-48-30" morph="none" pos="word" start_char="6804">all</TOKEN>
<TOKEN end_char="6809" id="token-48-31" morph="none" pos="word" start_char="6808">in</TOKEN>
<TOKEN end_char="6813" id="token-48-32" morph="none" pos="word" start_char="6811">the</TOKEN>
<TOKEN end_char="6823" id="token-48-33" morph="none" pos="word" start_char="6815">September</TOKEN>
<TOKEN end_char="6831" id="token-48-34" morph="none" pos="word" start_char="6825">through</TOKEN>
<TOKEN end_char="6840" id="token-48-35" morph="none" pos="word" start_char="6833">December</TOKEN>
<TOKEN end_char="6845" id="token-48-36" morph="none" pos="word" start_char="6842">2019</TOKEN>
<TOKEN end_char="6850" id="token-48-37" morph="none" pos="word" start_char="6847">time</TOKEN>
<TOKEN end_char="6856" id="token-48-38" morph="none" pos="word" start_char="6852">frame</TOKEN>
<TOKEN end_char="6858" id="token-48-39" morph="none" pos="punct" start_char="6857">,"</TOKEN>
<TOKEN end_char="6863" id="token-48-40" morph="none" pos="word" start_char="6860">said</TOKEN>
<TOKEN end_char="6867" id="token-48-41" morph="none" pos="word" start_char="6865">Tom</TOKEN>
<TOKEN end_char="6875" id="token-48-42" morph="none" pos="word" start_char="6869">Diamond</TOKEN>
<TOKEN end_char="6876" id="token-48-43" morph="none" pos="punct" start_char="6876">,</TOKEN>
<TOKEN end_char="6886" id="token-48-44" morph="none" pos="word" start_char="6878">president</TOKEN>
<TOKEN end_char="6889" id="token-48-45" morph="none" pos="word" start_char="6888">of</TOKEN>
<TOKEN end_char="6892" id="token-48-46" morph="none" pos="word" start_char="6891">RS</TOKEN>
<TOKEN end_char="6900" id="token-48-47" morph="none" pos="word" start_char="6894">Metrics</TOKEN>
<TOKEN end_char="6901" id="token-48-48" morph="none" pos="punct" start_char="6901">,</TOKEN>
<TOKEN end_char="6907" id="token-48-49" morph="none" pos="word" start_char="6903">which</TOKEN>
<TOKEN end_char="6914" id="token-48-50" morph="none" pos="word" start_char="6909">worked</TOKEN>
<TOKEN end_char="6919" id="token-48-51" morph="none" pos="word" start_char="6916">with</TOKEN>
<TOKEN end_char="6923" id="token-48-52" morph="none" pos="word" start_char="6921">the</TOKEN>
<TOKEN end_char="6934" id="token-48-53" morph="none" pos="word" start_char="6925">Brownstein</TOKEN>
<TOKEN end_char="6943" id="token-48-54" morph="none" pos="word" start_char="6936">research</TOKEN>
<TOKEN end_char="6948" id="token-48-55" morph="none" pos="word" start_char="6945">team</TOKEN>
<TOKEN end_char="6949" id="token-48-56" morph="none" pos="punct" start_char="6949">.</TOKEN>
</SEG>
<SEG end_char="7361" id="segment-49" start_char="6952">
<ORIGINAL_TEXT>As an initial "validation" of their methodology of extrapolating information about movement through the review of satellite images, researchers said they compared parking lot activity at the Huanan Seafood Market in mid-September, when the market was busy, and after the market was shut down by authorities after reports emerged that the wet market may have been ground zero for the novel coronavirus outbreak.</ORIGINAL_TEXT>
<TOKEN end_char="6953" id="token-49-0" morph="none" pos="word" start_char="6952">As</TOKEN>
<TOKEN end_char="6956" id="token-49-1" morph="none" pos="word" start_char="6955">an</TOKEN>
<TOKEN end_char="6964" id="token-49-2" morph="none" pos="word" start_char="6958">initial</TOKEN>
<TOKEN end_char="6966" id="token-49-3" morph="none" pos="punct" start_char="6966">"</TOKEN>
<TOKEN end_char="6976" id="token-49-4" morph="none" pos="word" start_char="6967">validation</TOKEN>
<TOKEN end_char="6977" id="token-49-5" morph="none" pos="punct" start_char="6977">"</TOKEN>
<TOKEN end_char="6980" id="token-49-6" morph="none" pos="word" start_char="6979">of</TOKEN>
<TOKEN end_char="6986" id="token-49-7" morph="none" pos="word" start_char="6982">their</TOKEN>
<TOKEN end_char="6998" id="token-49-8" morph="none" pos="word" start_char="6988">methodology</TOKEN>
<TOKEN end_char="7001" id="token-49-9" morph="none" pos="word" start_char="7000">of</TOKEN>
<TOKEN end_char="7015" id="token-49-10" morph="none" pos="word" start_char="7003">extrapolating</TOKEN>
<TOKEN end_char="7027" id="token-49-11" morph="none" pos="word" start_char="7017">information</TOKEN>
<TOKEN end_char="7033" id="token-49-12" morph="none" pos="word" start_char="7029">about</TOKEN>
<TOKEN end_char="7042" id="token-49-13" morph="none" pos="word" start_char="7035">movement</TOKEN>
<TOKEN end_char="7050" id="token-49-14" morph="none" pos="word" start_char="7044">through</TOKEN>
<TOKEN end_char="7054" id="token-49-15" morph="none" pos="word" start_char="7052">the</TOKEN>
<TOKEN end_char="7061" id="token-49-16" morph="none" pos="word" start_char="7056">review</TOKEN>
<TOKEN end_char="7064" id="token-49-17" morph="none" pos="word" start_char="7063">of</TOKEN>
<TOKEN end_char="7074" id="token-49-18" morph="none" pos="word" start_char="7066">satellite</TOKEN>
<TOKEN end_char="7081" id="token-49-19" morph="none" pos="word" start_char="7076">images</TOKEN>
<TOKEN end_char="7082" id="token-49-20" morph="none" pos="punct" start_char="7082">,</TOKEN>
<TOKEN end_char="7094" id="token-49-21" morph="none" pos="word" start_char="7084">researchers</TOKEN>
<TOKEN end_char="7099" id="token-49-22" morph="none" pos="word" start_char="7096">said</TOKEN>
<TOKEN end_char="7104" id="token-49-23" morph="none" pos="word" start_char="7101">they</TOKEN>
<TOKEN end_char="7113" id="token-49-24" morph="none" pos="word" start_char="7106">compared</TOKEN>
<TOKEN end_char="7121" id="token-49-25" morph="none" pos="word" start_char="7115">parking</TOKEN>
<TOKEN end_char="7125" id="token-49-26" morph="none" pos="word" start_char="7123">lot</TOKEN>
<TOKEN end_char="7134" id="token-49-27" morph="none" pos="word" start_char="7127">activity</TOKEN>
<TOKEN end_char="7137" id="token-49-28" morph="none" pos="word" start_char="7136">at</TOKEN>
<TOKEN end_char="7141" id="token-49-29" morph="none" pos="word" start_char="7139">the</TOKEN>
<TOKEN end_char="7148" id="token-49-30" morph="none" pos="word" start_char="7143">Huanan</TOKEN>
<TOKEN end_char="7156" id="token-49-31" morph="none" pos="word" start_char="7150">Seafood</TOKEN>
<TOKEN end_char="7163" id="token-49-32" morph="none" pos="word" start_char="7158">Market</TOKEN>
<TOKEN end_char="7166" id="token-49-33" morph="none" pos="word" start_char="7165">in</TOKEN>
<TOKEN end_char="7180" id="token-49-34" morph="none" pos="unknown" start_char="7168">mid-September</TOKEN>
<TOKEN end_char="7181" id="token-49-35" morph="none" pos="punct" start_char="7181">,</TOKEN>
<TOKEN end_char="7186" id="token-49-36" morph="none" pos="word" start_char="7183">when</TOKEN>
<TOKEN end_char="7190" id="token-49-37" morph="none" pos="word" start_char="7188">the</TOKEN>
<TOKEN end_char="7197" id="token-49-38" morph="none" pos="word" start_char="7192">market</TOKEN>
<TOKEN end_char="7201" id="token-49-39" morph="none" pos="word" start_char="7199">was</TOKEN>
<TOKEN end_char="7206" id="token-49-40" morph="none" pos="word" start_char="7203">busy</TOKEN>
<TOKEN end_char="7207" id="token-49-41" morph="none" pos="punct" start_char="7207">,</TOKEN>
<TOKEN end_char="7211" id="token-49-42" morph="none" pos="word" start_char="7209">and</TOKEN>
<TOKEN end_char="7217" id="token-49-43" morph="none" pos="word" start_char="7213">after</TOKEN>
<TOKEN end_char="7221" id="token-49-44" morph="none" pos="word" start_char="7219">the</TOKEN>
<TOKEN end_char="7228" id="token-49-45" morph="none" pos="word" start_char="7223">market</TOKEN>
<TOKEN end_char="7232" id="token-49-46" morph="none" pos="word" start_char="7230">was</TOKEN>
<TOKEN end_char="7237" id="token-49-47" morph="none" pos="word" start_char="7234">shut</TOKEN>
<TOKEN end_char="7242" id="token-49-48" morph="none" pos="word" start_char="7239">down</TOKEN>
<TOKEN end_char="7245" id="token-49-49" morph="none" pos="word" start_char="7244">by</TOKEN>
<TOKEN end_char="7257" id="token-49-50" morph="none" pos="word" start_char="7247">authorities</TOKEN>
<TOKEN end_char="7263" id="token-49-51" morph="none" pos="word" start_char="7259">after</TOKEN>
<TOKEN end_char="7271" id="token-49-52" morph="none" pos="word" start_char="7265">reports</TOKEN>
<TOKEN end_char="7279" id="token-49-53" morph="none" pos="word" start_char="7273">emerged</TOKEN>
<TOKEN end_char="7284" id="token-49-54" morph="none" pos="word" start_char="7281">that</TOKEN>
<TOKEN end_char="7288" id="token-49-55" morph="none" pos="word" start_char="7286">the</TOKEN>
<TOKEN end_char="7292" id="token-49-56" morph="none" pos="word" start_char="7290">wet</TOKEN>
<TOKEN end_char="7299" id="token-49-57" morph="none" pos="word" start_char="7294">market</TOKEN>
<TOKEN end_char="7303" id="token-49-58" morph="none" pos="word" start_char="7301">may</TOKEN>
<TOKEN end_char="7308" id="token-49-59" morph="none" pos="word" start_char="7305">have</TOKEN>
<TOKEN end_char="7313" id="token-49-60" morph="none" pos="word" start_char="7310">been</TOKEN>
<TOKEN end_char="7320" id="token-49-61" morph="none" pos="word" start_char="7315">ground</TOKEN>
<TOKEN end_char="7325" id="token-49-62" morph="none" pos="word" start_char="7322">zero</TOKEN>
<TOKEN end_char="7329" id="token-49-63" morph="none" pos="word" start_char="7327">for</TOKEN>
<TOKEN end_char="7333" id="token-49-64" morph="none" pos="word" start_char="7331">the</TOKEN>
<TOKEN end_char="7339" id="token-49-65" morph="none" pos="word" start_char="7335">novel</TOKEN>
<TOKEN end_char="7351" id="token-49-66" morph="none" pos="word" start_char="7341">coronavirus</TOKEN>
<TOKEN end_char="7360" id="token-49-67" morph="none" pos="word" start_char="7353">outbreak</TOKEN>
<TOKEN end_char="7361" id="token-49-68" morph="none" pos="punct" start_char="7361">.</TOKEN>
</SEG>
<SEG end_char="7399" id="segment-50" start_char="7363">
<ORIGINAL_TEXT>They said they found a marked change.</ORIGINAL_TEXT>
<TOKEN end_char="7366" id="token-50-0" morph="none" pos="word" start_char="7363">They</TOKEN>
<TOKEN end_char="7371" id="token-50-1" morph="none" pos="word" start_char="7368">said</TOKEN>
<TOKEN end_char="7376" id="token-50-2" morph="none" pos="word" start_char="7373">they</TOKEN>
<TOKEN end_char="7382" id="token-50-3" morph="none" pos="word" start_char="7378">found</TOKEN>
<TOKEN end_char="7384" id="token-50-4" morph="none" pos="word" start_char="7384">a</TOKEN>
<TOKEN end_char="7391" id="token-50-5" morph="none" pos="word" start_char="7386">marked</TOKEN>
<TOKEN end_char="7398" id="token-50-6" morph="none" pos="word" start_char="7393">change</TOKEN>
<TOKEN end_char="7399" id="token-50-7" morph="none" pos="punct" start_char="7399">.</TOKEN>
</SEG>
<SEG end_char="7534" id="segment-51" start_char="7401">
<ORIGINAL_TEXT>"The images validate the concept that activity and movement is shown through the lens of these sort of parking lots," said Brownstein.</ORIGINAL_TEXT>
<TOKEN end_char="7401" id="token-51-0" morph="none" pos="punct" start_char="7401">"</TOKEN>
<TOKEN end_char="7404" id="token-51-1" morph="none" pos="word" start_char="7402">The</TOKEN>
<TOKEN end_char="7411" id="token-51-2" morph="none" pos="word" start_char="7406">images</TOKEN>
<TOKEN end_char="7420" id="token-51-3" morph="none" pos="word" start_char="7413">validate</TOKEN>
<TOKEN end_char="7424" id="token-51-4" morph="none" pos="word" start_char="7422">the</TOKEN>
<TOKEN end_char="7432" id="token-51-5" morph="none" pos="word" start_char="7426">concept</TOKEN>
<TOKEN end_char="7437" id="token-51-6" morph="none" pos="word" start_char="7434">that</TOKEN>
<TOKEN end_char="7446" id="token-51-7" morph="none" pos="word" start_char="7439">activity</TOKEN>
<TOKEN end_char="7450" id="token-51-8" morph="none" pos="word" start_char="7448">and</TOKEN>
<TOKEN end_char="7459" id="token-51-9" morph="none" pos="word" start_char="7452">movement</TOKEN>
<TOKEN end_char="7462" id="token-51-10" morph="none" pos="word" start_char="7461">is</TOKEN>
<TOKEN end_char="7468" id="token-51-11" morph="none" pos="word" start_char="7464">shown</TOKEN>
<TOKEN end_char="7476" id="token-51-12" morph="none" pos="word" start_char="7470">through</TOKEN>
<TOKEN end_char="7480" id="token-51-13" morph="none" pos="word" start_char="7478">the</TOKEN>
<TOKEN end_char="7485" id="token-51-14" morph="none" pos="word" start_char="7482">lens</TOKEN>
<TOKEN end_char="7488" id="token-51-15" morph="none" pos="word" start_char="7487">of</TOKEN>
<TOKEN end_char="7494" id="token-51-16" morph="none" pos="word" start_char="7490">these</TOKEN>
<TOKEN end_char="7499" id="token-51-17" morph="none" pos="word" start_char="7496">sort</TOKEN>
<TOKEN end_char="7502" id="token-51-18" morph="none" pos="word" start_char="7501">of</TOKEN>
<TOKEN end_char="7510" id="token-51-19" morph="none" pos="word" start_char="7504">parking</TOKEN>
<TOKEN end_char="7515" id="token-51-20" morph="none" pos="word" start_char="7512">lots</TOKEN>
<TOKEN end_char="7517" id="token-51-21" morph="none" pos="punct" start_char="7516">,"</TOKEN>
<TOKEN end_char="7522" id="token-51-22" morph="none" pos="word" start_char="7519">said</TOKEN>
<TOKEN end_char="7533" id="token-51-23" morph="none" pos="word" start_char="7524">Brownstein</TOKEN>
<TOKEN end_char="7534" id="token-51-24" morph="none" pos="punct" start_char="7534">.</TOKEN>
</SEG>
<SEG end_char="7629" id="segment-52" start_char="7537">
<ORIGINAL_TEXT>The study has been submitted to the journal Nature Digital Medicine and is under peer review.</ORIGINAL_TEXT>
<TOKEN end_char="7539" id="token-52-0" morph="none" pos="word" start_char="7537">The</TOKEN>
<TOKEN end_char="7545" id="token-52-1" morph="none" pos="word" start_char="7541">study</TOKEN>
<TOKEN end_char="7549" id="token-52-2" morph="none" pos="word" start_char="7547">has</TOKEN>
<TOKEN end_char="7554" id="token-52-3" morph="none" pos="word" start_char="7551">been</TOKEN>
<TOKEN end_char="7564" id="token-52-4" morph="none" pos="word" start_char="7556">submitted</TOKEN>
<TOKEN end_char="7567" id="token-52-5" morph="none" pos="word" start_char="7566">to</TOKEN>
<TOKEN end_char="7571" id="token-52-6" morph="none" pos="word" start_char="7569">the</TOKEN>
<TOKEN end_char="7579" id="token-52-7" morph="none" pos="word" start_char="7573">journal</TOKEN>
<TOKEN end_char="7586" id="token-52-8" morph="none" pos="word" start_char="7581">Nature</TOKEN>
<TOKEN end_char="7594" id="token-52-9" morph="none" pos="word" start_char="7588">Digital</TOKEN>
<TOKEN end_char="7603" id="token-52-10" morph="none" pos="word" start_char="7596">Medicine</TOKEN>
<TOKEN end_char="7607" id="token-52-11" morph="none" pos="word" start_char="7605">and</TOKEN>
<TOKEN end_char="7610" id="token-52-12" morph="none" pos="word" start_char="7609">is</TOKEN>
<TOKEN end_char="7616" id="token-52-13" morph="none" pos="word" start_char="7612">under</TOKEN>
<TOKEN end_char="7621" id="token-52-14" morph="none" pos="word" start_char="7618">peer</TOKEN>
<TOKEN end_char="7628" id="token-52-15" morph="none" pos="word" start_char="7623">review</TOKEN>
<TOKEN end_char="7629" id="token-52-16" morph="none" pos="punct" start_char="7629">.</TOKEN>
</SEG>
<SEG end_char="7730" id="segment-53" start_char="7631">
<ORIGINAL_TEXT>It is scheduled to be posted Monday morning on "Dash," Harvard’s preprint server for medical papers.</ORIGINAL_TEXT>
<TOKEN end_char="7632" id="token-53-0" morph="none" pos="word" start_char="7631">It</TOKEN>
<TOKEN end_char="7635" id="token-53-1" morph="none" pos="word" start_char="7634">is</TOKEN>
<TOKEN end_char="7645" id="token-53-2" morph="none" pos="word" start_char="7637">scheduled</TOKEN>
<TOKEN end_char="7648" id="token-53-3" morph="none" pos="word" start_char="7647">to</TOKEN>
<TOKEN end_char="7651" id="token-53-4" morph="none" pos="word" start_char="7650">be</TOKEN>
<TOKEN end_char="7658" id="token-53-5" morph="none" pos="word" start_char="7653">posted</TOKEN>
<TOKEN end_char="7665" id="token-53-6" morph="none" pos="word" start_char="7660">Monday</TOKEN>
<TOKEN end_char="7673" id="token-53-7" morph="none" pos="word" start_char="7667">morning</TOKEN>
<TOKEN end_char="7676" id="token-53-8" morph="none" pos="word" start_char="7675">on</TOKEN>
<TOKEN end_char="7678" id="token-53-9" morph="none" pos="punct" start_char="7678">"</TOKEN>
<TOKEN end_char="7682" id="token-53-10" morph="none" pos="word" start_char="7679">Dash</TOKEN>
<TOKEN end_char="7684" id="token-53-11" morph="none" pos="punct" start_char="7683">,"</TOKEN>
<TOKEN end_char="7694" id="token-53-12" morph="none" pos="word" start_char="7686">Harvard’s</TOKEN>
<TOKEN end_char="7703" id="token-53-13" morph="none" pos="word" start_char="7696">preprint</TOKEN>
<TOKEN end_char="7710" id="token-53-14" morph="none" pos="word" start_char="7705">server</TOKEN>
<TOKEN end_char="7714" id="token-53-15" morph="none" pos="word" start_char="7712">for</TOKEN>
<TOKEN end_char="7722" id="token-53-16" morph="none" pos="word" start_char="7716">medical</TOKEN>
<TOKEN end_char="7729" id="token-53-17" morph="none" pos="word" start_char="7724">papers</TOKEN>
<TOKEN end_char="7730" id="token-53-18" morph="none" pos="punct" start_char="7730">.</TOKEN>
</SEG>
<SEG end_char="7801" id="segment-54" start_char="7733">
<ORIGINAL_TEXT>On Monday morning the website for "Dash" suffered a temporary outage.</ORIGINAL_TEXT>
<TOKEN end_char="7734" id="token-54-0" morph="none" pos="word" start_char="7733">On</TOKEN>
<TOKEN end_char="7741" id="token-54-1" morph="none" pos="word" start_char="7736">Monday</TOKEN>
<TOKEN end_char="7749" id="token-54-2" morph="none" pos="word" start_char="7743">morning</TOKEN>
<TOKEN end_char="7753" id="token-54-3" morph="none" pos="word" start_char="7751">the</TOKEN>
<TOKEN end_char="7761" id="token-54-4" morph="none" pos="word" start_char="7755">website</TOKEN>
<TOKEN end_char="7765" id="token-54-5" morph="none" pos="word" start_char="7763">for</TOKEN>
<TOKEN end_char="7767" id="token-54-6" morph="none" pos="punct" start_char="7767">"</TOKEN>
<TOKEN end_char="7771" id="token-54-7" morph="none" pos="word" start_char="7768">Dash</TOKEN>
<TOKEN end_char="7772" id="token-54-8" morph="none" pos="punct" start_char="7772">"</TOKEN>
<TOKEN end_char="7781" id="token-54-9" morph="none" pos="word" start_char="7774">suffered</TOKEN>
<TOKEN end_char="7783" id="token-54-10" morph="none" pos="word" start_char="7783">a</TOKEN>
<TOKEN end_char="7793" id="token-54-11" morph="none" pos="word" start_char="7785">temporary</TOKEN>
<TOKEN end_char="7800" id="token-54-12" morph="none" pos="word" start_char="7795">outage</TOKEN>
<TOKEN end_char="7801" id="token-54-13" morph="none" pos="punct" start_char="7801">.</TOKEN>
</SEG>
<SEG end_char="7895" id="segment-55" start_char="7803">
<ORIGINAL_TEXT>A spokesperson for Harvard Medical School told ABC News they were investigating the incident.</ORIGINAL_TEXT>
<TOKEN end_char="7803" id="token-55-0" morph="none" pos="word" start_char="7803">A</TOKEN>
<TOKEN end_char="7816" id="token-55-1" morph="none" pos="word" start_char="7805">spokesperson</TOKEN>
<TOKEN end_char="7820" id="token-55-2" morph="none" pos="word" start_char="7818">for</TOKEN>
<TOKEN end_char="7828" id="token-55-3" morph="none" pos="word" start_char="7822">Harvard</TOKEN>
<TOKEN end_char="7836" id="token-55-4" morph="none" pos="word" start_char="7830">Medical</TOKEN>
<TOKEN end_char="7843" id="token-55-5" morph="none" pos="word" start_char="7838">School</TOKEN>
<TOKEN end_char="7848" id="token-55-6" morph="none" pos="word" start_char="7845">told</TOKEN>
<TOKEN end_char="7852" id="token-55-7" morph="none" pos="word" start_char="7850">ABC</TOKEN>
<TOKEN end_char="7857" id="token-55-8" morph="none" pos="word" start_char="7854">News</TOKEN>
<TOKEN end_char="7862" id="token-55-9" morph="none" pos="word" start_char="7859">they</TOKEN>
<TOKEN end_char="7867" id="token-55-10" morph="none" pos="word" start_char="7864">were</TOKEN>
<TOKEN end_char="7881" id="token-55-11" morph="none" pos="word" start_char="7869">investigating</TOKEN>
<TOKEN end_char="7885" id="token-55-12" morph="none" pos="word" start_char="7883">the</TOKEN>
<TOKEN end_char="7894" id="token-55-13" morph="none" pos="word" start_char="7887">incident</TOKEN>
<TOKEN end_char="7895" id="token-55-14" morph="none" pos="punct" start_char="7895">.</TOKEN>
</SEG>
<SEG end_char="8119" id="segment-56" start_char="7898">
<ORIGINAL_TEXT>In conducting the project, RS Metrics, an intelligence-analysis firm that analyzes satellite imagery for corporate clients, employed techniques designed to identify and monitor changes in the patterns of life and business.</ORIGINAL_TEXT>
<TOKEN end_char="7899" id="token-56-0" morph="none" pos="word" start_char="7898">In</TOKEN>
<TOKEN end_char="7910" id="token-56-1" morph="none" pos="word" start_char="7901">conducting</TOKEN>
<TOKEN end_char="7914" id="token-56-2" morph="none" pos="word" start_char="7912">the</TOKEN>
<TOKEN end_char="7922" id="token-56-3" morph="none" pos="word" start_char="7916">project</TOKEN>
<TOKEN end_char="7923" id="token-56-4" morph="none" pos="punct" start_char="7923">,</TOKEN>
<TOKEN end_char="7926" id="token-56-5" morph="none" pos="word" start_char="7925">RS</TOKEN>
<TOKEN end_char="7934" id="token-56-6" morph="none" pos="word" start_char="7928">Metrics</TOKEN>
<TOKEN end_char="7935" id="token-56-7" morph="none" pos="punct" start_char="7935">,</TOKEN>
<TOKEN end_char="7938" id="token-56-8" morph="none" pos="word" start_char="7937">an</TOKEN>
<TOKEN end_char="7960" id="token-56-9" morph="none" pos="unknown" start_char="7940">intelligence-analysis</TOKEN>
<TOKEN end_char="7965" id="token-56-10" morph="none" pos="word" start_char="7962">firm</TOKEN>
<TOKEN end_char="7970" id="token-56-11" morph="none" pos="word" start_char="7967">that</TOKEN>
<TOKEN end_char="7979" id="token-56-12" morph="none" pos="word" start_char="7972">analyzes</TOKEN>
<TOKEN end_char="7989" id="token-56-13" morph="none" pos="word" start_char="7981">satellite</TOKEN>
<TOKEN end_char="7997" id="token-56-14" morph="none" pos="word" start_char="7991">imagery</TOKEN>
<TOKEN end_char="8001" id="token-56-15" morph="none" pos="word" start_char="7999">for</TOKEN>
<TOKEN end_char="8011" id="token-56-16" morph="none" pos="word" start_char="8003">corporate</TOKEN>
<TOKEN end_char="8019" id="token-56-17" morph="none" pos="word" start_char="8013">clients</TOKEN>
<TOKEN end_char="8020" id="token-56-18" morph="none" pos="punct" start_char="8020">,</TOKEN>
<TOKEN end_char="8029" id="token-56-19" morph="none" pos="word" start_char="8022">employed</TOKEN>
<TOKEN end_char="8040" id="token-56-20" morph="none" pos="word" start_char="8031">techniques</TOKEN>
<TOKEN end_char="8049" id="token-56-21" morph="none" pos="word" start_char="8042">designed</TOKEN>
<TOKEN end_char="8052" id="token-56-22" morph="none" pos="word" start_char="8051">to</TOKEN>
<TOKEN end_char="8061" id="token-56-23" morph="none" pos="word" start_char="8054">identify</TOKEN>
<TOKEN end_char="8065" id="token-56-24" morph="none" pos="word" start_char="8063">and</TOKEN>
<TOKEN end_char="8073" id="token-56-25" morph="none" pos="word" start_char="8067">monitor</TOKEN>
<TOKEN end_char="8081" id="token-56-26" morph="none" pos="word" start_char="8075">changes</TOKEN>
<TOKEN end_char="8084" id="token-56-27" morph="none" pos="word" start_char="8083">in</TOKEN>
<TOKEN end_char="8088" id="token-56-28" morph="none" pos="word" start_char="8086">the</TOKEN>
<TOKEN end_char="8097" id="token-56-29" morph="none" pos="word" start_char="8090">patterns</TOKEN>
<TOKEN end_char="8100" id="token-56-30" morph="none" pos="word" start_char="8099">of</TOKEN>
<TOKEN end_char="8105" id="token-56-31" morph="none" pos="word" start_char="8102">life</TOKEN>
<TOKEN end_char="8109" id="token-56-32" morph="none" pos="word" start_char="8107">and</TOKEN>
<TOKEN end_char="8118" id="token-56-33" morph="none" pos="word" start_char="8111">business</TOKEN>
<TOKEN end_char="8119" id="token-56-34" morph="none" pos="punct" start_char="8119">.</TOKEN>
</SEG>
<SEG end_char="8392" id="segment-57" start_char="8122">
<ORIGINAL_TEXT>It’s similar to work done by analysts at the Central Intelligence Agency and the Defense Intelligence Agency, who pore over images each day to try to figure out what is happening on the ground – especially in places where governments restrict the flow of people and news.</ORIGINAL_TEXT>
<TOKEN end_char="8125" id="token-57-0" morph="none" pos="word" start_char="8122">It’s</TOKEN>
<TOKEN end_char="8133" id="token-57-1" morph="none" pos="word" start_char="8127">similar</TOKEN>
<TOKEN end_char="8136" id="token-57-2" morph="none" pos="word" start_char="8135">to</TOKEN>
<TOKEN end_char="8141" id="token-57-3" morph="none" pos="word" start_char="8138">work</TOKEN>
<TOKEN end_char="8146" id="token-57-4" morph="none" pos="word" start_char="8143">done</TOKEN>
<TOKEN end_char="8149" id="token-57-5" morph="none" pos="word" start_char="8148">by</TOKEN>
<TOKEN end_char="8158" id="token-57-6" morph="none" pos="word" start_char="8151">analysts</TOKEN>
<TOKEN end_char="8161" id="token-57-7" morph="none" pos="word" start_char="8160">at</TOKEN>
<TOKEN end_char="8165" id="token-57-8" morph="none" pos="word" start_char="8163">the</TOKEN>
<TOKEN end_char="8173" id="token-57-9" morph="none" pos="word" start_char="8167">Central</TOKEN>
<TOKEN end_char="8186" id="token-57-10" morph="none" pos="word" start_char="8175">Intelligence</TOKEN>
<TOKEN end_char="8193" id="token-57-11" morph="none" pos="word" start_char="8188">Agency</TOKEN>
<TOKEN end_char="8197" id="token-57-12" morph="none" pos="word" start_char="8195">and</TOKEN>
<TOKEN end_char="8201" id="token-57-13" morph="none" pos="word" start_char="8199">the</TOKEN>
<TOKEN end_char="8209" id="token-57-14" morph="none" pos="word" start_char="8203">Defense</TOKEN>
<TOKEN end_char="8222" id="token-57-15" morph="none" pos="word" start_char="8211">Intelligence</TOKEN>
<TOKEN end_char="8229" id="token-57-16" morph="none" pos="word" start_char="8224">Agency</TOKEN>
<TOKEN end_char="8230" id="token-57-17" morph="none" pos="punct" start_char="8230">,</TOKEN>
<TOKEN end_char="8234" id="token-57-18" morph="none" pos="word" start_char="8232">who</TOKEN>
<TOKEN end_char="8239" id="token-57-19" morph="none" pos="word" start_char="8236">pore</TOKEN>
<TOKEN end_char="8244" id="token-57-20" morph="none" pos="word" start_char="8241">over</TOKEN>
<TOKEN end_char="8251" id="token-57-21" morph="none" pos="word" start_char="8246">images</TOKEN>
<TOKEN end_char="8256" id="token-57-22" morph="none" pos="word" start_char="8253">each</TOKEN>
<TOKEN end_char="8260" id="token-57-23" morph="none" pos="word" start_char="8258">day</TOKEN>
<TOKEN end_char="8263" id="token-57-24" morph="none" pos="word" start_char="8262">to</TOKEN>
<TOKEN end_char="8267" id="token-57-25" morph="none" pos="word" start_char="8265">try</TOKEN>
<TOKEN end_char="8270" id="token-57-26" morph="none" pos="word" start_char="8269">to</TOKEN>
<TOKEN end_char="8277" id="token-57-27" morph="none" pos="word" start_char="8272">figure</TOKEN>
<TOKEN end_char="8281" id="token-57-28" morph="none" pos="word" start_char="8279">out</TOKEN>
<TOKEN end_char="8286" id="token-57-29" morph="none" pos="word" start_char="8283">what</TOKEN>
<TOKEN end_char="8289" id="token-57-30" morph="none" pos="word" start_char="8288">is</TOKEN>
<TOKEN end_char="8299" id="token-57-31" morph="none" pos="word" start_char="8291">happening</TOKEN>
<TOKEN end_char="8302" id="token-57-32" morph="none" pos="word" start_char="8301">on</TOKEN>
<TOKEN end_char="8306" id="token-57-33" morph="none" pos="word" start_char="8304">the</TOKEN>
<TOKEN end_char="8313" id="token-57-34" morph="none" pos="word" start_char="8308">ground</TOKEN>
<TOKEN end_char="8315" id="token-57-35" morph="none" pos="punct" start_char="8315">–</TOKEN>
<TOKEN end_char="8326" id="token-57-36" morph="none" pos="word" start_char="8317">especially</TOKEN>
<TOKEN end_char="8329" id="token-57-37" morph="none" pos="word" start_char="8328">in</TOKEN>
<TOKEN end_char="8336" id="token-57-38" morph="none" pos="word" start_char="8331">places</TOKEN>
<TOKEN end_char="8342" id="token-57-39" morph="none" pos="word" start_char="8338">where</TOKEN>
<TOKEN end_char="8354" id="token-57-40" morph="none" pos="word" start_char="8344">governments</TOKEN>
<TOKEN end_char="8363" id="token-57-41" morph="none" pos="word" start_char="8356">restrict</TOKEN>
<TOKEN end_char="8367" id="token-57-42" morph="none" pos="word" start_char="8365">the</TOKEN>
<TOKEN end_char="8372" id="token-57-43" morph="none" pos="word" start_char="8369">flow</TOKEN>
<TOKEN end_char="8375" id="token-57-44" morph="none" pos="word" start_char="8374">of</TOKEN>
<TOKEN end_char="8382" id="token-57-45" morph="none" pos="word" start_char="8377">people</TOKEN>
<TOKEN end_char="8386" id="token-57-46" morph="none" pos="word" start_char="8384">and</TOKEN>
<TOKEN end_char="8391" id="token-57-47" morph="none" pos="word" start_char="8388">news</TOKEN>
<TOKEN end_char="8392" id="token-57-48" morph="none" pos="punct" start_char="8392">.</TOKEN>
</SEG>
<SEG end_char="8613" id="segment-58" start_char="8395">
<ORIGINAL_TEXT>Diamond told ABC News the Wuhan region was clearly experiencing a widespread health problem in the months before China’s government acknowledged publicly that a contagion was coursing through the densely populated city.</ORIGINAL_TEXT>
<TOKEN end_char="8401" id="token-58-0" morph="none" pos="word" start_char="8395">Diamond</TOKEN>
<TOKEN end_char="8406" id="token-58-1" morph="none" pos="word" start_char="8403">told</TOKEN>
<TOKEN end_char="8410" id="token-58-2" morph="none" pos="word" start_char="8408">ABC</TOKEN>
<TOKEN end_char="8415" id="token-58-3" morph="none" pos="word" start_char="8412">News</TOKEN>
<TOKEN end_char="8419" id="token-58-4" morph="none" pos="word" start_char="8417">the</TOKEN>
<TOKEN end_char="8425" id="token-58-5" morph="none" pos="word" start_char="8421">Wuhan</TOKEN>
<TOKEN end_char="8432" id="token-58-6" morph="none" pos="word" start_char="8427">region</TOKEN>
<TOKEN end_char="8436" id="token-58-7" morph="none" pos="word" start_char="8434">was</TOKEN>
<TOKEN end_char="8444" id="token-58-8" morph="none" pos="word" start_char="8438">clearly</TOKEN>
<TOKEN end_char="8457" id="token-58-9" morph="none" pos="word" start_char="8446">experiencing</TOKEN>
<TOKEN end_char="8459" id="token-58-10" morph="none" pos="word" start_char="8459">a</TOKEN>
<TOKEN end_char="8470" id="token-58-11" morph="none" pos="word" start_char="8461">widespread</TOKEN>
<TOKEN end_char="8477" id="token-58-12" morph="none" pos="word" start_char="8472">health</TOKEN>
<TOKEN end_char="8485" id="token-58-13" morph="none" pos="word" start_char="8479">problem</TOKEN>
<TOKEN end_char="8488" id="token-58-14" morph="none" pos="word" start_char="8487">in</TOKEN>
<TOKEN end_char="8492" id="token-58-15" morph="none" pos="word" start_char="8490">the</TOKEN>
<TOKEN end_char="8499" id="token-58-16" morph="none" pos="word" start_char="8494">months</TOKEN>
<TOKEN end_char="8506" id="token-58-17" morph="none" pos="word" start_char="8501">before</TOKEN>
<TOKEN end_char="8514" id="token-58-18" morph="none" pos="word" start_char="8508">China’s</TOKEN>
<TOKEN end_char="8525" id="token-58-19" morph="none" pos="word" start_char="8516">government</TOKEN>
<TOKEN end_char="8538" id="token-58-20" morph="none" pos="word" start_char="8527">acknowledged</TOKEN>
<TOKEN end_char="8547" id="token-58-21" morph="none" pos="word" start_char="8540">publicly</TOKEN>
<TOKEN end_char="8552" id="token-58-22" morph="none" pos="word" start_char="8549">that</TOKEN>
<TOKEN end_char="8554" id="token-58-23" morph="none" pos="word" start_char="8554">a</TOKEN>
<TOKEN end_char="8564" id="token-58-24" morph="none" pos="word" start_char="8556">contagion</TOKEN>
<TOKEN end_char="8568" id="token-58-25" morph="none" pos="word" start_char="8566">was</TOKEN>
<TOKEN end_char="8577" id="token-58-26" morph="none" pos="word" start_char="8570">coursing</TOKEN>
<TOKEN end_char="8585" id="token-58-27" morph="none" pos="word" start_char="8579">through</TOKEN>
<TOKEN end_char="8589" id="token-58-28" morph="none" pos="word" start_char="8587">the</TOKEN>
<TOKEN end_char="8597" id="token-58-29" morph="none" pos="word" start_char="8591">densely</TOKEN>
<TOKEN end_char="8607" id="token-58-30" morph="none" pos="word" start_char="8599">populated</TOKEN>
<TOKEN end_char="8612" id="token-58-31" morph="none" pos="word" start_char="8609">city</TOKEN>
<TOKEN end_char="8613" id="token-58-32" morph="none" pos="punct" start_char="8613">.</TOKEN>
</SEG>
<SEG end_char="8757" id="segment-59" start_char="8615">
<ORIGINAL_TEXT>That announcement came on New Year’s Eve when the Wuhan Municipal Health Commission, China reported a "cluster" of pneumonia cases in its city.</ORIGINAL_TEXT>
<TOKEN end_char="8618" id="token-59-0" morph="none" pos="word" start_char="8615">That</TOKEN>
<TOKEN end_char="8631" id="token-59-1" morph="none" pos="word" start_char="8620">announcement</TOKEN>
<TOKEN end_char="8636" id="token-59-2" morph="none" pos="word" start_char="8633">came</TOKEN>
<TOKEN end_char="8639" id="token-59-3" morph="none" pos="word" start_char="8638">on</TOKEN>
<TOKEN end_char="8643" id="token-59-4" morph="none" pos="word" start_char="8641">New</TOKEN>
<TOKEN end_char="8650" id="token-59-5" morph="none" pos="word" start_char="8645">Year’s</TOKEN>
<TOKEN end_char="8654" id="token-59-6" morph="none" pos="word" start_char="8652">Eve</TOKEN>
<TOKEN end_char="8659" id="token-59-7" morph="none" pos="word" start_char="8656">when</TOKEN>
<TOKEN end_char="8663" id="token-59-8" morph="none" pos="word" start_char="8661">the</TOKEN>
<TOKEN end_char="8669" id="token-59-9" morph="none" pos="word" start_char="8665">Wuhan</TOKEN>
<TOKEN end_char="8679" id="token-59-10" morph="none" pos="word" start_char="8671">Municipal</TOKEN>
<TOKEN end_char="8686" id="token-59-11" morph="none" pos="word" start_char="8681">Health</TOKEN>
<TOKEN end_char="8697" id="token-59-12" morph="none" pos="word" start_char="8688">Commission</TOKEN>
<TOKEN end_char="8698" id="token-59-13" morph="none" pos="punct" start_char="8698">,</TOKEN>
<TOKEN end_char="8704" id="token-59-14" morph="none" pos="word" start_char="8700">China</TOKEN>
<TOKEN end_char="8713" id="token-59-15" morph="none" pos="word" start_char="8706">reported</TOKEN>
<TOKEN end_char="8715" id="token-59-16" morph="none" pos="word" start_char="8715">a</TOKEN>
<TOKEN end_char="8717" id="token-59-17" morph="none" pos="punct" start_char="8717">"</TOKEN>
<TOKEN end_char="8724" id="token-59-18" morph="none" pos="word" start_char="8718">cluster</TOKEN>
<TOKEN end_char="8725" id="token-59-19" morph="none" pos="punct" start_char="8725">"</TOKEN>
<TOKEN end_char="8728" id="token-59-20" morph="none" pos="word" start_char="8727">of</TOKEN>
<TOKEN end_char="8738" id="token-59-21" morph="none" pos="word" start_char="8730">pneumonia</TOKEN>
<TOKEN end_char="8744" id="token-59-22" morph="none" pos="word" start_char="8740">cases</TOKEN>
<TOKEN end_char="8747" id="token-59-23" morph="none" pos="word" start_char="8746">in</TOKEN>
<TOKEN end_char="8751" id="token-59-24" morph="none" pos="word" start_char="8749">its</TOKEN>
<TOKEN end_char="8756" id="token-59-25" morph="none" pos="word" start_char="8753">city</TOKEN>
<TOKEN end_char="8757" id="token-59-26" morph="none" pos="punct" start_char="8757">.</TOKEN>
</SEG>
<SEG end_char="8928" id="segment-60" start_char="8760">
<ORIGINAL_TEXT>"At all the larger hospitals in Wuhan, we measured the highest traffic we’ve seen in over two years during the September through December 2019 time frame," Diamond said.</ORIGINAL_TEXT>
<TOKEN end_char="8760" id="token-60-0" morph="none" pos="punct" start_char="8760">"</TOKEN>
<TOKEN end_char="8762" id="token-60-1" morph="none" pos="word" start_char="8761">At</TOKEN>
<TOKEN end_char="8766" id="token-60-2" morph="none" pos="word" start_char="8764">all</TOKEN>
<TOKEN end_char="8770" id="token-60-3" morph="none" pos="word" start_char="8768">the</TOKEN>
<TOKEN end_char="8777" id="token-60-4" morph="none" pos="word" start_char="8772">larger</TOKEN>
<TOKEN end_char="8787" id="token-60-5" morph="none" pos="word" start_char="8779">hospitals</TOKEN>
<TOKEN end_char="8790" id="token-60-6" morph="none" pos="word" start_char="8789">in</TOKEN>
<TOKEN end_char="8796" id="token-60-7" morph="none" pos="word" start_char="8792">Wuhan</TOKEN>
<TOKEN end_char="8797" id="token-60-8" morph="none" pos="punct" start_char="8797">,</TOKEN>
<TOKEN end_char="8800" id="token-60-9" morph="none" pos="word" start_char="8799">we</TOKEN>
<TOKEN end_char="8809" id="token-60-10" morph="none" pos="word" start_char="8802">measured</TOKEN>
<TOKEN end_char="8813" id="token-60-11" morph="none" pos="word" start_char="8811">the</TOKEN>
<TOKEN end_char="8821" id="token-60-12" morph="none" pos="word" start_char="8815">highest</TOKEN>
<TOKEN end_char="8829" id="token-60-13" morph="none" pos="word" start_char="8823">traffic</TOKEN>
<TOKEN end_char="8835" id="token-60-14" morph="none" pos="word" start_char="8831">we’ve</TOKEN>
<TOKEN end_char="8840" id="token-60-15" morph="none" pos="word" start_char="8837">seen</TOKEN>
<TOKEN end_char="8843" id="token-60-16" morph="none" pos="word" start_char="8842">in</TOKEN>
<TOKEN end_char="8848" id="token-60-17" morph="none" pos="word" start_char="8845">over</TOKEN>
<TOKEN end_char="8852" id="token-60-18" morph="none" pos="word" start_char="8850">two</TOKEN>
<TOKEN end_char="8858" id="token-60-19" morph="none" pos="word" start_char="8854">years</TOKEN>
<TOKEN end_char="8865" id="token-60-20" morph="none" pos="word" start_char="8860">during</TOKEN>
<TOKEN end_char="8869" id="token-60-21" morph="none" pos="word" start_char="8867">the</TOKEN>
<TOKEN end_char="8879" id="token-60-22" morph="none" pos="word" start_char="8871">September</TOKEN>
<TOKEN end_char="8887" id="token-60-23" morph="none" pos="word" start_char="8881">through</TOKEN>
<TOKEN end_char="8896" id="token-60-24" morph="none" pos="word" start_char="8889">December</TOKEN>
<TOKEN end_char="8901" id="token-60-25" morph="none" pos="word" start_char="8898">2019</TOKEN>
<TOKEN end_char="8906" id="token-60-26" morph="none" pos="word" start_char="8903">time</TOKEN>
<TOKEN end_char="8912" id="token-60-27" morph="none" pos="word" start_char="8908">frame</TOKEN>
<TOKEN end_char="8914" id="token-60-28" morph="none" pos="punct" start_char="8913">,"</TOKEN>
<TOKEN end_char="8922" id="token-60-29" morph="none" pos="word" start_char="8916">Diamond</TOKEN>
<TOKEN end_char="8927" id="token-60-30" morph="none" pos="word" start_char="8924">said</TOKEN>
<TOKEN end_char="8928" id="token-60-31" morph="none" pos="punct" start_char="8928">.</TOKEN>
</SEG>
<SEG end_char="9037" id="segment-61" start_char="8930">
<ORIGINAL_TEXT>"Our company is used to measuring tiny changes, like 2% to 3% growth in a Cabella’s or Wal-Mart parking lot.</ORIGINAL_TEXT>
<TOKEN end_char="8930" id="token-61-0" morph="none" pos="punct" start_char="8930">"</TOKEN>
<TOKEN end_char="8933" id="token-61-1" morph="none" pos="word" start_char="8931">Our</TOKEN>
<TOKEN end_char="8941" id="token-61-2" morph="none" pos="word" start_char="8935">company</TOKEN>
<TOKEN end_char="8944" id="token-61-3" morph="none" pos="word" start_char="8943">is</TOKEN>
<TOKEN end_char="8949" id="token-61-4" morph="none" pos="word" start_char="8946">used</TOKEN>
<TOKEN end_char="8952" id="token-61-5" morph="none" pos="word" start_char="8951">to</TOKEN>
<TOKEN end_char="8962" id="token-61-6" morph="none" pos="word" start_char="8954">measuring</TOKEN>
<TOKEN end_char="8967" id="token-61-7" morph="none" pos="word" start_char="8964">tiny</TOKEN>
<TOKEN end_char="8975" id="token-61-8" morph="none" pos="word" start_char="8969">changes</TOKEN>
<TOKEN end_char="8976" id="token-61-9" morph="none" pos="punct" start_char="8976">,</TOKEN>
<TOKEN end_char="8981" id="token-61-10" morph="none" pos="word" start_char="8978">like</TOKEN>
<TOKEN end_char="8983" id="token-61-11" morph="none" pos="word" start_char="8983">2</TOKEN>
<TOKEN end_char="8984" id="token-61-12" morph="none" pos="punct" start_char="8984">%</TOKEN>
<TOKEN end_char="8987" id="token-61-13" morph="none" pos="word" start_char="8986">to</TOKEN>
<TOKEN end_char="8989" id="token-61-14" morph="none" pos="word" start_char="8989">3</TOKEN>
<TOKEN end_char="8990" id="token-61-15" morph="none" pos="punct" start_char="8990">%</TOKEN>
<TOKEN end_char="8997" id="token-61-16" morph="none" pos="word" start_char="8992">growth</TOKEN>
<TOKEN end_char="9000" id="token-61-17" morph="none" pos="word" start_char="8999">in</TOKEN>
<TOKEN end_char="9002" id="token-61-18" morph="none" pos="word" start_char="9002">a</TOKEN>
<TOKEN end_char="9012" id="token-61-19" morph="none" pos="word" start_char="9004">Cabella’s</TOKEN>
<TOKEN end_char="9015" id="token-61-20" morph="none" pos="word" start_char="9014">or</TOKEN>
<TOKEN end_char="9024" id="token-61-21" morph="none" pos="unknown" start_char="9017">Wal-Mart</TOKEN>
<TOKEN end_char="9032" id="token-61-22" morph="none" pos="word" start_char="9026">parking</TOKEN>
<TOKEN end_char="9036" id="token-61-23" morph="none" pos="word" start_char="9034">lot</TOKEN>
<TOKEN end_char="9037" id="token-61-24" morph="none" pos="punct" start_char="9037">.</TOKEN>
</SEG>
<SEG end_char="9065" id="segment-62" start_char="9039">
<ORIGINAL_TEXT>That was not the case here.</ORIGINAL_TEXT>
<TOKEN end_char="9042" id="token-62-0" morph="none" pos="word" start_char="9039">That</TOKEN>
<TOKEN end_char="9046" id="token-62-1" morph="none" pos="word" start_char="9044">was</TOKEN>
<TOKEN end_char="9050" id="token-62-2" morph="none" pos="word" start_char="9048">not</TOKEN>
<TOKEN end_char="9054" id="token-62-3" morph="none" pos="word" start_char="9052">the</TOKEN>
<TOKEN end_char="9059" id="token-62-4" morph="none" pos="word" start_char="9056">case</TOKEN>
<TOKEN end_char="9064" id="token-62-5" morph="none" pos="word" start_char="9061">here</TOKEN>
<TOKEN end_char="9065" id="token-62-6" morph="none" pos="punct" start_char="9065">.</TOKEN>
</SEG>
<SEG end_char="9101" id="segment-63" start_char="9067">
<ORIGINAL_TEXT>Here, there is a very clear trend."</ORIGINAL_TEXT>
<TOKEN end_char="9070" id="token-63-0" morph="none" pos="word" start_char="9067">Here</TOKEN>
<TOKEN end_char="9071" id="token-63-1" morph="none" pos="punct" start_char="9071">,</TOKEN>
<TOKEN end_char="9077" id="token-63-2" morph="none" pos="word" start_char="9073">there</TOKEN>
<TOKEN end_char="9080" id="token-63-3" morph="none" pos="word" start_char="9079">is</TOKEN>
<TOKEN end_char="9082" id="token-63-4" morph="none" pos="word" start_char="9082">a</TOKEN>
<TOKEN end_char="9087" id="token-63-5" morph="none" pos="word" start_char="9084">very</TOKEN>
<TOKEN end_char="9093" id="token-63-6" morph="none" pos="word" start_char="9089">clear</TOKEN>
<TOKEN end_char="9099" id="token-63-7" morph="none" pos="word" start_char="9095">trend</TOKEN>
<TOKEN end_char="9101" id="token-63-8" morph="none" pos="punct" start_char="9100">."</TOKEN>
</SEG>
<SEG end_char="9418" id="segment-64" start_char="9104">
<ORIGINAL_TEXT>Former acting Homeland Security Undersecretary John Cohen, who oversaw DHS intelligence operations during the Obama administration, said the new research suggests that COVID-19, which has already killed more than 110,000 Americans, was likely brought to the U.S. by travelers from Wuhan long before it was detected.</ORIGINAL_TEXT>
<TOKEN end_char="9109" id="token-64-0" morph="none" pos="word" start_char="9104">Former</TOKEN>
<TOKEN end_char="9116" id="token-64-1" morph="none" pos="word" start_char="9111">acting</TOKEN>
<TOKEN end_char="9125" id="token-64-2" morph="none" pos="word" start_char="9118">Homeland</TOKEN>
<TOKEN end_char="9134" id="token-64-3" morph="none" pos="word" start_char="9127">Security</TOKEN>
<TOKEN end_char="9149" id="token-64-4" morph="none" pos="word" start_char="9136">Undersecretary</TOKEN>
<TOKEN end_char="9154" id="token-64-5" morph="none" pos="word" start_char="9151">John</TOKEN>
<TOKEN end_char="9160" id="token-64-6" morph="none" pos="word" start_char="9156">Cohen</TOKEN>
<TOKEN end_char="9161" id="token-64-7" morph="none" pos="punct" start_char="9161">,</TOKEN>
<TOKEN end_char="9165" id="token-64-8" morph="none" pos="word" start_char="9163">who</TOKEN>
<TOKEN end_char="9173" id="token-64-9" morph="none" pos="word" start_char="9167">oversaw</TOKEN>
<TOKEN end_char="9177" id="token-64-10" morph="none" pos="word" start_char="9175">DHS</TOKEN>
<TOKEN end_char="9190" id="token-64-11" morph="none" pos="word" start_char="9179">intelligence</TOKEN>
<TOKEN end_char="9201" id="token-64-12" morph="none" pos="word" start_char="9192">operations</TOKEN>
<TOKEN end_char="9208" id="token-64-13" morph="none" pos="word" start_char="9203">during</TOKEN>
<TOKEN end_char="9212" id="token-64-14" morph="none" pos="word" start_char="9210">the</TOKEN>
<TOKEN end_char="9218" id="token-64-15" morph="none" pos="word" start_char="9214">Obama</TOKEN>
<TOKEN end_char="9233" id="token-64-16" morph="none" pos="word" start_char="9220">administration</TOKEN>
<TOKEN end_char="9234" id="token-64-17" morph="none" pos="punct" start_char="9234">,</TOKEN>
<TOKEN end_char="9239" id="token-64-18" morph="none" pos="word" start_char="9236">said</TOKEN>
<TOKEN end_char="9243" id="token-64-19" morph="none" pos="word" start_char="9241">the</TOKEN>
<TOKEN end_char="9247" id="token-64-20" morph="none" pos="word" start_char="9245">new</TOKEN>
<TOKEN end_char="9256" id="token-64-21" morph="none" pos="word" start_char="9249">research</TOKEN>
<TOKEN end_char="9265" id="token-64-22" morph="none" pos="word" start_char="9258">suggests</TOKEN>
<TOKEN end_char="9270" id="token-64-23" morph="none" pos="word" start_char="9267">that</TOKEN>
<TOKEN end_char="9279" id="token-64-24" morph="none" pos="unknown" start_char="9272">COVID-19</TOKEN>
<TOKEN end_char="9280" id="token-64-25" morph="none" pos="punct" start_char="9280">,</TOKEN>
<TOKEN end_char="9286" id="token-64-26" morph="none" pos="word" start_char="9282">which</TOKEN>
<TOKEN end_char="9290" id="token-64-27" morph="none" pos="word" start_char="9288">has</TOKEN>
<TOKEN end_char="9298" id="token-64-28" morph="none" pos="word" start_char="9292">already</TOKEN>
<TOKEN end_char="9305" id="token-64-29" morph="none" pos="word" start_char="9300">killed</TOKEN>
<TOKEN end_char="9310" id="token-64-30" morph="none" pos="word" start_char="9307">more</TOKEN>
<TOKEN end_char="9315" id="token-64-31" morph="none" pos="word" start_char="9312">than</TOKEN>
<TOKEN end_char="9323" id="token-64-32" morph="none" pos="unknown" start_char="9317">110,000</TOKEN>
<TOKEN end_char="9333" id="token-64-33" morph="none" pos="word" start_char="9325">Americans</TOKEN>
<TOKEN end_char="9334" id="token-64-34" morph="none" pos="punct" start_char="9334">,</TOKEN>
<TOKEN end_char="9338" id="token-64-35" morph="none" pos="word" start_char="9336">was</TOKEN>
<TOKEN end_char="9345" id="token-64-36" morph="none" pos="word" start_char="9340">likely</TOKEN>
<TOKEN end_char="9353" id="token-64-37" morph="none" pos="word" start_char="9347">brought</TOKEN>
<TOKEN end_char="9356" id="token-64-38" morph="none" pos="word" start_char="9355">to</TOKEN>
<TOKEN end_char="9360" id="token-64-39" morph="none" pos="word" start_char="9358">the</TOKEN>
<TOKEN end_char="9364" id="token-64-40" morph="none" pos="unknown" start_char="9362">U.S</TOKEN>
<TOKEN end_char="9365" id="token-64-41" morph="none" pos="punct" start_char="9365">.</TOKEN>
<TOKEN end_char="9368" id="token-64-42" morph="none" pos="word" start_char="9367">by</TOKEN>
<TOKEN end_char="9378" id="token-64-43" morph="none" pos="word" start_char="9370">travelers</TOKEN>
<TOKEN end_char="9383" id="token-64-44" morph="none" pos="word" start_char="9380">from</TOKEN>
<TOKEN end_char="9389" id="token-64-45" morph="none" pos="word" start_char="9385">Wuhan</TOKEN>
<TOKEN end_char="9394" id="token-64-46" morph="none" pos="word" start_char="9391">long</TOKEN>
<TOKEN end_char="9401" id="token-64-47" morph="none" pos="word" start_char="9396">before</TOKEN>
<TOKEN end_char="9404" id="token-64-48" morph="none" pos="word" start_char="9403">it</TOKEN>
<TOKEN end_char="9408" id="token-64-49" morph="none" pos="word" start_char="9406">was</TOKEN>
<TOKEN end_char="9417" id="token-64-50" morph="none" pos="word" start_char="9410">detected</TOKEN>
<TOKEN end_char="9418" id="token-64-51" morph="none" pos="punct" start_char="9418">.</TOKEN>
</SEG>
<SEG end_char="9708" id="segment-65" start_char="9421">
<ORIGINAL_TEXT>"This study raises serious questions about whether the coronavirus was first introduced into the United States earlier than previously reported and whether measures announced in late January restricting travel from China were too little too late," said Cohen, now an ABC News contributor.</ORIGINAL_TEXT>
<TOKEN end_char="9421" id="token-65-0" morph="none" pos="punct" start_char="9421">"</TOKEN>
<TOKEN end_char="9425" id="token-65-1" morph="none" pos="word" start_char="9422">This</TOKEN>
<TOKEN end_char="9431" id="token-65-2" morph="none" pos="word" start_char="9427">study</TOKEN>
<TOKEN end_char="9438" id="token-65-3" morph="none" pos="word" start_char="9433">raises</TOKEN>
<TOKEN end_char="9446" id="token-65-4" morph="none" pos="word" start_char="9440">serious</TOKEN>
<TOKEN end_char="9456" id="token-65-5" morph="none" pos="word" start_char="9448">questions</TOKEN>
<TOKEN end_char="9462" id="token-65-6" morph="none" pos="word" start_char="9458">about</TOKEN>
<TOKEN end_char="9470" id="token-65-7" morph="none" pos="word" start_char="9464">whether</TOKEN>
<TOKEN end_char="9474" id="token-65-8" morph="none" pos="word" start_char="9472">the</TOKEN>
<TOKEN end_char="9486" id="token-65-9" morph="none" pos="word" start_char="9476">coronavirus</TOKEN>
<TOKEN end_char="9490" id="token-65-10" morph="none" pos="word" start_char="9488">was</TOKEN>
<TOKEN end_char="9496" id="token-65-11" morph="none" pos="word" start_char="9492">first</TOKEN>
<TOKEN end_char="9507" id="token-65-12" morph="none" pos="word" start_char="9498">introduced</TOKEN>
<TOKEN end_char="9512" id="token-65-13" morph="none" pos="word" start_char="9509">into</TOKEN>
<TOKEN end_char="9516" id="token-65-14" morph="none" pos="word" start_char="9514">the</TOKEN>
<TOKEN end_char="9523" id="token-65-15" morph="none" pos="word" start_char="9518">United</TOKEN>
<TOKEN end_char="9530" id="token-65-16" morph="none" pos="word" start_char="9525">States</TOKEN>
<TOKEN end_char="9538" id="token-65-17" morph="none" pos="word" start_char="9532">earlier</TOKEN>
<TOKEN end_char="9543" id="token-65-18" morph="none" pos="word" start_char="9540">than</TOKEN>
<TOKEN end_char="9554" id="token-65-19" morph="none" pos="word" start_char="9545">previously</TOKEN>
<TOKEN end_char="9563" id="token-65-20" morph="none" pos="word" start_char="9556">reported</TOKEN>
<TOKEN end_char="9567" id="token-65-21" morph="none" pos="word" start_char="9565">and</TOKEN>
<TOKEN end_char="9575" id="token-65-22" morph="none" pos="word" start_char="9569">whether</TOKEN>
<TOKEN end_char="9584" id="token-65-23" morph="none" pos="word" start_char="9577">measures</TOKEN>
<TOKEN end_char="9594" id="token-65-24" morph="none" pos="word" start_char="9586">announced</TOKEN>
<TOKEN end_char="9597" id="token-65-25" morph="none" pos="word" start_char="9596">in</TOKEN>
<TOKEN end_char="9602" id="token-65-26" morph="none" pos="word" start_char="9599">late</TOKEN>
<TOKEN end_char="9610" id="token-65-27" morph="none" pos="word" start_char="9604">January</TOKEN>
<TOKEN end_char="9622" id="token-65-28" morph="none" pos="word" start_char="9612">restricting</TOKEN>
<TOKEN end_char="9629" id="token-65-29" morph="none" pos="word" start_char="9624">travel</TOKEN>
<TOKEN end_char="9634" id="token-65-30" morph="none" pos="word" start_char="9631">from</TOKEN>
<TOKEN end_char="9640" id="token-65-31" morph="none" pos="word" start_char="9636">China</TOKEN>
<TOKEN end_char="9645" id="token-65-32" morph="none" pos="word" start_char="9642">were</TOKEN>
<TOKEN end_char="9649" id="token-65-33" morph="none" pos="word" start_char="9647">too</TOKEN>
<TOKEN end_char="9656" id="token-65-34" morph="none" pos="word" start_char="9651">little</TOKEN>
<TOKEN end_char="9660" id="token-65-35" morph="none" pos="word" start_char="9658">too</TOKEN>
<TOKEN end_char="9665" id="token-65-36" morph="none" pos="word" start_char="9662">late</TOKEN>
<TOKEN end_char="9667" id="token-65-37" morph="none" pos="punct" start_char="9666">,"</TOKEN>
<TOKEN end_char="9672" id="token-65-38" morph="none" pos="word" start_char="9669">said</TOKEN>
<TOKEN end_char="9678" id="token-65-39" morph="none" pos="word" start_char="9674">Cohen</TOKEN>
<TOKEN end_char="9679" id="token-65-40" morph="none" pos="punct" start_char="9679">,</TOKEN>
<TOKEN end_char="9683" id="token-65-41" morph="none" pos="word" start_char="9681">now</TOKEN>
<TOKEN end_char="9686" id="token-65-42" morph="none" pos="word" start_char="9685">an</TOKEN>
<TOKEN end_char="9690" id="token-65-43" morph="none" pos="word" start_char="9688">ABC</TOKEN>
<TOKEN end_char="9695" id="token-65-44" morph="none" pos="word" start_char="9692">News</TOKEN>
<TOKEN end_char="9707" id="token-65-45" morph="none" pos="word" start_char="9697">contributor</TOKEN>
<TOKEN end_char="9708" id="token-65-46" morph="none" pos="punct" start_char="9708">.</TOKEN>
</SEG>
<SEG end_char="9844" id="segment-66" start_char="9711">
<ORIGINAL_TEXT>Satellite images suggesting a change in life patterns in Wuhan were also a key factor in classified early U.S. intelligence reporting.</ORIGINAL_TEXT>
<TOKEN end_char="9719" id="token-66-0" morph="none" pos="word" start_char="9711">Satellite</TOKEN>
<TOKEN end_char="9726" id="token-66-1" morph="none" pos="word" start_char="9721">images</TOKEN>
<TOKEN end_char="9737" id="token-66-2" morph="none" pos="word" start_char="9728">suggesting</TOKEN>
<TOKEN end_char="9739" id="token-66-3" morph="none" pos="word" start_char="9739">a</TOKEN>
<TOKEN end_char="9746" id="token-66-4" morph="none" pos="word" start_char="9741">change</TOKEN>
<TOKEN end_char="9749" id="token-66-5" morph="none" pos="word" start_char="9748">in</TOKEN>
<TOKEN end_char="9754" id="token-66-6" morph="none" pos="word" start_char="9751">life</TOKEN>
<TOKEN end_char="9763" id="token-66-7" morph="none" pos="word" start_char="9756">patterns</TOKEN>
<TOKEN end_char="9766" id="token-66-8" morph="none" pos="word" start_char="9765">in</TOKEN>
<TOKEN end_char="9772" id="token-66-9" morph="none" pos="word" start_char="9768">Wuhan</TOKEN>
<TOKEN end_char="9777" id="token-66-10" morph="none" pos="word" start_char="9774">were</TOKEN>
<TOKEN end_char="9782" id="token-66-11" morph="none" pos="word" start_char="9779">also</TOKEN>
<TOKEN end_char="9784" id="token-66-12" morph="none" pos="word" start_char="9784">a</TOKEN>
<TOKEN end_char="9788" id="token-66-13" morph="none" pos="word" start_char="9786">key</TOKEN>
<TOKEN end_char="9795" id="token-66-14" morph="none" pos="word" start_char="9790">factor</TOKEN>
<TOKEN end_char="9798" id="token-66-15" morph="none" pos="word" start_char="9797">in</TOKEN>
<TOKEN end_char="9809" id="token-66-16" morph="none" pos="word" start_char="9800">classified</TOKEN>
<TOKEN end_char="9815" id="token-66-17" morph="none" pos="word" start_char="9811">early</TOKEN>
<TOKEN end_char="9819" id="token-66-18" morph="none" pos="unknown" start_char="9817">U.S</TOKEN>
<TOKEN end_char="9820" id="token-66-19" morph="none" pos="punct" start_char="9820">.</TOKEN>
<TOKEN end_char="9833" id="token-66-20" morph="none" pos="word" start_char="9822">intelligence</TOKEN>
<TOKEN end_char="9843" id="token-66-21" morph="none" pos="word" start_char="9835">reporting</TOKEN>
<TOKEN end_char="9844" id="token-66-22" morph="none" pos="punct" start_char="9844">.</TOKEN>
</SEG>
<SEG end_char="10088" id="segment-67" start_char="9847">
<ORIGINAL_TEXT>In April, ABC News reported that the National Center for Medical Intelligence (NCMI) received word in late November that a contagion was sweeping through Wuhan, changing the patterns of life and business and posing a threat to the population.</ORIGINAL_TEXT>
<TOKEN end_char="9848" id="token-67-0" morph="none" pos="word" start_char="9847">In</TOKEN>
<TOKEN end_char="9854" id="token-67-1" morph="none" pos="word" start_char="9850">April</TOKEN>
<TOKEN end_char="9855" id="token-67-2" morph="none" pos="punct" start_char="9855">,</TOKEN>
<TOKEN end_char="9859" id="token-67-3" morph="none" pos="word" start_char="9857">ABC</TOKEN>
<TOKEN end_char="9864" id="token-67-4" morph="none" pos="word" start_char="9861">News</TOKEN>
<TOKEN end_char="9873" id="token-67-5" morph="none" pos="word" start_char="9866">reported</TOKEN>
<TOKEN end_char="9878" id="token-67-6" morph="none" pos="word" start_char="9875">that</TOKEN>
<TOKEN end_char="9882" id="token-67-7" morph="none" pos="word" start_char="9880">the</TOKEN>
<TOKEN end_char="9891" id="token-67-8" morph="none" pos="word" start_char="9884">National</TOKEN>
<TOKEN end_char="9898" id="token-67-9" morph="none" pos="word" start_char="9893">Center</TOKEN>
<TOKEN end_char="9902" id="token-67-10" morph="none" pos="word" start_char="9900">for</TOKEN>
<TOKEN end_char="9910" id="token-67-11" morph="none" pos="word" start_char="9904">Medical</TOKEN>
<TOKEN end_char="9923" id="token-67-12" morph="none" pos="word" start_char="9912">Intelligence</TOKEN>
<TOKEN end_char="9925" id="token-67-13" morph="none" pos="punct" start_char="9925">(</TOKEN>
<TOKEN end_char="9929" id="token-67-14" morph="none" pos="word" start_char="9926">NCMI</TOKEN>
<TOKEN end_char="9930" id="token-67-15" morph="none" pos="punct" start_char="9930">)</TOKEN>
<TOKEN end_char="9939" id="token-67-16" morph="none" pos="word" start_char="9932">received</TOKEN>
<TOKEN end_char="9944" id="token-67-17" morph="none" pos="word" start_char="9941">word</TOKEN>
<TOKEN end_char="9947" id="token-67-18" morph="none" pos="word" start_char="9946">in</TOKEN>
<TOKEN end_char="9952" id="token-67-19" morph="none" pos="word" start_char="9949">late</TOKEN>
<TOKEN end_char="9961" id="token-67-20" morph="none" pos="word" start_char="9954">November</TOKEN>
<TOKEN end_char="9966" id="token-67-21" morph="none" pos="word" start_char="9963">that</TOKEN>
<TOKEN end_char="9968" id="token-67-22" morph="none" pos="word" start_char="9968">a</TOKEN>
<TOKEN end_char="9978" id="token-67-23" morph="none" pos="word" start_char="9970">contagion</TOKEN>
<TOKEN end_char="9982" id="token-67-24" morph="none" pos="word" start_char="9980">was</TOKEN>
<TOKEN end_char="9991" id="token-67-25" morph="none" pos="word" start_char="9984">sweeping</TOKEN>
<TOKEN end_char="9999" id="token-67-26" morph="none" pos="word" start_char="9993">through</TOKEN>
<TOKEN end_char="10005" id="token-67-27" morph="none" pos="word" start_char="10001">Wuhan</TOKEN>
<TOKEN end_char="10006" id="token-67-28" morph="none" pos="punct" start_char="10006">,</TOKEN>
<TOKEN end_char="10015" id="token-67-29" morph="none" pos="word" start_char="10008">changing</TOKEN>
<TOKEN end_char="10019" id="token-67-30" morph="none" pos="word" start_char="10017">the</TOKEN>
<TOKEN end_char="10028" id="token-67-31" morph="none" pos="word" start_char="10021">patterns</TOKEN>
<TOKEN end_char="10031" id="token-67-32" morph="none" pos="word" start_char="10030">of</TOKEN>
<TOKEN end_char="10036" id="token-67-33" morph="none" pos="word" start_char="10033">life</TOKEN>
<TOKEN end_char="10040" id="token-67-34" morph="none" pos="word" start_char="10038">and</TOKEN>
<TOKEN end_char="10049" id="token-67-35" morph="none" pos="word" start_char="10042">business</TOKEN>
<TOKEN end_char="10053" id="token-67-36" morph="none" pos="word" start_char="10051">and</TOKEN>
<TOKEN end_char="10060" id="token-67-37" morph="none" pos="word" start_char="10055">posing</TOKEN>
<TOKEN end_char="10062" id="token-67-38" morph="none" pos="word" start_char="10062">a</TOKEN>
<TOKEN end_char="10069" id="token-67-39" morph="none" pos="word" start_char="10064">threat</TOKEN>
<TOKEN end_char="10072" id="token-67-40" morph="none" pos="word" start_char="10071">to</TOKEN>
<TOKEN end_char="10076" id="token-67-41" morph="none" pos="word" start_char="10074">the</TOKEN>
<TOKEN end_char="10087" id="token-67-42" morph="none" pos="word" start_char="10078">population</TOKEN>
<TOKEN end_char="10088" id="token-67-43" morph="none" pos="punct" start_char="10088">.</TOKEN>
</SEG>
<SEG end_char="10317" id="segment-68" start_char="10090">
<ORIGINAL_TEXT>Sources familiar with the reports said NCMI, a component of the military’s Defense Intelligence Agency, based the analysis on wire and computer intercepts coupled with satellite images similar to those used by Brownstein’s team.</ORIGINAL_TEXT>
<TOKEN end_char="10096" id="token-68-0" morph="none" pos="word" start_char="10090">Sources</TOKEN>
<TOKEN end_char="10105" id="token-68-1" morph="none" pos="word" start_char="10098">familiar</TOKEN>
<TOKEN end_char="10110" id="token-68-2" morph="none" pos="word" start_char="10107">with</TOKEN>
<TOKEN end_char="10114" id="token-68-3" morph="none" pos="word" start_char="10112">the</TOKEN>
<TOKEN end_char="10122" id="token-68-4" morph="none" pos="word" start_char="10116">reports</TOKEN>
<TOKEN end_char="10127" id="token-68-5" morph="none" pos="word" start_char="10124">said</TOKEN>
<TOKEN end_char="10132" id="token-68-6" morph="none" pos="word" start_char="10129">NCMI</TOKEN>
<TOKEN end_char="10133" id="token-68-7" morph="none" pos="punct" start_char="10133">,</TOKEN>
<TOKEN end_char="10135" id="token-68-8" morph="none" pos="word" start_char="10135">a</TOKEN>
<TOKEN end_char="10145" id="token-68-9" morph="none" pos="word" start_char="10137">component</TOKEN>
<TOKEN end_char="10148" id="token-68-10" morph="none" pos="word" start_char="10147">of</TOKEN>
<TOKEN end_char="10152" id="token-68-11" morph="none" pos="word" start_char="10150">the</TOKEN>
<TOKEN end_char="10163" id="token-68-12" morph="none" pos="word" start_char="10154">military’s</TOKEN>
<TOKEN end_char="10171" id="token-68-13" morph="none" pos="word" start_char="10165">Defense</TOKEN>
<TOKEN end_char="10184" id="token-68-14" morph="none" pos="word" start_char="10173">Intelligence</TOKEN>
<TOKEN end_char="10191" id="token-68-15" morph="none" pos="word" start_char="10186">Agency</TOKEN>
<TOKEN end_char="10192" id="token-68-16" morph="none" pos="punct" start_char="10192">,</TOKEN>
<TOKEN end_char="10198" id="token-68-17" morph="none" pos="word" start_char="10194">based</TOKEN>
<TOKEN end_char="10202" id="token-68-18" morph="none" pos="word" start_char="10200">the</TOKEN>
<TOKEN end_char="10211" id="token-68-19" morph="none" pos="word" start_char="10204">analysis</TOKEN>
<TOKEN end_char="10214" id="token-68-20" morph="none" pos="word" start_char="10213">on</TOKEN>
<TOKEN end_char="10219" id="token-68-21" morph="none" pos="word" start_char="10216">wire</TOKEN>
<TOKEN end_char="10223" id="token-68-22" morph="none" pos="word" start_char="10221">and</TOKEN>
<TOKEN end_char="10232" id="token-68-23" morph="none" pos="word" start_char="10225">computer</TOKEN>
<TOKEN end_char="10243" id="token-68-24" morph="none" pos="word" start_char="10234">intercepts</TOKEN>
<TOKEN end_char="10251" id="token-68-25" morph="none" pos="word" start_char="10245">coupled</TOKEN>
<TOKEN end_char="10256" id="token-68-26" morph="none" pos="word" start_char="10253">with</TOKEN>
<TOKEN end_char="10266" id="token-68-27" morph="none" pos="word" start_char="10258">satellite</TOKEN>
<TOKEN end_char="10273" id="token-68-28" morph="none" pos="word" start_char="10268">images</TOKEN>
<TOKEN end_char="10281" id="token-68-29" morph="none" pos="word" start_char="10275">similar</TOKEN>
<TOKEN end_char="10284" id="token-68-30" morph="none" pos="word" start_char="10283">to</TOKEN>
<TOKEN end_char="10290" id="token-68-31" morph="none" pos="word" start_char="10286">those</TOKEN>
<TOKEN end_char="10295" id="token-68-32" morph="none" pos="word" start_char="10292">used</TOKEN>
<TOKEN end_char="10298" id="token-68-33" morph="none" pos="word" start_char="10297">by</TOKEN>
<TOKEN end_char="10311" id="token-68-34" morph="none" pos="word" start_char="10300">Brownstein’s</TOKEN>
<TOKEN end_char="10316" id="token-68-35" morph="none" pos="word" start_char="10313">team</TOKEN>
<TOKEN end_char="10317" id="token-68-36" morph="none" pos="punct" start_char="10317">.</TOKEN>
</SEG>
<SEG end_char="10460" id="segment-69" start_char="10320">
<ORIGINAL_TEXT>After that story was broadcast, the NCMI’s director issued a statement, denying that a formal "product/assessment" was generated in November.</ORIGINAL_TEXT>
<TOKEN end_char="10324" id="token-69-0" morph="none" pos="word" start_char="10320">After</TOKEN>
<TOKEN end_char="10329" id="token-69-1" morph="none" pos="word" start_char="10326">that</TOKEN>
<TOKEN end_char="10335" id="token-69-2" morph="none" pos="word" start_char="10331">story</TOKEN>
<TOKEN end_char="10339" id="token-69-3" morph="none" pos="word" start_char="10337">was</TOKEN>
<TOKEN end_char="10349" id="token-69-4" morph="none" pos="word" start_char="10341">broadcast</TOKEN>
<TOKEN end_char="10350" id="token-69-5" morph="none" pos="punct" start_char="10350">,</TOKEN>
<TOKEN end_char="10354" id="token-69-6" morph="none" pos="word" start_char="10352">the</TOKEN>
<TOKEN end_char="10361" id="token-69-7" morph="none" pos="word" start_char="10356">NCMI’s</TOKEN>
<TOKEN end_char="10370" id="token-69-8" morph="none" pos="word" start_char="10363">director</TOKEN>
<TOKEN end_char="10377" id="token-69-9" morph="none" pos="word" start_char="10372">issued</TOKEN>
<TOKEN end_char="10379" id="token-69-10" morph="none" pos="word" start_char="10379">a</TOKEN>
<TOKEN end_char="10389" id="token-69-11" morph="none" pos="word" start_char="10381">statement</TOKEN>
<TOKEN end_char="10390" id="token-69-12" morph="none" pos="punct" start_char="10390">,</TOKEN>
<TOKEN end_char="10398" id="token-69-13" morph="none" pos="word" start_char="10392">denying</TOKEN>
<TOKEN end_char="10403" id="token-69-14" morph="none" pos="word" start_char="10400">that</TOKEN>
<TOKEN end_char="10405" id="token-69-15" morph="none" pos="word" start_char="10405">a</TOKEN>
<TOKEN end_char="10412" id="token-69-16" morph="none" pos="word" start_char="10407">formal</TOKEN>
<TOKEN end_char="10414" id="token-69-17" morph="none" pos="punct" start_char="10414">"</TOKEN>
<TOKEN end_char="10432" id="token-69-18" morph="none" pos="unknown" start_char="10415">product/assessment</TOKEN>
<TOKEN end_char="10433" id="token-69-19" morph="none" pos="punct" start_char="10433">"</TOKEN>
<TOKEN end_char="10437" id="token-69-20" morph="none" pos="word" start_char="10435">was</TOKEN>
<TOKEN end_char="10447" id="token-69-21" morph="none" pos="word" start_char="10439">generated</TOKEN>
<TOKEN end_char="10450" id="token-69-22" morph="none" pos="word" start_char="10449">in</TOKEN>
<TOKEN end_char="10459" id="token-69-23" morph="none" pos="word" start_char="10452">November</TOKEN>
<TOKEN end_char="10460" id="token-69-24" morph="none" pos="punct" start_char="10460">.</TOKEN>
</SEG>
<SEG end_char="10524" id="segment-70" start_char="10462">
<ORIGINAL_TEXT>The statement did not address preliminary intelligence reports.</ORIGINAL_TEXT>
<TOKEN end_char="10464" id="token-70-0" morph="none" pos="word" start_char="10462">The</TOKEN>
<TOKEN end_char="10474" id="token-70-1" morph="none" pos="word" start_char="10466">statement</TOKEN>
<TOKEN end_char="10478" id="token-70-2" morph="none" pos="word" start_char="10476">did</TOKEN>
<TOKEN end_char="10482" id="token-70-3" morph="none" pos="word" start_char="10480">not</TOKEN>
<TOKEN end_char="10490" id="token-70-4" morph="none" pos="word" start_char="10484">address</TOKEN>
<TOKEN end_char="10502" id="token-70-5" morph="none" pos="word" start_char="10492">preliminary</TOKEN>
<TOKEN end_char="10515" id="token-70-6" morph="none" pos="word" start_char="10504">intelligence</TOKEN>
<TOKEN end_char="10523" id="token-70-7" morph="none" pos="word" start_char="10517">reports</TOKEN>
<TOKEN end_char="10524" id="token-70-8" morph="none" pos="punct" start_char="10524">.</TOKEN>
</SEG>
<SEG end_char="10669" id="segment-71" start_char="10526">
<ORIGINAL_TEXT>When contacted Friday with the results of the new Harvard study, the Pentagon’s chief spokesman, Jonathan Hoffman, said he had "nothing to add."</ORIGINAL_TEXT>
<TOKEN end_char="10529" id="token-71-0" morph="none" pos="word" start_char="10526">When</TOKEN>
<TOKEN end_char="10539" id="token-71-1" morph="none" pos="word" start_char="10531">contacted</TOKEN>
<TOKEN end_char="10546" id="token-71-2" morph="none" pos="word" start_char="10541">Friday</TOKEN>
<TOKEN end_char="10551" id="token-71-3" morph="none" pos="word" start_char="10548">with</TOKEN>
<TOKEN end_char="10555" id="token-71-4" morph="none" pos="word" start_char="10553">the</TOKEN>
<TOKEN end_char="10563" id="token-71-5" morph="none" pos="word" start_char="10557">results</TOKEN>
<TOKEN end_char="10566" id="token-71-6" morph="none" pos="word" start_char="10565">of</TOKEN>
<TOKEN end_char="10570" id="token-71-7" morph="none" pos="word" start_char="10568">the</TOKEN>
<TOKEN end_char="10574" id="token-71-8" morph="none" pos="word" start_char="10572">new</TOKEN>
<TOKEN end_char="10582" id="token-71-9" morph="none" pos="word" start_char="10576">Harvard</TOKEN>
<TOKEN end_char="10588" id="token-71-10" morph="none" pos="word" start_char="10584">study</TOKEN>
<TOKEN end_char="10589" id="token-71-11" morph="none" pos="punct" start_char="10589">,</TOKEN>
<TOKEN end_char="10593" id="token-71-12" morph="none" pos="word" start_char="10591">the</TOKEN>
<TOKEN end_char="10604" id="token-71-13" morph="none" pos="word" start_char="10595">Pentagon’s</TOKEN>
<TOKEN end_char="10610" id="token-71-14" morph="none" pos="word" start_char="10606">chief</TOKEN>
<TOKEN end_char="10620" id="token-71-15" morph="none" pos="word" start_char="10612">spokesman</TOKEN>
<TOKEN end_char="10621" id="token-71-16" morph="none" pos="punct" start_char="10621">,</TOKEN>
<TOKEN end_char="10630" id="token-71-17" morph="none" pos="word" start_char="10623">Jonathan</TOKEN>
<TOKEN end_char="10638" id="token-71-18" morph="none" pos="word" start_char="10632">Hoffman</TOKEN>
<TOKEN end_char="10639" id="token-71-19" morph="none" pos="punct" start_char="10639">,</TOKEN>
<TOKEN end_char="10644" id="token-71-20" morph="none" pos="word" start_char="10641">said</TOKEN>
<TOKEN end_char="10647" id="token-71-21" morph="none" pos="word" start_char="10646">he</TOKEN>
<TOKEN end_char="10651" id="token-71-22" morph="none" pos="word" start_char="10649">had</TOKEN>
<TOKEN end_char="10653" id="token-71-23" morph="none" pos="punct" start_char="10653">"</TOKEN>
<TOKEN end_char="10660" id="token-71-24" morph="none" pos="word" start_char="10654">nothing</TOKEN>
<TOKEN end_char="10663" id="token-71-25" morph="none" pos="word" start_char="10662">to</TOKEN>
<TOKEN end_char="10667" id="token-71-26" morph="none" pos="word" start_char="10665">add</TOKEN>
<TOKEN end_char="10669" id="token-71-27" morph="none" pos="punct" start_char="10668">."</TOKEN>
</SEG>
<SEG end_char="10743" id="segment-72" start_char="10672">
<ORIGINAL_TEXT>The Office of the Director of National Intelligence declined to comment.</ORIGINAL_TEXT>
<TOKEN end_char="10674" id="token-72-0" morph="none" pos="word" start_char="10672">The</TOKEN>
<TOKEN end_char="10681" id="token-72-1" morph="none" pos="word" start_char="10676">Office</TOKEN>
<TOKEN end_char="10684" id="token-72-2" morph="none" pos="word" start_char="10683">of</TOKEN>
<TOKEN end_char="10688" id="token-72-3" morph="none" pos="word" start_char="10686">the</TOKEN>
<TOKEN end_char="10697" id="token-72-4" morph="none" pos="word" start_char="10690">Director</TOKEN>
<TOKEN end_char="10700" id="token-72-5" morph="none" pos="word" start_char="10699">of</TOKEN>
<TOKEN end_char="10709" id="token-72-6" morph="none" pos="word" start_char="10702">National</TOKEN>
<TOKEN end_char="10722" id="token-72-7" morph="none" pos="word" start_char="10711">Intelligence</TOKEN>
<TOKEN end_char="10731" id="token-72-8" morph="none" pos="word" start_char="10724">declined</TOKEN>
<TOKEN end_char="10734" id="token-72-9" morph="none" pos="word" start_char="10733">to</TOKEN>
<TOKEN end_char="10742" id="token-72-10" morph="none" pos="word" start_char="10736">comment</TOKEN>
<TOKEN end_char="10743" id="token-72-11" morph="none" pos="punct" start_char="10743">.</TOKEN>
</SEG>
<SEG end_char="10954" id="segment-73" start_char="10746">
<ORIGINAL_TEXT>In response to questions about the new Harvard Medical study, the State Department Sunday again criticized the government in Beijing for withholding from the world community critical public health information.</ORIGINAL_TEXT>
<TOKEN end_char="10747" id="token-73-0" morph="none" pos="word" start_char="10746">In</TOKEN>
<TOKEN end_char="10756" id="token-73-1" morph="none" pos="word" start_char="10749">response</TOKEN>
<TOKEN end_char="10759" id="token-73-2" morph="none" pos="word" start_char="10758">to</TOKEN>
<TOKEN end_char="10769" id="token-73-3" morph="none" pos="word" start_char="10761">questions</TOKEN>
<TOKEN end_char="10775" id="token-73-4" morph="none" pos="word" start_char="10771">about</TOKEN>
<TOKEN end_char="10779" id="token-73-5" morph="none" pos="word" start_char="10777">the</TOKEN>
<TOKEN end_char="10783" id="token-73-6" morph="none" pos="word" start_char="10781">new</TOKEN>
<TOKEN end_char="10791" id="token-73-7" morph="none" pos="word" start_char="10785">Harvard</TOKEN>
<TOKEN end_char="10799" id="token-73-8" morph="none" pos="word" start_char="10793">Medical</TOKEN>
<TOKEN end_char="10805" id="token-73-9" morph="none" pos="word" start_char="10801">study</TOKEN>
<TOKEN end_char="10806" id="token-73-10" morph="none" pos="punct" start_char="10806">,</TOKEN>
<TOKEN end_char="10810" id="token-73-11" morph="none" pos="word" start_char="10808">the</TOKEN>
<TOKEN end_char="10816" id="token-73-12" morph="none" pos="word" start_char="10812">State</TOKEN>
<TOKEN end_char="10827" id="token-73-13" morph="none" pos="word" start_char="10818">Department</TOKEN>
<TOKEN end_char="10834" id="token-73-14" morph="none" pos="word" start_char="10829">Sunday</TOKEN>
<TOKEN end_char="10840" id="token-73-15" morph="none" pos="word" start_char="10836">again</TOKEN>
<TOKEN end_char="10851" id="token-73-16" morph="none" pos="word" start_char="10842">criticized</TOKEN>
<TOKEN end_char="10855" id="token-73-17" morph="none" pos="word" start_char="10853">the</TOKEN>
<TOKEN end_char="10866" id="token-73-18" morph="none" pos="word" start_char="10857">government</TOKEN>
<TOKEN end_char="10869" id="token-73-19" morph="none" pos="word" start_char="10868">in</TOKEN>
<TOKEN end_char="10877" id="token-73-20" morph="none" pos="word" start_char="10871">Beijing</TOKEN>
<TOKEN end_char="10881" id="token-73-21" morph="none" pos="word" start_char="10879">for</TOKEN>
<TOKEN end_char="10893" id="token-73-22" morph="none" pos="word" start_char="10883">withholding</TOKEN>
<TOKEN end_char="10898" id="token-73-23" morph="none" pos="word" start_char="10895">from</TOKEN>
<TOKEN end_char="10902" id="token-73-24" morph="none" pos="word" start_char="10900">the</TOKEN>
<TOKEN end_char="10908" id="token-73-25" morph="none" pos="word" start_char="10904">world</TOKEN>
<TOKEN end_char="10918" id="token-73-26" morph="none" pos="word" start_char="10910">community</TOKEN>
<TOKEN end_char="10927" id="token-73-27" morph="none" pos="word" start_char="10920">critical</TOKEN>
<TOKEN end_char="10934" id="token-73-28" morph="none" pos="word" start_char="10929">public</TOKEN>
<TOKEN end_char="10941" id="token-73-29" morph="none" pos="word" start_char="10936">health</TOKEN>
<TOKEN end_char="10953" id="token-73-30" morph="none" pos="word" start_char="10943">information</TOKEN>
<TOKEN end_char="10954" id="token-73-31" morph="none" pos="punct" start_char="10954">.</TOKEN>
</SEG>
<SEG end_char="11189" id="segment-74" start_char="10957">
<ORIGINAL_TEXT>"The Chinese government's cover up of initial reporting on the virus is just one more example of the challenges presented by the Chinese Communist Party's hostility toward transparency," a State Department spokesperson told ABC News.</ORIGINAL_TEXT>
<TOKEN end_char="10957" id="token-74-0" morph="none" pos="punct" start_char="10957">"</TOKEN>
<TOKEN end_char="10960" id="token-74-1" morph="none" pos="word" start_char="10958">The</TOKEN>
<TOKEN end_char="10968" id="token-74-2" morph="none" pos="word" start_char="10962">Chinese</TOKEN>
<TOKEN end_char="10981" id="token-74-3" morph="none" pos="word" start_char="10970">government's</TOKEN>
<TOKEN end_char="10987" id="token-74-4" morph="none" pos="word" start_char="10983">cover</TOKEN>
<TOKEN end_char="10990" id="token-74-5" morph="none" pos="word" start_char="10989">up</TOKEN>
<TOKEN end_char="10993" id="token-74-6" morph="none" pos="word" start_char="10992">of</TOKEN>
<TOKEN end_char="11001" id="token-74-7" morph="none" pos="word" start_char="10995">initial</TOKEN>
<TOKEN end_char="11011" id="token-74-8" morph="none" pos="word" start_char="11003">reporting</TOKEN>
<TOKEN end_char="11014" id="token-74-9" morph="none" pos="word" start_char="11013">on</TOKEN>
<TOKEN end_char="11018" id="token-74-10" morph="none" pos="word" start_char="11016">the</TOKEN>
<TOKEN end_char="11024" id="token-74-11" morph="none" pos="word" start_char="11020">virus</TOKEN>
<TOKEN end_char="11027" id="token-74-12" morph="none" pos="word" start_char="11026">is</TOKEN>
<TOKEN end_char="11032" id="token-74-13" morph="none" pos="word" start_char="11029">just</TOKEN>
<TOKEN end_char="11036" id="token-74-14" morph="none" pos="word" start_char="11034">one</TOKEN>
<TOKEN end_char="11041" id="token-74-15" morph="none" pos="word" start_char="11038">more</TOKEN>
<TOKEN end_char="11049" id="token-74-16" morph="none" pos="word" start_char="11043">example</TOKEN>
<TOKEN end_char="11052" id="token-74-17" morph="none" pos="word" start_char="11051">of</TOKEN>
<TOKEN end_char="11056" id="token-74-18" morph="none" pos="word" start_char="11054">the</TOKEN>
<TOKEN end_char="11067" id="token-74-19" morph="none" pos="word" start_char="11058">challenges</TOKEN>
<TOKEN end_char="11077" id="token-74-20" morph="none" pos="word" start_char="11069">presented</TOKEN>
<TOKEN end_char="11080" id="token-74-21" morph="none" pos="word" start_char="11079">by</TOKEN>
<TOKEN end_char="11084" id="token-74-22" morph="none" pos="word" start_char="11082">the</TOKEN>
<TOKEN end_char="11092" id="token-74-23" morph="none" pos="word" start_char="11086">Chinese</TOKEN>
<TOKEN end_char="11102" id="token-74-24" morph="none" pos="word" start_char="11094">Communist</TOKEN>
<TOKEN end_char="11110" id="token-74-25" morph="none" pos="word" start_char="11104">Party's</TOKEN>
<TOKEN end_char="11120" id="token-74-26" morph="none" pos="word" start_char="11112">hostility</TOKEN>
<TOKEN end_char="11127" id="token-74-27" morph="none" pos="word" start_char="11122">toward</TOKEN>
<TOKEN end_char="11140" id="token-74-28" morph="none" pos="word" start_char="11129">transparency</TOKEN>
<TOKEN end_char="11142" id="token-74-29" morph="none" pos="punct" start_char="11141">,"</TOKEN>
<TOKEN end_char="11144" id="token-74-30" morph="none" pos="word" start_char="11144">a</TOKEN>
<TOKEN end_char="11150" id="token-74-31" morph="none" pos="word" start_char="11146">State</TOKEN>
<TOKEN end_char="11161" id="token-74-32" morph="none" pos="word" start_char="11152">Department</TOKEN>
<TOKEN end_char="11174" id="token-74-33" morph="none" pos="word" start_char="11163">spokesperson</TOKEN>
<TOKEN end_char="11179" id="token-74-34" morph="none" pos="word" start_char="11176">told</TOKEN>
<TOKEN end_char="11183" id="token-74-35" morph="none" pos="word" start_char="11181">ABC</TOKEN>
<TOKEN end_char="11188" id="token-74-36" morph="none" pos="word" start_char="11185">News</TOKEN>
<TOKEN end_char="11189" id="token-74-37" morph="none" pos="punct" start_char="11189">.</TOKEN>
</SEG>
<SEG end_char="11326" id="segment-75" start_char="11191">
<ORIGINAL_TEXT>"The Chinese government has a responsibility to share information on the virus and support countries as the world responds to COVID-19."</ORIGINAL_TEXT>
<TOKEN end_char="11191" id="token-75-0" morph="none" pos="punct" start_char="11191">"</TOKEN>
<TOKEN end_char="11194" id="token-75-1" morph="none" pos="word" start_char="11192">The</TOKEN>
<TOKEN end_char="11202" id="token-75-2" morph="none" pos="word" start_char="11196">Chinese</TOKEN>
<TOKEN end_char="11213" id="token-75-3" morph="none" pos="word" start_char="11204">government</TOKEN>
<TOKEN end_char="11217" id="token-75-4" morph="none" pos="word" start_char="11215">has</TOKEN>
<TOKEN end_char="11219" id="token-75-5" morph="none" pos="word" start_char="11219">a</TOKEN>
<TOKEN end_char="11234" id="token-75-6" morph="none" pos="word" start_char="11221">responsibility</TOKEN>
<TOKEN end_char="11237" id="token-75-7" morph="none" pos="word" start_char="11236">to</TOKEN>
<TOKEN end_char="11243" id="token-75-8" morph="none" pos="word" start_char="11239">share</TOKEN>
<TOKEN end_char="11255" id="token-75-9" morph="none" pos="word" start_char="11245">information</TOKEN>
<TOKEN end_char="11258" id="token-75-10" morph="none" pos="word" start_char="11257">on</TOKEN>
<TOKEN end_char="11262" id="token-75-11" morph="none" pos="word" start_char="11260">the</TOKEN>
<TOKEN end_char="11268" id="token-75-12" morph="none" pos="word" start_char="11264">virus</TOKEN>
<TOKEN end_char="11272" id="token-75-13" morph="none" pos="word" start_char="11270">and</TOKEN>
<TOKEN end_char="11280" id="token-75-14" morph="none" pos="word" start_char="11274">support</TOKEN>
<TOKEN end_char="11290" id="token-75-15" morph="none" pos="word" start_char="11282">countries</TOKEN>
<TOKEN end_char="11293" id="token-75-16" morph="none" pos="word" start_char="11292">as</TOKEN>
<TOKEN end_char="11297" id="token-75-17" morph="none" pos="word" start_char="11295">the</TOKEN>
<TOKEN end_char="11303" id="token-75-18" morph="none" pos="word" start_char="11299">world</TOKEN>
<TOKEN end_char="11312" id="token-75-19" morph="none" pos="word" start_char="11305">responds</TOKEN>
<TOKEN end_char="11315" id="token-75-20" morph="none" pos="word" start_char="11314">to</TOKEN>
<TOKEN end_char="11324" id="token-75-21" morph="none" pos="unknown" start_char="11317">COVID-19</TOKEN>
<TOKEN end_char="11326" id="token-75-22" morph="none" pos="punct" start_char="11325">."</TOKEN>
</SEG>
<SEG end_char="11509" id="segment-76" start_char="11329">
<ORIGINAL_TEXT>In March, the Hong Kong-based South China Morning Post newspaper, citing Chinese government data, reported that the first case of COVID-19 could be traced back to November 17, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="11330" id="token-76-0" morph="none" pos="word" start_char="11329">In</TOKEN>
<TOKEN end_char="11336" id="token-76-1" morph="none" pos="word" start_char="11332">March</TOKEN>
<TOKEN end_char="11337" id="token-76-2" morph="none" pos="punct" start_char="11337">,</TOKEN>
<TOKEN end_char="11341" id="token-76-3" morph="none" pos="word" start_char="11339">the</TOKEN>
<TOKEN end_char="11346" id="token-76-4" morph="none" pos="word" start_char="11343">Hong</TOKEN>
<TOKEN end_char="11357" id="token-76-5" morph="none" pos="unknown" start_char="11348">Kong-based</TOKEN>
<TOKEN end_char="11363" id="token-76-6" morph="none" pos="word" start_char="11359">South</TOKEN>
<TOKEN end_char="11369" id="token-76-7" morph="none" pos="word" start_char="11365">China</TOKEN>
<TOKEN end_char="11377" id="token-76-8" morph="none" pos="word" start_char="11371">Morning</TOKEN>
<TOKEN end_char="11382" id="token-76-9" morph="none" pos="word" start_char="11379">Post</TOKEN>
<TOKEN end_char="11392" id="token-76-10" morph="none" pos="word" start_char="11384">newspaper</TOKEN>
<TOKEN end_char="11393" id="token-76-11" morph="none" pos="punct" start_char="11393">,</TOKEN>
<TOKEN end_char="11400" id="token-76-12" morph="none" pos="word" start_char="11395">citing</TOKEN>
<TOKEN end_char="11408" id="token-76-13" morph="none" pos="word" start_char="11402">Chinese</TOKEN>
<TOKEN end_char="11419" id="token-76-14" morph="none" pos="word" start_char="11410">government</TOKEN>
<TOKEN end_char="11424" id="token-76-15" morph="none" pos="word" start_char="11421">data</TOKEN>
<TOKEN end_char="11425" id="token-76-16" morph="none" pos="punct" start_char="11425">,</TOKEN>
<TOKEN end_char="11434" id="token-76-17" morph="none" pos="word" start_char="11427">reported</TOKEN>
<TOKEN end_char="11439" id="token-76-18" morph="none" pos="word" start_char="11436">that</TOKEN>
<TOKEN end_char="11443" id="token-76-19" morph="none" pos="word" start_char="11441">the</TOKEN>
<TOKEN end_char="11449" id="token-76-20" morph="none" pos="word" start_char="11445">first</TOKEN>
<TOKEN end_char="11454" id="token-76-21" morph="none" pos="word" start_char="11451">case</TOKEN>
<TOKEN end_char="11457" id="token-76-22" morph="none" pos="word" start_char="11456">of</TOKEN>
<TOKEN end_char="11466" id="token-76-23" morph="none" pos="unknown" start_char="11459">COVID-19</TOKEN>
<TOKEN end_char="11472" id="token-76-24" morph="none" pos="word" start_char="11468">could</TOKEN>
<TOKEN end_char="11475" id="token-76-25" morph="none" pos="word" start_char="11474">be</TOKEN>
<TOKEN end_char="11482" id="token-76-26" morph="none" pos="word" start_char="11477">traced</TOKEN>
<TOKEN end_char="11487" id="token-76-27" morph="none" pos="word" start_char="11484">back</TOKEN>
<TOKEN end_char="11490" id="token-76-28" morph="none" pos="word" start_char="11489">to</TOKEN>
<TOKEN end_char="11499" id="token-76-29" morph="none" pos="word" start_char="11492">November</TOKEN>
<TOKEN end_char="11502" id="token-76-30" morph="none" pos="word" start_char="11501">17</TOKEN>
<TOKEN end_char="11503" id="token-76-31" morph="none" pos="punct" start_char="11503">,</TOKEN>
<TOKEN end_char="11508" id="token-76-32" morph="none" pos="word" start_char="11505">2019</TOKEN>
<TOKEN end_char="11509" id="token-76-33" morph="none" pos="punct" start_char="11509">.</TOKEN>
</SEG>
<SEG end_char="11667" id="segment-77" start_char="11511">
<ORIGINAL_TEXT>In recent days, Chinese health officials have told local media that the virus likely was spreading before they realized, though they have offered no details.</ORIGINAL_TEXT>
<TOKEN end_char="11512" id="token-77-0" morph="none" pos="word" start_char="11511">In</TOKEN>
<TOKEN end_char="11519" id="token-77-1" morph="none" pos="word" start_char="11514">recent</TOKEN>
<TOKEN end_char="11524" id="token-77-2" morph="none" pos="word" start_char="11521">days</TOKEN>
<TOKEN end_char="11525" id="token-77-3" morph="none" pos="punct" start_char="11525">,</TOKEN>
<TOKEN end_char="11533" id="token-77-4" morph="none" pos="word" start_char="11527">Chinese</TOKEN>
<TOKEN end_char="11540" id="token-77-5" morph="none" pos="word" start_char="11535">health</TOKEN>
<TOKEN end_char="11550" id="token-77-6" morph="none" pos="word" start_char="11542">officials</TOKEN>
<TOKEN end_char="11555" id="token-77-7" morph="none" pos="word" start_char="11552">have</TOKEN>
<TOKEN end_char="11560" id="token-77-8" morph="none" pos="word" start_char="11557">told</TOKEN>
<TOKEN end_char="11566" id="token-77-9" morph="none" pos="word" start_char="11562">local</TOKEN>
<TOKEN end_char="11572" id="token-77-10" morph="none" pos="word" start_char="11568">media</TOKEN>
<TOKEN end_char="11577" id="token-77-11" morph="none" pos="word" start_char="11574">that</TOKEN>
<TOKEN end_char="11581" id="token-77-12" morph="none" pos="word" start_char="11579">the</TOKEN>
<TOKEN end_char="11587" id="token-77-13" morph="none" pos="word" start_char="11583">virus</TOKEN>
<TOKEN end_char="11594" id="token-77-14" morph="none" pos="word" start_char="11589">likely</TOKEN>
<TOKEN end_char="11598" id="token-77-15" morph="none" pos="word" start_char="11596">was</TOKEN>
<TOKEN end_char="11608" id="token-77-16" morph="none" pos="word" start_char="11600">spreading</TOKEN>
<TOKEN end_char="11615" id="token-77-17" morph="none" pos="word" start_char="11610">before</TOKEN>
<TOKEN end_char="11620" id="token-77-18" morph="none" pos="word" start_char="11617">they</TOKEN>
<TOKEN end_char="11629" id="token-77-19" morph="none" pos="word" start_char="11622">realized</TOKEN>
<TOKEN end_char="11630" id="token-77-20" morph="none" pos="punct" start_char="11630">,</TOKEN>
<TOKEN end_char="11637" id="token-77-21" morph="none" pos="word" start_char="11632">though</TOKEN>
<TOKEN end_char="11642" id="token-77-22" morph="none" pos="word" start_char="11639">they</TOKEN>
<TOKEN end_char="11647" id="token-77-23" morph="none" pos="word" start_char="11644">have</TOKEN>
<TOKEN end_char="11655" id="token-77-24" morph="none" pos="word" start_char="11649">offered</TOKEN>
<TOKEN end_char="11658" id="token-77-25" morph="none" pos="word" start_char="11657">no</TOKEN>
<TOKEN end_char="11666" id="token-77-26" morph="none" pos="word" start_char="11660">details</TOKEN>
<TOKEN end_char="11667" id="token-77-27" morph="none" pos="punct" start_char="11667">.</TOKEN>
</SEG>
<SEG end_char="11827" id="segment-78" start_char="11670">
<ORIGINAL_TEXT>ABC News sought comment on the new study from the hospitals in Wuhan that were analyzed, the local public health agency and the Chinese embassy in Washington.</ORIGINAL_TEXT>
<TOKEN end_char="11672" id="token-78-0" morph="none" pos="word" start_char="11670">ABC</TOKEN>
<TOKEN end_char="11677" id="token-78-1" morph="none" pos="word" start_char="11674">News</TOKEN>
<TOKEN end_char="11684" id="token-78-2" morph="none" pos="word" start_char="11679">sought</TOKEN>
<TOKEN end_char="11692" id="token-78-3" morph="none" pos="word" start_char="11686">comment</TOKEN>
<TOKEN end_char="11695" id="token-78-4" morph="none" pos="word" start_char="11694">on</TOKEN>
<TOKEN end_char="11699" id="token-78-5" morph="none" pos="word" start_char="11697">the</TOKEN>
<TOKEN end_char="11703" id="token-78-6" morph="none" pos="word" start_char="11701">new</TOKEN>
<TOKEN end_char="11709" id="token-78-7" morph="none" pos="word" start_char="11705">study</TOKEN>
<TOKEN end_char="11714" id="token-78-8" morph="none" pos="word" start_char="11711">from</TOKEN>
<TOKEN end_char="11718" id="token-78-9" morph="none" pos="word" start_char="11716">the</TOKEN>
<TOKEN end_char="11728" id="token-78-10" morph="none" pos="word" start_char="11720">hospitals</TOKEN>
<TOKEN end_char="11731" id="token-78-11" morph="none" pos="word" start_char="11730">in</TOKEN>
<TOKEN end_char="11737" id="token-78-12" morph="none" pos="word" start_char="11733">Wuhan</TOKEN>
<TOKEN end_char="11742" id="token-78-13" morph="none" pos="word" start_char="11739">that</TOKEN>
<TOKEN end_char="11747" id="token-78-14" morph="none" pos="word" start_char="11744">were</TOKEN>
<TOKEN end_char="11756" id="token-78-15" morph="none" pos="word" start_char="11749">analyzed</TOKEN>
<TOKEN end_char="11757" id="token-78-16" morph="none" pos="punct" start_char="11757">,</TOKEN>
<TOKEN end_char="11761" id="token-78-17" morph="none" pos="word" start_char="11759">the</TOKEN>
<TOKEN end_char="11767" id="token-78-18" morph="none" pos="word" start_char="11763">local</TOKEN>
<TOKEN end_char="11774" id="token-78-19" morph="none" pos="word" start_char="11769">public</TOKEN>
<TOKEN end_char="11781" id="token-78-20" morph="none" pos="word" start_char="11776">health</TOKEN>
<TOKEN end_char="11788" id="token-78-21" morph="none" pos="word" start_char="11783">agency</TOKEN>
<TOKEN end_char="11792" id="token-78-22" morph="none" pos="word" start_char="11790">and</TOKEN>
<TOKEN end_char="11796" id="token-78-23" morph="none" pos="word" start_char="11794">the</TOKEN>
<TOKEN end_char="11804" id="token-78-24" morph="none" pos="word" start_char="11798">Chinese</TOKEN>
<TOKEN end_char="11812" id="token-78-25" morph="none" pos="word" start_char="11806">embassy</TOKEN>
<TOKEN end_char="11815" id="token-78-26" morph="none" pos="word" start_char="11814">in</TOKEN>
<TOKEN end_char="11826" id="token-78-27" morph="none" pos="word" start_char="11817">Washington</TOKEN>
<TOKEN end_char="11827" id="token-78-28" morph="none" pos="punct" start_char="11827">.</TOKEN>
</SEG>
<SEG end_char="11982" id="segment-79" start_char="11829">
<ORIGINAL_TEXT>The only response received by the network came from the Chinese embassy, where officials pointed to a white paper released Sunday the China State Council.</ORIGINAL_TEXT>
<TOKEN end_char="11831" id="token-79-0" morph="none" pos="word" start_char="11829">The</TOKEN>
<TOKEN end_char="11836" id="token-79-1" morph="none" pos="word" start_char="11833">only</TOKEN>
<TOKEN end_char="11845" id="token-79-2" morph="none" pos="word" start_char="11838">response</TOKEN>
<TOKEN end_char="11854" id="token-79-3" morph="none" pos="word" start_char="11847">received</TOKEN>
<TOKEN end_char="11857" id="token-79-4" morph="none" pos="word" start_char="11856">by</TOKEN>
<TOKEN end_char="11861" id="token-79-5" morph="none" pos="word" start_char="11859">the</TOKEN>
<TOKEN end_char="11869" id="token-79-6" morph="none" pos="word" start_char="11863">network</TOKEN>
<TOKEN end_char="11874" id="token-79-7" morph="none" pos="word" start_char="11871">came</TOKEN>
<TOKEN end_char="11879" id="token-79-8" morph="none" pos="word" start_char="11876">from</TOKEN>
<TOKEN end_char="11883" id="token-79-9" morph="none" pos="word" start_char="11881">the</TOKEN>
<TOKEN end_char="11891" id="token-79-10" morph="none" pos="word" start_char="11885">Chinese</TOKEN>
<TOKEN end_char="11899" id="token-79-11" morph="none" pos="word" start_char="11893">embassy</TOKEN>
<TOKEN end_char="11900" id="token-79-12" morph="none" pos="punct" start_char="11900">,</TOKEN>
<TOKEN end_char="11906" id="token-79-13" morph="none" pos="word" start_char="11902">where</TOKEN>
<TOKEN end_char="11916" id="token-79-14" morph="none" pos="word" start_char="11908">officials</TOKEN>
<TOKEN end_char="11924" id="token-79-15" morph="none" pos="word" start_char="11918">pointed</TOKEN>
<TOKEN end_char="11927" id="token-79-16" morph="none" pos="word" start_char="11926">to</TOKEN>
<TOKEN end_char="11929" id="token-79-17" morph="none" pos="word" start_char="11929">a</TOKEN>
<TOKEN end_char="11935" id="token-79-18" morph="none" pos="word" start_char="11931">white</TOKEN>
<TOKEN end_char="11941" id="token-79-19" morph="none" pos="word" start_char="11937">paper</TOKEN>
<TOKEN end_char="11950" id="token-79-20" morph="none" pos="word" start_char="11943">released</TOKEN>
<TOKEN end_char="11957" id="token-79-21" morph="none" pos="word" start_char="11952">Sunday</TOKEN>
<TOKEN end_char="11961" id="token-79-22" morph="none" pos="word" start_char="11959">the</TOKEN>
<TOKEN end_char="11967" id="token-79-23" morph="none" pos="word" start_char="11963">China</TOKEN>
<TOKEN end_char="11973" id="token-79-24" morph="none" pos="word" start_char="11969">State</TOKEN>
<TOKEN end_char="11981" id="token-79-25" morph="none" pos="word" start_char="11975">Council</TOKEN>
<TOKEN end_char="11982" id="token-79-26" morph="none" pos="punct" start_char="11982">.</TOKEN>
</SEG>
<SEG end_char="12060" id="segment-80" start_char="11985">
<ORIGINAL_TEXT>"The novel coronavirus is a previously unknown virus," the report documents.</ORIGINAL_TEXT>
<TOKEN end_char="11985" id="token-80-0" morph="none" pos="punct" start_char="11985">"</TOKEN>
<TOKEN end_char="11988" id="token-80-1" morph="none" pos="word" start_char="11986">The</TOKEN>
<TOKEN end_char="11994" id="token-80-2" morph="none" pos="word" start_char="11990">novel</TOKEN>
<TOKEN end_char="12006" id="token-80-3" morph="none" pos="word" start_char="11996">coronavirus</TOKEN>
<TOKEN end_char="12009" id="token-80-4" morph="none" pos="word" start_char="12008">is</TOKEN>
<TOKEN end_char="12011" id="token-80-5" morph="none" pos="word" start_char="12011">a</TOKEN>
<TOKEN end_char="12022" id="token-80-6" morph="none" pos="word" start_char="12013">previously</TOKEN>
<TOKEN end_char="12030" id="token-80-7" morph="none" pos="word" start_char="12024">unknown</TOKEN>
<TOKEN end_char="12036" id="token-80-8" morph="none" pos="word" start_char="12032">virus</TOKEN>
<TOKEN end_char="12038" id="token-80-9" morph="none" pos="punct" start_char="12037">,"</TOKEN>
<TOKEN end_char="12042" id="token-80-10" morph="none" pos="word" start_char="12040">the</TOKEN>
<TOKEN end_char="12049" id="token-80-11" morph="none" pos="word" start_char="12044">report</TOKEN>
<TOKEN end_char="12059" id="token-80-12" morph="none" pos="word" start_char="12051">documents</TOKEN>
<TOKEN end_char="12060" id="token-80-13" morph="none" pos="punct" start_char="12060">.</TOKEN>
</SEG>
<SEG end_char="12156" id="segment-81" start_char="12062">
<ORIGINAL_TEXT>"Determining its origin is a scientific issue that requires research by scientists and doctors.</ORIGINAL_TEXT>
<TOKEN end_char="12062" id="token-81-0" morph="none" pos="punct" start_char="12062">"</TOKEN>
<TOKEN end_char="12073" id="token-81-1" morph="none" pos="word" start_char="12063">Determining</TOKEN>
<TOKEN end_char="12077" id="token-81-2" morph="none" pos="word" start_char="12075">its</TOKEN>
<TOKEN end_char="12084" id="token-81-3" morph="none" pos="word" start_char="12079">origin</TOKEN>
<TOKEN end_char="12087" id="token-81-4" morph="none" pos="word" start_char="12086">is</TOKEN>
<TOKEN end_char="12089" id="token-81-5" morph="none" pos="word" start_char="12089">a</TOKEN>
<TOKEN end_char="12100" id="token-81-6" morph="none" pos="word" start_char="12091">scientific</TOKEN>
<TOKEN end_char="12106" id="token-81-7" morph="none" pos="word" start_char="12102">issue</TOKEN>
<TOKEN end_char="12111" id="token-81-8" morph="none" pos="word" start_char="12108">that</TOKEN>
<TOKEN end_char="12120" id="token-81-9" morph="none" pos="word" start_char="12113">requires</TOKEN>
<TOKEN end_char="12129" id="token-81-10" morph="none" pos="word" start_char="12122">research</TOKEN>
<TOKEN end_char="12132" id="token-81-11" morph="none" pos="word" start_char="12131">by</TOKEN>
<TOKEN end_char="12143" id="token-81-12" morph="none" pos="word" start_char="12134">scientists</TOKEN>
<TOKEN end_char="12147" id="token-81-13" morph="none" pos="word" start_char="12145">and</TOKEN>
<TOKEN end_char="12155" id="token-81-14" morph="none" pos="word" start_char="12149">doctors</TOKEN>
<TOKEN end_char="12156" id="token-81-15" morph="none" pos="punct" start_char="12156">.</TOKEN>
</SEG>
<SEG end_char="12209" id="segment-82" start_char="12158">
<ORIGINAL_TEXT>The conclusion must be based on facts and evidence."</ORIGINAL_TEXT>
<TOKEN end_char="12160" id="token-82-0" morph="none" pos="word" start_char="12158">The</TOKEN>
<TOKEN end_char="12171" id="token-82-1" morph="none" pos="word" start_char="12162">conclusion</TOKEN>
<TOKEN end_char="12176" id="token-82-2" morph="none" pos="word" start_char="12173">must</TOKEN>
<TOKEN end_char="12179" id="token-82-3" morph="none" pos="word" start_char="12178">be</TOKEN>
<TOKEN end_char="12185" id="token-82-4" morph="none" pos="word" start_char="12181">based</TOKEN>
<TOKEN end_char="12188" id="token-82-5" morph="none" pos="word" start_char="12187">on</TOKEN>
<TOKEN end_char="12194" id="token-82-6" morph="none" pos="word" start_char="12190">facts</TOKEN>
<TOKEN end_char="12198" id="token-82-7" morph="none" pos="word" start_char="12196">and</TOKEN>
<TOKEN end_char="12207" id="token-82-8" morph="none" pos="word" start_char="12200">evidence</TOKEN>
<TOKEN end_char="12209" id="token-82-9" morph="none" pos="punct" start_char="12208">."</TOKEN>
</SEG>
<SEG end_char="12408" id="segment-83" start_char="12212">
<ORIGINAL_TEXT>The council also defended the Chinese government’s response, writing, "China has also acted with a keen sense of responsibility to humanity, its people, posterity, and the international community."</ORIGINAL_TEXT>
<TOKEN end_char="12214" id="token-83-0" morph="none" pos="word" start_char="12212">The</TOKEN>
<TOKEN end_char="12222" id="token-83-1" morph="none" pos="word" start_char="12216">council</TOKEN>
<TOKEN end_char="12227" id="token-83-2" morph="none" pos="word" start_char="12224">also</TOKEN>
<TOKEN end_char="12236" id="token-83-3" morph="none" pos="word" start_char="12229">defended</TOKEN>
<TOKEN end_char="12240" id="token-83-4" morph="none" pos="word" start_char="12238">the</TOKEN>
<TOKEN end_char="12248" id="token-83-5" morph="none" pos="word" start_char="12242">Chinese</TOKEN>
<TOKEN end_char="12261" id="token-83-6" morph="none" pos="word" start_char="12250">government’s</TOKEN>
<TOKEN end_char="12270" id="token-83-7" morph="none" pos="word" start_char="12263">response</TOKEN>
<TOKEN end_char="12271" id="token-83-8" morph="none" pos="punct" start_char="12271">,</TOKEN>
<TOKEN end_char="12279" id="token-83-9" morph="none" pos="word" start_char="12273">writing</TOKEN>
<TOKEN end_char="12280" id="token-83-10" morph="none" pos="punct" start_char="12280">,</TOKEN>
<TOKEN end_char="12282" id="token-83-11" morph="none" pos="punct" start_char="12282">"</TOKEN>
<TOKEN end_char="12287" id="token-83-12" morph="none" pos="word" start_char="12283">China</TOKEN>
<TOKEN end_char="12291" id="token-83-13" morph="none" pos="word" start_char="12289">has</TOKEN>
<TOKEN end_char="12296" id="token-83-14" morph="none" pos="word" start_char="12293">also</TOKEN>
<TOKEN end_char="12302" id="token-83-15" morph="none" pos="word" start_char="12298">acted</TOKEN>
<TOKEN end_char="12307" id="token-83-16" morph="none" pos="word" start_char="12304">with</TOKEN>
<TOKEN end_char="12309" id="token-83-17" morph="none" pos="word" start_char="12309">a</TOKEN>
<TOKEN end_char="12314" id="token-83-18" morph="none" pos="word" start_char="12311">keen</TOKEN>
<TOKEN end_char="12320" id="token-83-19" morph="none" pos="word" start_char="12316">sense</TOKEN>
<TOKEN end_char="12323" id="token-83-20" morph="none" pos="word" start_char="12322">of</TOKEN>
<TOKEN end_char="12338" id="token-83-21" morph="none" pos="word" start_char="12325">responsibility</TOKEN>
<TOKEN end_char="12341" id="token-83-22" morph="none" pos="word" start_char="12340">to</TOKEN>
<TOKEN end_char="12350" id="token-83-23" morph="none" pos="word" start_char="12343">humanity</TOKEN>
<TOKEN end_char="12351" id="token-83-24" morph="none" pos="punct" start_char="12351">,</TOKEN>
<TOKEN end_char="12355" id="token-83-25" morph="none" pos="word" start_char="12353">its</TOKEN>
<TOKEN end_char="12362" id="token-83-26" morph="none" pos="word" start_char="12357">people</TOKEN>
<TOKEN end_char="12363" id="token-83-27" morph="none" pos="punct" start_char="12363">,</TOKEN>
<TOKEN end_char="12373" id="token-83-28" morph="none" pos="word" start_char="12365">posterity</TOKEN>
<TOKEN end_char="12374" id="token-83-29" morph="none" pos="punct" start_char="12374">,</TOKEN>
<TOKEN end_char="12378" id="token-83-30" morph="none" pos="word" start_char="12376">and</TOKEN>
<TOKEN end_char="12382" id="token-83-31" morph="none" pos="word" start_char="12380">the</TOKEN>
<TOKEN end_char="12396" id="token-83-32" morph="none" pos="word" start_char="12384">international</TOKEN>
<TOKEN end_char="12406" id="token-83-33" morph="none" pos="word" start_char="12398">community</TOKEN>
<TOKEN end_char="12408" id="token-83-34" morph="none" pos="punct" start_char="12407">."</TOKEN>
</SEG>
<SEG end_char="12644" id="segment-84" start_char="12411">
<ORIGINAL_TEXT>Tuesday a spokesperson for the Chinese Foreign Ministry told reporters she had not seen the Harvard study, but thought it was "ridiculous to come to this kind of conclusion based on superficial [observations], such as traffic volumes.</ORIGINAL_TEXT>
<TOKEN end_char="12417" id="token-84-0" morph="none" pos="word" start_char="12411">Tuesday</TOKEN>
<TOKEN end_char="12419" id="token-84-1" morph="none" pos="word" start_char="12419">a</TOKEN>
<TOKEN end_char="12432" id="token-84-2" morph="none" pos="word" start_char="12421">spokesperson</TOKEN>
<TOKEN end_char="12436" id="token-84-3" morph="none" pos="word" start_char="12434">for</TOKEN>
<TOKEN end_char="12440" id="token-84-4" morph="none" pos="word" start_char="12438">the</TOKEN>
<TOKEN end_char="12448" id="token-84-5" morph="none" pos="word" start_char="12442">Chinese</TOKEN>
<TOKEN end_char="12456" id="token-84-6" morph="none" pos="word" start_char="12450">Foreign</TOKEN>
<TOKEN end_char="12465" id="token-84-7" morph="none" pos="word" start_char="12458">Ministry</TOKEN>
<TOKEN end_char="12470" id="token-84-8" morph="none" pos="word" start_char="12467">told</TOKEN>
<TOKEN end_char="12480" id="token-84-9" morph="none" pos="word" start_char="12472">reporters</TOKEN>
<TOKEN end_char="12484" id="token-84-10" morph="none" pos="word" start_char="12482">she</TOKEN>
<TOKEN end_char="12488" id="token-84-11" morph="none" pos="word" start_char="12486">had</TOKEN>
<TOKEN end_char="12492" id="token-84-12" morph="none" pos="word" start_char="12490">not</TOKEN>
<TOKEN end_char="12497" id="token-84-13" morph="none" pos="word" start_char="12494">seen</TOKEN>
<TOKEN end_char="12501" id="token-84-14" morph="none" pos="word" start_char="12499">the</TOKEN>
<TOKEN end_char="12509" id="token-84-15" morph="none" pos="word" start_char="12503">Harvard</TOKEN>
<TOKEN end_char="12515" id="token-84-16" morph="none" pos="word" start_char="12511">study</TOKEN>
<TOKEN end_char="12516" id="token-84-17" morph="none" pos="punct" start_char="12516">,</TOKEN>
<TOKEN end_char="12520" id="token-84-18" morph="none" pos="word" start_char="12518">but</TOKEN>
<TOKEN end_char="12528" id="token-84-19" morph="none" pos="word" start_char="12522">thought</TOKEN>
<TOKEN end_char="12531" id="token-84-20" morph="none" pos="word" start_char="12530">it</TOKEN>
<TOKEN end_char="12535" id="token-84-21" morph="none" pos="word" start_char="12533">was</TOKEN>
<TOKEN end_char="12537" id="token-84-22" morph="none" pos="punct" start_char="12537">"</TOKEN>
<TOKEN end_char="12547" id="token-84-23" morph="none" pos="word" start_char="12538">ridiculous</TOKEN>
<TOKEN end_char="12550" id="token-84-24" morph="none" pos="word" start_char="12549">to</TOKEN>
<TOKEN end_char="12555" id="token-84-25" morph="none" pos="word" start_char="12552">come</TOKEN>
<TOKEN end_char="12558" id="token-84-26" morph="none" pos="word" start_char="12557">to</TOKEN>
<TOKEN end_char="12563" id="token-84-27" morph="none" pos="word" start_char="12560">this</TOKEN>
<TOKEN end_char="12568" id="token-84-28" morph="none" pos="word" start_char="12565">kind</TOKEN>
<TOKEN end_char="12571" id="token-84-29" morph="none" pos="word" start_char="12570">of</TOKEN>
<TOKEN end_char="12582" id="token-84-30" morph="none" pos="word" start_char="12573">conclusion</TOKEN>
<TOKEN end_char="12588" id="token-84-31" morph="none" pos="word" start_char="12584">based</TOKEN>
<TOKEN end_char="12591" id="token-84-32" morph="none" pos="word" start_char="12590">on</TOKEN>
<TOKEN end_char="12603" id="token-84-33" morph="none" pos="word" start_char="12593">superficial</TOKEN>
<TOKEN end_char="12605" id="token-84-34" morph="none" pos="punct" start_char="12605">[</TOKEN>
<TOKEN end_char="12617" id="token-84-35" morph="none" pos="word" start_char="12606">observations</TOKEN>
<TOKEN end_char="12619" id="token-84-36" morph="none" pos="punct" start_char="12618">],</TOKEN>
<TOKEN end_char="12624" id="token-84-37" morph="none" pos="word" start_char="12621">such</TOKEN>
<TOKEN end_char="12627" id="token-84-38" morph="none" pos="word" start_char="12626">as</TOKEN>
<TOKEN end_char="12635" id="token-84-39" morph="none" pos="word" start_char="12629">traffic</TOKEN>
<TOKEN end_char="12643" id="token-84-40" morph="none" pos="word" start_char="12637">volumes</TOKEN>
<TOKEN end_char="12644" id="token-84-41" morph="none" pos="punct" start_char="12644">.</TOKEN>
</SEG>
<SEG end_char="12674" id="segment-85" start_char="12646">
<ORIGINAL_TEXT>It is incredibly ridiculous."</ORIGINAL_TEXT>
<TOKEN end_char="12647" id="token-85-0" morph="none" pos="word" start_char="12646">It</TOKEN>
<TOKEN end_char="12650" id="token-85-1" morph="none" pos="word" start_char="12649">is</TOKEN>
<TOKEN end_char="12661" id="token-85-2" morph="none" pos="word" start_char="12652">incredibly</TOKEN>
<TOKEN end_char="12672" id="token-85-3" morph="none" pos="word" start_char="12663">ridiculous</TOKEN>
<TOKEN end_char="12674" id="token-85-4" morph="none" pos="punct" start_char="12673">."</TOKEN>
</SEG>
<SEG end_char="12891" id="segment-86" start_char="12677">
<ORIGINAL_TEXT>The spokesperson, Hua Chunying, said she was not a scientist, but found the conclusions "very far-fetched" and more generally urged the U.S. and China to work together on combating what she called false information.</ORIGINAL_TEXT>
<TOKEN end_char="12679" id="token-86-0" morph="none" pos="word" start_char="12677">The</TOKEN>
<TOKEN end_char="12692" id="token-86-1" morph="none" pos="word" start_char="12681">spokesperson</TOKEN>
<TOKEN end_char="12693" id="token-86-2" morph="none" pos="punct" start_char="12693">,</TOKEN>
<TOKEN end_char="12697" id="token-86-3" morph="none" pos="word" start_char="12695">Hua</TOKEN>
<TOKEN end_char="12706" id="token-86-4" morph="none" pos="word" start_char="12699">Chunying</TOKEN>
<TOKEN end_char="12707" id="token-86-5" morph="none" pos="punct" start_char="12707">,</TOKEN>
<TOKEN end_char="12712" id="token-86-6" morph="none" pos="word" start_char="12709">said</TOKEN>
<TOKEN end_char="12716" id="token-86-7" morph="none" pos="word" start_char="12714">she</TOKEN>
<TOKEN end_char="12720" id="token-86-8" morph="none" pos="word" start_char="12718">was</TOKEN>
<TOKEN end_char="12724" id="token-86-9" morph="none" pos="word" start_char="12722">not</TOKEN>
<TOKEN end_char="12726" id="token-86-10" morph="none" pos="word" start_char="12726">a</TOKEN>
<TOKEN end_char="12736" id="token-86-11" morph="none" pos="word" start_char="12728">scientist</TOKEN>
<TOKEN end_char="12737" id="token-86-12" morph="none" pos="punct" start_char="12737">,</TOKEN>
<TOKEN end_char="12741" id="token-86-13" morph="none" pos="word" start_char="12739">but</TOKEN>
<TOKEN end_char="12747" id="token-86-14" morph="none" pos="word" start_char="12743">found</TOKEN>
<TOKEN end_char="12751" id="token-86-15" morph="none" pos="word" start_char="12749">the</TOKEN>
<TOKEN end_char="12763" id="token-86-16" morph="none" pos="word" start_char="12753">conclusions</TOKEN>
<TOKEN end_char="12765" id="token-86-17" morph="none" pos="punct" start_char="12765">"</TOKEN>
<TOKEN end_char="12769" id="token-86-18" morph="none" pos="word" start_char="12766">very</TOKEN>
<TOKEN end_char="12781" id="token-86-19" morph="none" pos="unknown" start_char="12771">far-fetched</TOKEN>
<TOKEN end_char="12782" id="token-86-20" morph="none" pos="punct" start_char="12782">"</TOKEN>
<TOKEN end_char="12786" id="token-86-21" morph="none" pos="word" start_char="12784">and</TOKEN>
<TOKEN end_char="12791" id="token-86-22" morph="none" pos="word" start_char="12788">more</TOKEN>
<TOKEN end_char="12801" id="token-86-23" morph="none" pos="word" start_char="12793">generally</TOKEN>
<TOKEN end_char="12807" id="token-86-24" morph="none" pos="word" start_char="12803">urged</TOKEN>
<TOKEN end_char="12811" id="token-86-25" morph="none" pos="word" start_char="12809">the</TOKEN>
<TOKEN end_char="12815" id="token-86-26" morph="none" pos="unknown" start_char="12813">U.S</TOKEN>
<TOKEN end_char="12816" id="token-86-27" morph="none" pos="punct" start_char="12816">.</TOKEN>
<TOKEN end_char="12820" id="token-86-28" morph="none" pos="word" start_char="12818">and</TOKEN>
<TOKEN end_char="12826" id="token-86-29" morph="none" pos="word" start_char="12822">China</TOKEN>
<TOKEN end_char="12829" id="token-86-30" morph="none" pos="word" start_char="12828">to</TOKEN>
<TOKEN end_char="12834" id="token-86-31" morph="none" pos="word" start_char="12831">work</TOKEN>
<TOKEN end_char="12843" id="token-86-32" morph="none" pos="word" start_char="12836">together</TOKEN>
<TOKEN end_char="12846" id="token-86-33" morph="none" pos="word" start_char="12845">on</TOKEN>
<TOKEN end_char="12856" id="token-86-34" morph="none" pos="word" start_char="12848">combating</TOKEN>
<TOKEN end_char="12861" id="token-86-35" morph="none" pos="word" start_char="12858">what</TOKEN>
<TOKEN end_char="12865" id="token-86-36" morph="none" pos="word" start_char="12863">she</TOKEN>
<TOKEN end_char="12872" id="token-86-37" morph="none" pos="word" start_char="12867">called</TOKEN>
<TOKEN end_char="12878" id="token-86-38" morph="none" pos="word" start_char="12874">false</TOKEN>
<TOKEN end_char="12890" id="token-86-39" morph="none" pos="word" start_char="12880">information</TOKEN>
<TOKEN end_char="12891" id="token-86-40" morph="none" pos="punct" start_char="12891">.</TOKEN>
</SEG>
<SEG end_char="12960" id="segment-87" start_char="12894">
<ORIGINAL_TEXT>On the ground, internet searches for symptoms associated with COVID</ORIGINAL_TEXT>
<TOKEN end_char="12895" id="token-87-0" morph="none" pos="word" start_char="12894">On</TOKEN>
<TOKEN end_char="12899" id="token-87-1" morph="none" pos="word" start_char="12897">the</TOKEN>
<TOKEN end_char="12906" id="token-87-2" morph="none" pos="word" start_char="12901">ground</TOKEN>
<TOKEN end_char="12907" id="token-87-3" morph="none" pos="punct" start_char="12907">,</TOKEN>
<TOKEN end_char="12916" id="token-87-4" morph="none" pos="word" start_char="12909">internet</TOKEN>
<TOKEN end_char="12925" id="token-87-5" morph="none" pos="word" start_char="12918">searches</TOKEN>
<TOKEN end_char="12929" id="token-87-6" morph="none" pos="word" start_char="12927">for</TOKEN>
<TOKEN end_char="12938" id="token-87-7" morph="none" pos="word" start_char="12931">symptoms</TOKEN>
<TOKEN end_char="12949" id="token-87-8" morph="none" pos="word" start_char="12940">associated</TOKEN>
<TOKEN end_char="12954" id="token-87-9" morph="none" pos="word" start_char="12951">with</TOKEN>
<TOKEN end_char="12960" id="token-87-10" morph="none" pos="word" start_char="12956">COVID</TOKEN>
</SEG>
<SEG end_char="13104" id="segment-88" start_char="12963">
<ORIGINAL_TEXT>Brownstein said he and his researchers found the hospital-traffic data to be even more compelling after digging into internet search patterns.</ORIGINAL_TEXT>
<TOKEN end_char="12972" id="token-88-0" morph="none" pos="word" start_char="12963">Brownstein</TOKEN>
<TOKEN end_char="12977" id="token-88-1" morph="none" pos="word" start_char="12974">said</TOKEN>
<TOKEN end_char="12980" id="token-88-2" morph="none" pos="word" start_char="12979">he</TOKEN>
<TOKEN end_char="12984" id="token-88-3" morph="none" pos="word" start_char="12982">and</TOKEN>
<TOKEN end_char="12988" id="token-88-4" morph="none" pos="word" start_char="12986">his</TOKEN>
<TOKEN end_char="13000" id="token-88-5" morph="none" pos="word" start_char="12990">researchers</TOKEN>
<TOKEN end_char="13006" id="token-88-6" morph="none" pos="word" start_char="13002">found</TOKEN>
<TOKEN end_char="13010" id="token-88-7" morph="none" pos="word" start_char="13008">the</TOKEN>
<TOKEN end_char="13027" id="token-88-8" morph="none" pos="unknown" start_char="13012">hospital-traffic</TOKEN>
<TOKEN end_char="13032" id="token-88-9" morph="none" pos="word" start_char="13029">data</TOKEN>
<TOKEN end_char="13035" id="token-88-10" morph="none" pos="word" start_char="13034">to</TOKEN>
<TOKEN end_char="13038" id="token-88-11" morph="none" pos="word" start_char="13037">be</TOKEN>
<TOKEN end_char="13043" id="token-88-12" morph="none" pos="word" start_char="13040">even</TOKEN>
<TOKEN end_char="13048" id="token-88-13" morph="none" pos="word" start_char="13045">more</TOKEN>
<TOKEN end_char="13059" id="token-88-14" morph="none" pos="word" start_char="13050">compelling</TOKEN>
<TOKEN end_char="13065" id="token-88-15" morph="none" pos="word" start_char="13061">after</TOKEN>
<TOKEN end_char="13073" id="token-88-16" morph="none" pos="word" start_char="13067">digging</TOKEN>
<TOKEN end_char="13078" id="token-88-17" morph="none" pos="word" start_char="13075">into</TOKEN>
<TOKEN end_char="13087" id="token-88-18" morph="none" pos="word" start_char="13080">internet</TOKEN>
<TOKEN end_char="13094" id="token-88-19" morph="none" pos="word" start_char="13089">search</TOKEN>
<TOKEN end_char="13103" id="token-88-20" morph="none" pos="word" start_char="13096">patterns</TOKEN>
<TOKEN end_char="13104" id="token-88-21" morph="none" pos="punct" start_char="13104">.</TOKEN>
</SEG>
<SEG end_char="13300" id="segment-89" start_char="13106">
<ORIGINAL_TEXT>Around the time the hospital traffic was surging, there was a spike in online traffic in the Wuhan region among users asking China’s Baidu search engine for information on "cough" and "diarrhea."</ORIGINAL_TEXT>
<TOKEN end_char="13111" id="token-89-0" morph="none" pos="word" start_char="13106">Around</TOKEN>
<TOKEN end_char="13115" id="token-89-1" morph="none" pos="word" start_char="13113">the</TOKEN>
<TOKEN end_char="13120" id="token-89-2" morph="none" pos="word" start_char="13117">time</TOKEN>
<TOKEN end_char="13124" id="token-89-3" morph="none" pos="word" start_char="13122">the</TOKEN>
<TOKEN end_char="13133" id="token-89-4" morph="none" pos="word" start_char="13126">hospital</TOKEN>
<TOKEN end_char="13141" id="token-89-5" morph="none" pos="word" start_char="13135">traffic</TOKEN>
<TOKEN end_char="13145" id="token-89-6" morph="none" pos="word" start_char="13143">was</TOKEN>
<TOKEN end_char="13153" id="token-89-7" morph="none" pos="word" start_char="13147">surging</TOKEN>
<TOKEN end_char="13154" id="token-89-8" morph="none" pos="punct" start_char="13154">,</TOKEN>
<TOKEN end_char="13160" id="token-89-9" morph="none" pos="word" start_char="13156">there</TOKEN>
<TOKEN end_char="13164" id="token-89-10" morph="none" pos="word" start_char="13162">was</TOKEN>
<TOKEN end_char="13166" id="token-89-11" morph="none" pos="word" start_char="13166">a</TOKEN>
<TOKEN end_char="13172" id="token-89-12" morph="none" pos="word" start_char="13168">spike</TOKEN>
<TOKEN end_char="13175" id="token-89-13" morph="none" pos="word" start_char="13174">in</TOKEN>
<TOKEN end_char="13182" id="token-89-14" morph="none" pos="word" start_char="13177">online</TOKEN>
<TOKEN end_char="13190" id="token-89-15" morph="none" pos="word" start_char="13184">traffic</TOKEN>
<TOKEN end_char="13193" id="token-89-16" morph="none" pos="word" start_char="13192">in</TOKEN>
<TOKEN end_char="13197" id="token-89-17" morph="none" pos="word" start_char="13195">the</TOKEN>
<TOKEN end_char="13203" id="token-89-18" morph="none" pos="word" start_char="13199">Wuhan</TOKEN>
<TOKEN end_char="13210" id="token-89-19" morph="none" pos="word" start_char="13205">region</TOKEN>
<TOKEN end_char="13216" id="token-89-20" morph="none" pos="word" start_char="13212">among</TOKEN>
<TOKEN end_char="13222" id="token-89-21" morph="none" pos="word" start_char="13218">users</TOKEN>
<TOKEN end_char="13229" id="token-89-22" morph="none" pos="word" start_char="13224">asking</TOKEN>
<TOKEN end_char="13237" id="token-89-23" morph="none" pos="word" start_char="13231">China’s</TOKEN>
<TOKEN end_char="13243" id="token-89-24" morph="none" pos="word" start_char="13239">Baidu</TOKEN>
<TOKEN end_char="13250" id="token-89-25" morph="none" pos="word" start_char="13245">search</TOKEN>
<TOKEN end_char="13257" id="token-89-26" morph="none" pos="word" start_char="13252">engine</TOKEN>
<TOKEN end_char="13261" id="token-89-27" morph="none" pos="word" start_char="13259">for</TOKEN>
<TOKEN end_char="13273" id="token-89-28" morph="none" pos="word" start_char="13263">information</TOKEN>
<TOKEN end_char="13276" id="token-89-29" morph="none" pos="word" start_char="13275">on</TOKEN>
<TOKEN end_char="13278" id="token-89-30" morph="none" pos="punct" start_char="13278">"</TOKEN>
<TOKEN end_char="13283" id="token-89-31" morph="none" pos="word" start_char="13279">cough</TOKEN>
<TOKEN end_char="13284" id="token-89-32" morph="none" pos="punct" start_char="13284">"</TOKEN>
<TOKEN end_char="13288" id="token-89-33" morph="none" pos="word" start_char="13286">and</TOKEN>
<TOKEN end_char="13290" id="token-89-34" morph="none" pos="punct" start_char="13290">"</TOKEN>
<TOKEN end_char="13298" id="token-89-35" morph="none" pos="word" start_char="13291">diarrhea</TOKEN>
<TOKEN end_char="13300" id="token-89-36" morph="none" pos="punct" start_char="13299">."</TOKEN>
</SEG>
<SEG end_char="13549" id="segment-90" start_char="13303">
<ORIGINAL_TEXT>"While queries of the respiratory symptom ‘cough’ show seasonal fluctuations coinciding with yearly influenza seasons, ‘diarrhea’ is a more COVID-19-specific symptom and only shows an association with the current epidemic," according to the study.</ORIGINAL_TEXT>
<TOKEN end_char="13303" id="token-90-0" morph="none" pos="punct" start_char="13303">"</TOKEN>
<TOKEN end_char="13308" id="token-90-1" morph="none" pos="word" start_char="13304">While</TOKEN>
<TOKEN end_char="13316" id="token-90-2" morph="none" pos="word" start_char="13310">queries</TOKEN>
<TOKEN end_char="13319" id="token-90-3" morph="none" pos="word" start_char="13318">of</TOKEN>
<TOKEN end_char="13323" id="token-90-4" morph="none" pos="word" start_char="13321">the</TOKEN>
<TOKEN end_char="13335" id="token-90-5" morph="none" pos="word" start_char="13325">respiratory</TOKEN>
<TOKEN end_char="13343" id="token-90-6" morph="none" pos="word" start_char="13337">symptom</TOKEN>
<TOKEN end_char="13345" id="token-90-7" morph="none" pos="punct" start_char="13345">‘</TOKEN>
<TOKEN end_char="13350" id="token-90-8" morph="none" pos="word" start_char="13346">cough</TOKEN>
<TOKEN end_char="13351" id="token-90-9" morph="none" pos="punct" start_char="13351">’</TOKEN>
<TOKEN end_char="13356" id="token-90-10" morph="none" pos="word" start_char="13353">show</TOKEN>
<TOKEN end_char="13365" id="token-90-11" morph="none" pos="word" start_char="13358">seasonal</TOKEN>
<TOKEN end_char="13378" id="token-90-12" morph="none" pos="word" start_char="13367">fluctuations</TOKEN>
<TOKEN end_char="13389" id="token-90-13" morph="none" pos="word" start_char="13380">coinciding</TOKEN>
<TOKEN end_char="13394" id="token-90-14" morph="none" pos="word" start_char="13391">with</TOKEN>
<TOKEN end_char="13401" id="token-90-15" morph="none" pos="word" start_char="13396">yearly</TOKEN>
<TOKEN end_char="13411" id="token-90-16" morph="none" pos="word" start_char="13403">influenza</TOKEN>
<TOKEN end_char="13419" id="token-90-17" morph="none" pos="word" start_char="13413">seasons</TOKEN>
<TOKEN end_char="13420" id="token-90-18" morph="none" pos="punct" start_char="13420">,</TOKEN>
<TOKEN end_char="13422" id="token-90-19" morph="none" pos="punct" start_char="13422">‘</TOKEN>
<TOKEN end_char="13430" id="token-90-20" morph="none" pos="word" start_char="13423">diarrhea</TOKEN>
<TOKEN end_char="13431" id="token-90-21" morph="none" pos="punct" start_char="13431">’</TOKEN>
<TOKEN end_char="13434" id="token-90-22" morph="none" pos="word" start_char="13433">is</TOKEN>
<TOKEN end_char="13436" id="token-90-23" morph="none" pos="word" start_char="13436">a</TOKEN>
<TOKEN end_char="13441" id="token-90-24" morph="none" pos="word" start_char="13438">more</TOKEN>
<TOKEN end_char="13459" id="token-90-25" morph="none" pos="unknown" start_char="13443">COVID-19-specific</TOKEN>
<TOKEN end_char="13467" id="token-90-26" morph="none" pos="word" start_char="13461">symptom</TOKEN>
<TOKEN end_char="13471" id="token-90-27" morph="none" pos="word" start_char="13469">and</TOKEN>
<TOKEN end_char="13476" id="token-90-28" morph="none" pos="word" start_char="13473">only</TOKEN>
<TOKEN end_char="13482" id="token-90-29" morph="none" pos="word" start_char="13478">shows</TOKEN>
<TOKEN end_char="13485" id="token-90-30" morph="none" pos="word" start_char="13484">an</TOKEN>
<TOKEN end_char="13497" id="token-90-31" morph="none" pos="word" start_char="13487">association</TOKEN>
<TOKEN end_char="13502" id="token-90-32" morph="none" pos="word" start_char="13499">with</TOKEN>
<TOKEN end_char="13506" id="token-90-33" morph="none" pos="word" start_char="13504">the</TOKEN>
<TOKEN end_char="13514" id="token-90-34" morph="none" pos="word" start_char="13508">current</TOKEN>
<TOKEN end_char="13523" id="token-90-35" morph="none" pos="word" start_char="13516">epidemic</TOKEN>
<TOKEN end_char="13525" id="token-90-36" morph="none" pos="punct" start_char="13524">,"</TOKEN>
<TOKEN end_char="13535" id="token-90-37" morph="none" pos="word" start_char="13527">according</TOKEN>
<TOKEN end_char="13538" id="token-90-38" morph="none" pos="word" start_char="13537">to</TOKEN>
<TOKEN end_char="13542" id="token-90-39" morph="none" pos="word" start_char="13540">the</TOKEN>
<TOKEN end_char="13548" id="token-90-40" morph="none" pos="word" start_char="13544">study</TOKEN>
<TOKEN end_char="13549" id="token-90-41" morph="none" pos="punct" start_char="13549">.</TOKEN>
</SEG>
<SEG end_char="13647" id="segment-91" start_char="13551">
<ORIGINAL_TEXT>"The increase of both signals precede the documented start of the COVID-19 pandemic in December."</ORIGINAL_TEXT>
<TOKEN end_char="13551" id="token-91-0" morph="none" pos="punct" start_char="13551">"</TOKEN>
<TOKEN end_char="13554" id="token-91-1" morph="none" pos="word" start_char="13552">The</TOKEN>
<TOKEN end_char="13563" id="token-91-2" morph="none" pos="word" start_char="13556">increase</TOKEN>
<TOKEN end_char="13566" id="token-91-3" morph="none" pos="word" start_char="13565">of</TOKEN>
<TOKEN end_char="13571" id="token-91-4" morph="none" pos="word" start_char="13568">both</TOKEN>
<TOKEN end_char="13579" id="token-91-5" morph="none" pos="word" start_char="13573">signals</TOKEN>
<TOKEN end_char="13587" id="token-91-6" morph="none" pos="word" start_char="13581">precede</TOKEN>
<TOKEN end_char="13591" id="token-91-7" morph="none" pos="word" start_char="13589">the</TOKEN>
<TOKEN end_char="13602" id="token-91-8" morph="none" pos="word" start_char="13593">documented</TOKEN>
<TOKEN end_char="13608" id="token-91-9" morph="none" pos="word" start_char="13604">start</TOKEN>
<TOKEN end_char="13611" id="token-91-10" morph="none" pos="word" start_char="13610">of</TOKEN>
<TOKEN end_char="13615" id="token-91-11" morph="none" pos="word" start_char="13613">the</TOKEN>
<TOKEN end_char="13624" id="token-91-12" morph="none" pos="unknown" start_char="13617">COVID-19</TOKEN>
<TOKEN end_char="13633" id="token-91-13" morph="none" pos="word" start_char="13626">pandemic</TOKEN>
<TOKEN end_char="13636" id="token-91-14" morph="none" pos="word" start_char="13635">in</TOKEN>
<TOKEN end_char="13645" id="token-91-15" morph="none" pos="word" start_char="13638">December</TOKEN>
<TOKEN end_char="13647" id="token-91-16" morph="none" pos="punct" start_char="13646">."</TOKEN>
</SEG>
<SEG end_char="13796" id="segment-92" start_char="13650">
<ORIGINAL_TEXT>"We've done previous studies where we could show that what people search for online is an indicator of disease in the population," Brownstein said.</ORIGINAL_TEXT>
<TOKEN end_char="13650" id="token-92-0" morph="none" pos="punct" start_char="13650">"</TOKEN>
<TOKEN end_char="13655" id="token-92-1" morph="none" pos="word" start_char="13651">We've</TOKEN>
<TOKEN end_char="13660" id="token-92-2" morph="none" pos="word" start_char="13657">done</TOKEN>
<TOKEN end_char="13669" id="token-92-3" morph="none" pos="word" start_char="13662">previous</TOKEN>
<TOKEN end_char="13677" id="token-92-4" morph="none" pos="word" start_char="13671">studies</TOKEN>
<TOKEN end_char="13683" id="token-92-5" morph="none" pos="word" start_char="13679">where</TOKEN>
<TOKEN end_char="13686" id="token-92-6" morph="none" pos="word" start_char="13685">we</TOKEN>
<TOKEN end_char="13692" id="token-92-7" morph="none" pos="word" start_char="13688">could</TOKEN>
<TOKEN end_char="13697" id="token-92-8" morph="none" pos="word" start_char="13694">show</TOKEN>
<TOKEN end_char="13702" id="token-92-9" morph="none" pos="word" start_char="13699">that</TOKEN>
<TOKEN end_char="13707" id="token-92-10" morph="none" pos="word" start_char="13704">what</TOKEN>
<TOKEN end_char="13714" id="token-92-11" morph="none" pos="word" start_char="13709">people</TOKEN>
<TOKEN end_char="13721" id="token-92-12" morph="none" pos="word" start_char="13716">search</TOKEN>
<TOKEN end_char="13725" id="token-92-13" morph="none" pos="word" start_char="13723">for</TOKEN>
<TOKEN end_char="13732" id="token-92-14" morph="none" pos="word" start_char="13727">online</TOKEN>
<TOKEN end_char="13735" id="token-92-15" morph="none" pos="word" start_char="13734">is</TOKEN>
<TOKEN end_char="13738" id="token-92-16" morph="none" pos="word" start_char="13737">an</TOKEN>
<TOKEN end_char="13748" id="token-92-17" morph="none" pos="word" start_char="13740">indicator</TOKEN>
<TOKEN end_char="13751" id="token-92-18" morph="none" pos="word" start_char="13750">of</TOKEN>
<TOKEN end_char="13759" id="token-92-19" morph="none" pos="word" start_char="13753">disease</TOKEN>
<TOKEN end_char="13762" id="token-92-20" morph="none" pos="word" start_char="13761">in</TOKEN>
<TOKEN end_char="13766" id="token-92-21" morph="none" pos="word" start_char="13764">the</TOKEN>
<TOKEN end_char="13777" id="token-92-22" morph="none" pos="word" start_char="13768">population</TOKEN>
<TOKEN end_char="13779" id="token-92-23" morph="none" pos="punct" start_char="13778">,"</TOKEN>
<TOKEN end_char="13790" id="token-92-24" morph="none" pos="word" start_char="13781">Brownstein</TOKEN>
<TOKEN end_char="13795" id="token-92-25" morph="none" pos="word" start_char="13792">said</TOKEN>
<TOKEN end_char="13796" id="token-92-26" morph="none" pos="punct" start_char="13796">.</TOKEN>
</SEG>
<SEG end_char="13905" id="segment-93" start_char="13798">
<ORIGINAL_TEXT>"And we actually saw people searching for symptoms that might be related to COVID: diarrheal disease, cough.</ORIGINAL_TEXT>
<TOKEN end_char="13798" id="token-93-0" morph="none" pos="punct" start_char="13798">"</TOKEN>
<TOKEN end_char="13801" id="token-93-1" morph="none" pos="word" start_char="13799">And</TOKEN>
<TOKEN end_char="13804" id="token-93-2" morph="none" pos="word" start_char="13803">we</TOKEN>
<TOKEN end_char="13813" id="token-93-3" morph="none" pos="word" start_char="13806">actually</TOKEN>
<TOKEN end_char="13817" id="token-93-4" morph="none" pos="word" start_char="13815">saw</TOKEN>
<TOKEN end_char="13824" id="token-93-5" morph="none" pos="word" start_char="13819">people</TOKEN>
<TOKEN end_char="13834" id="token-93-6" morph="none" pos="word" start_char="13826">searching</TOKEN>
<TOKEN end_char="13838" id="token-93-7" morph="none" pos="word" start_char="13836">for</TOKEN>
<TOKEN end_char="13847" id="token-93-8" morph="none" pos="word" start_char="13840">symptoms</TOKEN>
<TOKEN end_char="13852" id="token-93-9" morph="none" pos="word" start_char="13849">that</TOKEN>
<TOKEN end_char="13858" id="token-93-10" morph="none" pos="word" start_char="13854">might</TOKEN>
<TOKEN end_char="13861" id="token-93-11" morph="none" pos="word" start_char="13860">be</TOKEN>
<TOKEN end_char="13869" id="token-93-12" morph="none" pos="word" start_char="13863">related</TOKEN>
<TOKEN end_char="13872" id="token-93-13" morph="none" pos="word" start_char="13871">to</TOKEN>
<TOKEN end_char="13878" id="token-93-14" morph="none" pos="word" start_char="13874">COVID</TOKEN>
<TOKEN end_char="13879" id="token-93-15" morph="none" pos="punct" start_char="13879">:</TOKEN>
<TOKEN end_char="13889" id="token-93-16" morph="none" pos="word" start_char="13881">diarrheal</TOKEN>
<TOKEN end_char="13897" id="token-93-17" morph="none" pos="word" start_char="13891">disease</TOKEN>
<TOKEN end_char="13898" id="token-93-18" morph="none" pos="punct" start_char="13898">,</TOKEN>
<TOKEN end_char="13904" id="token-93-19" morph="none" pos="word" start_char="13900">cough</TOKEN>
<TOKEN end_char="13905" id="token-93-20" morph="none" pos="punct" start_char="13905">.</TOKEN>
</SEG>
<SEG end_char="13953" id="segment-94" start_char="13907">
<ORIGINAL_TEXT>That was even starting as early as late summer.</ORIGINAL_TEXT>
<TOKEN end_char="13910" id="token-94-0" morph="none" pos="word" start_char="13907">That</TOKEN>
<TOKEN end_char="13914" id="token-94-1" morph="none" pos="word" start_char="13912">was</TOKEN>
<TOKEN end_char="13919" id="token-94-2" morph="none" pos="word" start_char="13916">even</TOKEN>
<TOKEN end_char="13928" id="token-94-3" morph="none" pos="word" start_char="13921">starting</TOKEN>
<TOKEN end_char="13931" id="token-94-4" morph="none" pos="word" start_char="13930">as</TOKEN>
<TOKEN end_char="13937" id="token-94-5" morph="none" pos="word" start_char="13933">early</TOKEN>
<TOKEN end_char="13940" id="token-94-6" morph="none" pos="word" start_char="13939">as</TOKEN>
<TOKEN end_char="13945" id="token-94-7" morph="none" pos="word" start_char="13942">late</TOKEN>
<TOKEN end_char="13952" id="token-94-8" morph="none" pos="word" start_char="13947">summer</TOKEN>
<TOKEN end_char="13953" id="token-94-9" morph="none" pos="punct" start_char="13953">.</TOKEN>
</SEG>
<SEG end_char="14098" id="segment-95" start_char="13956">
<ORIGINAL_TEXT>"Now, we can't confirm 100% what the virus was that was causing this illness and what was causing this business in hospitals," Brownstein said.</ORIGINAL_TEXT>
<TOKEN end_char="13956" id="token-95-0" morph="none" pos="punct" start_char="13956">"</TOKEN>
<TOKEN end_char="13959" id="token-95-1" morph="none" pos="word" start_char="13957">Now</TOKEN>
<TOKEN end_char="13960" id="token-95-2" morph="none" pos="punct" start_char="13960">,</TOKEN>
<TOKEN end_char="13963" id="token-95-3" morph="none" pos="word" start_char="13962">we</TOKEN>
<TOKEN end_char="13969" id="token-95-4" morph="none" pos="word" start_char="13965">can't</TOKEN>
<TOKEN end_char="13977" id="token-95-5" morph="none" pos="word" start_char="13971">confirm</TOKEN>
<TOKEN end_char="13981" id="token-95-6" morph="none" pos="word" start_char="13979">100</TOKEN>
<TOKEN end_char="13982" id="token-95-7" morph="none" pos="punct" start_char="13982">%</TOKEN>
<TOKEN end_char="13987" id="token-95-8" morph="none" pos="word" start_char="13984">what</TOKEN>
<TOKEN end_char="13991" id="token-95-9" morph="none" pos="word" start_char="13989">the</TOKEN>
<TOKEN end_char="13997" id="token-95-10" morph="none" pos="word" start_char="13993">virus</TOKEN>
<TOKEN end_char="14001" id="token-95-11" morph="none" pos="word" start_char="13999">was</TOKEN>
<TOKEN end_char="14006" id="token-95-12" morph="none" pos="word" start_char="14003">that</TOKEN>
<TOKEN end_char="14010" id="token-95-13" morph="none" pos="word" start_char="14008">was</TOKEN>
<TOKEN end_char="14018" id="token-95-14" morph="none" pos="word" start_char="14012">causing</TOKEN>
<TOKEN end_char="14023" id="token-95-15" morph="none" pos="word" start_char="14020">this</TOKEN>
<TOKEN end_char="14031" id="token-95-16" morph="none" pos="word" start_char="14025">illness</TOKEN>
<TOKEN end_char="14035" id="token-95-17" morph="none" pos="word" start_char="14033">and</TOKEN>
<TOKEN end_char="14040" id="token-95-18" morph="none" pos="word" start_char="14037">what</TOKEN>
<TOKEN end_char="14044" id="token-95-19" morph="none" pos="word" start_char="14042">was</TOKEN>
<TOKEN end_char="14052" id="token-95-20" morph="none" pos="word" start_char="14046">causing</TOKEN>
<TOKEN end_char="14057" id="token-95-21" morph="none" pos="word" start_char="14054">this</TOKEN>
<TOKEN end_char="14066" id="token-95-22" morph="none" pos="word" start_char="14059">business</TOKEN>
<TOKEN end_char="14069" id="token-95-23" morph="none" pos="word" start_char="14068">in</TOKEN>
<TOKEN end_char="14079" id="token-95-24" morph="none" pos="word" start_char="14071">hospitals</TOKEN>
<TOKEN end_char="14081" id="token-95-25" morph="none" pos="punct" start_char="14080">,"</TOKEN>
<TOKEN end_char="14092" id="token-95-26" morph="none" pos="word" start_char="14083">Brownstein</TOKEN>
<TOKEN end_char="14097" id="token-95-27" morph="none" pos="word" start_char="14094">said</TOKEN>
<TOKEN end_char="14098" id="token-95-28" morph="none" pos="punct" start_char="14098">.</TOKEN>
</SEG>
<SEG end_char="14197" id="segment-96" start_char="14100">
<ORIGINAL_TEXT>"But something was going on that looked very different than any other time that we had looked at."</ORIGINAL_TEXT>
<TOKEN end_char="14100" id="token-96-0" morph="none" pos="punct" start_char="14100">"</TOKEN>
<TOKEN end_char="14103" id="token-96-1" morph="none" pos="word" start_char="14101">But</TOKEN>
<TOKEN end_char="14113" id="token-96-2" morph="none" pos="word" start_char="14105">something</TOKEN>
<TOKEN end_char="14117" id="token-96-3" morph="none" pos="word" start_char="14115">was</TOKEN>
<TOKEN end_char="14123" id="token-96-4" morph="none" pos="word" start_char="14119">going</TOKEN>
<TOKEN end_char="14126" id="token-96-5" morph="none" pos="word" start_char="14125">on</TOKEN>
<TOKEN end_char="14131" id="token-96-6" morph="none" pos="word" start_char="14128">that</TOKEN>
<TOKEN end_char="14138" id="token-96-7" morph="none" pos="word" start_char="14133">looked</TOKEN>
<TOKEN end_char="14143" id="token-96-8" morph="none" pos="word" start_char="14140">very</TOKEN>
<TOKEN end_char="14153" id="token-96-9" morph="none" pos="word" start_char="14145">different</TOKEN>
<TOKEN end_char="14158" id="token-96-10" morph="none" pos="word" start_char="14155">than</TOKEN>
<TOKEN end_char="14162" id="token-96-11" morph="none" pos="word" start_char="14160">any</TOKEN>
<TOKEN end_char="14168" id="token-96-12" morph="none" pos="word" start_char="14164">other</TOKEN>
<TOKEN end_char="14173" id="token-96-13" morph="none" pos="word" start_char="14170">time</TOKEN>
<TOKEN end_char="14178" id="token-96-14" morph="none" pos="word" start_char="14175">that</TOKEN>
<TOKEN end_char="14181" id="token-96-15" morph="none" pos="word" start_char="14180">we</TOKEN>
<TOKEN end_char="14185" id="token-96-16" morph="none" pos="word" start_char="14183">had</TOKEN>
<TOKEN end_char="14192" id="token-96-17" morph="none" pos="word" start_char="14187">looked</TOKEN>
<TOKEN end_char="14195" id="token-96-18" morph="none" pos="word" start_char="14194">at</TOKEN>
<TOKEN end_char="14197" id="token-96-19" morph="none" pos="punct" start_char="14196">."</TOKEN>
</SEG>
<SEG end_char="14368" id="segment-97" start_char="14200">
<ORIGINAL_TEXT>Brownstein and his research team used satellite imagery in 2015 to investigate how health care systems could predict outbreaks of influenza-like illnesses as they occur.</ORIGINAL_TEXT>
<TOKEN end_char="14209" id="token-97-0" morph="none" pos="word" start_char="14200">Brownstein</TOKEN>
<TOKEN end_char="14213" id="token-97-1" morph="none" pos="word" start_char="14211">and</TOKEN>
<TOKEN end_char="14217" id="token-97-2" morph="none" pos="word" start_char="14215">his</TOKEN>
<TOKEN end_char="14226" id="token-97-3" morph="none" pos="word" start_char="14219">research</TOKEN>
<TOKEN end_char="14231" id="token-97-4" morph="none" pos="word" start_char="14228">team</TOKEN>
<TOKEN end_char="14236" id="token-97-5" morph="none" pos="word" start_char="14233">used</TOKEN>
<TOKEN end_char="14246" id="token-97-6" morph="none" pos="word" start_char="14238">satellite</TOKEN>
<TOKEN end_char="14254" id="token-97-7" morph="none" pos="word" start_char="14248">imagery</TOKEN>
<TOKEN end_char="14257" id="token-97-8" morph="none" pos="word" start_char="14256">in</TOKEN>
<TOKEN end_char="14262" id="token-97-9" morph="none" pos="word" start_char="14259">2015</TOKEN>
<TOKEN end_char="14265" id="token-97-10" morph="none" pos="word" start_char="14264">to</TOKEN>
<TOKEN end_char="14277" id="token-97-11" morph="none" pos="word" start_char="14267">investigate</TOKEN>
<TOKEN end_char="14281" id="token-97-12" morph="none" pos="word" start_char="14279">how</TOKEN>
<TOKEN end_char="14288" id="token-97-13" morph="none" pos="word" start_char="14283">health</TOKEN>
<TOKEN end_char="14293" id="token-97-14" morph="none" pos="word" start_char="14290">care</TOKEN>
<TOKEN end_char="14301" id="token-97-15" morph="none" pos="word" start_char="14295">systems</TOKEN>
<TOKEN end_char="14307" id="token-97-16" morph="none" pos="word" start_char="14303">could</TOKEN>
<TOKEN end_char="14315" id="token-97-17" morph="none" pos="word" start_char="14309">predict</TOKEN>
<TOKEN end_char="14325" id="token-97-18" morph="none" pos="word" start_char="14317">outbreaks</TOKEN>
<TOKEN end_char="14328" id="token-97-19" morph="none" pos="word" start_char="14327">of</TOKEN>
<TOKEN end_char="14343" id="token-97-20" morph="none" pos="unknown" start_char="14330">influenza-like</TOKEN>
<TOKEN end_char="14353" id="token-97-21" morph="none" pos="word" start_char="14345">illnesses</TOKEN>
<TOKEN end_char="14356" id="token-97-22" morph="none" pos="word" start_char="14355">as</TOKEN>
<TOKEN end_char="14361" id="token-97-23" morph="none" pos="word" start_char="14358">they</TOKEN>
<TOKEN end_char="14367" id="token-97-24" morph="none" pos="word" start_char="14363">occur</TOKEN>
<TOKEN end_char="14368" id="token-97-25" morph="none" pos="punct" start_char="14368">.</TOKEN>
</SEG>
<SEG end_char="14645" id="segment-98" start_char="14371">
<ORIGINAL_TEXT>"We previously validated this method of indirectly measuring disease activity by monitoring hospital parking lot usage in Chile, Argentina and Mexico," said researcher Elaine Nsoesie, a global health professor at Boston University who worked with Brownstein on both projects.</ORIGINAL_TEXT>
<TOKEN end_char="14371" id="token-98-0" morph="none" pos="punct" start_char="14371">"</TOKEN>
<TOKEN end_char="14373" id="token-98-1" morph="none" pos="word" start_char="14372">We</TOKEN>
<TOKEN end_char="14384" id="token-98-2" morph="none" pos="word" start_char="14375">previously</TOKEN>
<TOKEN end_char="14394" id="token-98-3" morph="none" pos="word" start_char="14386">validated</TOKEN>
<TOKEN end_char="14399" id="token-98-4" morph="none" pos="word" start_char="14396">this</TOKEN>
<TOKEN end_char="14406" id="token-98-5" morph="none" pos="word" start_char="14401">method</TOKEN>
<TOKEN end_char="14409" id="token-98-6" morph="none" pos="word" start_char="14408">of</TOKEN>
<TOKEN end_char="14420" id="token-98-7" morph="none" pos="word" start_char="14411">indirectly</TOKEN>
<TOKEN end_char="14430" id="token-98-8" morph="none" pos="word" start_char="14422">measuring</TOKEN>
<TOKEN end_char="14438" id="token-98-9" morph="none" pos="word" start_char="14432">disease</TOKEN>
<TOKEN end_char="14447" id="token-98-10" morph="none" pos="word" start_char="14440">activity</TOKEN>
<TOKEN end_char="14450" id="token-98-11" morph="none" pos="word" start_char="14449">by</TOKEN>
<TOKEN end_char="14461" id="token-98-12" morph="none" pos="word" start_char="14452">monitoring</TOKEN>
<TOKEN end_char="14470" id="token-98-13" morph="none" pos="word" start_char="14463">hospital</TOKEN>
<TOKEN end_char="14478" id="token-98-14" morph="none" pos="word" start_char="14472">parking</TOKEN>
<TOKEN end_char="14482" id="token-98-15" morph="none" pos="word" start_char="14480">lot</TOKEN>
<TOKEN end_char="14488" id="token-98-16" morph="none" pos="word" start_char="14484">usage</TOKEN>
<TOKEN end_char="14491" id="token-98-17" morph="none" pos="word" start_char="14490">in</TOKEN>
<TOKEN end_char="14497" id="token-98-18" morph="none" pos="word" start_char="14493">Chile</TOKEN>
<TOKEN end_char="14498" id="token-98-19" morph="none" pos="punct" start_char="14498">,</TOKEN>
<TOKEN end_char="14508" id="token-98-20" morph="none" pos="word" start_char="14500">Argentina</TOKEN>
<TOKEN end_char="14512" id="token-98-21" morph="none" pos="word" start_char="14510">and</TOKEN>
<TOKEN end_char="14519" id="token-98-22" morph="none" pos="word" start_char="14514">Mexico</TOKEN>
<TOKEN end_char="14521" id="token-98-23" morph="none" pos="punct" start_char="14520">,"</TOKEN>
<TOKEN end_char="14526" id="token-98-24" morph="none" pos="word" start_char="14523">said</TOKEN>
<TOKEN end_char="14537" id="token-98-25" morph="none" pos="word" start_char="14528">researcher</TOKEN>
<TOKEN end_char="14544" id="token-98-26" morph="none" pos="word" start_char="14539">Elaine</TOKEN>
<TOKEN end_char="14552" id="token-98-27" morph="none" pos="word" start_char="14546">Nsoesie</TOKEN>
<TOKEN end_char="14553" id="token-98-28" morph="none" pos="punct" start_char="14553">,</TOKEN>
<TOKEN end_char="14555" id="token-98-29" morph="none" pos="word" start_char="14555">a</TOKEN>
<TOKEN end_char="14562" id="token-98-30" morph="none" pos="word" start_char="14557">global</TOKEN>
<TOKEN end_char="14569" id="token-98-31" morph="none" pos="word" start_char="14564">health</TOKEN>
<TOKEN end_char="14579" id="token-98-32" morph="none" pos="word" start_char="14571">professor</TOKEN>
<TOKEN end_char="14582" id="token-98-33" morph="none" pos="word" start_char="14581">at</TOKEN>
<TOKEN end_char="14589" id="token-98-34" morph="none" pos="word" start_char="14584">Boston</TOKEN>
<TOKEN end_char="14600" id="token-98-35" morph="none" pos="word" start_char="14591">University</TOKEN>
<TOKEN end_char="14604" id="token-98-36" morph="none" pos="word" start_char="14602">who</TOKEN>
<TOKEN end_char="14611" id="token-98-37" morph="none" pos="word" start_char="14606">worked</TOKEN>
<TOKEN end_char="14616" id="token-98-38" morph="none" pos="word" start_char="14613">with</TOKEN>
<TOKEN end_char="14627" id="token-98-39" morph="none" pos="word" start_char="14618">Brownstein</TOKEN>
<TOKEN end_char="14630" id="token-98-40" morph="none" pos="word" start_char="14629">on</TOKEN>
<TOKEN end_char="14635" id="token-98-41" morph="none" pos="word" start_char="14632">both</TOKEN>
<TOKEN end_char="14644" id="token-98-42" morph="none" pos="word" start_char="14637">projects</TOKEN>
<TOKEN end_char="14645" id="token-98-43" morph="none" pos="punct" start_char="14645">.</TOKEN>
</SEG>
<SEG end_char="14743" id="segment-99" start_char="14647">
<ORIGINAL_TEXT>"Using the data, we were able to forecast trends in influenza-like illnesses over several years."</ORIGINAL_TEXT>
<TOKEN end_char="14647" id="token-99-0" morph="none" pos="punct" start_char="14647">"</TOKEN>
<TOKEN end_char="14652" id="token-99-1" morph="none" pos="word" start_char="14648">Using</TOKEN>
<TOKEN end_char="14656" id="token-99-2" morph="none" pos="word" start_char="14654">the</TOKEN>
<TOKEN end_char="14661" id="token-99-3" morph="none" pos="word" start_char="14658">data</TOKEN>
<TOKEN end_char="14662" id="token-99-4" morph="none" pos="punct" start_char="14662">,</TOKEN>
<TOKEN end_char="14665" id="token-99-5" morph="none" pos="word" start_char="14664">we</TOKEN>
<TOKEN end_char="14670" id="token-99-6" morph="none" pos="word" start_char="14667">were</TOKEN>
<TOKEN end_char="14675" id="token-99-7" morph="none" pos="word" start_char="14672">able</TOKEN>
<TOKEN end_char="14678" id="token-99-8" morph="none" pos="word" start_char="14677">to</TOKEN>
<TOKEN end_char="14687" id="token-99-9" morph="none" pos="word" start_char="14680">forecast</TOKEN>
<TOKEN end_char="14694" id="token-99-10" morph="none" pos="word" start_char="14689">trends</TOKEN>
<TOKEN end_char="14697" id="token-99-11" morph="none" pos="word" start_char="14696">in</TOKEN>
<TOKEN end_char="14712" id="token-99-12" morph="none" pos="unknown" start_char="14699">influenza-like</TOKEN>
<TOKEN end_char="14722" id="token-99-13" morph="none" pos="word" start_char="14714">illnesses</TOKEN>
<TOKEN end_char="14727" id="token-99-14" morph="none" pos="word" start_char="14724">over</TOKEN>
<TOKEN end_char="14735" id="token-99-15" morph="none" pos="word" start_char="14729">several</TOKEN>
<TOKEN end_char="14741" id="token-99-16" morph="none" pos="word" start_char="14737">years</TOKEN>
<TOKEN end_char="14743" id="token-99-17" morph="none" pos="punct" start_char="14742">."</TOKEN>
</SEG>
<SEG end_char="14876" id="segment-100" start_char="14746">
<ORIGINAL_TEXT>For that study, the scientists reviewed nearly 3,000 satellite images from 2010 to 2013, again, measuring car traffic at hospitals.</ORIGINAL_TEXT>
<TOKEN end_char="14748" id="token-100-0" morph="none" pos="word" start_char="14746">For</TOKEN>
<TOKEN end_char="14753" id="token-100-1" morph="none" pos="word" start_char="14750">that</TOKEN>
<TOKEN end_char="14759" id="token-100-2" morph="none" pos="word" start_char="14755">study</TOKEN>
<TOKEN end_char="14760" id="token-100-3" morph="none" pos="punct" start_char="14760">,</TOKEN>
<TOKEN end_char="14764" id="token-100-4" morph="none" pos="word" start_char="14762">the</TOKEN>
<TOKEN end_char="14775" id="token-100-5" morph="none" pos="word" start_char="14766">scientists</TOKEN>
<TOKEN end_char="14784" id="token-100-6" morph="none" pos="word" start_char="14777">reviewed</TOKEN>
<TOKEN end_char="14791" id="token-100-7" morph="none" pos="word" start_char="14786">nearly</TOKEN>
<TOKEN end_char="14797" id="token-100-8" morph="none" pos="unknown" start_char="14793">3,000</TOKEN>
<TOKEN end_char="14807" id="token-100-9" morph="none" pos="word" start_char="14799">satellite</TOKEN>
<TOKEN end_char="14814" id="token-100-10" morph="none" pos="word" start_char="14809">images</TOKEN>
<TOKEN end_char="14819" id="token-100-11" morph="none" pos="word" start_char="14816">from</TOKEN>
<TOKEN end_char="14824" id="token-100-12" morph="none" pos="word" start_char="14821">2010</TOKEN>
<TOKEN end_char="14827" id="token-100-13" morph="none" pos="word" start_char="14826">to</TOKEN>
<TOKEN end_char="14832" id="token-100-14" morph="none" pos="word" start_char="14829">2013</TOKEN>
<TOKEN end_char="14833" id="token-100-15" morph="none" pos="punct" start_char="14833">,</TOKEN>
<TOKEN end_char="14839" id="token-100-16" morph="none" pos="word" start_char="14835">again</TOKEN>
<TOKEN end_char="14840" id="token-100-17" morph="none" pos="punct" start_char="14840">,</TOKEN>
<TOKEN end_char="14850" id="token-100-18" morph="none" pos="word" start_char="14842">measuring</TOKEN>
<TOKEN end_char="14854" id="token-100-19" morph="none" pos="word" start_char="14852">car</TOKEN>
<TOKEN end_char="14862" id="token-100-20" morph="none" pos="word" start_char="14856">traffic</TOKEN>
<TOKEN end_char="14865" id="token-100-21" morph="none" pos="word" start_char="14864">at</TOKEN>
<TOKEN end_char="14875" id="token-100-22" morph="none" pos="word" start_char="14867">hospitals</TOKEN>
<TOKEN end_char="14876" id="token-100-23" morph="none" pos="punct" start_char="14876">.</TOKEN>
</SEG>
<SEG end_char="15091" id="segment-101" start_char="14878">
<ORIGINAL_TEXT>They concluded that traffic spikes coincide with an outbreak of influenza-like illness, so public health officials could use parking-lot data to help them prepare for something that could strain medical facilities.</ORIGINAL_TEXT>
<TOKEN end_char="14881" id="token-101-0" morph="none" pos="word" start_char="14878">They</TOKEN>
<TOKEN end_char="14891" id="token-101-1" morph="none" pos="word" start_char="14883">concluded</TOKEN>
<TOKEN end_char="14896" id="token-101-2" morph="none" pos="word" start_char="14893">that</TOKEN>
<TOKEN end_char="14904" id="token-101-3" morph="none" pos="word" start_char="14898">traffic</TOKEN>
<TOKEN end_char="14911" id="token-101-4" morph="none" pos="word" start_char="14906">spikes</TOKEN>
<TOKEN end_char="14920" id="token-101-5" morph="none" pos="word" start_char="14913">coincide</TOKEN>
<TOKEN end_char="14925" id="token-101-6" morph="none" pos="word" start_char="14922">with</TOKEN>
<TOKEN end_char="14928" id="token-101-7" morph="none" pos="word" start_char="14927">an</TOKEN>
<TOKEN end_char="14937" id="token-101-8" morph="none" pos="word" start_char="14930">outbreak</TOKEN>
<TOKEN end_char="14940" id="token-101-9" morph="none" pos="word" start_char="14939">of</TOKEN>
<TOKEN end_char="14955" id="token-101-10" morph="none" pos="unknown" start_char="14942">influenza-like</TOKEN>
<TOKEN end_char="14963" id="token-101-11" morph="none" pos="word" start_char="14957">illness</TOKEN>
<TOKEN end_char="14964" id="token-101-12" morph="none" pos="punct" start_char="14964">,</TOKEN>
<TOKEN end_char="14967" id="token-101-13" morph="none" pos="word" start_char="14966">so</TOKEN>
<TOKEN end_char="14974" id="token-101-14" morph="none" pos="word" start_char="14969">public</TOKEN>
<TOKEN end_char="14981" id="token-101-15" morph="none" pos="word" start_char="14976">health</TOKEN>
<TOKEN end_char="14991" id="token-101-16" morph="none" pos="word" start_char="14983">officials</TOKEN>
<TOKEN end_char="14997" id="token-101-17" morph="none" pos="word" start_char="14993">could</TOKEN>
<TOKEN end_char="15001" id="token-101-18" morph="none" pos="word" start_char="14999">use</TOKEN>
<TOKEN end_char="15013" id="token-101-19" morph="none" pos="unknown" start_char="15003">parking-lot</TOKEN>
<TOKEN end_char="15018" id="token-101-20" morph="none" pos="word" start_char="15015">data</TOKEN>
<TOKEN end_char="15021" id="token-101-21" morph="none" pos="word" start_char="15020">to</TOKEN>
<TOKEN end_char="15026" id="token-101-22" morph="none" pos="word" start_char="15023">help</TOKEN>
<TOKEN end_char="15031" id="token-101-23" morph="none" pos="word" start_char="15028">them</TOKEN>
<TOKEN end_char="15039" id="token-101-24" morph="none" pos="word" start_char="15033">prepare</TOKEN>
<TOKEN end_char="15043" id="token-101-25" morph="none" pos="word" start_char="15041">for</TOKEN>
<TOKEN end_char="15053" id="token-101-26" morph="none" pos="word" start_char="15045">something</TOKEN>
<TOKEN end_char="15058" id="token-101-27" morph="none" pos="word" start_char="15055">that</TOKEN>
<TOKEN end_char="15064" id="token-101-28" morph="none" pos="word" start_char="15060">could</TOKEN>
<TOKEN end_char="15071" id="token-101-29" morph="none" pos="word" start_char="15066">strain</TOKEN>
<TOKEN end_char="15079" id="token-101-30" morph="none" pos="word" start_char="15073">medical</TOKEN>
<TOKEN end_char="15090" id="token-101-31" morph="none" pos="word" start_char="15081">facilities</TOKEN>
<TOKEN end_char="15091" id="token-101-32" morph="none" pos="punct" start_char="15091">.</TOKEN>
</SEG>
<SEG end_char="15320" id="segment-102" start_char="15094">
<ORIGINAL_TEXT>"We are in need of new and innovative methods for predicting disease," said epidemiology professor Anne Rimoin, the director of the Center for Global and Immigrant Health at UCLA, who was not connected with the research effort.</ORIGINAL_TEXT>
<TOKEN end_char="15094" id="token-102-0" morph="none" pos="punct" start_char="15094">"</TOKEN>
<TOKEN end_char="15096" id="token-102-1" morph="none" pos="word" start_char="15095">We</TOKEN>
<TOKEN end_char="15100" id="token-102-2" morph="none" pos="word" start_char="15098">are</TOKEN>
<TOKEN end_char="15103" id="token-102-3" morph="none" pos="word" start_char="15102">in</TOKEN>
<TOKEN end_char="15108" id="token-102-4" morph="none" pos="word" start_char="15105">need</TOKEN>
<TOKEN end_char="15111" id="token-102-5" morph="none" pos="word" start_char="15110">of</TOKEN>
<TOKEN end_char="15115" id="token-102-6" morph="none" pos="word" start_char="15113">new</TOKEN>
<TOKEN end_char="15119" id="token-102-7" morph="none" pos="word" start_char="15117">and</TOKEN>
<TOKEN end_char="15130" id="token-102-8" morph="none" pos="word" start_char="15121">innovative</TOKEN>
<TOKEN end_char="15138" id="token-102-9" morph="none" pos="word" start_char="15132">methods</TOKEN>
<TOKEN end_char="15142" id="token-102-10" morph="none" pos="word" start_char="15140">for</TOKEN>
<TOKEN end_char="15153" id="token-102-11" morph="none" pos="word" start_char="15144">predicting</TOKEN>
<TOKEN end_char="15161" id="token-102-12" morph="none" pos="word" start_char="15155">disease</TOKEN>
<TOKEN end_char="15163" id="token-102-13" morph="none" pos="punct" start_char="15162">,"</TOKEN>
<TOKEN end_char="15168" id="token-102-14" morph="none" pos="word" start_char="15165">said</TOKEN>
<TOKEN end_char="15181" id="token-102-15" morph="none" pos="word" start_char="15170">epidemiology</TOKEN>
<TOKEN end_char="15191" id="token-102-16" morph="none" pos="word" start_char="15183">professor</TOKEN>
<TOKEN end_char="15196" id="token-102-17" morph="none" pos="word" start_char="15193">Anne</TOKEN>
<TOKEN end_char="15203" id="token-102-18" morph="none" pos="word" start_char="15198">Rimoin</TOKEN>
<TOKEN end_char="15204" id="token-102-19" morph="none" pos="punct" start_char="15204">,</TOKEN>
<TOKEN end_char="15208" id="token-102-20" morph="none" pos="word" start_char="15206">the</TOKEN>
<TOKEN end_char="15217" id="token-102-21" morph="none" pos="word" start_char="15210">director</TOKEN>
<TOKEN end_char="15220" id="token-102-22" morph="none" pos="word" start_char="15219">of</TOKEN>
<TOKEN end_char="15224" id="token-102-23" morph="none" pos="word" start_char="15222">the</TOKEN>
<TOKEN end_char="15231" id="token-102-24" morph="none" pos="word" start_char="15226">Center</TOKEN>
<TOKEN end_char="15235" id="token-102-25" morph="none" pos="word" start_char="15233">for</TOKEN>
<TOKEN end_char="15242" id="token-102-26" morph="none" pos="word" start_char="15237">Global</TOKEN>
<TOKEN end_char="15246" id="token-102-27" morph="none" pos="word" start_char="15244">and</TOKEN>
<TOKEN end_char="15256" id="token-102-28" morph="none" pos="word" start_char="15248">Immigrant</TOKEN>
<TOKEN end_char="15263" id="token-102-29" morph="none" pos="word" start_char="15258">Health</TOKEN>
<TOKEN end_char="15266" id="token-102-30" morph="none" pos="word" start_char="15265">at</TOKEN>
<TOKEN end_char="15271" id="token-102-31" morph="none" pos="word" start_char="15268">UCLA</TOKEN>
<TOKEN end_char="15272" id="token-102-32" morph="none" pos="punct" start_char="15272">,</TOKEN>
<TOKEN end_char="15276" id="token-102-33" morph="none" pos="word" start_char="15274">who</TOKEN>
<TOKEN end_char="15280" id="token-102-34" morph="none" pos="word" start_char="15278">was</TOKEN>
<TOKEN end_char="15284" id="token-102-35" morph="none" pos="word" start_char="15282">not</TOKEN>
<TOKEN end_char="15294" id="token-102-36" morph="none" pos="word" start_char="15286">connected</TOKEN>
<TOKEN end_char="15299" id="token-102-37" morph="none" pos="word" start_char="15296">with</TOKEN>
<TOKEN end_char="15303" id="token-102-38" morph="none" pos="word" start_char="15301">the</TOKEN>
<TOKEN end_char="15312" id="token-102-39" morph="none" pos="word" start_char="15305">research</TOKEN>
<TOKEN end_char="15319" id="token-102-40" morph="none" pos="word" start_char="15314">effort</TOKEN>
<TOKEN end_char="15320" id="token-102-41" morph="none" pos="punct" start_char="15320">.</TOKEN>
</SEG>
<SEG end_char="15474" id="segment-103" start_char="15322">
<ORIGINAL_TEXT>"In this specific case, data on events such as increases in hospital traffic could serve as early indicators of social disruption resulting from disease.</ORIGINAL_TEXT>
<TOKEN end_char="15322" id="token-103-0" morph="none" pos="punct" start_char="15322">"</TOKEN>
<TOKEN end_char="15324" id="token-103-1" morph="none" pos="word" start_char="15323">In</TOKEN>
<TOKEN end_char="15329" id="token-103-2" morph="none" pos="word" start_char="15326">this</TOKEN>
<TOKEN end_char="15338" id="token-103-3" morph="none" pos="word" start_char="15331">specific</TOKEN>
<TOKEN end_char="15343" id="token-103-4" morph="none" pos="word" start_char="15340">case</TOKEN>
<TOKEN end_char="15344" id="token-103-5" morph="none" pos="punct" start_char="15344">,</TOKEN>
<TOKEN end_char="15349" id="token-103-6" morph="none" pos="word" start_char="15346">data</TOKEN>
<TOKEN end_char="15352" id="token-103-7" morph="none" pos="word" start_char="15351">on</TOKEN>
<TOKEN end_char="15359" id="token-103-8" morph="none" pos="word" start_char="15354">events</TOKEN>
<TOKEN end_char="15364" id="token-103-9" morph="none" pos="word" start_char="15361">such</TOKEN>
<TOKEN end_char="15367" id="token-103-10" morph="none" pos="word" start_char="15366">as</TOKEN>
<TOKEN end_char="15377" id="token-103-11" morph="none" pos="word" start_char="15369">increases</TOKEN>
<TOKEN end_char="15380" id="token-103-12" morph="none" pos="word" start_char="15379">in</TOKEN>
<TOKEN end_char="15389" id="token-103-13" morph="none" pos="word" start_char="15382">hospital</TOKEN>
<TOKEN end_char="15397" id="token-103-14" morph="none" pos="word" start_char="15391">traffic</TOKEN>
<TOKEN end_char="15403" id="token-103-15" morph="none" pos="word" start_char="15399">could</TOKEN>
<TOKEN end_char="15409" id="token-103-16" morph="none" pos="word" start_char="15405">serve</TOKEN>
<TOKEN end_char="15412" id="token-103-17" morph="none" pos="word" start_char="15411">as</TOKEN>
<TOKEN end_char="15418" id="token-103-18" morph="none" pos="word" start_char="15414">early</TOKEN>
<TOKEN end_char="15429" id="token-103-19" morph="none" pos="word" start_char="15420">indicators</TOKEN>
<TOKEN end_char="15432" id="token-103-20" morph="none" pos="word" start_char="15431">of</TOKEN>
<TOKEN end_char="15439" id="token-103-21" morph="none" pos="word" start_char="15434">social</TOKEN>
<TOKEN end_char="15450" id="token-103-22" morph="none" pos="word" start_char="15441">disruption</TOKEN>
<TOKEN end_char="15460" id="token-103-23" morph="none" pos="word" start_char="15452">resulting</TOKEN>
<TOKEN end_char="15465" id="token-103-24" morph="none" pos="word" start_char="15462">from</TOKEN>
<TOKEN end_char="15473" id="token-103-25" morph="none" pos="word" start_char="15467">disease</TOKEN>
<TOKEN end_char="15474" id="token-103-26" morph="none" pos="punct" start_char="15474">.</TOKEN>
</SEG>
<SEG end_char="15606" id="segment-104" start_char="15476">
<ORIGINAL_TEXT>High-resolution satellite imagery can be extremely useful for understanding disease spread and implementation of control measures."</ORIGINAL_TEXT>
<TOKEN end_char="15490" id="token-104-0" morph="none" pos="unknown" start_char="15476">High-resolution</TOKEN>
<TOKEN end_char="15500" id="token-104-1" morph="none" pos="word" start_char="15492">satellite</TOKEN>
<TOKEN end_char="15508" id="token-104-2" morph="none" pos="word" start_char="15502">imagery</TOKEN>
<TOKEN end_char="15512" id="token-104-3" morph="none" pos="word" start_char="15510">can</TOKEN>
<TOKEN end_char="15515" id="token-104-4" morph="none" pos="word" start_char="15514">be</TOKEN>
<TOKEN end_char="15525" id="token-104-5" morph="none" pos="word" start_char="15517">extremely</TOKEN>
<TOKEN end_char="15532" id="token-104-6" morph="none" pos="word" start_char="15527">useful</TOKEN>
<TOKEN end_char="15536" id="token-104-7" morph="none" pos="word" start_char="15534">for</TOKEN>
<TOKEN end_char="15550" id="token-104-8" morph="none" pos="word" start_char="15538">understanding</TOKEN>
<TOKEN end_char="15558" id="token-104-9" morph="none" pos="word" start_char="15552">disease</TOKEN>
<TOKEN end_char="15565" id="token-104-10" morph="none" pos="word" start_char="15560">spread</TOKEN>
<TOKEN end_char="15569" id="token-104-11" morph="none" pos="word" start_char="15567">and</TOKEN>
<TOKEN end_char="15584" id="token-104-12" morph="none" pos="word" start_char="15571">implementation</TOKEN>
<TOKEN end_char="15587" id="token-104-13" morph="none" pos="word" start_char="15586">of</TOKEN>
<TOKEN end_char="15595" id="token-104-14" morph="none" pos="word" start_char="15589">control</TOKEN>
<TOKEN end_char="15604" id="token-104-15" morph="none" pos="word" start_char="15597">measures</TOKEN>
<TOKEN end_char="15606" id="token-104-16" morph="none" pos="punct" start_char="15605">."</TOKEN>
</SEG>
<SEG end_char="15699" id="segment-105" start_char="15609">
<ORIGINAL_TEXT>Karson Yiu, Conor Finnegan, Luis Martinez and James Gordon Meek contributed to this report.</ORIGINAL_TEXT>
<TOKEN end_char="15614" id="token-105-0" morph="none" pos="word" start_char="15609">Karson</TOKEN>
<TOKEN end_char="15618" id="token-105-1" morph="none" pos="word" start_char="15616">Yiu</TOKEN>
<TOKEN end_char="15619" id="token-105-2" morph="none" pos="punct" start_char="15619">,</TOKEN>
<TOKEN end_char="15625" id="token-105-3" morph="none" pos="word" start_char="15621">Conor</TOKEN>
<TOKEN end_char="15634" id="token-105-4" morph="none" pos="word" start_char="15627">Finnegan</TOKEN>
<TOKEN end_char="15635" id="token-105-5" morph="none" pos="punct" start_char="15635">,</TOKEN>
<TOKEN end_char="15640" id="token-105-6" morph="none" pos="word" start_char="15637">Luis</TOKEN>
<TOKEN end_char="15649" id="token-105-7" morph="none" pos="word" start_char="15642">Martinez</TOKEN>
<TOKEN end_char="15653" id="token-105-8" morph="none" pos="word" start_char="15651">and</TOKEN>
<TOKEN end_char="15659" id="token-105-9" morph="none" pos="word" start_char="15655">James</TOKEN>
<TOKEN end_char="15666" id="token-105-10" morph="none" pos="word" start_char="15661">Gordon</TOKEN>
<TOKEN end_char="15671" id="token-105-11" morph="none" pos="word" start_char="15668">Meek</TOKEN>
<TOKEN end_char="15683" id="token-105-12" morph="none" pos="word" start_char="15673">contributed</TOKEN>
<TOKEN end_char="15686" id="token-105-13" morph="none" pos="word" start_char="15685">to</TOKEN>
<TOKEN end_char="15691" id="token-105-14" morph="none" pos="word" start_char="15688">this</TOKEN>
<TOKEN end_char="15698" id="token-105-15" morph="none" pos="word" start_char="15693">report</TOKEN>
<TOKEN end_char="15699" id="token-105-16" morph="none" pos="punct" start_char="15699">.</TOKEN>
</SEG>
<SEG end_char="15786" id="segment-106" start_char="15701">
<ORIGINAL_TEXT>This report was updated Tuesday to include comments from the Chinese Foreign Ministry.</ORIGINAL_TEXT>
<TOKEN end_char="15704" id="token-106-0" morph="none" pos="word" start_char="15701">This</TOKEN>
<TOKEN end_char="15711" id="token-106-1" morph="none" pos="word" start_char="15706">report</TOKEN>
<TOKEN end_char="15715" id="token-106-2" morph="none" pos="word" start_char="15713">was</TOKEN>
<TOKEN end_char="15723" id="token-106-3" morph="none" pos="word" start_char="15717">updated</TOKEN>
<TOKEN end_char="15731" id="token-106-4" morph="none" pos="word" start_char="15725">Tuesday</TOKEN>
<TOKEN end_char="15734" id="token-106-5" morph="none" pos="word" start_char="15733">to</TOKEN>
<TOKEN end_char="15742" id="token-106-6" morph="none" pos="word" start_char="15736">include</TOKEN>
<TOKEN end_char="15751" id="token-106-7" morph="none" pos="word" start_char="15744">comments</TOKEN>
<TOKEN end_char="15756" id="token-106-8" morph="none" pos="word" start_char="15753">from</TOKEN>
<TOKEN end_char="15760" id="token-106-9" morph="none" pos="word" start_char="15758">the</TOKEN>
<TOKEN end_char="15768" id="token-106-10" morph="none" pos="word" start_char="15762">Chinese</TOKEN>
<TOKEN end_char="15776" id="token-106-11" morph="none" pos="word" start_char="15770">Foreign</TOKEN>
<TOKEN end_char="15785" id="token-106-12" morph="none" pos="word" start_char="15778">Ministry</TOKEN>
<TOKEN end_char="15786" id="token-106-13" morph="none" pos="punct" start_char="15786">.</TOKEN>
</SEG>
<SEG end_char="15897" id="segment-107" start_char="15789">
<ORIGINAL_TEXT>This report was featured in the Tuesday, June 9, 2020, episode of "Start Here," ABC News’ daily news podcast.</ORIGINAL_TEXT>
<TOKEN end_char="15792" id="token-107-0" morph="none" pos="word" start_char="15789">This</TOKEN>
<TOKEN end_char="15799" id="token-107-1" morph="none" pos="word" start_char="15794">report</TOKEN>
<TOKEN end_char="15803" id="token-107-2" morph="none" pos="word" start_char="15801">was</TOKEN>
<TOKEN end_char="15812" id="token-107-3" morph="none" pos="word" start_char="15805">featured</TOKEN>
<TOKEN end_char="15815" id="token-107-4" morph="none" pos="word" start_char="15814">in</TOKEN>
<TOKEN end_char="15819" id="token-107-5" morph="none" pos="word" start_char="15817">the</TOKEN>
<TOKEN end_char="15827" id="token-107-6" morph="none" pos="word" start_char="15821">Tuesday</TOKEN>
<TOKEN end_char="15828" id="token-107-7" morph="none" pos="punct" start_char="15828">,</TOKEN>
<TOKEN end_char="15833" id="token-107-8" morph="none" pos="word" start_char="15830">June</TOKEN>
<TOKEN end_char="15835" id="token-107-9" morph="none" pos="word" start_char="15835">9</TOKEN>
<TOKEN end_char="15836" id="token-107-10" morph="none" pos="punct" start_char="15836">,</TOKEN>
<TOKEN end_char="15841" id="token-107-11" morph="none" pos="word" start_char="15838">2020</TOKEN>
<TOKEN end_char="15842" id="token-107-12" morph="none" pos="punct" start_char="15842">,</TOKEN>
<TOKEN end_char="15850" id="token-107-13" morph="none" pos="word" start_char="15844">episode</TOKEN>
<TOKEN end_char="15853" id="token-107-14" morph="none" pos="word" start_char="15852">of</TOKEN>
<TOKEN end_char="15855" id="token-107-15" morph="none" pos="punct" start_char="15855">"</TOKEN>
<TOKEN end_char="15860" id="token-107-16" morph="none" pos="word" start_char="15856">Start</TOKEN>
<TOKEN end_char="15865" id="token-107-17" morph="none" pos="word" start_char="15862">Here</TOKEN>
<TOKEN end_char="15867" id="token-107-18" morph="none" pos="punct" start_char="15866">,"</TOKEN>
<TOKEN end_char="15871" id="token-107-19" morph="none" pos="word" start_char="15869">ABC</TOKEN>
<TOKEN end_char="15876" id="token-107-20" morph="none" pos="word" start_char="15873">News</TOKEN>
<TOKEN end_char="15877" id="token-107-21" morph="none" pos="punct" start_char="15877">’</TOKEN>
<TOKEN end_char="15883" id="token-107-22" morph="none" pos="word" start_char="15879">daily</TOKEN>
<TOKEN end_char="15888" id="token-107-23" morph="none" pos="word" start_char="15885">news</TOKEN>
<TOKEN end_char="15896" id="token-107-24" morph="none" pos="word" start_char="15890">podcast</TOKEN>
<TOKEN end_char="15897" id="token-107-25" morph="none" pos="punct" start_char="15897">.</TOKEN>
</SEG>
<SEG end_char="15981" id="segment-108" start_char="15900">
<ORIGINAL_TEXT>"Start Here" offers a straightforward look at the day's top stories in 20 minutes.</ORIGINAL_TEXT>
<TOKEN end_char="15900" id="token-108-0" morph="none" pos="punct" start_char="15900">"</TOKEN>
<TOKEN end_char="15905" id="token-108-1" morph="none" pos="word" start_char="15901">Start</TOKEN>
<TOKEN end_char="15910" id="token-108-2" morph="none" pos="word" start_char="15907">Here</TOKEN>
<TOKEN end_char="15911" id="token-108-3" morph="none" pos="punct" start_char="15911">"</TOKEN>
<TOKEN end_char="15918" id="token-108-4" morph="none" pos="word" start_char="15913">offers</TOKEN>
<TOKEN end_char="15920" id="token-108-5" morph="none" pos="word" start_char="15920">a</TOKEN>
<TOKEN end_char="15936" id="token-108-6" morph="none" pos="word" start_char="15922">straightforward</TOKEN>
<TOKEN end_char="15941" id="token-108-7" morph="none" pos="word" start_char="15938">look</TOKEN>
<TOKEN end_char="15944" id="token-108-8" morph="none" pos="word" start_char="15943">at</TOKEN>
<TOKEN end_char="15948" id="token-108-9" morph="none" pos="word" start_char="15946">the</TOKEN>
<TOKEN end_char="15954" id="token-108-10" morph="none" pos="word" start_char="15950">day's</TOKEN>
<TOKEN end_char="15958" id="token-108-11" morph="none" pos="word" start_char="15956">top</TOKEN>
<TOKEN end_char="15966" id="token-108-12" morph="none" pos="word" start_char="15960">stories</TOKEN>
<TOKEN end_char="15969" id="token-108-13" morph="none" pos="word" start_char="15968">in</TOKEN>
<TOKEN end_char="15972" id="token-108-14" morph="none" pos="word" start_char="15971">20</TOKEN>
<TOKEN end_char="15980" id="token-108-15" morph="none" pos="word" start_char="15974">minutes</TOKEN>
<TOKEN end_char="15981" id="token-108-16" morph="none" pos="punct" start_char="15981">.</TOKEN>
</SEG>
<SEG end_char="16108" id="segment-109" start_char="15983">
<ORIGINAL_TEXT>Listen for free every weekday on Apple Podcasts, Google Podcasts, Spotify, the ABC News app or wherever you get your podcasts.</ORIGINAL_TEXT>
<TOKEN end_char="15988" id="token-109-0" morph="none" pos="word" start_char="15983">Listen</TOKEN>
<TOKEN end_char="15992" id="token-109-1" morph="none" pos="word" start_char="15990">for</TOKEN>
<TOKEN end_char="15997" id="token-109-2" morph="none" pos="word" start_char="15994">free</TOKEN>
<TOKEN end_char="16003" id="token-109-3" morph="none" pos="word" start_char="15999">every</TOKEN>
<TOKEN end_char="16011" id="token-109-4" morph="none" pos="word" start_char="16005">weekday</TOKEN>
<TOKEN end_char="16014" id="token-109-5" morph="none" pos="word" start_char="16013">on</TOKEN>
<TOKEN end_char="16020" id="token-109-6" morph="none" pos="word" start_char="16016">Apple</TOKEN>
<TOKEN end_char="16029" id="token-109-7" morph="none" pos="word" start_char="16022">Podcasts</TOKEN>
<TOKEN end_char="16030" id="token-109-8" morph="none" pos="punct" start_char="16030">,</TOKEN>
<TOKEN end_char="16037" id="token-109-9" morph="none" pos="word" start_char="16032">Google</TOKEN>
<TOKEN end_char="16046" id="token-109-10" morph="none" pos="word" start_char="16039">Podcasts</TOKEN>
<TOKEN end_char="16047" id="token-109-11" morph="none" pos="punct" start_char="16047">,</TOKEN>
<TOKEN end_char="16055" id="token-109-12" morph="none" pos="word" start_char="16049">Spotify</TOKEN>
<TOKEN end_char="16056" id="token-109-13" morph="none" pos="punct" start_char="16056">,</TOKEN>
<TOKEN end_char="16060" id="token-109-14" morph="none" pos="word" start_char="16058">the</TOKEN>
<TOKEN end_char="16064" id="token-109-15" morph="none" pos="word" start_char="16062">ABC</TOKEN>
<TOKEN end_char="16069" id="token-109-16" morph="none" pos="word" start_char="16066">News</TOKEN>
<TOKEN end_char="16073" id="token-109-17" morph="none" pos="word" start_char="16071">app</TOKEN>
<TOKEN end_char="16076" id="token-109-18" morph="none" pos="word" start_char="16075">or</TOKEN>
<TOKEN end_char="16085" id="token-109-19" morph="none" pos="word" start_char="16078">wherever</TOKEN>
<TOKEN end_char="16089" id="token-109-20" morph="none" pos="word" start_char="16087">you</TOKEN>
<TOKEN end_char="16093" id="token-109-21" morph="none" pos="word" start_char="16091">get</TOKEN>
<TOKEN end_char="16098" id="token-109-22" morph="none" pos="word" start_char="16095">your</TOKEN>
<TOKEN end_char="16107" id="token-109-23" morph="none" pos="word" start_char="16100">podcasts</TOKEN>
<TOKEN end_char="16108" id="token-109-24" morph="none" pos="punct" start_char="16108">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>