<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAAW" lang="spa" raw_text_char_length="15750" raw_text_md5="1c467e1f142e91812cbf4abc2563508b" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="29" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN end_char="5" id="token-0-0" morph="none" pos="word" start_char="1">Where</TOKEN>
<TOKEN end_char="9" id="token-0-1" morph="none" pos="word" start_char="7">did</TOKEN>
<TOKEN end_char="15" id="token-0-2" morph="none" pos="word" start_char="11">covid</TOKEN>
<TOKEN end_char="18" id="token-0-3" morph="none" pos="word" start_char="17">19</TOKEN>
<TOKEN end_char="28" id="token-0-4" morph="none" pos="word" start_char="20">originate</TOKEN>
<TOKEN end_char="29" id="token-0-5" morph="none" pos="punct" start_char="29">?</TOKEN>
</SEG>
<SEG end_char="150" id="segment-1" start_char="33">
<ORIGINAL_TEXT>There have been numerous reports related to cases earlier then December of 2019 when China first identified the virus.</ORIGINAL_TEXT>
<TOKEN end_char="37" id="token-1-0" morph="none" pos="word" start_char="33">There</TOKEN>
<TOKEN end_char="42" id="token-1-1" morph="none" pos="word" start_char="39">have</TOKEN>
<TOKEN end_char="47" id="token-1-2" morph="none" pos="word" start_char="44">been</TOKEN>
<TOKEN end_char="56" id="token-1-3" morph="none" pos="word" start_char="49">numerous</TOKEN>
<TOKEN end_char="64" id="token-1-4" morph="none" pos="word" start_char="58">reports</TOKEN>
<TOKEN end_char="72" id="token-1-5" morph="none" pos="word" start_char="66">related</TOKEN>
<TOKEN end_char="75" id="token-1-6" morph="none" pos="word" start_char="74">to</TOKEN>
<TOKEN end_char="81" id="token-1-7" morph="none" pos="word" start_char="77">cases</TOKEN>
<TOKEN end_char="89" id="token-1-8" morph="none" pos="word" start_char="83">earlier</TOKEN>
<TOKEN end_char="94" id="token-1-9" morph="none" pos="word" start_char="91">then</TOKEN>
<TOKEN end_char="103" id="token-1-10" morph="none" pos="word" start_char="96">December</TOKEN>
<TOKEN end_char="106" id="token-1-11" morph="none" pos="word" start_char="105">of</TOKEN>
<TOKEN end_char="111" id="token-1-12" morph="none" pos="word" start_char="108">2019</TOKEN>
<TOKEN end_char="116" id="token-1-13" morph="none" pos="word" start_char="113">when</TOKEN>
<TOKEN end_char="122" id="token-1-14" morph="none" pos="word" start_char="118">China</TOKEN>
<TOKEN end_char="128" id="token-1-15" morph="none" pos="word" start_char="124">first</TOKEN>
<TOKEN end_char="139" id="token-1-16" morph="none" pos="word" start_char="130">identified</TOKEN>
<TOKEN end_char="143" id="token-1-17" morph="none" pos="word" start_char="141">the</TOKEN>
<TOKEN end_char="149" id="token-1-18" morph="none" pos="word" start_char="145">virus</TOKEN>
<TOKEN end_char="150" id="token-1-19" morph="none" pos="punct" start_char="150">.</TOKEN>
</SEG>
<SEG end_char="235" id="segment-2" start_char="153">
<ORIGINAL_TEXT>This cambridge study suggested Covid had been in China atleast since September 2019</ORIGINAL_TEXT>
<TOKEN end_char="156" id="token-2-0" morph="none" pos="word" start_char="153">This</TOKEN>
<TOKEN end_char="166" id="token-2-1" morph="none" pos="word" start_char="158">cambridge</TOKEN>
<TOKEN end_char="172" id="token-2-2" morph="none" pos="word" start_char="168">study</TOKEN>
<TOKEN end_char="182" id="token-2-3" morph="none" pos="word" start_char="174">suggested</TOKEN>
<TOKEN end_char="188" id="token-2-4" morph="none" pos="word" start_char="184">Covid</TOKEN>
<TOKEN end_char="192" id="token-2-5" morph="none" pos="word" start_char="190">had</TOKEN>
<TOKEN end_char="197" id="token-2-6" morph="none" pos="word" start_char="194">been</TOKEN>
<TOKEN end_char="200" id="token-2-7" morph="none" pos="word" start_char="199">in</TOKEN>
<TOKEN end_char="206" id="token-2-8" morph="none" pos="word" start_char="202">China</TOKEN>
<TOKEN end_char="214" id="token-2-9" morph="none" pos="word" start_char="208">atleast</TOKEN>
<TOKEN end_char="220" id="token-2-10" morph="none" pos="word" start_char="216">since</TOKEN>
<TOKEN end_char="230" id="token-2-11" morph="none" pos="word" start_char="222">September</TOKEN>
<TOKEN end_char="235" id="token-2-12" morph="none" pos="word" start_char="232">2019</TOKEN>
</SEG>
<SEG end_char="305" id="segment-3" start_char="238">
<ORIGINAL_TEXT>Yet there is also evidence that covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="240" id="token-3-0" morph="none" pos="word" start_char="238">Yet</TOKEN>
<TOKEN end_char="246" id="token-3-1" morph="none" pos="word" start_char="242">there</TOKEN>
<TOKEN end_char="249" id="token-3-2" morph="none" pos="word" start_char="248">is</TOKEN>
<TOKEN end_char="254" id="token-3-3" morph="none" pos="word" start_char="251">also</TOKEN>
<TOKEN end_char="263" id="token-3-4" morph="none" pos="word" start_char="256">evidence</TOKEN>
<TOKEN end_char="268" id="token-3-5" morph="none" pos="word" start_char="265">that</TOKEN>
<TOKEN end_char="274" id="token-3-6" morph="none" pos="word" start_char="270">covid</TOKEN>
<TOKEN end_char="278" id="token-3-7" morph="none" pos="word" start_char="276">was</TOKEN>
<TOKEN end_char="281" id="token-3-8" morph="none" pos="word" start_char="280">in</TOKEN>
<TOKEN end_char="288" id="token-3-9" morph="none" pos="word" start_char="283">Brazil</TOKEN>
<TOKEN end_char="291" id="token-3-10" morph="none" pos="word" start_char="290">in</TOKEN>
<TOKEN end_char="300" id="token-3-11" morph="none" pos="word" start_char="293">November</TOKEN>
<TOKEN end_char="305" id="token-3-12" morph="none" pos="word" start_char="302">2019</TOKEN>
</SEG>
<SEG end_char="330" id="segment-4" start_char="308">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="313" id="token-4-0" morph="none" pos="word" start_char="308">France</TOKEN>
<TOKEN end_char="316" id="token-4-1" morph="none" pos="word" start_char="315">in</TOKEN>
<TOKEN end_char="325" id="token-4-2" morph="none" pos="word" start_char="318">November</TOKEN>
<TOKEN end_char="330" id="token-4-3" morph="none" pos="word" start_char="327">2019</TOKEN>
</SEG>
<SEG end_char="364" id="segment-5" start_char="333">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN end_char="335" id="token-5-0" morph="none" pos="word" start_char="333">And</TOKEN>
<TOKEN end_char="341" id="token-5-1" morph="none" pos="word" start_char="337">spain</TOKEN>
<TOKEN end_char="344" id="token-5-2" morph="none" pos="word" start_char="343">as</TOKEN>
<TOKEN end_char="350" id="token-5-3" morph="none" pos="word" start_char="346">early</TOKEN>
<TOKEN end_char="353" id="token-5-4" morph="none" pos="word" start_char="352">as</TOKEN>
<TOKEN end_char="359" id="token-5-5" morph="none" pos="word" start_char="355">March</TOKEN>
<TOKEN end_char="364" id="token-5-6" morph="none" pos="word" start_char="361">2019</TOKEN>
</SEG>
<SEG end_char="436" id="segment-6" start_char="367">
<ORIGINAL_TEXT>So is Spain the most likely source as it is now the earliest sighting?</ORIGINAL_TEXT>
<TOKEN end_char="368" id="token-6-0" morph="none" pos="word" start_char="367">So</TOKEN>
<TOKEN end_char="371" id="token-6-1" morph="none" pos="word" start_char="370">is</TOKEN>
<TOKEN end_char="377" id="token-6-2" morph="none" pos="word" start_char="373">Spain</TOKEN>
<TOKEN end_char="381" id="token-6-3" morph="none" pos="word" start_char="379">the</TOKEN>
<TOKEN end_char="386" id="token-6-4" morph="none" pos="word" start_char="383">most</TOKEN>
<TOKEN end_char="393" id="token-6-5" morph="none" pos="word" start_char="388">likely</TOKEN>
<TOKEN end_char="400" id="token-6-6" morph="none" pos="word" start_char="395">source</TOKEN>
<TOKEN end_char="403" id="token-6-7" morph="none" pos="word" start_char="402">as</TOKEN>
<TOKEN end_char="406" id="token-6-8" morph="none" pos="word" start_char="405">it</TOKEN>
<TOKEN end_char="409" id="token-6-9" morph="none" pos="word" start_char="408">is</TOKEN>
<TOKEN end_char="413" id="token-6-10" morph="none" pos="word" start_char="411">now</TOKEN>
<TOKEN end_char="417" id="token-6-11" morph="none" pos="word" start_char="415">the</TOKEN>
<TOKEN end_char="426" id="token-6-12" morph="none" pos="word" start_char="419">earliest</TOKEN>
<TOKEN end_char="435" id="token-6-13" morph="none" pos="word" start_char="428">sighting</TOKEN>
<TOKEN end_char="436" id="token-6-14" morph="none" pos="punct" start_char="436">?</TOKEN>
</SEG>
<SEG end_char="531" id="segment-7" start_char="440">
<ORIGINAL_TEXT>Fort Detrick was closed in July 2019 because of 6 cases of mishandling biological materials.</ORIGINAL_TEXT>
<TOKEN end_char="443" id="token-7-0" morph="none" pos="word" start_char="440">Fort</TOKEN>
<TOKEN end_char="451" id="token-7-1" morph="none" pos="word" start_char="445">Detrick</TOKEN>
<TOKEN end_char="455" id="token-7-2" morph="none" pos="word" start_char="453">was</TOKEN>
<TOKEN end_char="462" id="token-7-3" morph="none" pos="word" start_char="457">closed</TOKEN>
<TOKEN end_char="465" id="token-7-4" morph="none" pos="word" start_char="464">in</TOKEN>
<TOKEN end_char="470" id="token-7-5" morph="none" pos="word" start_char="467">July</TOKEN>
<TOKEN end_char="475" id="token-7-6" morph="none" pos="word" start_char="472">2019</TOKEN>
<TOKEN end_char="483" id="token-7-7" morph="none" pos="word" start_char="477">because</TOKEN>
<TOKEN end_char="486" id="token-7-8" morph="none" pos="word" start_char="485">of</TOKEN>
<TOKEN end_char="488" id="token-7-9" morph="none" pos="word" start_char="488">6</TOKEN>
<TOKEN end_char="494" id="token-7-10" morph="none" pos="word" start_char="490">cases</TOKEN>
<TOKEN end_char="497" id="token-7-11" morph="none" pos="word" start_char="496">of</TOKEN>
<TOKEN end_char="509" id="token-7-12" morph="none" pos="word" start_char="499">mishandling</TOKEN>
<TOKEN end_char="520" id="token-7-13" morph="none" pos="word" start_char="511">biological</TOKEN>
<TOKEN end_char="530" id="token-7-14" morph="none" pos="word" start_char="522">materials</TOKEN>
<TOKEN end_char="531" id="token-7-15" morph="none" pos="punct" start_char="531">.</TOKEN>
</SEG>
<SEG end_char="563" id="segment-8" start_char="533">
<ORIGINAL_TEXT>There were also leaks of agents</ORIGINAL_TEXT>
<TOKEN end_char="537" id="token-8-0" morph="none" pos="word" start_char="533">There</TOKEN>
<TOKEN end_char="542" id="token-8-1" morph="none" pos="word" start_char="539">were</TOKEN>
<TOKEN end_char="547" id="token-8-2" morph="none" pos="word" start_char="544">also</TOKEN>
<TOKEN end_char="553" id="token-8-3" morph="none" pos="word" start_char="549">leaks</TOKEN>
<TOKEN end_char="556" id="token-8-4" morph="none" pos="word" start_char="555">of</TOKEN>
<TOKEN end_char="563" id="token-8-5" morph="none" pos="word" start_char="558">agents</TOKEN>
</SEG>
<SEG end_char="659" id="segment-9" start_char="566">
<ORIGINAL_TEXT>Considering it was closed after the leaks happened, could fort Detrick be the original source?</ORIGINAL_TEXT>
<TOKEN end_char="576" id="token-9-0" morph="none" pos="word" start_char="566">Considering</TOKEN>
<TOKEN end_char="579" id="token-9-1" morph="none" pos="word" start_char="578">it</TOKEN>
<TOKEN end_char="583" id="token-9-2" morph="none" pos="word" start_char="581">was</TOKEN>
<TOKEN end_char="590" id="token-9-3" morph="none" pos="word" start_char="585">closed</TOKEN>
<TOKEN end_char="596" id="token-9-4" morph="none" pos="word" start_char="592">after</TOKEN>
<TOKEN end_char="600" id="token-9-5" morph="none" pos="word" start_char="598">the</TOKEN>
<TOKEN end_char="606" id="token-9-6" morph="none" pos="word" start_char="602">leaks</TOKEN>
<TOKEN end_char="615" id="token-9-7" morph="none" pos="word" start_char="608">happened</TOKEN>
<TOKEN end_char="616" id="token-9-8" morph="none" pos="punct" start_char="616">,</TOKEN>
<TOKEN end_char="622" id="token-9-9" morph="none" pos="word" start_char="618">could</TOKEN>
<TOKEN end_char="627" id="token-9-10" morph="none" pos="word" start_char="624">fort</TOKEN>
<TOKEN end_char="635" id="token-9-11" morph="none" pos="word" start_char="629">Detrick</TOKEN>
<TOKEN end_char="638" id="token-9-12" morph="none" pos="word" start_char="637">be</TOKEN>
<TOKEN end_char="642" id="token-9-13" morph="none" pos="word" start_char="640">the</TOKEN>
<TOKEN end_char="651" id="token-9-14" morph="none" pos="word" start_char="644">original</TOKEN>
<TOKEN end_char="658" id="token-9-15" morph="none" pos="word" start_char="653">source</TOKEN>
<TOKEN end_char="659" id="token-9-16" morph="none" pos="punct" start_char="659">?</TOKEN>
</SEG>
<SEG end_char="736" id="segment-10" start_char="662">
<ORIGINAL_TEXT>https://www.nytimes.com/2019/08/05/health/germs-fort-detrick-biohazard.html</ORIGINAL_TEXT>
<TOKEN end_char="736" id="token-10-0" morph="none" pos="url" start_char="662">https://www.nytimes.com/2019/08/05/health/germs-fort-detrick-biohazard.html</TOKEN>
<TRANSLATED_TEXT>https: / / www.nytimes.com / 2019 / 08 / 05 / health / germs-fort-detrick-biohazard .html</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="939" id="segment-11" start_char="739">
<ORIGINAL_TEXT>Most shockingly a residential home near fort Detrick suffered a respiratory outbreak https://abcnews.go.com/US/respiratory-outbreak-investigated-retirement-community-54-residents-fall/story?id=64275865</ORIGINAL_TEXT>
<TOKEN end_char="742" id="token-11-0" morph="none" pos="word" start_char="739">Most</TOKEN>
<TOKEN end_char="753" id="token-11-1" morph="none" pos="word" start_char="744">shockingly</TOKEN>
<TOKEN end_char="755" id="token-11-2" morph="none" pos="word" start_char="755">a</TOKEN>
<TOKEN end_char="767" id="token-11-3" morph="none" pos="word" start_char="757">residential</TOKEN>
<TOKEN end_char="772" id="token-11-4" morph="none" pos="word" start_char="769">home</TOKEN>
<TOKEN end_char="777" id="token-11-5" morph="none" pos="word" start_char="774">near</TOKEN>
<TOKEN end_char="782" id="token-11-6" morph="none" pos="word" start_char="779">fort</TOKEN>
<TOKEN end_char="790" id="token-11-7" morph="none" pos="word" start_char="784">Detrick</TOKEN>
<TOKEN end_char="799" id="token-11-8" morph="none" pos="word" start_char="792">suffered</TOKEN>
<TOKEN end_char="801" id="token-11-9" morph="none" pos="word" start_char="801">a</TOKEN>
<TOKEN end_char="813" id="token-11-10" morph="none" pos="word" start_char="803">respiratory</TOKEN>
<TOKEN end_char="822" id="token-11-11" morph="none" pos="word" start_char="815">outbreak</TOKEN>
<TOKEN end_char="939" id="token-11-12" morph="none" pos="url" start_char="824">https://abcnews.go.com/US/respiratory-outbreak-investigated-retirement-community-54-residents-fall/story?id=64275865</TOKEN>
</SEG>
<SEG end_char="1001" id="segment-12" start_char="942">
<ORIGINAL_TEXT>"He said the outbreak began with the first case on June 30."</ORIGINAL_TEXT>
<TOKEN end_char="942" id="token-12-0" morph="none" pos="punct" start_char="942">"</TOKEN>
<TOKEN end_char="944" id="token-12-1" morph="none" pos="word" start_char="943">He</TOKEN>
<TOKEN end_char="949" id="token-12-2" morph="none" pos="word" start_char="946">said</TOKEN>
<TOKEN end_char="953" id="token-12-3" morph="none" pos="word" start_char="951">the</TOKEN>
<TOKEN end_char="962" id="token-12-4" morph="none" pos="word" start_char="955">outbreak</TOKEN>
<TOKEN end_char="968" id="token-12-5" morph="none" pos="word" start_char="964">began</TOKEN>
<TOKEN end_char="973" id="token-12-6" morph="none" pos="word" start_char="970">with</TOKEN>
<TOKEN end_char="977" id="token-12-7" morph="none" pos="word" start_char="975">the</TOKEN>
<TOKEN end_char="983" id="token-12-8" morph="none" pos="word" start_char="979">first</TOKEN>
<TOKEN end_char="988" id="token-12-9" morph="none" pos="word" start_char="985">case</TOKEN>
<TOKEN end_char="991" id="token-12-10" morph="none" pos="word" start_char="990">on</TOKEN>
<TOKEN end_char="996" id="token-12-11" morph="none" pos="word" start_char="993">June</TOKEN>
<TOKEN end_char="999" id="token-12-12" morph="none" pos="word" start_char="998">30</TOKEN>
<TOKEN end_char="1001" id="token-12-13" morph="none" pos="punct" start_char="1000">."</TOKEN>
</SEG>
<SEG end_char="1100" id="segment-13" start_char="1004">
<ORIGINAL_TEXT>This is a few weeks before fort Detrick was closed due to mis management of biological materials.</ORIGINAL_TEXT>
<TOKEN end_char="1007" id="token-13-0" morph="none" pos="word" start_char="1004">This</TOKEN>
<TOKEN end_char="1010" id="token-13-1" morph="none" pos="word" start_char="1009">is</TOKEN>
<TOKEN end_char="1012" id="token-13-2" morph="none" pos="word" start_char="1012">a</TOKEN>
<TOKEN end_char="1016" id="token-13-3" morph="none" pos="word" start_char="1014">few</TOKEN>
<TOKEN end_char="1022" id="token-13-4" morph="none" pos="word" start_char="1018">weeks</TOKEN>
<TOKEN end_char="1029" id="token-13-5" morph="none" pos="word" start_char="1024">before</TOKEN>
<TOKEN end_char="1034" id="token-13-6" morph="none" pos="word" start_char="1031">fort</TOKEN>
<TOKEN end_char="1042" id="token-13-7" morph="none" pos="word" start_char="1036">Detrick</TOKEN>
<TOKEN end_char="1046" id="token-13-8" morph="none" pos="word" start_char="1044">was</TOKEN>
<TOKEN end_char="1053" id="token-13-9" morph="none" pos="word" start_char="1048">closed</TOKEN>
<TOKEN end_char="1057" id="token-13-10" morph="none" pos="word" start_char="1055">due</TOKEN>
<TOKEN end_char="1060" id="token-13-11" morph="none" pos="word" start_char="1059">to</TOKEN>
<TOKEN end_char="1064" id="token-13-12" morph="none" pos="word" start_char="1062">mis</TOKEN>
<TOKEN end_char="1075" id="token-13-13" morph="none" pos="word" start_char="1066">management</TOKEN>
<TOKEN end_char="1078" id="token-13-14" morph="none" pos="word" start_char="1077">of</TOKEN>
<TOKEN end_char="1089" id="token-13-15" morph="none" pos="word" start_char="1080">biological</TOKEN>
<TOKEN end_char="1099" id="token-13-16" morph="none" pos="word" start_char="1091">materials</TOKEN>
<TOKEN end_char="1100" id="token-13-17" morph="none" pos="punct" start_char="1100">.</TOKEN>
</SEG>
<SEG end_char="1158" id="segment-14" start_char="1103">
<ORIGINAL_TEXT>The retirement home is a 4 hour drive from the facility.</ORIGINAL_TEXT>
<TOKEN end_char="1105" id="token-14-0" morph="none" pos="word" start_char="1103">The</TOKEN>
<TOKEN end_char="1116" id="token-14-1" morph="none" pos="word" start_char="1107">retirement</TOKEN>
<TOKEN end_char="1121" id="token-14-2" morph="none" pos="word" start_char="1118">home</TOKEN>
<TOKEN end_char="1124" id="token-14-3" morph="none" pos="word" start_char="1123">is</TOKEN>
<TOKEN end_char="1126" id="token-14-4" morph="none" pos="word" start_char="1126">a</TOKEN>
<TOKEN end_char="1128" id="token-14-5" morph="none" pos="word" start_char="1128">4</TOKEN>
<TOKEN end_char="1133" id="token-14-6" morph="none" pos="word" start_char="1130">hour</TOKEN>
<TOKEN end_char="1139" id="token-14-7" morph="none" pos="word" start_char="1135">drive</TOKEN>
<TOKEN end_char="1144" id="token-14-8" morph="none" pos="word" start_char="1141">from</TOKEN>
<TOKEN end_char="1148" id="token-14-9" morph="none" pos="word" start_char="1146">the</TOKEN>
<TOKEN end_char="1157" id="token-14-10" morph="none" pos="word" start_char="1150">facility</TOKEN>
<TOKEN end_char="1158" id="token-14-11" morph="none" pos="punct" start_char="1158">.</TOKEN>
</SEG>
<SEG end_char="1298" id="segment-15" start_char="1161">
<ORIGINAL_TEXT>We know they closed fort Detrick in July due to.mis management of materials, we dont know how long that miss management was happening for.</ORIGINAL_TEXT>
<TOKEN end_char="1162" id="token-15-0" morph="none" pos="word" start_char="1161">We</TOKEN>
<TOKEN end_char="1167" id="token-15-1" morph="none" pos="word" start_char="1164">know</TOKEN>
<TOKEN end_char="1172" id="token-15-2" morph="none" pos="word" start_char="1169">they</TOKEN>
<TOKEN end_char="1179" id="token-15-3" morph="none" pos="word" start_char="1174">closed</TOKEN>
<TOKEN end_char="1184" id="token-15-4" morph="none" pos="word" start_char="1181">fort</TOKEN>
<TOKEN end_char="1192" id="token-15-5" morph="none" pos="word" start_char="1186">Detrick</TOKEN>
<TOKEN end_char="1195" id="token-15-6" morph="none" pos="word" start_char="1194">in</TOKEN>
<TOKEN end_char="1200" id="token-15-7" morph="none" pos="word" start_char="1197">July</TOKEN>
<TOKEN end_char="1204" id="token-15-8" morph="none" pos="word" start_char="1202">due</TOKEN>
<TOKEN end_char="1211" id="token-15-9" morph="none" pos="unknown" start_char="1206">to.mis</TOKEN>
<TOKEN end_char="1222" id="token-15-10" morph="none" pos="word" start_char="1213">management</TOKEN>
<TOKEN end_char="1225" id="token-15-11" morph="none" pos="word" start_char="1224">of</TOKEN>
<TOKEN end_char="1235" id="token-15-12" morph="none" pos="word" start_char="1227">materials</TOKEN>
<TOKEN end_char="1236" id="token-15-13" morph="none" pos="punct" start_char="1236">,</TOKEN>
<TOKEN end_char="1239" id="token-15-14" morph="none" pos="word" start_char="1238">we</TOKEN>
<TOKEN end_char="1244" id="token-15-15" morph="none" pos="word" start_char="1241">dont</TOKEN>
<TOKEN end_char="1249" id="token-15-16" morph="none" pos="word" start_char="1246">know</TOKEN>
<TOKEN end_char="1253" id="token-15-17" morph="none" pos="word" start_char="1251">how</TOKEN>
<TOKEN end_char="1258" id="token-15-18" morph="none" pos="word" start_char="1255">long</TOKEN>
<TOKEN end_char="1263" id="token-15-19" morph="none" pos="word" start_char="1260">that</TOKEN>
<TOKEN end_char="1268" id="token-15-20" morph="none" pos="word" start_char="1265">miss</TOKEN>
<TOKEN end_char="1279" id="token-15-21" morph="none" pos="word" start_char="1270">management</TOKEN>
<TOKEN end_char="1283" id="token-15-22" morph="none" pos="word" start_char="1281">was</TOKEN>
<TOKEN end_char="1293" id="token-15-23" morph="none" pos="word" start_char="1285">happening</TOKEN>
<TOKEN end_char="1297" id="token-15-24" morph="none" pos="word" start_char="1295">for</TOKEN>
<TOKEN end_char="1298" id="token-15-25" morph="none" pos="punct" start_char="1298">.</TOKEN>
</SEG>
<SEG end_char="1474" id="segment-16" start_char="1300">
<ORIGINAL_TEXT>Details as to what actually was released and mishandled is also sketchy as under national security the America government has refused to comment or allow a open investigation.</ORIGINAL_TEXT>
<TOKEN end_char="1306" id="token-16-0" morph="none" pos="word" start_char="1300">Details</TOKEN>
<TOKEN end_char="1309" id="token-16-1" morph="none" pos="word" start_char="1308">as</TOKEN>
<TOKEN end_char="1312" id="token-16-2" morph="none" pos="word" start_char="1311">to</TOKEN>
<TOKEN end_char="1317" id="token-16-3" morph="none" pos="word" start_char="1314">what</TOKEN>
<TOKEN end_char="1326" id="token-16-4" morph="none" pos="word" start_char="1319">actually</TOKEN>
<TOKEN end_char="1330" id="token-16-5" morph="none" pos="word" start_char="1328">was</TOKEN>
<TOKEN end_char="1339" id="token-16-6" morph="none" pos="word" start_char="1332">released</TOKEN>
<TOKEN end_char="1343" id="token-16-7" morph="none" pos="word" start_char="1341">and</TOKEN>
<TOKEN end_char="1354" id="token-16-8" morph="none" pos="word" start_char="1345">mishandled</TOKEN>
<TOKEN end_char="1357" id="token-16-9" morph="none" pos="word" start_char="1356">is</TOKEN>
<TOKEN end_char="1362" id="token-16-10" morph="none" pos="word" start_char="1359">also</TOKEN>
<TOKEN end_char="1370" id="token-16-11" morph="none" pos="word" start_char="1364">sketchy</TOKEN>
<TOKEN end_char="1373" id="token-16-12" morph="none" pos="word" start_char="1372">as</TOKEN>
<TOKEN end_char="1379" id="token-16-13" morph="none" pos="word" start_char="1375">under</TOKEN>
<TOKEN end_char="1388" id="token-16-14" morph="none" pos="word" start_char="1381">national</TOKEN>
<TOKEN end_char="1397" id="token-16-15" morph="none" pos="word" start_char="1390">security</TOKEN>
<TOKEN end_char="1401" id="token-16-16" morph="none" pos="word" start_char="1399">the</TOKEN>
<TOKEN end_char="1409" id="token-16-17" morph="none" pos="word" start_char="1403">America</TOKEN>
<TOKEN end_char="1420" id="token-16-18" morph="none" pos="word" start_char="1411">government</TOKEN>
<TOKEN end_char="1424" id="token-16-19" morph="none" pos="word" start_char="1422">has</TOKEN>
<TOKEN end_char="1432" id="token-16-20" morph="none" pos="word" start_char="1426">refused</TOKEN>
<TOKEN end_char="1435" id="token-16-21" morph="none" pos="word" start_char="1434">to</TOKEN>
<TOKEN end_char="1443" id="token-16-22" morph="none" pos="word" start_char="1437">comment</TOKEN>
<TOKEN end_char="1446" id="token-16-23" morph="none" pos="word" start_char="1445">or</TOKEN>
<TOKEN end_char="1452" id="token-16-24" morph="none" pos="word" start_char="1448">allow</TOKEN>
<TOKEN end_char="1454" id="token-16-25" morph="none" pos="word" start_char="1454">a</TOKEN>
<TOKEN end_char="1459" id="token-16-26" morph="none" pos="word" start_char="1456">open</TOKEN>
<TOKEN end_char="1473" id="token-16-27" morph="none" pos="word" start_char="1461">investigation</TOKEN>
<TOKEN end_char="1474" id="token-16-28" morph="none" pos="punct" start_char="1474">.</TOKEN>
</SEG>
<SEG end_char="1587" id="segment-17" start_char="1477">
<ORIGINAL_TEXT>Another question:- We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="1483" id="token-17-0" morph="none" pos="word" start_char="1477">Another</TOKEN>
<TOKEN end_char="1492" id="token-17-1" morph="none" pos="word" start_char="1485">question</TOKEN>
<TOKEN end_char="1494" id="token-17-2" morph="none" pos="punct" start_char="1493">:-</TOKEN>
<TOKEN end_char="1497" id="token-17-3" morph="none" pos="word" start_char="1496">We</TOKEN>
<TOKEN end_char="1502" id="token-17-4" morph="none" pos="word" start_char="1499">know</TOKEN>
<TOKEN end_char="1509" id="token-17-5" morph="none" pos="word" start_char="1504">Donald</TOKEN>
<TOKEN end_char="1515" id="token-17-6" morph="none" pos="word" start_char="1511">Trump</TOKEN>
<TOKEN end_char="1523" id="token-17-7" morph="none" pos="word" start_char="1517">claimed</TOKEN>
<TOKEN end_char="1526" id="token-17-8" morph="none" pos="word" start_char="1525">to</TOKEN>
<TOKEN end_char="1531" id="token-17-9" morph="none" pos="word" start_char="1528">have</TOKEN>
<TOKEN end_char="1536" id="token-17-10" morph="none" pos="word" start_char="1533">seen</TOKEN>
<TOKEN end_char="1545" id="token-17-11" morph="none" pos="word" start_char="1538">evidence</TOKEN>
<TOKEN end_char="1550" id="token-17-12" morph="none" pos="word" start_char="1547">that</TOKEN>
<TOKEN end_char="1556" id="token-17-13" morph="none" pos="word" start_char="1552">covid</TOKEN>
<TOKEN end_char="1559" id="token-17-14" morph="none" pos="word" start_char="1558">19</TOKEN>
<TOKEN end_char="1563" id="token-17-15" morph="none" pos="word" start_char="1561">had</TOKEN>
<TOKEN end_char="1568" id="token-17-16" morph="none" pos="word" start_char="1565">come</TOKEN>
<TOKEN end_char="1573" id="token-17-17" morph="none" pos="word" start_char="1570">from</TOKEN>
<TOKEN end_char="1575" id="token-17-18" morph="none" pos="word" start_char="1575">a</TOKEN>
<TOKEN end_char="1586" id="token-17-19" morph="none" pos="word" start_char="1577">laboratory</TOKEN>
<TOKEN end_char="1587" id="token-17-20" morph="none" pos="punct" start_char="1587">.</TOKEN>
</SEG>
<SEG end_char="1647" id="segment-18" start_char="1589">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN end_char="1593" id="token-18-0" morph="none" pos="word" start_char="1589">Could</TOKEN>
<TOKEN end_char="1596" id="token-18-1" morph="none" pos="word" start_char="1595">it</TOKEN>
<TOKEN end_char="1599" id="token-18-2" morph="none" pos="word" start_char="1598">be</TOKEN>
<TOKEN end_char="1604" id="token-18-3" morph="none" pos="word" start_char="1601">that</TOKEN>
<TOKEN end_char="1608" id="token-18-4" morph="none" pos="word" start_char="1606">the</TOKEN>
<TOKEN end_char="1617" id="token-18-5" morph="none" pos="word" start_char="1610">evidence</TOKEN>
<TOKEN end_char="1620" id="token-18-6" morph="none" pos="word" start_char="1619">he</TOKEN>
<TOKEN end_char="1624" id="token-18-7" morph="none" pos="word" start_char="1622">saw</TOKEN>
<TOKEN end_char="1628" id="token-18-8" morph="none" pos="word" start_char="1626">was</TOKEN>
<TOKEN end_char="1633" id="token-18-9" morph="none" pos="word" start_char="1630">from</TOKEN>
<TOKEN end_char="1638" id="token-18-10" morph="none" pos="word" start_char="1635">fort</TOKEN>
<TOKEN end_char="1646" id="token-18-11" morph="none" pos="word" start_char="1640">Detrick</TOKEN>
<TOKEN end_char="1647" id="token-18-12" morph="none" pos="punct" start_char="1647">?</TOKEN>
</SEG>
<SEG end_char="1732" id="segment-19" start_char="1649">
<ORIGINAL_TEXT>But rather then accept responsibility he sort to take an opportunity to blame China?</ORIGINAL_TEXT>
<TOKEN end_char="1651" id="token-19-0" morph="none" pos="word" start_char="1649">But</TOKEN>
<TOKEN end_char="1658" id="token-19-1" morph="none" pos="word" start_char="1653">rather</TOKEN>
<TOKEN end_char="1663" id="token-19-2" morph="none" pos="word" start_char="1660">then</TOKEN>
<TOKEN end_char="1670" id="token-19-3" morph="none" pos="word" start_char="1665">accept</TOKEN>
<TOKEN end_char="1685" id="token-19-4" morph="none" pos="word" start_char="1672">responsibility</TOKEN>
<TOKEN end_char="1688" id="token-19-5" morph="none" pos="word" start_char="1687">he</TOKEN>
<TOKEN end_char="1693" id="token-19-6" morph="none" pos="word" start_char="1690">sort</TOKEN>
<TOKEN end_char="1696" id="token-19-7" morph="none" pos="word" start_char="1695">to</TOKEN>
<TOKEN end_char="1701" id="token-19-8" morph="none" pos="word" start_char="1698">take</TOKEN>
<TOKEN end_char="1704" id="token-19-9" morph="none" pos="word" start_char="1703">an</TOKEN>
<TOKEN end_char="1716" id="token-19-10" morph="none" pos="word" start_char="1706">opportunity</TOKEN>
<TOKEN end_char="1719" id="token-19-11" morph="none" pos="word" start_char="1718">to</TOKEN>
<TOKEN end_char="1725" id="token-19-12" morph="none" pos="word" start_char="1721">blame</TOKEN>
<TOKEN end_char="1731" id="token-19-13" morph="none" pos="word" start_char="1727">China</TOKEN>
<TOKEN end_char="1732" id="token-19-14" morph="none" pos="punct" start_char="1732">?</TOKEN>
</SEG>
<SEG end_char="1766" id="segment-20" start_char="1736">
<ORIGINAL_TEXT>"Where did covid 19 originate?"</ORIGINAL_TEXT>
<TOKEN end_char="1736" id="token-20-0" morph="none" pos="punct" start_char="1736">"</TOKEN>
<TOKEN end_char="1741" id="token-20-1" morph="none" pos="word" start_char="1737">Where</TOKEN>
<TOKEN end_char="1745" id="token-20-2" morph="none" pos="word" start_char="1743">did</TOKEN>
<TOKEN end_char="1751" id="token-20-3" morph="none" pos="word" start_char="1747">covid</TOKEN>
<TOKEN end_char="1754" id="token-20-4" morph="none" pos="word" start_char="1753">19</TOKEN>
<TOKEN end_char="1764" id="token-20-5" morph="none" pos="word" start_char="1756">originate</TOKEN>
<TOKEN end_char="1766" id="token-20-6" morph="none" pos="punct" start_char="1765">?"</TOKEN>
</SEG>
<SEG end_char="1773" id="segment-21" start_char="1768">
<ORIGINAL_TEXT>Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="1772" id="token-21-0" morph="none" pos="word" start_char="1768">Wuhan</TOKEN>
<TOKEN end_char="1773" id="token-21-1" morph="none" pos="punct" start_char="1773">.</TOKEN>
<TRANSLATED_TEXT>Wuhan</TRANSLATED_TEXT><DETECTED_LANGUAGE>so</DETECTED_LANGUAGE></SEG>
<SEG end_char="1824" id="segment-22" start_char="1777">
<ORIGINAL_TEXT>Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN end_char="1781" id="token-22-0" morph="none" pos="word" start_char="1777">Quote</TOKEN>
<TOKEN end_char="1786" id="token-22-1" morph="none" pos="word" start_char="1783">from</TOKEN>
<TOKEN end_char="1787" id="token-22-2" morph="none" pos="punct" start_char="1787">:</TOKEN>
<TOKEN end_char="1793" id="token-22-3" morph="none" pos="word" start_char="1789">Bored</TOKEN>
<TOKEN end_char="1801" id="token-22-4" morph="none" pos="word" start_char="1795">chemist</TOKEN>
<TOKEN end_char="1804" id="token-22-5" morph="none" pos="word" start_char="1803">on</TOKEN>
<TOKEN end_char="1815" id="token-22-6" morph="none" pos="unknown" start_char="1806">05/02/2021</TOKEN>
<TOKEN end_char="1824" id="token-22-7" morph="none" pos="unknown" start_char="1817">18:56:50</TOKEN>
</SEG>
<SEG end_char="1856" id="segment-23" start_char="1827">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN end_char="1827" id="token-23-0" morph="none" pos="punct" start_char="1827">"</TOKEN>
<TOKEN end_char="1832" id="token-23-1" morph="none" pos="word" start_char="1828">Where</TOKEN>
<TOKEN end_char="1836" id="token-23-2" morph="none" pos="word" start_char="1834">did</TOKEN>
<TOKEN end_char="1842" id="token-23-3" morph="none" pos="word" start_char="1838">covid</TOKEN>
<TOKEN end_char="1845" id="token-23-4" morph="none" pos="word" start_char="1844">19</TOKEN>
<TOKEN end_char="1855" id="token-23-5" morph="none" pos="word" start_char="1847">originate</TOKEN>
<TOKEN end_char="1856" id="token-23-6" morph="none" pos="punct" start_char="1856">?</TOKEN>
</SEG>
<SEG end_char="1864" id="segment-24" start_char="1858">
<ORIGINAL_TEXT>"Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="1858" id="token-24-0" morph="none" pos="punct" start_char="1858">"</TOKEN>
<TOKEN end_char="1863" id="token-24-1" morph="none" pos="word" start_char="1859">Wuhan</TOKEN>
<TOKEN end_char="1864" id="token-24-2" morph="none" pos="punct" start_char="1864">.</TOKEN>
<TRANSLATED_TEXT>Wuhan.</TRANSLATED_TEXT><DETECTED_LANGUAGE>so</DETECTED_LANGUAGE></SEG>
<SEG end_char="1916" id="segment-25" start_char="1867">
<ORIGINAL_TEXT>A theory not a fact you should be more scientific.</ORIGINAL_TEXT>
<TOKEN end_char="1867" id="token-25-0" morph="none" pos="word" start_char="1867">A</TOKEN>
<TOKEN end_char="1874" id="token-25-1" morph="none" pos="word" start_char="1869">theory</TOKEN>
<TOKEN end_char="1878" id="token-25-2" morph="none" pos="word" start_char="1876">not</TOKEN>
<TOKEN end_char="1880" id="token-25-3" morph="none" pos="word" start_char="1880">a</TOKEN>
<TOKEN end_char="1885" id="token-25-4" morph="none" pos="word" start_char="1882">fact</TOKEN>
<TOKEN end_char="1889" id="token-25-5" morph="none" pos="word" start_char="1887">you</TOKEN>
<TOKEN end_char="1896" id="token-25-6" morph="none" pos="word" start_char="1891">should</TOKEN>
<TOKEN end_char="1899" id="token-25-7" morph="none" pos="word" start_char="1898">be</TOKEN>
<TOKEN end_char="1904" id="token-25-8" morph="none" pos="word" start_char="1901">more</TOKEN>
<TOKEN end_char="1915" id="token-25-9" morph="none" pos="word" start_char="1906">scientific</TOKEN>
<TOKEN end_char="1916" id="token-25-10" morph="none" pos="punct" start_char="1916">.</TOKEN>
</SEG>
<SEG end_char="1960" id="segment-26" start_char="1920">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 19:56:55</ORIGINAL_TEXT>
<TOKEN end_char="1924" id="token-26-0" morph="none" pos="word" start_char="1920">Quote</TOKEN>
<TOKEN end_char="1929" id="token-26-1" morph="none" pos="word" start_char="1926">from</TOKEN>
<TOKEN end_char="1930" id="token-26-2" morph="none" pos="punct" start_char="1930">:</TOKEN>
<TOKEN end_char="1937" id="token-26-3" morph="none" pos="word" start_char="1932">Jolly2</TOKEN>
<TOKEN end_char="1940" id="token-26-4" morph="none" pos="word" start_char="1939">on</TOKEN>
<TOKEN end_char="1951" id="token-26-5" morph="none" pos="unknown" start_char="1942">06/02/2021</TOKEN>
<TOKEN end_char="1960" id="token-26-6" morph="none" pos="unknown" start_char="1953">19:56:55</TOKEN>
</SEG>
<SEG end_char="1981" id="segment-27" start_char="1963">
<ORIGINAL_TEXT>A theory not a fact</ORIGINAL_TEXT>
<TOKEN end_char="1963" id="token-27-0" morph="none" pos="word" start_char="1963">A</TOKEN>
<TOKEN end_char="1970" id="token-27-1" morph="none" pos="word" start_char="1965">theory</TOKEN>
<TOKEN end_char="1974" id="token-27-2" morph="none" pos="word" start_char="1972">not</TOKEN>
<TOKEN end_char="1976" id="token-27-3" morph="none" pos="word" start_char="1976">a</TOKEN>
<TOKEN end_char="1981" id="token-27-4" morph="none" pos="word" start_char="1978">fact</TOKEN>
</SEG>
<SEG end_char="1994" id="segment-28" start_char="1985">
<ORIGINAL_TEXT>A bit like</ORIGINAL_TEXT>
<TOKEN end_char="1985" id="token-28-0" morph="none" pos="word" start_char="1985">A</TOKEN>
<TOKEN end_char="1989" id="token-28-1" morph="none" pos="word" start_char="1987">bit</TOKEN>
<TOKEN end_char="1994" id="token-28-2" morph="none" pos="word" start_char="1991">like</TOKEN>
</SEG>
<SEG end_char="2037" id="segment-29" start_char="1997">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2001" id="token-29-0" morph="none" pos="word" start_char="1997">Quote</TOKEN>
<TOKEN end_char="2006" id="token-29-1" morph="none" pos="word" start_char="2003">from</TOKEN>
<TOKEN end_char="2007" id="token-29-2" morph="none" pos="punct" start_char="2007">:</TOKEN>
<TOKEN end_char="2014" id="token-29-3" morph="none" pos="word" start_char="2009">Jolly2</TOKEN>
<TOKEN end_char="2017" id="token-29-4" morph="none" pos="word" start_char="2016">on</TOKEN>
<TOKEN end_char="2028" id="token-29-5" morph="none" pos="unknown" start_char="2019">05/02/2021</TOKEN>
<TOKEN end_char="2037" id="token-29-6" morph="none" pos="unknown" start_char="2030">18:12:36</TOKEN>
</SEG>
<SEG end_char="2075" id="segment-30" start_char="2040">
<ORIGINAL_TEXT>covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="2044" id="token-30-0" morph="none" pos="word" start_char="2040">covid</TOKEN>
<TOKEN end_char="2048" id="token-30-1" morph="none" pos="word" start_char="2046">was</TOKEN>
<TOKEN end_char="2051" id="token-30-2" morph="none" pos="word" start_char="2050">in</TOKEN>
<TOKEN end_char="2058" id="token-30-3" morph="none" pos="word" start_char="2053">Brazil</TOKEN>
<TOKEN end_char="2061" id="token-30-4" morph="none" pos="word" start_char="2060">in</TOKEN>
<TOKEN end_char="2070" id="token-30-5" morph="none" pos="word" start_char="2063">November</TOKEN>
<TOKEN end_char="2075" id="token-30-6" morph="none" pos="word" start_char="2072">2019</TOKEN>
</SEG>
<SEG end_char="2119" id="segment-31" start_char="2079">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2083" id="token-31-0" morph="none" pos="word" start_char="2079">Quote</TOKEN>
<TOKEN end_char="2088" id="token-31-1" morph="none" pos="word" start_char="2085">from</TOKEN>
<TOKEN end_char="2089" id="token-31-2" morph="none" pos="punct" start_char="2089">:</TOKEN>
<TOKEN end_char="2096" id="token-31-3" morph="none" pos="word" start_char="2091">Jolly2</TOKEN>
<TOKEN end_char="2099" id="token-31-4" morph="none" pos="word" start_char="2098">on</TOKEN>
<TOKEN end_char="2110" id="token-31-5" morph="none" pos="unknown" start_char="2101">05/02/2021</TOKEN>
<TOKEN end_char="2119" id="token-31-6" morph="none" pos="unknown" start_char="2112">18:12:36</TOKEN>
</SEG>
<SEG end_char="2144" id="segment-32" start_char="2122">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="2127" id="token-32-0" morph="none" pos="word" start_char="2122">France</TOKEN>
<TOKEN end_char="2130" id="token-32-1" morph="none" pos="word" start_char="2129">in</TOKEN>
<TOKEN end_char="2139" id="token-32-2" morph="none" pos="word" start_char="2132">November</TOKEN>
<TOKEN end_char="2144" id="token-32-3" morph="none" pos="word" start_char="2141">2019</TOKEN>
</SEG>
<SEG end_char="2188" id="segment-33" start_char="2148">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2152" id="token-33-0" morph="none" pos="word" start_char="2148">Quote</TOKEN>
<TOKEN end_char="2157" id="token-33-1" morph="none" pos="word" start_char="2154">from</TOKEN>
<TOKEN end_char="2158" id="token-33-2" morph="none" pos="punct" start_char="2158">:</TOKEN>
<TOKEN end_char="2165" id="token-33-3" morph="none" pos="word" start_char="2160">Jolly2</TOKEN>
<TOKEN end_char="2168" id="token-33-4" morph="none" pos="word" start_char="2167">on</TOKEN>
<TOKEN end_char="2179" id="token-33-5" morph="none" pos="unknown" start_char="2170">05/02/2021</TOKEN>
<TOKEN end_char="2188" id="token-33-6" morph="none" pos="unknown" start_char="2181">18:12:36</TOKEN>
</SEG>
<SEG end_char="2222" id="segment-34" start_char="2191">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN end_char="2193" id="token-34-0" morph="none" pos="word" start_char="2191">And</TOKEN>
<TOKEN end_char="2199" id="token-34-1" morph="none" pos="word" start_char="2195">spain</TOKEN>
<TOKEN end_char="2202" id="token-34-2" morph="none" pos="word" start_char="2201">as</TOKEN>
<TOKEN end_char="2208" id="token-34-3" morph="none" pos="word" start_char="2204">early</TOKEN>
<TOKEN end_char="2211" id="token-34-4" morph="none" pos="word" start_char="2210">as</TOKEN>
<TOKEN end_char="2217" id="token-34-5" morph="none" pos="word" start_char="2213">March</TOKEN>
<TOKEN end_char="2222" id="token-34-6" morph="none" pos="word" start_char="2219">2019</TOKEN>
</SEG>
<SEG end_char="2266" id="segment-35" start_char="2226">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:15:55</ORIGINAL_TEXT>
<TOKEN end_char="2230" id="token-35-0" morph="none" pos="word" start_char="2226">Quote</TOKEN>
<TOKEN end_char="2235" id="token-35-1" morph="none" pos="word" start_char="2232">from</TOKEN>
<TOKEN end_char="2236" id="token-35-2" morph="none" pos="punct" start_char="2236">:</TOKEN>
<TOKEN end_char="2243" id="token-35-3" morph="none" pos="word" start_char="2238">Jolly2</TOKEN>
<TOKEN end_char="2246" id="token-35-4" morph="none" pos="word" start_char="2245">on</TOKEN>
<TOKEN end_char="2257" id="token-35-5" morph="none" pos="unknown" start_char="2248">05/02/2021</TOKEN>
<TOKEN end_char="2266" id="token-35-6" morph="none" pos="unknown" start_char="2259">18:15:55</TOKEN>
</SEG>
<SEG end_char="2360" id="segment-36" start_char="2269">
<ORIGINAL_TEXT>We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="2270" id="token-36-0" morph="none" pos="word" start_char="2269">We</TOKEN>
<TOKEN end_char="2275" id="token-36-1" morph="none" pos="word" start_char="2272">know</TOKEN>
<TOKEN end_char="2282" id="token-36-2" morph="none" pos="word" start_char="2277">Donald</TOKEN>
<TOKEN end_char="2288" id="token-36-3" morph="none" pos="word" start_char="2284">Trump</TOKEN>
<TOKEN end_char="2296" id="token-36-4" morph="none" pos="word" start_char="2290">claimed</TOKEN>
<TOKEN end_char="2299" id="token-36-5" morph="none" pos="word" start_char="2298">to</TOKEN>
<TOKEN end_char="2304" id="token-36-6" morph="none" pos="word" start_char="2301">have</TOKEN>
<TOKEN end_char="2309" id="token-36-7" morph="none" pos="word" start_char="2306">seen</TOKEN>
<TOKEN end_char="2318" id="token-36-8" morph="none" pos="word" start_char="2311">evidence</TOKEN>
<TOKEN end_char="2323" id="token-36-9" morph="none" pos="word" start_char="2320">that</TOKEN>
<TOKEN end_char="2329" id="token-36-10" morph="none" pos="word" start_char="2325">covid</TOKEN>
<TOKEN end_char="2332" id="token-36-11" morph="none" pos="word" start_char="2331">19</TOKEN>
<TOKEN end_char="2336" id="token-36-12" morph="none" pos="word" start_char="2334">had</TOKEN>
<TOKEN end_char="2341" id="token-36-13" morph="none" pos="word" start_char="2338">come</TOKEN>
<TOKEN end_char="2346" id="token-36-14" morph="none" pos="word" start_char="2343">from</TOKEN>
<TOKEN end_char="2348" id="token-36-15" morph="none" pos="word" start_char="2348">a</TOKEN>
<TOKEN end_char="2359" id="token-36-16" morph="none" pos="word" start_char="2350">laboratory</TOKEN>
<TOKEN end_char="2360" id="token-36-17" morph="none" pos="punct" start_char="2360">.</TOKEN>
</SEG>
<SEG end_char="2420" id="segment-37" start_char="2362">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN end_char="2366" id="token-37-0" morph="none" pos="word" start_char="2362">Could</TOKEN>
<TOKEN end_char="2369" id="token-37-1" morph="none" pos="word" start_char="2368">it</TOKEN>
<TOKEN end_char="2372" id="token-37-2" morph="none" pos="word" start_char="2371">be</TOKEN>
<TOKEN end_char="2377" id="token-37-3" morph="none" pos="word" start_char="2374">that</TOKEN>
<TOKEN end_char="2381" id="token-37-4" morph="none" pos="word" start_char="2379">the</TOKEN>
<TOKEN end_char="2390" id="token-37-5" morph="none" pos="word" start_char="2383">evidence</TOKEN>
<TOKEN end_char="2393" id="token-37-6" morph="none" pos="word" start_char="2392">he</TOKEN>
<TOKEN end_char="2397" id="token-37-7" morph="none" pos="word" start_char="2395">saw</TOKEN>
<TOKEN end_char="2401" id="token-37-8" morph="none" pos="word" start_char="2399">was</TOKEN>
<TOKEN end_char="2406" id="token-37-9" morph="none" pos="word" start_char="2403">from</TOKEN>
<TOKEN end_char="2411" id="token-37-10" morph="none" pos="word" start_char="2408">fort</TOKEN>
<TOKEN end_char="2419" id="token-37-11" morph="none" pos="word" start_char="2413">Detrick</TOKEN>
<TOKEN end_char="2420" id="token-37-12" morph="none" pos="punct" start_char="2420">?</TOKEN>
</SEG>
<SEG end_char="2453" id="segment-38" start_char="2424">
<ORIGINAL_TEXT>you should be more scientific.</ORIGINAL_TEXT>
<TOKEN end_char="2426" id="token-38-0" morph="none" pos="word" start_char="2424">you</TOKEN>
<TOKEN end_char="2433" id="token-38-1" morph="none" pos="word" start_char="2428">should</TOKEN>
<TOKEN end_char="2436" id="token-38-2" morph="none" pos="word" start_char="2435">be</TOKEN>
<TOKEN end_char="2441" id="token-38-3" morph="none" pos="word" start_char="2438">more</TOKEN>
<TOKEN end_char="2452" id="token-38-4" morph="none" pos="word" start_char="2443">scientific</TOKEN>
<TOKEN end_char="2453" id="token-38-5" morph="none" pos="punct" start_char="2453">.</TOKEN>
</SEG>
<SEG end_char="2504" id="segment-39" start_char="2457">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN end_char="2461" id="token-39-0" morph="none" pos="word" start_char="2457">Quote</TOKEN>
<TOKEN end_char="2466" id="token-39-1" morph="none" pos="word" start_char="2463">from</TOKEN>
<TOKEN end_char="2467" id="token-39-2" morph="none" pos="punct" start_char="2467">:</TOKEN>
<TOKEN end_char="2473" id="token-39-3" morph="none" pos="word" start_char="2469">Bored</TOKEN>
<TOKEN end_char="2481" id="token-39-4" morph="none" pos="word" start_char="2475">chemist</TOKEN>
<TOKEN end_char="2484" id="token-39-5" morph="none" pos="word" start_char="2483">on</TOKEN>
<TOKEN end_char="2495" id="token-39-6" morph="none" pos="unknown" start_char="2486">06/02/2021</TOKEN>
<TOKEN end_char="2504" id="token-39-7" morph="none" pos="unknown" start_char="2497">20:13:01</TOKEN>
</SEG>
<SEG end_char="2547" id="segment-40" start_char="2507">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 19:56:55</ORIGINAL_TEXT>
<TOKEN end_char="2511" id="token-40-0" morph="none" pos="word" start_char="2507">Quote</TOKEN>
<TOKEN end_char="2516" id="token-40-1" morph="none" pos="word" start_char="2513">from</TOKEN>
<TOKEN end_char="2517" id="token-40-2" morph="none" pos="punct" start_char="2517">:</TOKEN>
<TOKEN end_char="2524" id="token-40-3" morph="none" pos="word" start_char="2519">Jolly2</TOKEN>
<TOKEN end_char="2527" id="token-40-4" morph="none" pos="word" start_char="2526">on</TOKEN>
<TOKEN end_char="2538" id="token-40-5" morph="none" pos="unknown" start_char="2529">06/02/2021</TOKEN>
<TOKEN end_char="2547" id="token-40-6" morph="none" pos="unknown" start_char="2540">19:56:55</TOKEN>
</SEG>
<SEG end_char="2568" id="segment-41" start_char="2550">
<ORIGINAL_TEXT>A theory not a fact</ORIGINAL_TEXT>
<TOKEN end_char="2550" id="token-41-0" morph="none" pos="word" start_char="2550">A</TOKEN>
<TOKEN end_char="2557" id="token-41-1" morph="none" pos="word" start_char="2552">theory</TOKEN>
<TOKEN end_char="2561" id="token-41-2" morph="none" pos="word" start_char="2559">not</TOKEN>
<TOKEN end_char="2563" id="token-41-3" morph="none" pos="word" start_char="2563">a</TOKEN>
<TOKEN end_char="2568" id="token-41-4" morph="none" pos="word" start_char="2565">fact</TOKEN>
</SEG>
<SEG end_char="2621" id="segment-42" start_char="2571">
<ORIGINAL_TEXT>A bit likeQuote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2571" id="token-42-0" morph="none" pos="word" start_char="2571">A</TOKEN>
<TOKEN end_char="2575" id="token-42-1" morph="none" pos="word" start_char="2573">bit</TOKEN>
<TOKEN end_char="2585" id="token-42-2" morph="none" pos="word" start_char="2577">likeQuote</TOKEN>
<TOKEN end_char="2590" id="token-42-3" morph="none" pos="word" start_char="2587">from</TOKEN>
<TOKEN end_char="2591" id="token-42-4" morph="none" pos="punct" start_char="2591">:</TOKEN>
<TOKEN end_char="2598" id="token-42-5" morph="none" pos="word" start_char="2593">Jolly2</TOKEN>
<TOKEN end_char="2601" id="token-42-6" morph="none" pos="word" start_char="2600">on</TOKEN>
<TOKEN end_char="2612" id="token-42-7" morph="none" pos="unknown" start_char="2603">05/02/2021</TOKEN>
<TOKEN end_char="2621" id="token-42-8" morph="none" pos="unknown" start_char="2614">18:12:36</TOKEN>
</SEG>
<SEG end_char="2659" id="segment-43" start_char="2624">
<ORIGINAL_TEXT>covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="2628" id="token-43-0" morph="none" pos="word" start_char="2624">covid</TOKEN>
<TOKEN end_char="2632" id="token-43-1" morph="none" pos="word" start_char="2630">was</TOKEN>
<TOKEN end_char="2635" id="token-43-2" morph="none" pos="word" start_char="2634">in</TOKEN>
<TOKEN end_char="2642" id="token-43-3" morph="none" pos="word" start_char="2637">Brazil</TOKEN>
<TOKEN end_char="2645" id="token-43-4" morph="none" pos="word" start_char="2644">in</TOKEN>
<TOKEN end_char="2654" id="token-43-5" morph="none" pos="word" start_char="2647">November</TOKEN>
<TOKEN end_char="2659" id="token-43-6" morph="none" pos="word" start_char="2656">2019</TOKEN>
</SEG>
<SEG end_char="2702" id="segment-44" start_char="2662">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2666" id="token-44-0" morph="none" pos="word" start_char="2662">Quote</TOKEN>
<TOKEN end_char="2671" id="token-44-1" morph="none" pos="word" start_char="2668">from</TOKEN>
<TOKEN end_char="2672" id="token-44-2" morph="none" pos="punct" start_char="2672">:</TOKEN>
<TOKEN end_char="2679" id="token-44-3" morph="none" pos="word" start_char="2674">Jolly2</TOKEN>
<TOKEN end_char="2682" id="token-44-4" morph="none" pos="word" start_char="2681">on</TOKEN>
<TOKEN end_char="2693" id="token-44-5" morph="none" pos="unknown" start_char="2684">05/02/2021</TOKEN>
<TOKEN end_char="2702" id="token-44-6" morph="none" pos="unknown" start_char="2695">18:12:36</TOKEN>
</SEG>
<SEG end_char="2727" id="segment-45" start_char="2705">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN end_char="2710" id="token-45-0" morph="none" pos="word" start_char="2705">France</TOKEN>
<TOKEN end_char="2713" id="token-45-1" morph="none" pos="word" start_char="2712">in</TOKEN>
<TOKEN end_char="2722" id="token-45-2" morph="none" pos="word" start_char="2715">November</TOKEN>
<TOKEN end_char="2727" id="token-45-3" morph="none" pos="word" start_char="2724">2019</TOKEN>
</SEG>
<SEG end_char="2770" id="segment-46" start_char="2730">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN end_char="2734" id="token-46-0" morph="none" pos="word" start_char="2730">Quote</TOKEN>
<TOKEN end_char="2739" id="token-46-1" morph="none" pos="word" start_char="2736">from</TOKEN>
<TOKEN end_char="2740" id="token-46-2" morph="none" pos="punct" start_char="2740">:</TOKEN>
<TOKEN end_char="2747" id="token-46-3" morph="none" pos="word" start_char="2742">Jolly2</TOKEN>
<TOKEN end_char="2750" id="token-46-4" morph="none" pos="word" start_char="2749">on</TOKEN>
<TOKEN end_char="2761" id="token-46-5" morph="none" pos="unknown" start_char="2752">05/02/2021</TOKEN>
<TOKEN end_char="2770" id="token-46-6" morph="none" pos="unknown" start_char="2763">18:12:36</TOKEN>
</SEG>
<SEG end_char="2804" id="segment-47" start_char="2773">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN end_char="2775" id="token-47-0" morph="none" pos="word" start_char="2773">And</TOKEN>
<TOKEN end_char="2781" id="token-47-1" morph="none" pos="word" start_char="2777">spain</TOKEN>
<TOKEN end_char="2784" id="token-47-2" morph="none" pos="word" start_char="2783">as</TOKEN>
<TOKEN end_char="2790" id="token-47-3" morph="none" pos="word" start_char="2786">early</TOKEN>
<TOKEN end_char="2793" id="token-47-4" morph="none" pos="word" start_char="2792">as</TOKEN>
<TOKEN end_char="2799" id="token-47-5" morph="none" pos="word" start_char="2795">March</TOKEN>
<TOKEN end_char="2804" id="token-47-6" morph="none" pos="word" start_char="2801">2019</TOKEN>
</SEG>
<SEG end_char="2847" id="segment-48" start_char="2807">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:15:55</ORIGINAL_TEXT>
<TOKEN end_char="2811" id="token-48-0" morph="none" pos="word" start_char="2807">Quote</TOKEN>
<TOKEN end_char="2816" id="token-48-1" morph="none" pos="word" start_char="2813">from</TOKEN>
<TOKEN end_char="2817" id="token-48-2" morph="none" pos="punct" start_char="2817">:</TOKEN>
<TOKEN end_char="2824" id="token-48-3" morph="none" pos="word" start_char="2819">Jolly2</TOKEN>
<TOKEN end_char="2827" id="token-48-4" morph="none" pos="word" start_char="2826">on</TOKEN>
<TOKEN end_char="2838" id="token-48-5" morph="none" pos="unknown" start_char="2829">05/02/2021</TOKEN>
<TOKEN end_char="2847" id="token-48-6" morph="none" pos="unknown" start_char="2840">18:15:55</TOKEN>
</SEG>
<SEG end_char="2941" id="segment-49" start_char="2850">
<ORIGINAL_TEXT>We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="2851" id="token-49-0" morph="none" pos="word" start_char="2850">We</TOKEN>
<TOKEN end_char="2856" id="token-49-1" morph="none" pos="word" start_char="2853">know</TOKEN>
<TOKEN end_char="2863" id="token-49-2" morph="none" pos="word" start_char="2858">Donald</TOKEN>
<TOKEN end_char="2869" id="token-49-3" morph="none" pos="word" start_char="2865">Trump</TOKEN>
<TOKEN end_char="2877" id="token-49-4" morph="none" pos="word" start_char="2871">claimed</TOKEN>
<TOKEN end_char="2880" id="token-49-5" morph="none" pos="word" start_char="2879">to</TOKEN>
<TOKEN end_char="2885" id="token-49-6" morph="none" pos="word" start_char="2882">have</TOKEN>
<TOKEN end_char="2890" id="token-49-7" morph="none" pos="word" start_char="2887">seen</TOKEN>
<TOKEN end_char="2899" id="token-49-8" morph="none" pos="word" start_char="2892">evidence</TOKEN>
<TOKEN end_char="2904" id="token-49-9" morph="none" pos="word" start_char="2901">that</TOKEN>
<TOKEN end_char="2910" id="token-49-10" morph="none" pos="word" start_char="2906">covid</TOKEN>
<TOKEN end_char="2913" id="token-49-11" morph="none" pos="word" start_char="2912">19</TOKEN>
<TOKEN end_char="2917" id="token-49-12" morph="none" pos="word" start_char="2915">had</TOKEN>
<TOKEN end_char="2922" id="token-49-13" morph="none" pos="word" start_char="2919">come</TOKEN>
<TOKEN end_char="2927" id="token-49-14" morph="none" pos="word" start_char="2924">from</TOKEN>
<TOKEN end_char="2929" id="token-49-15" morph="none" pos="word" start_char="2929">a</TOKEN>
<TOKEN end_char="2940" id="token-49-16" morph="none" pos="word" start_char="2931">laboratory</TOKEN>
<TOKEN end_char="2941" id="token-49-17" morph="none" pos="punct" start_char="2941">.</TOKEN>
</SEG>
<SEG end_char="3001" id="segment-50" start_char="2943">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN end_char="2947" id="token-50-0" morph="none" pos="word" start_char="2943">Could</TOKEN>
<TOKEN end_char="2950" id="token-50-1" morph="none" pos="word" start_char="2949">it</TOKEN>
<TOKEN end_char="2953" id="token-50-2" morph="none" pos="word" start_char="2952">be</TOKEN>
<TOKEN end_char="2958" id="token-50-3" morph="none" pos="word" start_char="2955">that</TOKEN>
<TOKEN end_char="2962" id="token-50-4" morph="none" pos="word" start_char="2960">the</TOKEN>
<TOKEN end_char="2971" id="token-50-5" morph="none" pos="word" start_char="2964">evidence</TOKEN>
<TOKEN end_char="2974" id="token-50-6" morph="none" pos="word" start_char="2973">he</TOKEN>
<TOKEN end_char="2978" id="token-50-7" morph="none" pos="word" start_char="2976">saw</TOKEN>
<TOKEN end_char="2982" id="token-50-8" morph="none" pos="word" start_char="2980">was</TOKEN>
<TOKEN end_char="2987" id="token-50-9" morph="none" pos="word" start_char="2984">from</TOKEN>
<TOKEN end_char="2992" id="token-50-10" morph="none" pos="word" start_char="2989">fort</TOKEN>
<TOKEN end_char="3000" id="token-50-11" morph="none" pos="word" start_char="2994">Detrick</TOKEN>
<TOKEN end_char="3001" id="token-50-12" morph="none" pos="punct" start_char="3001">?</TOKEN>
</SEG>
<SEG end_char="3033" id="segment-51" start_char="3004">
<ORIGINAL_TEXT>you should be more scientific.</ORIGINAL_TEXT>
<TOKEN end_char="3006" id="token-51-0" morph="none" pos="word" start_char="3004">you</TOKEN>
<TOKEN end_char="3013" id="token-51-1" morph="none" pos="word" start_char="3008">should</TOKEN>
<TOKEN end_char="3016" id="token-51-2" morph="none" pos="word" start_char="3015">be</TOKEN>
<TOKEN end_char="3021" id="token-51-3" morph="none" pos="word" start_char="3018">more</TOKEN>
<TOKEN end_char="3032" id="token-51-4" morph="none" pos="word" start_char="3023">scientific</TOKEN>
<TOKEN end_char="3033" id="token-51-5" morph="none" pos="punct" start_char="3033">.</TOKEN>
</SEG>
<SEG end_char="3071" id="segment-52" start_char="3036">
<ORIGINAL_TEXT>Hardly they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="3041" id="token-52-0" morph="none" pos="word" start_char="3036">Hardly</TOKEN>
<TOKEN end_char="3046" id="token-52-1" morph="none" pos="word" start_char="3043">they</TOKEN>
<TOKEN end_char="3052" id="token-52-2" morph="none" pos="word" start_char="3048">found</TOKEN>
<TOKEN end_char="3060" id="token-52-3" morph="none" pos="word" start_char="3054">samples</TOKEN>
<TOKEN end_char="3063" id="token-52-4" morph="none" pos="word" start_char="3062">in</TOKEN>
<TOKEN end_char="3070" id="token-52-5" morph="none" pos="word" start_char="3065">sewage</TOKEN>
<TOKEN end_char="3071" id="token-52-6" morph="none" pos="punct" start_char="3071">.</TOKEN>
</SEG>
<SEG end_char="3122" id="segment-53" start_char="3073">
<ORIGINAL_TEXT>I'm not claiming any facts for the origin You are.</ORIGINAL_TEXT>
<TOKEN end_char="3075" id="token-53-0" morph="none" pos="word" start_char="3073">I'm</TOKEN>
<TOKEN end_char="3079" id="token-53-1" morph="none" pos="word" start_char="3077">not</TOKEN>
<TOKEN end_char="3088" id="token-53-2" morph="none" pos="word" start_char="3081">claiming</TOKEN>
<TOKEN end_char="3092" id="token-53-3" morph="none" pos="word" start_char="3090">any</TOKEN>
<TOKEN end_char="3098" id="token-53-4" morph="none" pos="word" start_char="3094">facts</TOKEN>
<TOKEN end_char="3102" id="token-53-5" morph="none" pos="word" start_char="3100">for</TOKEN>
<TOKEN end_char="3106" id="token-53-6" morph="none" pos="word" start_char="3104">the</TOKEN>
<TOKEN end_char="3113" id="token-53-7" morph="none" pos="word" start_char="3108">origin</TOKEN>
<TOKEN end_char="3117" id="token-53-8" morph="none" pos="word" start_char="3115">You</TOKEN>
<TOKEN end_char="3121" id="token-53-9" morph="none" pos="word" start_char="3119">are</TOKEN>
<TOKEN end_char="3122" id="token-53-10" morph="none" pos="punct" start_char="3122">.</TOKEN>
</SEG>
<SEG end_char="3166" id="segment-54" start_char="3126">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN end_char="3130" id="token-54-0" morph="none" pos="word" start_char="3126">Quote</TOKEN>
<TOKEN end_char="3135" id="token-54-1" morph="none" pos="word" start_char="3132">from</TOKEN>
<TOKEN end_char="3136" id="token-54-2" morph="none" pos="punct" start_char="3136">:</TOKEN>
<TOKEN end_char="3143" id="token-54-3" morph="none" pos="word" start_char="3138">Jolly2</TOKEN>
<TOKEN end_char="3146" id="token-54-4" morph="none" pos="word" start_char="3145">on</TOKEN>
<TOKEN end_char="3157" id="token-54-5" morph="none" pos="unknown" start_char="3148">06/02/2021</TOKEN>
<TOKEN end_char="3166" id="token-54-6" morph="none" pos="unknown" start_char="3159">20:16:16</TOKEN>
</SEG>
<SEG end_char="3197" id="segment-55" start_char="3169">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="3172" id="token-55-0" morph="none" pos="word" start_char="3169">they</TOKEN>
<TOKEN end_char="3178" id="token-55-1" morph="none" pos="word" start_char="3174">found</TOKEN>
<TOKEN end_char="3186" id="token-55-2" morph="none" pos="word" start_char="3180">samples</TOKEN>
<TOKEN end_char="3189" id="token-55-3" morph="none" pos="word" start_char="3188">in</TOKEN>
<TOKEN end_char="3196" id="token-55-4" morph="none" pos="word" start_char="3191">sewage</TOKEN>
<TOKEN end_char="3197" id="token-55-5" morph="none" pos="punct" start_char="3197">.</TOKEN>
</SEG>
<SEG end_char="3232" id="segment-56" start_char="3201">
<ORIGINAL_TEXT>How sure are you of that "fact"?</ORIGINAL_TEXT>
<TOKEN end_char="3203" id="token-56-0" morph="none" pos="word" start_char="3201">How</TOKEN>
<TOKEN end_char="3208" id="token-56-1" morph="none" pos="word" start_char="3205">sure</TOKEN>
<TOKEN end_char="3212" id="token-56-2" morph="none" pos="word" start_char="3210">are</TOKEN>
<TOKEN end_char="3216" id="token-56-3" morph="none" pos="word" start_char="3214">you</TOKEN>
<TOKEN end_char="3219" id="token-56-4" morph="none" pos="word" start_char="3218">of</TOKEN>
<TOKEN end_char="3224" id="token-56-5" morph="none" pos="word" start_char="3221">that</TOKEN>
<TOKEN end_char="3226" id="token-56-6" morph="none" pos="punct" start_char="3226">"</TOKEN>
<TOKEN end_char="3230" id="token-56-7" morph="none" pos="word" start_char="3227">fact</TOKEN>
<TOKEN end_char="3232" id="token-56-8" morph="none" pos="punct" start_char="3231">"?</TOKEN>
</SEG>
<SEG end_char="3283" id="segment-57" start_char="3236">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN end_char="3240" id="token-57-0" morph="none" pos="word" start_char="3236">Quote</TOKEN>
<TOKEN end_char="3245" id="token-57-1" morph="none" pos="word" start_char="3242">from</TOKEN>
<TOKEN end_char="3246" id="token-57-2" morph="none" pos="punct" start_char="3246">:</TOKEN>
<TOKEN end_char="3252" id="token-57-3" morph="none" pos="word" start_char="3248">Bored</TOKEN>
<TOKEN end_char="3260" id="token-57-4" morph="none" pos="word" start_char="3254">chemist</TOKEN>
<TOKEN end_char="3263" id="token-57-5" morph="none" pos="word" start_char="3262">on</TOKEN>
<TOKEN end_char="3274" id="token-57-6" morph="none" pos="unknown" start_char="3265">06/02/2021</TOKEN>
<TOKEN end_char="3283" id="token-57-7" morph="none" pos="unknown" start_char="3276">20:19:01</TOKEN>
</SEG>
<SEG end_char="3326" id="segment-58" start_char="3286">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN end_char="3290" id="token-58-0" morph="none" pos="word" start_char="3286">Quote</TOKEN>
<TOKEN end_char="3295" id="token-58-1" morph="none" pos="word" start_char="3292">from</TOKEN>
<TOKEN end_char="3296" id="token-58-2" morph="none" pos="punct" start_char="3296">:</TOKEN>
<TOKEN end_char="3303" id="token-58-3" morph="none" pos="word" start_char="3298">Jolly2</TOKEN>
<TOKEN end_char="3306" id="token-58-4" morph="none" pos="word" start_char="3305">on</TOKEN>
<TOKEN end_char="3317" id="token-58-5" morph="none" pos="unknown" start_char="3308">06/02/2021</TOKEN>
<TOKEN end_char="3326" id="token-58-6" morph="none" pos="unknown" start_char="3319">20:16:16</TOKEN>
</SEG>
<SEG end_char="3357" id="segment-59" start_char="3329">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="3332" id="token-59-0" morph="none" pos="word" start_char="3329">they</TOKEN>
<TOKEN end_char="3338" id="token-59-1" morph="none" pos="word" start_char="3334">found</TOKEN>
<TOKEN end_char="3346" id="token-59-2" morph="none" pos="word" start_char="3340">samples</TOKEN>
<TOKEN end_char="3349" id="token-59-3" morph="none" pos="word" start_char="3348">in</TOKEN>
<TOKEN end_char="3356" id="token-59-4" morph="none" pos="word" start_char="3351">sewage</TOKEN>
<TOKEN end_char="3357" id="token-59-5" morph="none" pos="punct" start_char="3357">.</TOKEN>
</SEG>
<SEG end_char="3391" id="segment-60" start_char="3360">
<ORIGINAL_TEXT>How sure are you of that "fact"?</ORIGINAL_TEXT>
<TOKEN end_char="3362" id="token-60-0" morph="none" pos="word" start_char="3360">How</TOKEN>
<TOKEN end_char="3367" id="token-60-1" morph="none" pos="word" start_char="3364">sure</TOKEN>
<TOKEN end_char="3371" id="token-60-2" morph="none" pos="word" start_char="3369">are</TOKEN>
<TOKEN end_char="3375" id="token-60-3" morph="none" pos="word" start_char="3373">you</TOKEN>
<TOKEN end_char="3378" id="token-60-4" morph="none" pos="word" start_char="3377">of</TOKEN>
<TOKEN end_char="3383" id="token-60-5" morph="none" pos="word" start_char="3380">that</TOKEN>
<TOKEN end_char="3385" id="token-60-6" morph="none" pos="punct" start_char="3385">"</TOKEN>
<TOKEN end_char="3389" id="token-60-7" morph="none" pos="word" start_char="3386">fact</TOKEN>
<TOKEN end_char="3391" id="token-60-8" morph="none" pos="punct" start_char="3390">"?</TOKEN>
</SEG>
<SEG end_char="3435" id="segment-61" start_char="3394">
<ORIGINAL_TEXT>Sure enough to post it here for discussion</ORIGINAL_TEXT>
<TOKEN end_char="3397" id="token-61-0" morph="none" pos="word" start_char="3394">Sure</TOKEN>
<TOKEN end_char="3404" id="token-61-1" morph="none" pos="word" start_char="3399">enough</TOKEN>
<TOKEN end_char="3407" id="token-61-2" morph="none" pos="word" start_char="3406">to</TOKEN>
<TOKEN end_char="3412" id="token-61-3" morph="none" pos="word" start_char="3409">post</TOKEN>
<TOKEN end_char="3415" id="token-61-4" morph="none" pos="word" start_char="3414">it</TOKEN>
<TOKEN end_char="3420" id="token-61-5" morph="none" pos="word" start_char="3417">here</TOKEN>
<TOKEN end_char="3424" id="token-61-6" morph="none" pos="word" start_char="3422">for</TOKEN>
<TOKEN end_char="3435" id="token-61-7" morph="none" pos="word" start_char="3426">discussion</TOKEN>
</SEG>
<SEG end_char="3479" id="segment-62" start_char="3439">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:27:47</ORIGINAL_TEXT>
<TOKEN end_char="3443" id="token-62-0" morph="none" pos="word" start_char="3439">Quote</TOKEN>
<TOKEN end_char="3448" id="token-62-1" morph="none" pos="word" start_char="3445">from</TOKEN>
<TOKEN end_char="3449" id="token-62-2" morph="none" pos="punct" start_char="3449">:</TOKEN>
<TOKEN end_char="3456" id="token-62-3" morph="none" pos="word" start_char="3451">Jolly2</TOKEN>
<TOKEN end_char="3459" id="token-62-4" morph="none" pos="word" start_char="3458">on</TOKEN>
<TOKEN end_char="3470" id="token-62-5" morph="none" pos="unknown" start_char="3461">06/02/2021</TOKEN>
<TOKEN end_char="3479" id="token-62-6" morph="none" pos="unknown" start_char="3472">20:27:47</TOKEN>
</SEG>
<SEG end_char="3529" id="segment-63" start_char="3482">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN end_char="3486" id="token-63-0" morph="none" pos="word" start_char="3482">Quote</TOKEN>
<TOKEN end_char="3491" id="token-63-1" morph="none" pos="word" start_char="3488">from</TOKEN>
<TOKEN end_char="3492" id="token-63-2" morph="none" pos="punct" start_char="3492">:</TOKEN>
<TOKEN end_char="3498" id="token-63-3" morph="none" pos="word" start_char="3494">Bored</TOKEN>
<TOKEN end_char="3506" id="token-63-4" morph="none" pos="word" start_char="3500">chemist</TOKEN>
<TOKEN end_char="3509" id="token-63-5" morph="none" pos="word" start_char="3508">on</TOKEN>
<TOKEN end_char="3520" id="token-63-6" morph="none" pos="unknown" start_char="3511">06/02/2021</TOKEN>
<TOKEN end_char="3529" id="token-63-7" morph="none" pos="unknown" start_char="3522">20:19:01</TOKEN>
</SEG>
<SEG end_char="3572" id="segment-64" start_char="3532">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN end_char="3536" id="token-64-0" morph="none" pos="word" start_char="3532">Quote</TOKEN>
<TOKEN end_char="3541" id="token-64-1" morph="none" pos="word" start_char="3538">from</TOKEN>
<TOKEN end_char="3542" id="token-64-2" morph="none" pos="punct" start_char="3542">:</TOKEN>
<TOKEN end_char="3549" id="token-64-3" morph="none" pos="word" start_char="3544">Jolly2</TOKEN>
<TOKEN end_char="3552" id="token-64-4" morph="none" pos="word" start_char="3551">on</TOKEN>
<TOKEN end_char="3563" id="token-64-5" morph="none" pos="unknown" start_char="3554">06/02/2021</TOKEN>
<TOKEN end_char="3572" id="token-64-6" morph="none" pos="unknown" start_char="3565">20:16:16</TOKEN>
</SEG>
<SEG end_char="3603" id="segment-65" start_char="3575">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="3578" id="token-65-0" morph="none" pos="word" start_char="3575">they</TOKEN>
<TOKEN end_char="3584" id="token-65-1" morph="none" pos="word" start_char="3580">found</TOKEN>
<TOKEN end_char="3592" id="token-65-2" morph="none" pos="word" start_char="3586">samples</TOKEN>
<TOKEN end_char="3595" id="token-65-3" morph="none" pos="word" start_char="3594">in</TOKEN>
<TOKEN end_char="3602" id="token-65-4" morph="none" pos="word" start_char="3597">sewage</TOKEN>
<TOKEN end_char="3603" id="token-65-5" morph="none" pos="punct" start_char="3603">.</TOKEN>
</SEG>
<SEG end_char="3679" id="segment-66" start_char="3606">
<ORIGINAL_TEXT>How sure are you of that "fact"?Sure enough to post it here for discussion</ORIGINAL_TEXT>
<TOKEN end_char="3608" id="token-66-0" morph="none" pos="word" start_char="3606">How</TOKEN>
<TOKEN end_char="3613" id="token-66-1" morph="none" pos="word" start_char="3610">sure</TOKEN>
<TOKEN end_char="3617" id="token-66-2" morph="none" pos="word" start_char="3615">are</TOKEN>
<TOKEN end_char="3621" id="token-66-3" morph="none" pos="word" start_char="3619">you</TOKEN>
<TOKEN end_char="3624" id="token-66-4" morph="none" pos="word" start_char="3623">of</TOKEN>
<TOKEN end_char="3629" id="token-66-5" morph="none" pos="word" start_char="3626">that</TOKEN>
<TOKEN end_char="3631" id="token-66-6" morph="none" pos="punct" start_char="3631">"</TOKEN>
<TOKEN end_char="3641" id="token-66-7" morph="none" pos="unknown" start_char="3632">fact"?Sure</TOKEN>
<TOKEN end_char="3648" id="token-66-8" morph="none" pos="word" start_char="3643">enough</TOKEN>
<TOKEN end_char="3651" id="token-66-9" morph="none" pos="word" start_char="3650">to</TOKEN>
<TOKEN end_char="3656" id="token-66-10" morph="none" pos="word" start_char="3653">post</TOKEN>
<TOKEN end_char="3659" id="token-66-11" morph="none" pos="word" start_char="3658">it</TOKEN>
<TOKEN end_char="3664" id="token-66-12" morph="none" pos="word" start_char="3661">here</TOKEN>
<TOKEN end_char="3668" id="token-66-13" morph="none" pos="word" start_char="3666">for</TOKEN>
<TOKEN end_char="3679" id="token-66-14" morph="none" pos="word" start_char="3670">discussion</TOKEN>
</SEG>
<SEG end_char="3712" id="segment-67" start_char="3682">
<ORIGINAL_TEXT>Then I guess I can do the same.</ORIGINAL_TEXT>
<TOKEN end_char="3685" id="token-67-0" morph="none" pos="word" start_char="3682">Then</TOKEN>
<TOKEN end_char="3687" id="token-67-1" morph="none" pos="word" start_char="3687">I</TOKEN>
<TOKEN end_char="3693" id="token-67-2" morph="none" pos="word" start_char="3689">guess</TOKEN>
<TOKEN end_char="3695" id="token-67-3" morph="none" pos="word" start_char="3695">I</TOKEN>
<TOKEN end_char="3699" id="token-67-4" morph="none" pos="word" start_char="3697">can</TOKEN>
<TOKEN end_char="3702" id="token-67-5" morph="none" pos="word" start_char="3701">do</TOKEN>
<TOKEN end_char="3706" id="token-67-6" morph="none" pos="word" start_char="3704">the</TOKEN>
<TOKEN end_char="3711" id="token-67-7" morph="none" pos="word" start_char="3708">same</TOKEN>
<TOKEN end_char="3712" id="token-67-8" morph="none" pos="punct" start_char="3712">.</TOKEN>
</SEG>
<SEG end_char="3762" id="segment-68" start_char="3715">
<ORIGINAL_TEXT>Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN end_char="3719" id="token-68-0" morph="none" pos="word" start_char="3715">Quote</TOKEN>
<TOKEN end_char="3724" id="token-68-1" morph="none" pos="word" start_char="3721">from</TOKEN>
<TOKEN end_char="3725" id="token-68-2" morph="none" pos="punct" start_char="3725">:</TOKEN>
<TOKEN end_char="3731" id="token-68-3" morph="none" pos="word" start_char="3727">Bored</TOKEN>
<TOKEN end_char="3739" id="token-68-4" morph="none" pos="word" start_char="3733">chemist</TOKEN>
<TOKEN end_char="3742" id="token-68-5" morph="none" pos="word" start_char="3741">on</TOKEN>
<TOKEN end_char="3753" id="token-68-6" morph="none" pos="unknown" start_char="3744">05/02/2021</TOKEN>
<TOKEN end_char="3762" id="token-68-7" morph="none" pos="unknown" start_char="3755">18:56:50</TOKEN>
</SEG>
<SEG end_char="3794" id="segment-69" start_char="3765">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN end_char="3765" id="token-69-0" morph="none" pos="punct" start_char="3765">"</TOKEN>
<TOKEN end_char="3770" id="token-69-1" morph="none" pos="word" start_char="3766">Where</TOKEN>
<TOKEN end_char="3774" id="token-69-2" morph="none" pos="word" start_char="3772">did</TOKEN>
<TOKEN end_char="3780" id="token-69-3" morph="none" pos="word" start_char="3776">covid</TOKEN>
<TOKEN end_char="3783" id="token-69-4" morph="none" pos="word" start_char="3782">19</TOKEN>
<TOKEN end_char="3793" id="token-69-5" morph="none" pos="word" start_char="3785">originate</TOKEN>
<TOKEN end_char="3794" id="token-69-6" morph="none" pos="punct" start_char="3794">?</TOKEN>
</SEG>
<SEG end_char="3802" id="segment-70" start_char="3796">
<ORIGINAL_TEXT>"Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="3796" id="token-70-0" morph="none" pos="punct" start_char="3796">"</TOKEN>
<TOKEN end_char="3801" id="token-70-1" morph="none" pos="word" start_char="3797">Wuhan</TOKEN>
<TOKEN end_char="3802" id="token-70-2" morph="none" pos="punct" start_char="3802">.</TOKEN>
<TRANSLATED_TEXT>Wuhan.</TRANSLATED_TEXT><DETECTED_LANGUAGE>so</DETECTED_LANGUAGE></SEG>
<SEG end_char="3851" id="segment-71" start_char="3805">
<ORIGINAL_TEXT>Do you have any actual evidence that I'm wrong?</ORIGINAL_TEXT>
<TOKEN end_char="3806" id="token-71-0" morph="none" pos="word" start_char="3805">Do</TOKEN>
<TOKEN end_char="3810" id="token-71-1" morph="none" pos="word" start_char="3808">you</TOKEN>
<TOKEN end_char="3815" id="token-71-2" morph="none" pos="word" start_char="3812">have</TOKEN>
<TOKEN end_char="3819" id="token-71-3" morph="none" pos="word" start_char="3817">any</TOKEN>
<TOKEN end_char="3826" id="token-71-4" morph="none" pos="word" start_char="3821">actual</TOKEN>
<TOKEN end_char="3835" id="token-71-5" morph="none" pos="word" start_char="3828">evidence</TOKEN>
<TOKEN end_char="3840" id="token-71-6" morph="none" pos="word" start_char="3837">that</TOKEN>
<TOKEN end_char="3844" id="token-71-7" morph="none" pos="word" start_char="3842">I'm</TOKEN>
<TOKEN end_char="3850" id="token-71-8" morph="none" pos="word" start_char="3846">wrong</TOKEN>
<TOKEN end_char="3851" id="token-71-9" morph="none" pos="punct" start_char="3851">?</TOKEN>
</SEG>
<SEG end_char="4000" id="segment-72" start_char="3853">
<ORIGINAL_TEXT>(To do that you would have to prove that it was- for example- possible for the virus to transfer from Spain to china, but not the other way round.).</ORIGINAL_TEXT>
<TOKEN end_char="3853" id="token-72-0" morph="none" pos="punct" start_char="3853">(</TOKEN>
<TOKEN end_char="3855" id="token-72-1" morph="none" pos="word" start_char="3854">To</TOKEN>
<TOKEN end_char="3858" id="token-72-2" morph="none" pos="word" start_char="3857">do</TOKEN>
<TOKEN end_char="3863" id="token-72-3" morph="none" pos="word" start_char="3860">that</TOKEN>
<TOKEN end_char="3867" id="token-72-4" morph="none" pos="word" start_char="3865">you</TOKEN>
<TOKEN end_char="3873" id="token-72-5" morph="none" pos="word" start_char="3869">would</TOKEN>
<TOKEN end_char="3878" id="token-72-6" morph="none" pos="word" start_char="3875">have</TOKEN>
<TOKEN end_char="3881" id="token-72-7" morph="none" pos="word" start_char="3880">to</TOKEN>
<TOKEN end_char="3887" id="token-72-8" morph="none" pos="word" start_char="3883">prove</TOKEN>
<TOKEN end_char="3892" id="token-72-9" morph="none" pos="word" start_char="3889">that</TOKEN>
<TOKEN end_char="3895" id="token-72-10" morph="none" pos="word" start_char="3894">it</TOKEN>
<TOKEN end_char="3899" id="token-72-11" morph="none" pos="word" start_char="3897">was</TOKEN>
<TOKEN end_char="3900" id="token-72-12" morph="none" pos="punct" start_char="3900">-</TOKEN>
<TOKEN end_char="3904" id="token-72-13" morph="none" pos="word" start_char="3902">for</TOKEN>
<TOKEN end_char="3912" id="token-72-14" morph="none" pos="word" start_char="3906">example</TOKEN>
<TOKEN end_char="3913" id="token-72-15" morph="none" pos="punct" start_char="3913">-</TOKEN>
<TOKEN end_char="3922" id="token-72-16" morph="none" pos="word" start_char="3915">possible</TOKEN>
<TOKEN end_char="3926" id="token-72-17" morph="none" pos="word" start_char="3924">for</TOKEN>
<TOKEN end_char="3930" id="token-72-18" morph="none" pos="word" start_char="3928">the</TOKEN>
<TOKEN end_char="3936" id="token-72-19" morph="none" pos="word" start_char="3932">virus</TOKEN>
<TOKEN end_char="3939" id="token-72-20" morph="none" pos="word" start_char="3938">to</TOKEN>
<TOKEN end_char="3948" id="token-72-21" morph="none" pos="word" start_char="3941">transfer</TOKEN>
<TOKEN end_char="3953" id="token-72-22" morph="none" pos="word" start_char="3950">from</TOKEN>
<TOKEN end_char="3959" id="token-72-23" morph="none" pos="word" start_char="3955">Spain</TOKEN>
<TOKEN end_char="3962" id="token-72-24" morph="none" pos="word" start_char="3961">to</TOKEN>
<TOKEN end_char="3968" id="token-72-25" morph="none" pos="word" start_char="3964">china</TOKEN>
<TOKEN end_char="3969" id="token-72-26" morph="none" pos="punct" start_char="3969">,</TOKEN>
<TOKEN end_char="3973" id="token-72-27" morph="none" pos="word" start_char="3971">but</TOKEN>
<TOKEN end_char="3977" id="token-72-28" morph="none" pos="word" start_char="3975">not</TOKEN>
<TOKEN end_char="3981" id="token-72-29" morph="none" pos="word" start_char="3979">the</TOKEN>
<TOKEN end_char="3987" id="token-72-30" morph="none" pos="word" start_char="3983">other</TOKEN>
<TOKEN end_char="3991" id="token-72-31" morph="none" pos="word" start_char="3989">way</TOKEN>
<TOKEN end_char="3997" id="token-72-32" morph="none" pos="word" start_char="3993">round</TOKEN>
<TOKEN end_char="4000" id="token-72-33" morph="none" pos="punct" start_char="3998">.).</TOKEN>
</SEG>
<SEG end_char="4179" id="segment-73" start_char="4003">
<ORIGINAL_TEXT>I also look forward to your explanation of how the virus didn't spread much while it was in Spain, France, Brazil or wherever, but suddenly became massively infections in China.</ORIGINAL_TEXT>
<TOKEN end_char="4003" id="token-73-0" morph="none" pos="word" start_char="4003">I</TOKEN>
<TOKEN end_char="4008" id="token-73-1" morph="none" pos="word" start_char="4005">also</TOKEN>
<TOKEN end_char="4013" id="token-73-2" morph="none" pos="word" start_char="4010">look</TOKEN>
<TOKEN end_char="4021" id="token-73-3" morph="none" pos="word" start_char="4015">forward</TOKEN>
<TOKEN end_char="4024" id="token-73-4" morph="none" pos="word" start_char="4023">to</TOKEN>
<TOKEN end_char="4029" id="token-73-5" morph="none" pos="word" start_char="4026">your</TOKEN>
<TOKEN end_char="4041" id="token-73-6" morph="none" pos="word" start_char="4031">explanation</TOKEN>
<TOKEN end_char="4044" id="token-73-7" morph="none" pos="word" start_char="4043">of</TOKEN>
<TOKEN end_char="4048" id="token-73-8" morph="none" pos="word" start_char="4046">how</TOKEN>
<TOKEN end_char="4052" id="token-73-9" morph="none" pos="word" start_char="4050">the</TOKEN>
<TOKEN end_char="4058" id="token-73-10" morph="none" pos="word" start_char="4054">virus</TOKEN>
<TOKEN end_char="4065" id="token-73-11" morph="none" pos="word" start_char="4060">didn't</TOKEN>
<TOKEN end_char="4072" id="token-73-12" morph="none" pos="word" start_char="4067">spread</TOKEN>
<TOKEN end_char="4077" id="token-73-13" morph="none" pos="word" start_char="4074">much</TOKEN>
<TOKEN end_char="4083" id="token-73-14" morph="none" pos="word" start_char="4079">while</TOKEN>
<TOKEN end_char="4086" id="token-73-15" morph="none" pos="word" start_char="4085">it</TOKEN>
<TOKEN end_char="4090" id="token-73-16" morph="none" pos="word" start_char="4088">was</TOKEN>
<TOKEN end_char="4093" id="token-73-17" morph="none" pos="word" start_char="4092">in</TOKEN>
<TOKEN end_char="4099" id="token-73-18" morph="none" pos="word" start_char="4095">Spain</TOKEN>
<TOKEN end_char="4100" id="token-73-19" morph="none" pos="punct" start_char="4100">,</TOKEN>
<TOKEN end_char="4107" id="token-73-20" morph="none" pos="word" start_char="4102">France</TOKEN>
<TOKEN end_char="4108" id="token-73-21" morph="none" pos="punct" start_char="4108">,</TOKEN>
<TOKEN end_char="4115" id="token-73-22" morph="none" pos="word" start_char="4110">Brazil</TOKEN>
<TOKEN end_char="4118" id="token-73-23" morph="none" pos="word" start_char="4117">or</TOKEN>
<TOKEN end_char="4127" id="token-73-24" morph="none" pos="word" start_char="4120">wherever</TOKEN>
<TOKEN end_char="4128" id="token-73-25" morph="none" pos="punct" start_char="4128">,</TOKEN>
<TOKEN end_char="4132" id="token-73-26" morph="none" pos="word" start_char="4130">but</TOKEN>
<TOKEN end_char="4141" id="token-73-27" morph="none" pos="word" start_char="4134">suddenly</TOKEN>
<TOKEN end_char="4148" id="token-73-28" morph="none" pos="word" start_char="4143">became</TOKEN>
<TOKEN end_char="4158" id="token-73-29" morph="none" pos="word" start_char="4150">massively</TOKEN>
<TOKEN end_char="4169" id="token-73-30" morph="none" pos="word" start_char="4160">infections</TOKEN>
<TOKEN end_char="4172" id="token-73-31" morph="none" pos="word" start_char="4171">in</TOKEN>
<TOKEN end_char="4178" id="token-73-32" morph="none" pos="word" start_char="4174">China</TOKEN>
<TOKEN end_char="4179" id="token-73-33" morph="none" pos="punct" start_char="4179">.</TOKEN>
</SEG>
<SEG end_char="4240" id="segment-74" start_char="4182">
<ORIGINAL_TEXT>But, those observations and deductions are merely evidence.</ORIGINAL_TEXT>
<TOKEN end_char="4184" id="token-74-0" morph="none" pos="word" start_char="4182">But</TOKEN>
<TOKEN end_char="4185" id="token-74-1" morph="none" pos="punct" start_char="4185">,</TOKEN>
<TOKEN end_char="4191" id="token-74-2" morph="none" pos="word" start_char="4187">those</TOKEN>
<TOKEN end_char="4204" id="token-74-3" morph="none" pos="word" start_char="4193">observations</TOKEN>
<TOKEN end_char="4208" id="token-74-4" morph="none" pos="word" start_char="4206">and</TOKEN>
<TOKEN end_char="4219" id="token-74-5" morph="none" pos="word" start_char="4210">deductions</TOKEN>
<TOKEN end_char="4223" id="token-74-6" morph="none" pos="word" start_char="4221">are</TOKEN>
<TOKEN end_char="4230" id="token-74-7" morph="none" pos="word" start_char="4225">merely</TOKEN>
<TOKEN end_char="4239" id="token-74-8" morph="none" pos="word" start_char="4232">evidence</TOKEN>
<TOKEN end_char="4240" id="token-74-9" morph="none" pos="punct" start_char="4240">.</TOKEN>
</SEG>
<SEG end_char="4286" id="segment-75" start_char="4242">
<ORIGINAL_TEXT>I don't expect you to pay them any attention.</ORIGINAL_TEXT>
<TOKEN end_char="4242" id="token-75-0" morph="none" pos="word" start_char="4242">I</TOKEN>
<TOKEN end_char="4248" id="token-75-1" morph="none" pos="word" start_char="4244">don't</TOKEN>
<TOKEN end_char="4255" id="token-75-2" morph="none" pos="word" start_char="4250">expect</TOKEN>
<TOKEN end_char="4259" id="token-75-3" morph="none" pos="word" start_char="4257">you</TOKEN>
<TOKEN end_char="4262" id="token-75-4" morph="none" pos="word" start_char="4261">to</TOKEN>
<TOKEN end_char="4266" id="token-75-5" morph="none" pos="word" start_char="4264">pay</TOKEN>
<TOKEN end_char="4271" id="token-75-6" morph="none" pos="word" start_char="4268">them</TOKEN>
<TOKEN end_char="4275" id="token-75-7" morph="none" pos="word" start_char="4273">any</TOKEN>
<TOKEN end_char="4285" id="token-75-8" morph="none" pos="word" start_char="4277">attention</TOKEN>
<TOKEN end_char="4286" id="token-75-9" morph="none" pos="punct" start_char="4286">.</TOKEN>
</SEG>
<SEG end_char="4346" id="segment-76" start_char="4291">
<ORIGINAL_TEXT>The retirement home is a 4 hour drive from the facility.</ORIGINAL_TEXT>
<TOKEN end_char="4293" id="token-76-0" morph="none" pos="word" start_char="4291">The</TOKEN>
<TOKEN end_char="4304" id="token-76-1" morph="none" pos="word" start_char="4295">retirement</TOKEN>
<TOKEN end_char="4309" id="token-76-2" morph="none" pos="word" start_char="4306">home</TOKEN>
<TOKEN end_char="4312" id="token-76-3" morph="none" pos="word" start_char="4311">is</TOKEN>
<TOKEN end_char="4314" id="token-76-4" morph="none" pos="word" start_char="4314">a</TOKEN>
<TOKEN end_char="4316" id="token-76-5" morph="none" pos="word" start_char="4316">4</TOKEN>
<TOKEN end_char="4321" id="token-76-6" morph="none" pos="word" start_char="4318">hour</TOKEN>
<TOKEN end_char="4327" id="token-76-7" morph="none" pos="word" start_char="4323">drive</TOKEN>
<TOKEN end_char="4332" id="token-76-8" morph="none" pos="word" start_char="4329">from</TOKEN>
<TOKEN end_char="4336" id="token-76-9" morph="none" pos="word" start_char="4334">the</TOKEN>
<TOKEN end_char="4345" id="token-76-10" morph="none" pos="word" start_char="4338">facility</TOKEN>
<TOKEN end_char="4346" id="token-76-11" morph="none" pos="punct" start_char="4346">.</TOKEN>
</SEG>
<SEG end_char="4431" id="segment-77" start_char="4351">
<ORIGINAL_TEXT>https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</ORIGINAL_TEXT>
<TOKEN end_char="4431" id="token-77-0" morph="none" pos="url" start_char="4351">https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</TOKEN>
<TRANSLATED_TEXT>https: / / theconversation.com / was-coronavirus-really-in-europe-in-march-2019-141582</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="4482" id="segment-78" start_char="4435">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:57:05</ORIGINAL_TEXT>
<TOKEN end_char="4439" id="token-78-0" morph="none" pos="word" start_char="4435">Quote</TOKEN>
<TOKEN end_char="4444" id="token-78-1" morph="none" pos="word" start_char="4441">from</TOKEN>
<TOKEN end_char="4445" id="token-78-2" morph="none" pos="punct" start_char="4445">:</TOKEN>
<TOKEN end_char="4451" id="token-78-3" morph="none" pos="word" start_char="4447">Bored</TOKEN>
<TOKEN end_char="4459" id="token-78-4" morph="none" pos="word" start_char="4453">chemist</TOKEN>
<TOKEN end_char="4462" id="token-78-5" morph="none" pos="word" start_char="4461">on</TOKEN>
<TOKEN end_char="4473" id="token-78-6" morph="none" pos="unknown" start_char="4464">06/02/2021</TOKEN>
<TOKEN end_char="4482" id="token-78-7" morph="none" pos="unknown" start_char="4475">20:57:05</TOKEN>
</SEG>
<SEG end_char="4525" id="segment-79" start_char="4485">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:27:47</ORIGINAL_TEXT>
<TOKEN end_char="4489" id="token-79-0" morph="none" pos="word" start_char="4485">Quote</TOKEN>
<TOKEN end_char="4494" id="token-79-1" morph="none" pos="word" start_char="4491">from</TOKEN>
<TOKEN end_char="4495" id="token-79-2" morph="none" pos="punct" start_char="4495">:</TOKEN>
<TOKEN end_char="4502" id="token-79-3" morph="none" pos="word" start_char="4497">Jolly2</TOKEN>
<TOKEN end_char="4505" id="token-79-4" morph="none" pos="word" start_char="4504">on</TOKEN>
<TOKEN end_char="4516" id="token-79-5" morph="none" pos="unknown" start_char="4507">06/02/2021</TOKEN>
<TOKEN end_char="4525" id="token-79-6" morph="none" pos="unknown" start_char="4518">20:27:47</TOKEN>
</SEG>
<SEG end_char="4575" id="segment-80" start_char="4528">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN end_char="4532" id="token-80-0" morph="none" pos="word" start_char="4528">Quote</TOKEN>
<TOKEN end_char="4537" id="token-80-1" morph="none" pos="word" start_char="4534">from</TOKEN>
<TOKEN end_char="4538" id="token-80-2" morph="none" pos="punct" start_char="4538">:</TOKEN>
<TOKEN end_char="4544" id="token-80-3" morph="none" pos="word" start_char="4540">Bored</TOKEN>
<TOKEN end_char="4552" id="token-80-4" morph="none" pos="word" start_char="4546">chemist</TOKEN>
<TOKEN end_char="4555" id="token-80-5" morph="none" pos="word" start_char="4554">on</TOKEN>
<TOKEN end_char="4566" id="token-80-6" morph="none" pos="unknown" start_char="4557">06/02/2021</TOKEN>
<TOKEN end_char="4575" id="token-80-7" morph="none" pos="unknown" start_char="4568">20:19:01</TOKEN>
</SEG>
<SEG end_char="4618" id="segment-81" start_char="4578">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN end_char="4582" id="token-81-0" morph="none" pos="word" start_char="4578">Quote</TOKEN>
<TOKEN end_char="4587" id="token-81-1" morph="none" pos="word" start_char="4584">from</TOKEN>
<TOKEN end_char="4588" id="token-81-2" morph="none" pos="punct" start_char="4588">:</TOKEN>
<TOKEN end_char="4595" id="token-81-3" morph="none" pos="word" start_char="4590">Jolly2</TOKEN>
<TOKEN end_char="4598" id="token-81-4" morph="none" pos="word" start_char="4597">on</TOKEN>
<TOKEN end_char="4609" id="token-81-5" morph="none" pos="unknown" start_char="4600">06/02/2021</TOKEN>
<TOKEN end_char="4618" id="token-81-6" morph="none" pos="unknown" start_char="4611">20:16:16</TOKEN>
</SEG>
<SEG end_char="4649" id="segment-82" start_char="4621">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="4624" id="token-82-0" morph="none" pos="word" start_char="4621">they</TOKEN>
<TOKEN end_char="4630" id="token-82-1" morph="none" pos="word" start_char="4626">found</TOKEN>
<TOKEN end_char="4638" id="token-82-2" morph="none" pos="word" start_char="4632">samples</TOKEN>
<TOKEN end_char="4641" id="token-82-3" morph="none" pos="word" start_char="4640">in</TOKEN>
<TOKEN end_char="4648" id="token-82-4" morph="none" pos="word" start_char="4643">sewage</TOKEN>
<TOKEN end_char="4649" id="token-82-5" morph="none" pos="punct" start_char="4649">.</TOKEN>
</SEG>
<SEG end_char="4805" id="segment-83" start_char="4652">
<ORIGINAL_TEXT>How sure are you of that "fact"?Sure enough to post it here for discussion Then I guess I can do the same.Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN end_char="4654" id="token-83-0" morph="none" pos="word" start_char="4652">How</TOKEN>
<TOKEN end_char="4659" id="token-83-1" morph="none" pos="word" start_char="4656">sure</TOKEN>
<TOKEN end_char="4663" id="token-83-2" morph="none" pos="word" start_char="4661">are</TOKEN>
<TOKEN end_char="4667" id="token-83-3" morph="none" pos="word" start_char="4665">you</TOKEN>
<TOKEN end_char="4670" id="token-83-4" morph="none" pos="word" start_char="4669">of</TOKEN>
<TOKEN end_char="4675" id="token-83-5" morph="none" pos="word" start_char="4672">that</TOKEN>
<TOKEN end_char="4677" id="token-83-6" morph="none" pos="punct" start_char="4677">"</TOKEN>
<TOKEN end_char="4687" id="token-83-7" morph="none" pos="unknown" start_char="4678">fact"?Sure</TOKEN>
<TOKEN end_char="4694" id="token-83-8" morph="none" pos="word" start_char="4689">enough</TOKEN>
<TOKEN end_char="4697" id="token-83-9" morph="none" pos="word" start_char="4696">to</TOKEN>
<TOKEN end_char="4702" id="token-83-10" morph="none" pos="word" start_char="4699">post</TOKEN>
<TOKEN end_char="4705" id="token-83-11" morph="none" pos="word" start_char="4704">it</TOKEN>
<TOKEN end_char="4710" id="token-83-12" morph="none" pos="word" start_char="4707">here</TOKEN>
<TOKEN end_char="4714" id="token-83-13" morph="none" pos="word" start_char="4712">for</TOKEN>
<TOKEN end_char="4725" id="token-83-14" morph="none" pos="word" start_char="4716">discussion</TOKEN>
<TOKEN end_char="4730" id="token-83-15" morph="none" pos="word" start_char="4727">Then</TOKEN>
<TOKEN end_char="4732" id="token-83-16" morph="none" pos="word" start_char="4732">I</TOKEN>
<TOKEN end_char="4738" id="token-83-17" morph="none" pos="word" start_char="4734">guess</TOKEN>
<TOKEN end_char="4740" id="token-83-18" morph="none" pos="word" start_char="4740">I</TOKEN>
<TOKEN end_char="4744" id="token-83-19" morph="none" pos="word" start_char="4742">can</TOKEN>
<TOKEN end_char="4747" id="token-83-20" morph="none" pos="word" start_char="4746">do</TOKEN>
<TOKEN end_char="4751" id="token-83-21" morph="none" pos="word" start_char="4749">the</TOKEN>
<TOKEN end_char="4762" id="token-83-22" morph="none" pos="unknown" start_char="4753">same.Quote</TOKEN>
<TOKEN end_char="4767" id="token-83-23" morph="none" pos="word" start_char="4764">from</TOKEN>
<TOKEN end_char="4768" id="token-83-24" morph="none" pos="punct" start_char="4768">:</TOKEN>
<TOKEN end_char="4774" id="token-83-25" morph="none" pos="word" start_char="4770">Bored</TOKEN>
<TOKEN end_char="4782" id="token-83-26" morph="none" pos="word" start_char="4776">chemist</TOKEN>
<TOKEN end_char="4785" id="token-83-27" morph="none" pos="word" start_char="4784">on</TOKEN>
<TOKEN end_char="4796" id="token-83-28" morph="none" pos="unknown" start_char="4787">05/02/2021</TOKEN>
<TOKEN end_char="4805" id="token-83-29" morph="none" pos="unknown" start_char="4798">18:56:50</TOKEN>
</SEG>
<SEG end_char="4837" id="segment-84" start_char="4808">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN end_char="4808" id="token-84-0" morph="none" pos="punct" start_char="4808">"</TOKEN>
<TOKEN end_char="4813" id="token-84-1" morph="none" pos="word" start_char="4809">Where</TOKEN>
<TOKEN end_char="4817" id="token-84-2" morph="none" pos="word" start_char="4815">did</TOKEN>
<TOKEN end_char="4823" id="token-84-3" morph="none" pos="word" start_char="4819">covid</TOKEN>
<TOKEN end_char="4826" id="token-84-4" morph="none" pos="word" start_char="4825">19</TOKEN>
<TOKEN end_char="4836" id="token-84-5" morph="none" pos="word" start_char="4828">originate</TOKEN>
<TOKEN end_char="4837" id="token-84-6" morph="none" pos="punct" start_char="4837">?</TOKEN>
</SEG>
<SEG end_char="4892" id="segment-85" start_char="4839">
<ORIGINAL_TEXT>"Wuhan.Do you have any actual evidence that I'm wrong?</ORIGINAL_TEXT>
<TOKEN end_char="4839" id="token-85-0" morph="none" pos="punct" start_char="4839">"</TOKEN>
<TOKEN end_char="4847" id="token-85-1" morph="none" pos="unknown" start_char="4840">Wuhan.Do</TOKEN>
<TOKEN end_char="4851" id="token-85-2" morph="none" pos="word" start_char="4849">you</TOKEN>
<TOKEN end_char="4856" id="token-85-3" morph="none" pos="word" start_char="4853">have</TOKEN>
<TOKEN end_char="4860" id="token-85-4" morph="none" pos="word" start_char="4858">any</TOKEN>
<TOKEN end_char="4867" id="token-85-5" morph="none" pos="word" start_char="4862">actual</TOKEN>
<TOKEN end_char="4876" id="token-85-6" morph="none" pos="word" start_char="4869">evidence</TOKEN>
<TOKEN end_char="4881" id="token-85-7" morph="none" pos="word" start_char="4878">that</TOKEN>
<TOKEN end_char="4885" id="token-85-8" morph="none" pos="word" start_char="4883">I'm</TOKEN>
<TOKEN end_char="4891" id="token-85-9" morph="none" pos="word" start_char="4887">wrong</TOKEN>
<TOKEN end_char="4892" id="token-85-10" morph="none" pos="punct" start_char="4892">?</TOKEN>
</SEG>
<SEG end_char="5039" id="segment-86" start_char="4894">
<ORIGINAL_TEXT>(To do that you would have to prove that it was- for example- possible for the virus to transfer from Spain to china, but not the other way round.</ORIGINAL_TEXT>
<TOKEN end_char="4894" id="token-86-0" morph="none" pos="punct" start_char="4894">(</TOKEN>
<TOKEN end_char="4896" id="token-86-1" morph="none" pos="word" start_char="4895">To</TOKEN>
<TOKEN end_char="4899" id="token-86-2" morph="none" pos="word" start_char="4898">do</TOKEN>
<TOKEN end_char="4904" id="token-86-3" morph="none" pos="word" start_char="4901">that</TOKEN>
<TOKEN end_char="4908" id="token-86-4" morph="none" pos="word" start_char="4906">you</TOKEN>
<TOKEN end_char="4914" id="token-86-5" morph="none" pos="word" start_char="4910">would</TOKEN>
<TOKEN end_char="4919" id="token-86-6" morph="none" pos="word" start_char="4916">have</TOKEN>
<TOKEN end_char="4922" id="token-86-7" morph="none" pos="word" start_char="4921">to</TOKEN>
<TOKEN end_char="4928" id="token-86-8" morph="none" pos="word" start_char="4924">prove</TOKEN>
<TOKEN end_char="4933" id="token-86-9" morph="none" pos="word" start_char="4930">that</TOKEN>
<TOKEN end_char="4936" id="token-86-10" morph="none" pos="word" start_char="4935">it</TOKEN>
<TOKEN end_char="4940" id="token-86-11" morph="none" pos="word" start_char="4938">was</TOKEN>
<TOKEN end_char="4941" id="token-86-12" morph="none" pos="punct" start_char="4941">-</TOKEN>
<TOKEN end_char="4945" id="token-86-13" morph="none" pos="word" start_char="4943">for</TOKEN>
<TOKEN end_char="4953" id="token-86-14" morph="none" pos="word" start_char="4947">example</TOKEN>
<TOKEN end_char="4954" id="token-86-15" morph="none" pos="punct" start_char="4954">-</TOKEN>
<TOKEN end_char="4963" id="token-86-16" morph="none" pos="word" start_char="4956">possible</TOKEN>
<TOKEN end_char="4967" id="token-86-17" morph="none" pos="word" start_char="4965">for</TOKEN>
<TOKEN end_char="4971" id="token-86-18" morph="none" pos="word" start_char="4969">the</TOKEN>
<TOKEN end_char="4977" id="token-86-19" morph="none" pos="word" start_char="4973">virus</TOKEN>
<TOKEN end_char="4980" id="token-86-20" morph="none" pos="word" start_char="4979">to</TOKEN>
<TOKEN end_char="4989" id="token-86-21" morph="none" pos="word" start_char="4982">transfer</TOKEN>
<TOKEN end_char="4994" id="token-86-22" morph="none" pos="word" start_char="4991">from</TOKEN>
<TOKEN end_char="5000" id="token-86-23" morph="none" pos="word" start_char="4996">Spain</TOKEN>
<TOKEN end_char="5003" id="token-86-24" morph="none" pos="word" start_char="5002">to</TOKEN>
<TOKEN end_char="5009" id="token-86-25" morph="none" pos="word" start_char="5005">china</TOKEN>
<TOKEN end_char="5010" id="token-86-26" morph="none" pos="punct" start_char="5010">,</TOKEN>
<TOKEN end_char="5014" id="token-86-27" morph="none" pos="word" start_char="5012">but</TOKEN>
<TOKEN end_char="5018" id="token-86-28" morph="none" pos="word" start_char="5016">not</TOKEN>
<TOKEN end_char="5022" id="token-86-29" morph="none" pos="word" start_char="5020">the</TOKEN>
<TOKEN end_char="5028" id="token-86-30" morph="none" pos="word" start_char="5024">other</TOKEN>
<TOKEN end_char="5032" id="token-86-31" morph="none" pos="word" start_char="5030">way</TOKEN>
<TOKEN end_char="5038" id="token-86-32" morph="none" pos="word" start_char="5034">round</TOKEN>
<TOKEN end_char="5039" id="token-86-33" morph="none" pos="punct" start_char="5039">.</TOKEN>
</SEG>
<SEG end_char="5323" id="segment-87" start_char="5041">
<ORIGINAL_TEXT>).I also look forward to your explanation of how the virus didn't spread much while it was in Spain, France, Brazil or wherever, but suddenly became massively infections in China.But, those observations and deductions are merely evidence.I don't expect you to pay them any attention.</ORIGINAL_TEXT>
<TOKEN end_char="5042" id="token-87-0" morph="none" pos="punct" start_char="5041">).</TOKEN>
<TOKEN end_char="5043" id="token-87-1" morph="none" pos="word" start_char="5043">I</TOKEN>
<TOKEN end_char="5048" id="token-87-2" morph="none" pos="word" start_char="5045">also</TOKEN>
<TOKEN end_char="5053" id="token-87-3" morph="none" pos="word" start_char="5050">look</TOKEN>
<TOKEN end_char="5061" id="token-87-4" morph="none" pos="word" start_char="5055">forward</TOKEN>
<TOKEN end_char="5064" id="token-87-5" morph="none" pos="word" start_char="5063">to</TOKEN>
<TOKEN end_char="5069" id="token-87-6" morph="none" pos="word" start_char="5066">your</TOKEN>
<TOKEN end_char="5081" id="token-87-7" morph="none" pos="word" start_char="5071">explanation</TOKEN>
<TOKEN end_char="5084" id="token-87-8" morph="none" pos="word" start_char="5083">of</TOKEN>
<TOKEN end_char="5088" id="token-87-9" morph="none" pos="word" start_char="5086">how</TOKEN>
<TOKEN end_char="5092" id="token-87-10" morph="none" pos="word" start_char="5090">the</TOKEN>
<TOKEN end_char="5098" id="token-87-11" morph="none" pos="word" start_char="5094">virus</TOKEN>
<TOKEN end_char="5105" id="token-87-12" morph="none" pos="word" start_char="5100">didn't</TOKEN>
<TOKEN end_char="5112" id="token-87-13" morph="none" pos="word" start_char="5107">spread</TOKEN>
<TOKEN end_char="5117" id="token-87-14" morph="none" pos="word" start_char="5114">much</TOKEN>
<TOKEN end_char="5123" id="token-87-15" morph="none" pos="word" start_char="5119">while</TOKEN>
<TOKEN end_char="5126" id="token-87-16" morph="none" pos="word" start_char="5125">it</TOKEN>
<TOKEN end_char="5130" id="token-87-17" morph="none" pos="word" start_char="5128">was</TOKEN>
<TOKEN end_char="5133" id="token-87-18" morph="none" pos="word" start_char="5132">in</TOKEN>
<TOKEN end_char="5139" id="token-87-19" morph="none" pos="word" start_char="5135">Spain</TOKEN>
<TOKEN end_char="5140" id="token-87-20" morph="none" pos="punct" start_char="5140">,</TOKEN>
<TOKEN end_char="5147" id="token-87-21" morph="none" pos="word" start_char="5142">France</TOKEN>
<TOKEN end_char="5148" id="token-87-22" morph="none" pos="punct" start_char="5148">,</TOKEN>
<TOKEN end_char="5155" id="token-87-23" morph="none" pos="word" start_char="5150">Brazil</TOKEN>
<TOKEN end_char="5158" id="token-87-24" morph="none" pos="word" start_char="5157">or</TOKEN>
<TOKEN end_char="5167" id="token-87-25" morph="none" pos="word" start_char="5160">wherever</TOKEN>
<TOKEN end_char="5168" id="token-87-26" morph="none" pos="punct" start_char="5168">,</TOKEN>
<TOKEN end_char="5172" id="token-87-27" morph="none" pos="word" start_char="5170">but</TOKEN>
<TOKEN end_char="5181" id="token-87-28" morph="none" pos="word" start_char="5174">suddenly</TOKEN>
<TOKEN end_char="5188" id="token-87-29" morph="none" pos="word" start_char="5183">became</TOKEN>
<TOKEN end_char="5198" id="token-87-30" morph="none" pos="word" start_char="5190">massively</TOKEN>
<TOKEN end_char="5209" id="token-87-31" morph="none" pos="word" start_char="5200">infections</TOKEN>
<TOKEN end_char="5212" id="token-87-32" morph="none" pos="word" start_char="5211">in</TOKEN>
<TOKEN end_char="5222" id="token-87-33" morph="none" pos="unknown" start_char="5214">China.But</TOKEN>
<TOKEN end_char="5223" id="token-87-34" morph="none" pos="punct" start_char="5223">,</TOKEN>
<TOKEN end_char="5229" id="token-87-35" morph="none" pos="word" start_char="5225">those</TOKEN>
<TOKEN end_char="5242" id="token-87-36" morph="none" pos="word" start_char="5231">observations</TOKEN>
<TOKEN end_char="5246" id="token-87-37" morph="none" pos="word" start_char="5244">and</TOKEN>
<TOKEN end_char="5257" id="token-87-38" morph="none" pos="word" start_char="5248">deductions</TOKEN>
<TOKEN end_char="5261" id="token-87-39" morph="none" pos="word" start_char="5259">are</TOKEN>
<TOKEN end_char="5268" id="token-87-40" morph="none" pos="word" start_char="5263">merely</TOKEN>
<TOKEN end_char="5279" id="token-87-41" morph="none" pos="unknown" start_char="5270">evidence.I</TOKEN>
<TOKEN end_char="5285" id="token-87-42" morph="none" pos="word" start_char="5281">don't</TOKEN>
<TOKEN end_char="5292" id="token-87-43" morph="none" pos="word" start_char="5287">expect</TOKEN>
<TOKEN end_char="5296" id="token-87-44" morph="none" pos="word" start_char="5294">you</TOKEN>
<TOKEN end_char="5299" id="token-87-45" morph="none" pos="word" start_char="5298">to</TOKEN>
<TOKEN end_char="5303" id="token-87-46" morph="none" pos="word" start_char="5301">pay</TOKEN>
<TOKEN end_char="5308" id="token-87-47" morph="none" pos="word" start_char="5305">them</TOKEN>
<TOKEN end_char="5312" id="token-87-48" morph="none" pos="word" start_char="5310">any</TOKEN>
<TOKEN end_char="5322" id="token-87-49" morph="none" pos="word" start_char="5314">attention</TOKEN>
<TOKEN end_char="5323" id="token-87-50" morph="none" pos="punct" start_char="5323">.</TOKEN>
</SEG>
<SEG end_char="5360" id="segment-88" start_char="5326">
<ORIGINAL_TEXT>China was the first to identify it.</ORIGINAL_TEXT>
<TOKEN end_char="5330" id="token-88-0" morph="none" pos="word" start_char="5326">China</TOKEN>
<TOKEN end_char="5334" id="token-88-1" morph="none" pos="word" start_char="5332">was</TOKEN>
<TOKEN end_char="5338" id="token-88-2" morph="none" pos="word" start_char="5336">the</TOKEN>
<TOKEN end_char="5344" id="token-88-3" morph="none" pos="word" start_char="5340">first</TOKEN>
<TOKEN end_char="5347" id="token-88-4" morph="none" pos="word" start_char="5346">to</TOKEN>
<TOKEN end_char="5356" id="token-88-5" morph="none" pos="word" start_char="5349">identify</TOKEN>
<TOKEN end_char="5359" id="token-88-6" morph="none" pos="word" start_char="5358">it</TOKEN>
<TOKEN end_char="5360" id="token-88-7" morph="none" pos="punct" start_char="5360">.</TOKEN>
</SEG>
<SEG end_char="5487" id="segment-89" start_char="5362">
<ORIGINAL_TEXT>Means it spread undetected, as the cambridge study suggested covid had been in China for around 4 months before they found it.</ORIGINAL_TEXT>
<TOKEN end_char="5366" id="token-89-0" morph="none" pos="word" start_char="5362">Means</TOKEN>
<TOKEN end_char="5369" id="token-89-1" morph="none" pos="word" start_char="5368">it</TOKEN>
<TOKEN end_char="5376" id="token-89-2" morph="none" pos="word" start_char="5371">spread</TOKEN>
<TOKEN end_char="5387" id="token-89-3" morph="none" pos="word" start_char="5378">undetected</TOKEN>
<TOKEN end_char="5388" id="token-89-4" morph="none" pos="punct" start_char="5388">,</TOKEN>
<TOKEN end_char="5391" id="token-89-5" morph="none" pos="word" start_char="5390">as</TOKEN>
<TOKEN end_char="5395" id="token-89-6" morph="none" pos="word" start_char="5393">the</TOKEN>
<TOKEN end_char="5405" id="token-89-7" morph="none" pos="word" start_char="5397">cambridge</TOKEN>
<TOKEN end_char="5411" id="token-89-8" morph="none" pos="word" start_char="5407">study</TOKEN>
<TOKEN end_char="5421" id="token-89-9" morph="none" pos="word" start_char="5413">suggested</TOKEN>
<TOKEN end_char="5427" id="token-89-10" morph="none" pos="word" start_char="5423">covid</TOKEN>
<TOKEN end_char="5431" id="token-89-11" morph="none" pos="word" start_char="5429">had</TOKEN>
<TOKEN end_char="5436" id="token-89-12" morph="none" pos="word" start_char="5433">been</TOKEN>
<TOKEN end_char="5439" id="token-89-13" morph="none" pos="word" start_char="5438">in</TOKEN>
<TOKEN end_char="5445" id="token-89-14" morph="none" pos="word" start_char="5441">China</TOKEN>
<TOKEN end_char="5449" id="token-89-15" morph="none" pos="word" start_char="5447">for</TOKEN>
<TOKEN end_char="5456" id="token-89-16" morph="none" pos="word" start_char="5451">around</TOKEN>
<TOKEN end_char="5458" id="token-89-17" morph="none" pos="word" start_char="5458">4</TOKEN>
<TOKEN end_char="5465" id="token-89-18" morph="none" pos="word" start_char="5460">months</TOKEN>
<TOKEN end_char="5472" id="token-89-19" morph="none" pos="word" start_char="5467">before</TOKEN>
<TOKEN end_char="5477" id="token-89-20" morph="none" pos="word" start_char="5474">they</TOKEN>
<TOKEN end_char="5483" id="token-89-21" morph="none" pos="word" start_char="5479">found</TOKEN>
<TOKEN end_char="5486" id="token-89-22" morph="none" pos="word" start_char="5485">it</TOKEN>
<TOKEN end_char="5487" id="token-89-23" morph="none" pos="punct" start_char="5487">.</TOKEN>
</SEG>
<SEG end_char="5508" id="segment-90" start_char="5491">
<ORIGINAL_TEXT>Quote from: Jolly2</ORIGINAL_TEXT>
<TOKEN end_char="5495" id="token-90-0" morph="none" pos="word" start_char="5491">Quote</TOKEN>
<TOKEN end_char="5500" id="token-90-1" morph="none" pos="word" start_char="5497">from</TOKEN>
<TOKEN end_char="5501" id="token-90-2" morph="none" pos="punct" start_char="5501">:</TOKEN>
<TOKEN end_char="5508" id="token-90-3" morph="none" pos="word" start_char="5503">Jolly2</TOKEN>
</SEG>
<SEG end_char="5569" id="segment-91" start_char="5511">
<ORIGINAL_TEXT>spain as early as March 2019...they found samples in sewage</ORIGINAL_TEXT>
<TOKEN end_char="5515" id="token-91-0" morph="none" pos="word" start_char="5511">spain</TOKEN>
<TOKEN end_char="5518" id="token-91-1" morph="none" pos="word" start_char="5517">as</TOKEN>
<TOKEN end_char="5524" id="token-91-2" morph="none" pos="word" start_char="5520">early</TOKEN>
<TOKEN end_char="5527" id="token-91-3" morph="none" pos="word" start_char="5526">as</TOKEN>
<TOKEN end_char="5533" id="token-91-4" morph="none" pos="word" start_char="5529">March</TOKEN>
<TOKEN end_char="5545" id="token-91-5" morph="none" pos="unknown" start_char="5535">2019...they</TOKEN>
<TOKEN end_char="5551" id="token-91-6" morph="none" pos="word" start_char="5547">found</TOKEN>
<TOKEN end_char="5559" id="token-91-7" morph="none" pos="word" start_char="5553">samples</TOKEN>
<TOKEN end_char="5562" id="token-91-8" morph="none" pos="word" start_char="5561">in</TOKEN>
<TOKEN end_char="5569" id="token-91-9" morph="none" pos="word" start_char="5564">sewage</TOKEN>
</SEG>
<SEG end_char="5618" id="segment-92" start_char="5573">
<ORIGINAL_TEXT>The quality of RNA in sewage is very variable.</ORIGINAL_TEXT>
<TOKEN end_char="5575" id="token-92-0" morph="none" pos="word" start_char="5573">The</TOKEN>
<TOKEN end_char="5583" id="token-92-1" morph="none" pos="word" start_char="5577">quality</TOKEN>
<TOKEN end_char="5586" id="token-92-2" morph="none" pos="word" start_char="5585">of</TOKEN>
<TOKEN end_char="5590" id="token-92-3" morph="none" pos="word" start_char="5588">RNA</TOKEN>
<TOKEN end_char="5593" id="token-92-4" morph="none" pos="word" start_char="5592">in</TOKEN>
<TOKEN end_char="5600" id="token-92-5" morph="none" pos="word" start_char="5595">sewage</TOKEN>
<TOKEN end_char="5603" id="token-92-6" morph="none" pos="word" start_char="5602">is</TOKEN>
<TOKEN end_char="5608" id="token-92-7" morph="none" pos="word" start_char="5605">very</TOKEN>
<TOKEN end_char="5617" id="token-92-8" morph="none" pos="word" start_char="5610">variable</TOKEN>
<TOKEN end_char="5618" id="token-92-9" morph="none" pos="punct" start_char="5618">.</TOKEN>
</SEG>
<SEG end_char="6150" id="segment-93" start_char="5620">
<ORIGINAL_TEXT>- We know that the fatty coat of the SARS-COV2 virus is broken down by soaps and detergents - Most people flush soap down the sewer when they take a bath or shower - Most people flush detergents down the sewer when they wash the dishes (and use even more destructive chemicals in the dishwasher) - Industrial processes can also flush destructive chemicals into sewers - So RNA is badly degraded when it is collected (within 1 week) - And, depending on how it is stored, may continue degrading if it is stored for months afterwards.</ORIGINAL_TEXT>
<TOKEN end_char="5620" id="token-93-0" morph="none" pos="punct" start_char="5620">-</TOKEN>
<TOKEN end_char="5623" id="token-93-1" morph="none" pos="word" start_char="5622">We</TOKEN>
<TOKEN end_char="5628" id="token-93-2" morph="none" pos="word" start_char="5625">know</TOKEN>
<TOKEN end_char="5633" id="token-93-3" morph="none" pos="word" start_char="5630">that</TOKEN>
<TOKEN end_char="5637" id="token-93-4" morph="none" pos="word" start_char="5635">the</TOKEN>
<TOKEN end_char="5643" id="token-93-5" morph="none" pos="word" start_char="5639">fatty</TOKEN>
<TOKEN end_char="5648" id="token-93-6" morph="none" pos="word" start_char="5645">coat</TOKEN>
<TOKEN end_char="5651" id="token-93-7" morph="none" pos="word" start_char="5650">of</TOKEN>
<TOKEN end_char="5655" id="token-93-8" morph="none" pos="word" start_char="5653">the</TOKEN>
<TOKEN end_char="5665" id="token-93-9" morph="none" pos="unknown" start_char="5657">SARS-COV2</TOKEN>
<TOKEN end_char="5671" id="token-93-10" morph="none" pos="word" start_char="5667">virus</TOKEN>
<TOKEN end_char="5674" id="token-93-11" morph="none" pos="word" start_char="5673">is</TOKEN>
<TOKEN end_char="5681" id="token-93-12" morph="none" pos="word" start_char="5676">broken</TOKEN>
<TOKEN end_char="5686" id="token-93-13" morph="none" pos="word" start_char="5683">down</TOKEN>
<TOKEN end_char="5689" id="token-93-14" morph="none" pos="word" start_char="5688">by</TOKEN>
<TOKEN end_char="5695" id="token-93-15" morph="none" pos="word" start_char="5691">soaps</TOKEN>
<TOKEN end_char="5699" id="token-93-16" morph="none" pos="word" start_char="5697">and</TOKEN>
<TOKEN end_char="5710" id="token-93-17" morph="none" pos="word" start_char="5701">detergents</TOKEN>
<TOKEN end_char="5712" id="token-93-18" morph="none" pos="punct" start_char="5712">-</TOKEN>
<TOKEN end_char="5717" id="token-93-19" morph="none" pos="word" start_char="5714">Most</TOKEN>
<TOKEN end_char="5724" id="token-93-20" morph="none" pos="word" start_char="5719">people</TOKEN>
<TOKEN end_char="5730" id="token-93-21" morph="none" pos="word" start_char="5726">flush</TOKEN>
<TOKEN end_char="5735" id="token-93-22" morph="none" pos="word" start_char="5732">soap</TOKEN>
<TOKEN end_char="5740" id="token-93-23" morph="none" pos="word" start_char="5737">down</TOKEN>
<TOKEN end_char="5744" id="token-93-24" morph="none" pos="word" start_char="5742">the</TOKEN>
<TOKEN end_char="5750" id="token-93-25" morph="none" pos="word" start_char="5746">sewer</TOKEN>
<TOKEN end_char="5755" id="token-93-26" morph="none" pos="word" start_char="5752">when</TOKEN>
<TOKEN end_char="5760" id="token-93-27" morph="none" pos="word" start_char="5757">they</TOKEN>
<TOKEN end_char="5765" id="token-93-28" morph="none" pos="word" start_char="5762">take</TOKEN>
<TOKEN end_char="5767" id="token-93-29" morph="none" pos="word" start_char="5767">a</TOKEN>
<TOKEN end_char="5772" id="token-93-30" morph="none" pos="word" start_char="5769">bath</TOKEN>
<TOKEN end_char="5775" id="token-93-31" morph="none" pos="word" start_char="5774">or</TOKEN>
<TOKEN end_char="5782" id="token-93-32" morph="none" pos="word" start_char="5777">shower</TOKEN>
<TOKEN end_char="5784" id="token-93-33" morph="none" pos="punct" start_char="5784">-</TOKEN>
<TOKEN end_char="5789" id="token-93-34" morph="none" pos="word" start_char="5786">Most</TOKEN>
<TOKEN end_char="5796" id="token-93-35" morph="none" pos="word" start_char="5791">people</TOKEN>
<TOKEN end_char="5802" id="token-93-36" morph="none" pos="word" start_char="5798">flush</TOKEN>
<TOKEN end_char="5813" id="token-93-37" morph="none" pos="word" start_char="5804">detergents</TOKEN>
<TOKEN end_char="5818" id="token-93-38" morph="none" pos="word" start_char="5815">down</TOKEN>
<TOKEN end_char="5822" id="token-93-39" morph="none" pos="word" start_char="5820">the</TOKEN>
<TOKEN end_char="5828" id="token-93-40" morph="none" pos="word" start_char="5824">sewer</TOKEN>
<TOKEN end_char="5833" id="token-93-41" morph="none" pos="word" start_char="5830">when</TOKEN>
<TOKEN end_char="5838" id="token-93-42" morph="none" pos="word" start_char="5835">they</TOKEN>
<TOKEN end_char="5843" id="token-93-43" morph="none" pos="word" start_char="5840">wash</TOKEN>
<TOKEN end_char="5847" id="token-93-44" morph="none" pos="word" start_char="5845">the</TOKEN>
<TOKEN end_char="5854" id="token-93-45" morph="none" pos="word" start_char="5849">dishes</TOKEN>
<TOKEN end_char="5856" id="token-93-46" morph="none" pos="punct" start_char="5856">(</TOKEN>
<TOKEN end_char="5859" id="token-93-47" morph="none" pos="word" start_char="5857">and</TOKEN>
<TOKEN end_char="5863" id="token-93-48" morph="none" pos="word" start_char="5861">use</TOKEN>
<TOKEN end_char="5868" id="token-93-49" morph="none" pos="word" start_char="5865">even</TOKEN>
<TOKEN end_char="5873" id="token-93-50" morph="none" pos="word" start_char="5870">more</TOKEN>
<TOKEN end_char="5885" id="token-93-51" morph="none" pos="word" start_char="5875">destructive</TOKEN>
<TOKEN end_char="5895" id="token-93-52" morph="none" pos="word" start_char="5887">chemicals</TOKEN>
<TOKEN end_char="5898" id="token-93-53" morph="none" pos="word" start_char="5897">in</TOKEN>
<TOKEN end_char="5902" id="token-93-54" morph="none" pos="word" start_char="5900">the</TOKEN>
<TOKEN end_char="5913" id="token-93-55" morph="none" pos="word" start_char="5904">dishwasher</TOKEN>
<TOKEN end_char="5914" id="token-93-56" morph="none" pos="punct" start_char="5914">)</TOKEN>
<TOKEN end_char="5916" id="token-93-57" morph="none" pos="punct" start_char="5916">-</TOKEN>
<TOKEN end_char="5927" id="token-93-58" morph="none" pos="word" start_char="5918">Industrial</TOKEN>
<TOKEN end_char="5937" id="token-93-59" morph="none" pos="word" start_char="5929">processes</TOKEN>
<TOKEN end_char="5941" id="token-93-60" morph="none" pos="word" start_char="5939">can</TOKEN>
<TOKEN end_char="5946" id="token-93-61" morph="none" pos="word" start_char="5943">also</TOKEN>
<TOKEN end_char="5952" id="token-93-62" morph="none" pos="word" start_char="5948">flush</TOKEN>
<TOKEN end_char="5964" id="token-93-63" morph="none" pos="word" start_char="5954">destructive</TOKEN>
<TOKEN end_char="5974" id="token-93-64" morph="none" pos="word" start_char="5966">chemicals</TOKEN>
<TOKEN end_char="5979" id="token-93-65" morph="none" pos="word" start_char="5976">into</TOKEN>
<TOKEN end_char="5986" id="token-93-66" morph="none" pos="word" start_char="5981">sewers</TOKEN>
<TOKEN end_char="5988" id="token-93-67" morph="none" pos="punct" start_char="5988">-</TOKEN>
<TOKEN end_char="5991" id="token-93-68" morph="none" pos="word" start_char="5990">So</TOKEN>
<TOKEN end_char="5995" id="token-93-69" morph="none" pos="word" start_char="5993">RNA</TOKEN>
<TOKEN end_char="5998" id="token-93-70" morph="none" pos="word" start_char="5997">is</TOKEN>
<TOKEN end_char="6004" id="token-93-71" morph="none" pos="word" start_char="6000">badly</TOKEN>
<TOKEN end_char="6013" id="token-93-72" morph="none" pos="word" start_char="6006">degraded</TOKEN>
<TOKEN end_char="6018" id="token-93-73" morph="none" pos="word" start_char="6015">when</TOKEN>
<TOKEN end_char="6021" id="token-93-74" morph="none" pos="word" start_char="6020">it</TOKEN>
<TOKEN end_char="6024" id="token-93-75" morph="none" pos="word" start_char="6023">is</TOKEN>
<TOKEN end_char="6034" id="token-93-76" morph="none" pos="word" start_char="6026">collected</TOKEN>
<TOKEN end_char="6036" id="token-93-77" morph="none" pos="punct" start_char="6036">(</TOKEN>
<TOKEN end_char="6042" id="token-93-78" morph="none" pos="word" start_char="6037">within</TOKEN>
<TOKEN end_char="6044" id="token-93-79" morph="none" pos="word" start_char="6044">1</TOKEN>
<TOKEN end_char="6049" id="token-93-80" morph="none" pos="word" start_char="6046">week</TOKEN>
<TOKEN end_char="6050" id="token-93-81" morph="none" pos="punct" start_char="6050">)</TOKEN>
<TOKEN end_char="6052" id="token-93-82" morph="none" pos="punct" start_char="6052">-</TOKEN>
<TOKEN end_char="6056" id="token-93-83" morph="none" pos="word" start_char="6054">And</TOKEN>
<TOKEN end_char="6057" id="token-93-84" morph="none" pos="punct" start_char="6057">,</TOKEN>
<TOKEN end_char="6067" id="token-93-85" morph="none" pos="word" start_char="6059">depending</TOKEN>
<TOKEN end_char="6070" id="token-93-86" morph="none" pos="word" start_char="6069">on</TOKEN>
<TOKEN end_char="6074" id="token-93-87" morph="none" pos="word" start_char="6072">how</TOKEN>
<TOKEN end_char="6077" id="token-93-88" morph="none" pos="word" start_char="6076">it</TOKEN>
<TOKEN end_char="6080" id="token-93-89" morph="none" pos="word" start_char="6079">is</TOKEN>
<TOKEN end_char="6087" id="token-93-90" morph="none" pos="word" start_char="6082">stored</TOKEN>
<TOKEN end_char="6088" id="token-93-91" morph="none" pos="punct" start_char="6088">,</TOKEN>
<TOKEN end_char="6092" id="token-93-92" morph="none" pos="word" start_char="6090">may</TOKEN>
<TOKEN end_char="6101" id="token-93-93" morph="none" pos="word" start_char="6094">continue</TOKEN>
<TOKEN end_char="6111" id="token-93-94" morph="none" pos="word" start_char="6103">degrading</TOKEN>
<TOKEN end_char="6114" id="token-93-95" morph="none" pos="word" start_char="6113">if</TOKEN>
<TOKEN end_char="6117" id="token-93-96" morph="none" pos="word" start_char="6116">it</TOKEN>
<TOKEN end_char="6120" id="token-93-97" morph="none" pos="word" start_char="6119">is</TOKEN>
<TOKEN end_char="6127" id="token-93-98" morph="none" pos="word" start_char="6122">stored</TOKEN>
<TOKEN end_char="6131" id="token-93-99" morph="none" pos="word" start_char="6129">for</TOKEN>
<TOKEN end_char="6138" id="token-93-100" morph="none" pos="word" start_char="6133">months</TOKEN>
<TOKEN end_char="6149" id="token-93-101" morph="none" pos="word" start_char="6140">afterwards</TOKEN>
<TOKEN end_char="6150" id="token-93-102" morph="none" pos="punct" start_char="6150">.</TOKEN>
</SEG>
<SEG end_char="6258" id="segment-94" start_char="6153">
<ORIGINAL_TEXT>So we have degraded RNA from stored sewage samples, which is compared to a new viral sequence (SARS-COV2).</ORIGINAL_TEXT>
<TOKEN end_char="6154" id="token-94-0" morph="none" pos="word" start_char="6153">So</TOKEN>
<TOKEN end_char="6157" id="token-94-1" morph="none" pos="word" start_char="6156">we</TOKEN>
<TOKEN end_char="6162" id="token-94-2" morph="none" pos="word" start_char="6159">have</TOKEN>
<TOKEN end_char="6171" id="token-94-3" morph="none" pos="word" start_char="6164">degraded</TOKEN>
<TOKEN end_char="6175" id="token-94-4" morph="none" pos="word" start_char="6173">RNA</TOKEN>
<TOKEN end_char="6180" id="token-94-5" morph="none" pos="word" start_char="6177">from</TOKEN>
<TOKEN end_char="6187" id="token-94-6" morph="none" pos="word" start_char="6182">stored</TOKEN>
<TOKEN end_char="6194" id="token-94-7" morph="none" pos="word" start_char="6189">sewage</TOKEN>
<TOKEN end_char="6202" id="token-94-8" morph="none" pos="word" start_char="6196">samples</TOKEN>
<TOKEN end_char="6203" id="token-94-9" morph="none" pos="punct" start_char="6203">,</TOKEN>
<TOKEN end_char="6209" id="token-94-10" morph="none" pos="word" start_char="6205">which</TOKEN>
<TOKEN end_char="6212" id="token-94-11" morph="none" pos="word" start_char="6211">is</TOKEN>
<TOKEN end_char="6221" id="token-94-12" morph="none" pos="word" start_char="6214">compared</TOKEN>
<TOKEN end_char="6224" id="token-94-13" morph="none" pos="word" start_char="6223">to</TOKEN>
<TOKEN end_char="6226" id="token-94-14" morph="none" pos="word" start_char="6226">a</TOKEN>
<TOKEN end_char="6230" id="token-94-15" morph="none" pos="word" start_char="6228">new</TOKEN>
<TOKEN end_char="6236" id="token-94-16" morph="none" pos="word" start_char="6232">viral</TOKEN>
<TOKEN end_char="6245" id="token-94-17" morph="none" pos="word" start_char="6238">sequence</TOKEN>
<TOKEN end_char="6247" id="token-94-18" morph="none" pos="punct" start_char="6247">(</TOKEN>
<TOKEN end_char="6256" id="token-94-19" morph="none" pos="unknown" start_char="6248">SARS-COV2</TOKEN>
<TOKEN end_char="6258" id="token-94-20" morph="none" pos="punct" start_char="6257">).</TOKEN>
</SEG>
<SEG end_char="6507" id="segment-95" start_char="6260">
<ORIGINAL_TEXT>- This comparison must exclude the 4 common coronaviruses that are responsible for something like 30% of the "common cold" infections - And residual infections of MERS and SARS, which have 50% (MERS) or 80% (SARS) genetic similarities to SARS-COV2.</ORIGINAL_TEXT>
<TOKEN end_char="6260" id="token-95-0" morph="none" pos="punct" start_char="6260">-</TOKEN>
<TOKEN end_char="6265" id="token-95-1" morph="none" pos="word" start_char="6262">This</TOKEN>
<TOKEN end_char="6276" id="token-95-2" morph="none" pos="word" start_char="6267">comparison</TOKEN>
<TOKEN end_char="6281" id="token-95-3" morph="none" pos="word" start_char="6278">must</TOKEN>
<TOKEN end_char="6289" id="token-95-4" morph="none" pos="word" start_char="6283">exclude</TOKEN>
<TOKEN end_char="6293" id="token-95-5" morph="none" pos="word" start_char="6291">the</TOKEN>
<TOKEN end_char="6295" id="token-95-6" morph="none" pos="word" start_char="6295">4</TOKEN>
<TOKEN end_char="6302" id="token-95-7" morph="none" pos="word" start_char="6297">common</TOKEN>
<TOKEN end_char="6316" id="token-95-8" morph="none" pos="word" start_char="6304">coronaviruses</TOKEN>
<TOKEN end_char="6321" id="token-95-9" morph="none" pos="word" start_char="6318">that</TOKEN>
<TOKEN end_char="6325" id="token-95-10" morph="none" pos="word" start_char="6323">are</TOKEN>
<TOKEN end_char="6337" id="token-95-11" morph="none" pos="word" start_char="6327">responsible</TOKEN>
<TOKEN end_char="6341" id="token-95-12" morph="none" pos="word" start_char="6339">for</TOKEN>
<TOKEN end_char="6351" id="token-95-13" morph="none" pos="word" start_char="6343">something</TOKEN>
<TOKEN end_char="6356" id="token-95-14" morph="none" pos="word" start_char="6353">like</TOKEN>
<TOKEN end_char="6359" id="token-95-15" morph="none" pos="word" start_char="6358">30</TOKEN>
<TOKEN end_char="6360" id="token-95-16" morph="none" pos="punct" start_char="6360">%</TOKEN>
<TOKEN end_char="6363" id="token-95-17" morph="none" pos="word" start_char="6362">of</TOKEN>
<TOKEN end_char="6367" id="token-95-18" morph="none" pos="word" start_char="6365">the</TOKEN>
<TOKEN end_char="6369" id="token-95-19" morph="none" pos="punct" start_char="6369">"</TOKEN>
<TOKEN end_char="6375" id="token-95-20" morph="none" pos="word" start_char="6370">common</TOKEN>
<TOKEN end_char="6380" id="token-95-21" morph="none" pos="word" start_char="6377">cold</TOKEN>
<TOKEN end_char="6381" id="token-95-22" morph="none" pos="punct" start_char="6381">"</TOKEN>
<TOKEN end_char="6392" id="token-95-23" morph="none" pos="word" start_char="6383">infections</TOKEN>
<TOKEN end_char="6394" id="token-95-24" morph="none" pos="punct" start_char="6394">-</TOKEN>
<TOKEN end_char="6398" id="token-95-25" morph="none" pos="word" start_char="6396">And</TOKEN>
<TOKEN end_char="6407" id="token-95-26" morph="none" pos="word" start_char="6400">residual</TOKEN>
<TOKEN end_char="6418" id="token-95-27" morph="none" pos="word" start_char="6409">infections</TOKEN>
<TOKEN end_char="6421" id="token-95-28" morph="none" pos="word" start_char="6420">of</TOKEN>
<TOKEN end_char="6426" id="token-95-29" morph="none" pos="word" start_char="6423">MERS</TOKEN>
<TOKEN end_char="6430" id="token-95-30" morph="none" pos="word" start_char="6428">and</TOKEN>
<TOKEN end_char="6435" id="token-95-31" morph="none" pos="word" start_char="6432">SARS</TOKEN>
<TOKEN end_char="6436" id="token-95-32" morph="none" pos="punct" start_char="6436">,</TOKEN>
<TOKEN end_char="6442" id="token-95-33" morph="none" pos="word" start_char="6438">which</TOKEN>
<TOKEN end_char="6447" id="token-95-34" morph="none" pos="word" start_char="6444">have</TOKEN>
<TOKEN end_char="6450" id="token-95-35" morph="none" pos="word" start_char="6449">50</TOKEN>
<TOKEN end_char="6451" id="token-95-36" morph="none" pos="punct" start_char="6451">%</TOKEN>
<TOKEN end_char="6453" id="token-95-37" morph="none" pos="punct" start_char="6453">(</TOKEN>
<TOKEN end_char="6457" id="token-95-38" morph="none" pos="word" start_char="6454">MERS</TOKEN>
<TOKEN end_char="6458" id="token-95-39" morph="none" pos="punct" start_char="6458">)</TOKEN>
<TOKEN end_char="6461" id="token-95-40" morph="none" pos="word" start_char="6460">or</TOKEN>
<TOKEN end_char="6464" id="token-95-41" morph="none" pos="word" start_char="6463">80</TOKEN>
<TOKEN end_char="6465" id="token-95-42" morph="none" pos="punct" start_char="6465">%</TOKEN>
<TOKEN end_char="6467" id="token-95-43" morph="none" pos="punct" start_char="6467">(</TOKEN>
<TOKEN end_char="6471" id="token-95-44" morph="none" pos="word" start_char="6468">SARS</TOKEN>
<TOKEN end_char="6472" id="token-95-45" morph="none" pos="punct" start_char="6472">)</TOKEN>
<TOKEN end_char="6480" id="token-95-46" morph="none" pos="word" start_char="6474">genetic</TOKEN>
<TOKEN end_char="6493" id="token-95-47" morph="none" pos="word" start_char="6482">similarities</TOKEN>
<TOKEN end_char="6496" id="token-95-48" morph="none" pos="word" start_char="6495">to</TOKEN>
<TOKEN end_char="6506" id="token-95-49" morph="none" pos="unknown" start_char="6498">SARS-COV2</TOKEN>
<TOKEN end_char="6507" id="token-95-50" morph="none" pos="punct" start_char="6507">.</TOKEN>
</SEG>
<SEG end_char="6619" id="segment-96" start_char="6509">
<ORIGINAL_TEXT>These viruses were never eliminated, but since R 1, the occasional local outbreak will occur, and then die out.</ORIGINAL_TEXT>
<TOKEN end_char="6513" id="token-96-0" morph="none" pos="word" start_char="6509">These</TOKEN>
<TOKEN end_char="6521" id="token-96-1" morph="none" pos="word" start_char="6515">viruses</TOKEN>
<TOKEN end_char="6526" id="token-96-2" morph="none" pos="word" start_char="6523">were</TOKEN>
<TOKEN end_char="6532" id="token-96-3" morph="none" pos="word" start_char="6528">never</TOKEN>
<TOKEN end_char="6543" id="token-96-4" morph="none" pos="word" start_char="6534">eliminated</TOKEN>
<TOKEN end_char="6544" id="token-96-5" morph="none" pos="punct" start_char="6544">,</TOKEN>
<TOKEN end_char="6548" id="token-96-6" morph="none" pos="word" start_char="6546">but</TOKEN>
<TOKEN end_char="6554" id="token-96-7" morph="none" pos="word" start_char="6550">since</TOKEN>
<TOKEN end_char="6556" id="token-96-8" morph="none" pos="word" start_char="6556">R</TOKEN>
<TOKEN end_char="6558" id="token-96-9" morph="none" pos="word" start_char="6558">1</TOKEN>
<TOKEN end_char="6559" id="token-96-10" morph="none" pos="punct" start_char="6559">,</TOKEN>
<TOKEN end_char="6563" id="token-96-11" morph="none" pos="word" start_char="6561">the</TOKEN>
<TOKEN end_char="6574" id="token-96-12" morph="none" pos="word" start_char="6565">occasional</TOKEN>
<TOKEN end_char="6580" id="token-96-13" morph="none" pos="word" start_char="6576">local</TOKEN>
<TOKEN end_char="6589" id="token-96-14" morph="none" pos="word" start_char="6582">outbreak</TOKEN>
<TOKEN end_char="6594" id="token-96-15" morph="none" pos="word" start_char="6591">will</TOKEN>
<TOKEN end_char="6600" id="token-96-16" morph="none" pos="word" start_char="6596">occur</TOKEN>
<TOKEN end_char="6601" id="token-96-17" morph="none" pos="punct" start_char="6601">,</TOKEN>
<TOKEN end_char="6605" id="token-96-18" morph="none" pos="word" start_char="6603">and</TOKEN>
<TOKEN end_char="6610" id="token-96-19" morph="none" pos="word" start_char="6607">then</TOKEN>
<TOKEN end_char="6614" id="token-96-20" morph="none" pos="word" start_char="6612">die</TOKEN>
<TOKEN end_char="6618" id="token-96-21" morph="none" pos="word" start_char="6616">out</TOKEN>
<TOKEN end_char="6619" id="token-96-22" morph="none" pos="punct" start_char="6619">.</TOKEN>
</SEG>
<SEG end_char="6717" id="segment-97" start_char="6621">
<ORIGINAL_TEXT>- There are other coronaviruses circulating in bats that may occasionally spill over into humans.</ORIGINAL_TEXT>
<TOKEN end_char="6621" id="token-97-0" morph="none" pos="punct" start_char="6621">-</TOKEN>
<TOKEN end_char="6627" id="token-97-1" morph="none" pos="word" start_char="6623">There</TOKEN>
<TOKEN end_char="6631" id="token-97-2" morph="none" pos="word" start_char="6629">are</TOKEN>
<TOKEN end_char="6637" id="token-97-3" morph="none" pos="word" start_char="6633">other</TOKEN>
<TOKEN end_char="6651" id="token-97-4" morph="none" pos="word" start_char="6639">coronaviruses</TOKEN>
<TOKEN end_char="6663" id="token-97-5" morph="none" pos="word" start_char="6653">circulating</TOKEN>
<TOKEN end_char="6666" id="token-97-6" morph="none" pos="word" start_char="6665">in</TOKEN>
<TOKEN end_char="6671" id="token-97-7" morph="none" pos="word" start_char="6668">bats</TOKEN>
<TOKEN end_char="6676" id="token-97-8" morph="none" pos="word" start_char="6673">that</TOKEN>
<TOKEN end_char="6680" id="token-97-9" morph="none" pos="word" start_char="6678">may</TOKEN>
<TOKEN end_char="6693" id="token-97-10" morph="none" pos="word" start_char="6682">occasionally</TOKEN>
<TOKEN end_char="6699" id="token-97-11" morph="none" pos="word" start_char="6695">spill</TOKEN>
<TOKEN end_char="6704" id="token-97-12" morph="none" pos="word" start_char="6701">over</TOKEN>
<TOKEN end_char="6709" id="token-97-13" morph="none" pos="word" start_char="6706">into</TOKEN>
<TOKEN end_char="6716" id="token-97-14" morph="none" pos="word" start_char="6711">humans</TOKEN>
<TOKEN end_char="6717" id="token-97-15" morph="none" pos="punct" start_char="6717">.</TOKEN>
</SEG>
<SEG end_char="6919" id="segment-98" start_char="6719">
<ORIGINAL_TEXT>But provided they aren't spread by human-to-human contact, these will show up as sporadic positive samples for coronavirus, without going on to create a pandemic, an epidemic, or even a local outbreak.</ORIGINAL_TEXT>
<TOKEN end_char="6721" id="token-98-0" morph="none" pos="word" start_char="6719">But</TOKEN>
<TOKEN end_char="6730" id="token-98-1" morph="none" pos="word" start_char="6723">provided</TOKEN>
<TOKEN end_char="6735" id="token-98-2" morph="none" pos="word" start_char="6732">they</TOKEN>
<TOKEN end_char="6742" id="token-98-3" morph="none" pos="word" start_char="6737">aren't</TOKEN>
<TOKEN end_char="6749" id="token-98-4" morph="none" pos="word" start_char="6744">spread</TOKEN>
<TOKEN end_char="6752" id="token-98-5" morph="none" pos="word" start_char="6751">by</TOKEN>
<TOKEN end_char="6767" id="token-98-6" morph="none" pos="unknown" start_char="6754">human-to-human</TOKEN>
<TOKEN end_char="6775" id="token-98-7" morph="none" pos="word" start_char="6769">contact</TOKEN>
<TOKEN end_char="6776" id="token-98-8" morph="none" pos="punct" start_char="6776">,</TOKEN>
<TOKEN end_char="6782" id="token-98-9" morph="none" pos="word" start_char="6778">these</TOKEN>
<TOKEN end_char="6787" id="token-98-10" morph="none" pos="word" start_char="6784">will</TOKEN>
<TOKEN end_char="6792" id="token-98-11" morph="none" pos="word" start_char="6789">show</TOKEN>
<TOKEN end_char="6795" id="token-98-12" morph="none" pos="word" start_char="6794">up</TOKEN>
<TOKEN end_char="6798" id="token-98-13" morph="none" pos="word" start_char="6797">as</TOKEN>
<TOKEN end_char="6807" id="token-98-14" morph="none" pos="word" start_char="6800">sporadic</TOKEN>
<TOKEN end_char="6816" id="token-98-15" morph="none" pos="word" start_char="6809">positive</TOKEN>
<TOKEN end_char="6824" id="token-98-16" morph="none" pos="word" start_char="6818">samples</TOKEN>
<TOKEN end_char="6828" id="token-98-17" morph="none" pos="word" start_char="6826">for</TOKEN>
<TOKEN end_char="6840" id="token-98-18" morph="none" pos="word" start_char="6830">coronavirus</TOKEN>
<TOKEN end_char="6841" id="token-98-19" morph="none" pos="punct" start_char="6841">,</TOKEN>
<TOKEN end_char="6849" id="token-98-20" morph="none" pos="word" start_char="6843">without</TOKEN>
<TOKEN end_char="6855" id="token-98-21" morph="none" pos="word" start_char="6851">going</TOKEN>
<TOKEN end_char="6858" id="token-98-22" morph="none" pos="word" start_char="6857">on</TOKEN>
<TOKEN end_char="6861" id="token-98-23" morph="none" pos="word" start_char="6860">to</TOKEN>
<TOKEN end_char="6868" id="token-98-24" morph="none" pos="word" start_char="6863">create</TOKEN>
<TOKEN end_char="6870" id="token-98-25" morph="none" pos="word" start_char="6870">a</TOKEN>
<TOKEN end_char="6879" id="token-98-26" morph="none" pos="word" start_char="6872">pandemic</TOKEN>
<TOKEN end_char="6880" id="token-98-27" morph="none" pos="punct" start_char="6880">,</TOKEN>
<TOKEN end_char="6883" id="token-98-28" morph="none" pos="word" start_char="6882">an</TOKEN>
<TOKEN end_char="6892" id="token-98-29" morph="none" pos="word" start_char="6885">epidemic</TOKEN>
<TOKEN end_char="6893" id="token-98-30" morph="none" pos="punct" start_char="6893">,</TOKEN>
<TOKEN end_char="6896" id="token-98-31" morph="none" pos="word" start_char="6895">or</TOKEN>
<TOKEN end_char="6901" id="token-98-32" morph="none" pos="word" start_char="6898">even</TOKEN>
<TOKEN end_char="6903" id="token-98-33" morph="none" pos="word" start_char="6903">a</TOKEN>
<TOKEN end_char="6909" id="token-98-34" morph="none" pos="word" start_char="6905">local</TOKEN>
<TOKEN end_char="6918" id="token-98-35" morph="none" pos="word" start_char="6911">outbreak</TOKEN>
<TOKEN end_char="6919" id="token-98-36" morph="none" pos="punct" start_char="6919">.</TOKEN>
</SEG>
<SEG end_char="7141" id="segment-99" start_char="6922">
<ORIGINAL_TEXT>It is pretty clear that the pandemic form of the coronavirus spread out from Wuhan (although previous coronavirus samples suggest it may have been carried to Wuhan from bats in a warmer province of China, further south).</ORIGINAL_TEXT>
<TOKEN end_char="6923" id="token-99-0" morph="none" pos="word" start_char="6922">It</TOKEN>
<TOKEN end_char="6926" id="token-99-1" morph="none" pos="word" start_char="6925">is</TOKEN>
<TOKEN end_char="6933" id="token-99-2" morph="none" pos="word" start_char="6928">pretty</TOKEN>
<TOKEN end_char="6939" id="token-99-3" morph="none" pos="word" start_char="6935">clear</TOKEN>
<TOKEN end_char="6944" id="token-99-4" morph="none" pos="word" start_char="6941">that</TOKEN>
<TOKEN end_char="6948" id="token-99-5" morph="none" pos="word" start_char="6946">the</TOKEN>
<TOKEN end_char="6957" id="token-99-6" morph="none" pos="word" start_char="6950">pandemic</TOKEN>
<TOKEN end_char="6962" id="token-99-7" morph="none" pos="word" start_char="6959">form</TOKEN>
<TOKEN end_char="6965" id="token-99-8" morph="none" pos="word" start_char="6964">of</TOKEN>
<TOKEN end_char="6969" id="token-99-9" morph="none" pos="word" start_char="6967">the</TOKEN>
<TOKEN end_char="6981" id="token-99-10" morph="none" pos="word" start_char="6971">coronavirus</TOKEN>
<TOKEN end_char="6988" id="token-99-11" morph="none" pos="word" start_char="6983">spread</TOKEN>
<TOKEN end_char="6992" id="token-99-12" morph="none" pos="word" start_char="6990">out</TOKEN>
<TOKEN end_char="6997" id="token-99-13" morph="none" pos="word" start_char="6994">from</TOKEN>
<TOKEN end_char="7003" id="token-99-14" morph="none" pos="word" start_char="6999">Wuhan</TOKEN>
<TOKEN end_char="7005" id="token-99-15" morph="none" pos="punct" start_char="7005">(</TOKEN>
<TOKEN end_char="7013" id="token-99-16" morph="none" pos="word" start_char="7006">although</TOKEN>
<TOKEN end_char="7022" id="token-99-17" morph="none" pos="word" start_char="7015">previous</TOKEN>
<TOKEN end_char="7034" id="token-99-18" morph="none" pos="word" start_char="7024">coronavirus</TOKEN>
<TOKEN end_char="7042" id="token-99-19" morph="none" pos="word" start_char="7036">samples</TOKEN>
<TOKEN end_char="7050" id="token-99-20" morph="none" pos="word" start_char="7044">suggest</TOKEN>
<TOKEN end_char="7053" id="token-99-21" morph="none" pos="word" start_char="7052">it</TOKEN>
<TOKEN end_char="7057" id="token-99-22" morph="none" pos="word" start_char="7055">may</TOKEN>
<TOKEN end_char="7062" id="token-99-23" morph="none" pos="word" start_char="7059">have</TOKEN>
<TOKEN end_char="7067" id="token-99-24" morph="none" pos="word" start_char="7064">been</TOKEN>
<TOKEN end_char="7075" id="token-99-25" morph="none" pos="word" start_char="7069">carried</TOKEN>
<TOKEN end_char="7078" id="token-99-26" morph="none" pos="word" start_char="7077">to</TOKEN>
<TOKEN end_char="7084" id="token-99-27" morph="none" pos="word" start_char="7080">Wuhan</TOKEN>
<TOKEN end_char="7089" id="token-99-28" morph="none" pos="word" start_char="7086">from</TOKEN>
<TOKEN end_char="7094" id="token-99-29" morph="none" pos="word" start_char="7091">bats</TOKEN>
<TOKEN end_char="7097" id="token-99-30" morph="none" pos="word" start_char="7096">in</TOKEN>
<TOKEN end_char="7099" id="token-99-31" morph="none" pos="word" start_char="7099">a</TOKEN>
<TOKEN end_char="7106" id="token-99-32" morph="none" pos="word" start_char="7101">warmer</TOKEN>
<TOKEN end_char="7115" id="token-99-33" morph="none" pos="word" start_char="7108">province</TOKEN>
<TOKEN end_char="7118" id="token-99-34" morph="none" pos="word" start_char="7117">of</TOKEN>
<TOKEN end_char="7124" id="token-99-35" morph="none" pos="word" start_char="7120">China</TOKEN>
<TOKEN end_char="7125" id="token-99-36" morph="none" pos="punct" start_char="7125">,</TOKEN>
<TOKEN end_char="7133" id="token-99-37" morph="none" pos="word" start_char="7127">further</TOKEN>
<TOKEN end_char="7139" id="token-99-38" morph="none" pos="word" start_char="7135">south</TOKEN>
<TOKEN end_char="7141" id="token-99-39" morph="none" pos="punct" start_char="7140">).</TOKEN>
</SEG>
<SEG end_char="7258" id="segment-100" start_char="7144">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Australia was a man who flew from Wuhan to visit his parents in Melbourne.</ORIGINAL_TEXT>
<TOKEN end_char="7146" id="token-100-0" morph="none" pos="word" start_char="7144">The</TOKEN>
<TOKEN end_char="7151" id="token-100-1" morph="none" pos="word" start_char="7148">very</TOKEN>
<TOKEN end_char="7157" id="token-100-2" morph="none" pos="word" start_char="7153">first</TOKEN>
<TOKEN end_char="7166" id="token-100-3" morph="none" pos="unknown" start_char="7159">COVID-19</TOKEN>
<TOKEN end_char="7171" id="token-100-4" morph="none" pos="word" start_char="7168">case</TOKEN>
<TOKEN end_char="7180" id="token-100-5" morph="none" pos="word" start_char="7173">detected</TOKEN>
<TOKEN end_char="7183" id="token-100-6" morph="none" pos="word" start_char="7182">in</TOKEN>
<TOKEN end_char="7193" id="token-100-7" morph="none" pos="word" start_char="7185">Australia</TOKEN>
<TOKEN end_char="7197" id="token-100-8" morph="none" pos="word" start_char="7195">was</TOKEN>
<TOKEN end_char="7199" id="token-100-9" morph="none" pos="word" start_char="7199">a</TOKEN>
<TOKEN end_char="7203" id="token-100-10" morph="none" pos="word" start_char="7201">man</TOKEN>
<TOKEN end_char="7207" id="token-100-11" morph="none" pos="word" start_char="7205">who</TOKEN>
<TOKEN end_char="7212" id="token-100-12" morph="none" pos="word" start_char="7209">flew</TOKEN>
<TOKEN end_char="7217" id="token-100-13" morph="none" pos="word" start_char="7214">from</TOKEN>
<TOKEN end_char="7223" id="token-100-14" morph="none" pos="word" start_char="7219">Wuhan</TOKEN>
<TOKEN end_char="7226" id="token-100-15" morph="none" pos="word" start_char="7225">to</TOKEN>
<TOKEN end_char="7232" id="token-100-16" morph="none" pos="word" start_char="7228">visit</TOKEN>
<TOKEN end_char="7236" id="token-100-17" morph="none" pos="word" start_char="7234">his</TOKEN>
<TOKEN end_char="7244" id="token-100-18" morph="none" pos="word" start_char="7238">parents</TOKEN>
<TOKEN end_char="7247" id="token-100-19" morph="none" pos="word" start_char="7246">in</TOKEN>
<TOKEN end_char="7257" id="token-100-20" morph="none" pos="word" start_char="7249">Melbourne</TOKEN>
<TOKEN end_char="7258" id="token-100-21" morph="none" pos="punct" start_char="7258">.</TOKEN>
</SEG>
<SEG end_char="7428" id="segment-101" start_char="7260">
<ORIGINAL_TEXT>- Fortunately, he knew of the pandemic in Wuhan (he had just come from there), self-isolated, wore a mask, and was hospitalized, all without spreading it to anyone else.</ORIGINAL_TEXT>
<TOKEN end_char="7260" id="token-101-0" morph="none" pos="punct" start_char="7260">-</TOKEN>
<TOKEN end_char="7272" id="token-101-1" morph="none" pos="word" start_char="7262">Fortunately</TOKEN>
<TOKEN end_char="7273" id="token-101-2" morph="none" pos="punct" start_char="7273">,</TOKEN>
<TOKEN end_char="7276" id="token-101-3" morph="none" pos="word" start_char="7275">he</TOKEN>
<TOKEN end_char="7281" id="token-101-4" morph="none" pos="word" start_char="7278">knew</TOKEN>
<TOKEN end_char="7284" id="token-101-5" morph="none" pos="word" start_char="7283">of</TOKEN>
<TOKEN end_char="7288" id="token-101-6" morph="none" pos="word" start_char="7286">the</TOKEN>
<TOKEN end_char="7297" id="token-101-7" morph="none" pos="word" start_char="7290">pandemic</TOKEN>
<TOKEN end_char="7300" id="token-101-8" morph="none" pos="word" start_char="7299">in</TOKEN>
<TOKEN end_char="7306" id="token-101-9" morph="none" pos="word" start_char="7302">Wuhan</TOKEN>
<TOKEN end_char="7308" id="token-101-10" morph="none" pos="punct" start_char="7308">(</TOKEN>
<TOKEN end_char="7310" id="token-101-11" morph="none" pos="word" start_char="7309">he</TOKEN>
<TOKEN end_char="7314" id="token-101-12" morph="none" pos="word" start_char="7312">had</TOKEN>
<TOKEN end_char="7319" id="token-101-13" morph="none" pos="word" start_char="7316">just</TOKEN>
<TOKEN end_char="7324" id="token-101-14" morph="none" pos="word" start_char="7321">come</TOKEN>
<TOKEN end_char="7329" id="token-101-15" morph="none" pos="word" start_char="7326">from</TOKEN>
<TOKEN end_char="7335" id="token-101-16" morph="none" pos="word" start_char="7331">there</TOKEN>
<TOKEN end_char="7337" id="token-101-17" morph="none" pos="punct" start_char="7336">),</TOKEN>
<TOKEN end_char="7351" id="token-101-18" morph="none" pos="unknown" start_char="7339">self-isolated</TOKEN>
<TOKEN end_char="7352" id="token-101-19" morph="none" pos="punct" start_char="7352">,</TOKEN>
<TOKEN end_char="7357" id="token-101-20" morph="none" pos="word" start_char="7354">wore</TOKEN>
<TOKEN end_char="7359" id="token-101-21" morph="none" pos="word" start_char="7359">a</TOKEN>
<TOKEN end_char="7364" id="token-101-22" morph="none" pos="word" start_char="7361">mask</TOKEN>
<TOKEN end_char="7365" id="token-101-23" morph="none" pos="punct" start_char="7365">,</TOKEN>
<TOKEN end_char="7369" id="token-101-24" morph="none" pos="word" start_char="7367">and</TOKEN>
<TOKEN end_char="7373" id="token-101-25" morph="none" pos="word" start_char="7371">was</TOKEN>
<TOKEN end_char="7386" id="token-101-26" morph="none" pos="word" start_char="7375">hospitalized</TOKEN>
<TOKEN end_char="7387" id="token-101-27" morph="none" pos="punct" start_char="7387">,</TOKEN>
<TOKEN end_char="7391" id="token-101-28" morph="none" pos="word" start_char="7389">all</TOKEN>
<TOKEN end_char="7399" id="token-101-29" morph="none" pos="word" start_char="7393">without</TOKEN>
<TOKEN end_char="7409" id="token-101-30" morph="none" pos="word" start_char="7401">spreading</TOKEN>
<TOKEN end_char="7412" id="token-101-31" morph="none" pos="word" start_char="7411">it</TOKEN>
<TOKEN end_char="7415" id="token-101-32" morph="none" pos="word" start_char="7414">to</TOKEN>
<TOKEN end_char="7422" id="token-101-33" morph="none" pos="word" start_char="7417">anyone</TOKEN>
<TOKEN end_char="7427" id="token-101-34" morph="none" pos="word" start_char="7424">else</TOKEN>
<TOKEN end_char="7428" id="token-101-35" morph="none" pos="punct" start_char="7428">.</TOKEN>
</SEG>
<SEG end_char="7546" id="segment-102" start_char="7431">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Germany was a woman who flew from Wuhan to run a training course in Munich.</ORIGINAL_TEXT>
<TOKEN end_char="7433" id="token-102-0" morph="none" pos="word" start_char="7431">The</TOKEN>
<TOKEN end_char="7438" id="token-102-1" morph="none" pos="word" start_char="7435">very</TOKEN>
<TOKEN end_char="7444" id="token-102-2" morph="none" pos="word" start_char="7440">first</TOKEN>
<TOKEN end_char="7453" id="token-102-3" morph="none" pos="unknown" start_char="7446">COVID-19</TOKEN>
<TOKEN end_char="7458" id="token-102-4" morph="none" pos="word" start_char="7455">case</TOKEN>
<TOKEN end_char="7467" id="token-102-5" morph="none" pos="word" start_char="7460">detected</TOKEN>
<TOKEN end_char="7470" id="token-102-6" morph="none" pos="word" start_char="7469">in</TOKEN>
<TOKEN end_char="7478" id="token-102-7" morph="none" pos="word" start_char="7472">Germany</TOKEN>
<TOKEN end_char="7482" id="token-102-8" morph="none" pos="word" start_char="7480">was</TOKEN>
<TOKEN end_char="7484" id="token-102-9" morph="none" pos="word" start_char="7484">a</TOKEN>
<TOKEN end_char="7490" id="token-102-10" morph="none" pos="word" start_char="7486">woman</TOKEN>
<TOKEN end_char="7494" id="token-102-11" morph="none" pos="word" start_char="7492">who</TOKEN>
<TOKEN end_char="7499" id="token-102-12" morph="none" pos="word" start_char="7496">flew</TOKEN>
<TOKEN end_char="7504" id="token-102-13" morph="none" pos="word" start_char="7501">from</TOKEN>
<TOKEN end_char="7510" id="token-102-14" morph="none" pos="word" start_char="7506">Wuhan</TOKEN>
<TOKEN end_char="7513" id="token-102-15" morph="none" pos="word" start_char="7512">to</TOKEN>
<TOKEN end_char="7517" id="token-102-16" morph="none" pos="word" start_char="7515">run</TOKEN>
<TOKEN end_char="7519" id="token-102-17" morph="none" pos="word" start_char="7519">a</TOKEN>
<TOKEN end_char="7528" id="token-102-18" morph="none" pos="word" start_char="7521">training</TOKEN>
<TOKEN end_char="7535" id="token-102-19" morph="none" pos="word" start_char="7530">course</TOKEN>
<TOKEN end_char="7538" id="token-102-20" morph="none" pos="word" start_char="7537">in</TOKEN>
<TOKEN end_char="7545" id="token-102-21" morph="none" pos="word" start_char="7540">Munich</TOKEN>
<TOKEN end_char="7546" id="token-102-22" morph="none" pos="punct" start_char="7546">.</TOKEN>
</SEG>
<SEG end_char="7655" id="segment-103" start_char="7548">
<ORIGINAL_TEXT>- She only developed symptoms on the return flight to China, but several people on the course were infected.</ORIGINAL_TEXT>
<TOKEN end_char="7548" id="token-103-0" morph="none" pos="punct" start_char="7548">-</TOKEN>
<TOKEN end_char="7552" id="token-103-1" morph="none" pos="word" start_char="7550">She</TOKEN>
<TOKEN end_char="7557" id="token-103-2" morph="none" pos="word" start_char="7554">only</TOKEN>
<TOKEN end_char="7567" id="token-103-3" morph="none" pos="word" start_char="7559">developed</TOKEN>
<TOKEN end_char="7576" id="token-103-4" morph="none" pos="word" start_char="7569">symptoms</TOKEN>
<TOKEN end_char="7579" id="token-103-5" morph="none" pos="word" start_char="7578">on</TOKEN>
<TOKEN end_char="7583" id="token-103-6" morph="none" pos="word" start_char="7581">the</TOKEN>
<TOKEN end_char="7590" id="token-103-7" morph="none" pos="word" start_char="7585">return</TOKEN>
<TOKEN end_char="7597" id="token-103-8" morph="none" pos="word" start_char="7592">flight</TOKEN>
<TOKEN end_char="7600" id="token-103-9" morph="none" pos="word" start_char="7599">to</TOKEN>
<TOKEN end_char="7606" id="token-103-10" morph="none" pos="word" start_char="7602">China</TOKEN>
<TOKEN end_char="7607" id="token-103-11" morph="none" pos="punct" start_char="7607">,</TOKEN>
<TOKEN end_char="7611" id="token-103-12" morph="none" pos="word" start_char="7609">but</TOKEN>
<TOKEN end_char="7619" id="token-103-13" morph="none" pos="word" start_char="7613">several</TOKEN>
<TOKEN end_char="7626" id="token-103-14" morph="none" pos="word" start_char="7621">people</TOKEN>
<TOKEN end_char="7629" id="token-103-15" morph="none" pos="word" start_char="7628">on</TOKEN>
<TOKEN end_char="7633" id="token-103-16" morph="none" pos="word" start_char="7631">the</TOKEN>
<TOKEN end_char="7640" id="token-103-17" morph="none" pos="word" start_char="7635">course</TOKEN>
<TOKEN end_char="7645" id="token-103-18" morph="none" pos="word" start_char="7642">were</TOKEN>
<TOKEN end_char="7654" id="token-103-19" morph="none" pos="word" start_char="7647">infected</TOKEN>
<TOKEN end_char="7655" id="token-103-20" morph="none" pos="punct" start_char="7655">.</TOKEN>
</SEG>
<SEG end_char="7757" id="segment-104" start_char="7658">
<ORIGINAL_TEXT>Listen (43 minutes): https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</ORIGINAL_TEXT>
<TOKEN end_char="7663" id="token-104-0" morph="none" pos="word" start_char="7658">Listen</TOKEN>
<TOKEN end_char="7665" id="token-104-1" morph="none" pos="punct" start_char="7665">(</TOKEN>
<TOKEN end_char="7667" id="token-104-2" morph="none" pos="word" start_char="7666">43</TOKEN>
<TOKEN end_char="7675" id="token-104-3" morph="none" pos="word" start_char="7669">minutes</TOKEN>
<TOKEN end_char="7677" id="token-104-4" morph="none" pos="punct" start_char="7676">):</TOKEN>
<TOKEN end_char="7757" id="token-104-5" morph="none" pos="url" start_char="7679">https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</TOKEN>
<TRANSLATED_TEXT>Listen (43 minutes): https: / / www.abc.net.au / radionational / programs / rn-presents / patient-zero / 12523222</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="7801" id="segment-105" start_char="7761">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 04:17:23</ORIGINAL_TEXT>
<TOKEN end_char="7765" id="token-105-0" morph="none" pos="word" start_char="7761">Quote</TOKEN>
<TOKEN end_char="7770" id="token-105-1" morph="none" pos="word" start_char="7767">from</TOKEN>
<TOKEN end_char="7771" id="token-105-2" morph="none" pos="punct" start_char="7771">:</TOKEN>
<TOKEN end_char="7778" id="token-105-3" morph="none" pos="word" start_char="7773">Jolly2</TOKEN>
<TOKEN end_char="7781" id="token-105-4" morph="none" pos="word" start_char="7780">on</TOKEN>
<TOKEN end_char="7792" id="token-105-5" morph="none" pos="unknown" start_char="7783">07/02/2021</TOKEN>
<TOKEN end_char="7801" id="token-105-6" morph="none" pos="unknown" start_char="7794">04:17:23</TOKEN>
</SEG>
<SEG end_char="7804" id="segment-106" start_char="7804">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN end_char="7804" id="token-106-0" morph="none" pos="punct" start_char="7804">.</TOKEN>
</SEG>
<SEG end_char="7832" id="segment-107" start_char="7806">
<ORIGINAL_TEXT>Means it spread undetected,</ORIGINAL_TEXT>
<TOKEN end_char="7810" id="token-107-0" morph="none" pos="word" start_char="7806">Means</TOKEN>
<TOKEN end_char="7813" id="token-107-1" morph="none" pos="word" start_char="7812">it</TOKEN>
<TOKEN end_char="7820" id="token-107-2" morph="none" pos="word" start_char="7815">spread</TOKEN>
<TOKEN end_char="7831" id="token-107-3" morph="none" pos="word" start_char="7822">undetected</TOKEN>
<TOKEN end_char="7832" id="token-107-4" morph="none" pos="punct" start_char="7832">,</TOKEN>
</SEG>
<SEG end_char="7940" id="segment-108" start_char="7836">
<ORIGINAL_TEXT>You may be stunned to realise this, but Chines people notice if all the grannies and granddads are dying.</ORIGINAL_TEXT>
<TOKEN end_char="7838" id="token-108-0" morph="none" pos="word" start_char="7836">You</TOKEN>
<TOKEN end_char="7842" id="token-108-1" morph="none" pos="word" start_char="7840">may</TOKEN>
<TOKEN end_char="7845" id="token-108-2" morph="none" pos="word" start_char="7844">be</TOKEN>
<TOKEN end_char="7853" id="token-108-3" morph="none" pos="word" start_char="7847">stunned</TOKEN>
<TOKEN end_char="7856" id="token-108-4" morph="none" pos="word" start_char="7855">to</TOKEN>
<TOKEN end_char="7864" id="token-108-5" morph="none" pos="word" start_char="7858">realise</TOKEN>
<TOKEN end_char="7869" id="token-108-6" morph="none" pos="word" start_char="7866">this</TOKEN>
<TOKEN end_char="7870" id="token-108-7" morph="none" pos="punct" start_char="7870">,</TOKEN>
<TOKEN end_char="7874" id="token-108-8" morph="none" pos="word" start_char="7872">but</TOKEN>
<TOKEN end_char="7881" id="token-108-9" morph="none" pos="word" start_char="7876">Chines</TOKEN>
<TOKEN end_char="7888" id="token-108-10" morph="none" pos="word" start_char="7883">people</TOKEN>
<TOKEN end_char="7895" id="token-108-11" morph="none" pos="word" start_char="7890">notice</TOKEN>
<TOKEN end_char="7898" id="token-108-12" morph="none" pos="word" start_char="7897">if</TOKEN>
<TOKEN end_char="7902" id="token-108-13" morph="none" pos="word" start_char="7900">all</TOKEN>
<TOKEN end_char="7906" id="token-108-14" morph="none" pos="word" start_char="7904">the</TOKEN>
<TOKEN end_char="7915" id="token-108-15" morph="none" pos="word" start_char="7908">grannies</TOKEN>
<TOKEN end_char="7919" id="token-108-16" morph="none" pos="word" start_char="7917">and</TOKEN>
<TOKEN end_char="7929" id="token-108-17" morph="none" pos="word" start_char="7921">granddads</TOKEN>
<TOKEN end_char="7933" id="token-108-18" morph="none" pos="word" start_char="7931">are</TOKEN>
<TOKEN end_char="7939" id="token-108-19" morph="none" pos="word" start_char="7935">dying</TOKEN>
<TOKEN end_char="7940" id="token-108-20" morph="none" pos="punct" start_char="7940">.</TOKEN>
</SEG>
<SEG end_char="8140" id="segment-109" start_char="7942">
<ORIGINAL_TEXT>So it's essentially impossible that it spread undetected through China but somehow magically got there from Spain (or wherever) - where it was also magically showing restraint and not killing anyone.</ORIGINAL_TEXT>
<TOKEN end_char="7943" id="token-109-0" morph="none" pos="word" start_char="7942">So</TOKEN>
<TOKEN end_char="7948" id="token-109-1" morph="none" pos="word" start_char="7945">it's</TOKEN>
<TOKEN end_char="7960" id="token-109-2" morph="none" pos="word" start_char="7950">essentially</TOKEN>
<TOKEN end_char="7971" id="token-109-3" morph="none" pos="word" start_char="7962">impossible</TOKEN>
<TOKEN end_char="7976" id="token-109-4" morph="none" pos="word" start_char="7973">that</TOKEN>
<TOKEN end_char="7979" id="token-109-5" morph="none" pos="word" start_char="7978">it</TOKEN>
<TOKEN end_char="7986" id="token-109-6" morph="none" pos="word" start_char="7981">spread</TOKEN>
<TOKEN end_char="7997" id="token-109-7" morph="none" pos="word" start_char="7988">undetected</TOKEN>
<TOKEN end_char="8005" id="token-109-8" morph="none" pos="word" start_char="7999">through</TOKEN>
<TOKEN end_char="8011" id="token-109-9" morph="none" pos="word" start_char="8007">China</TOKEN>
<TOKEN end_char="8015" id="token-109-10" morph="none" pos="word" start_char="8013">but</TOKEN>
<TOKEN end_char="8023" id="token-109-11" morph="none" pos="word" start_char="8017">somehow</TOKEN>
<TOKEN end_char="8033" id="token-109-12" morph="none" pos="word" start_char="8025">magically</TOKEN>
<TOKEN end_char="8037" id="token-109-13" morph="none" pos="word" start_char="8035">got</TOKEN>
<TOKEN end_char="8043" id="token-109-14" morph="none" pos="word" start_char="8039">there</TOKEN>
<TOKEN end_char="8048" id="token-109-15" morph="none" pos="word" start_char="8045">from</TOKEN>
<TOKEN end_char="8054" id="token-109-16" morph="none" pos="word" start_char="8050">Spain</TOKEN>
<TOKEN end_char="8056" id="token-109-17" morph="none" pos="punct" start_char="8056">(</TOKEN>
<TOKEN end_char="8058" id="token-109-18" morph="none" pos="word" start_char="8057">or</TOKEN>
<TOKEN end_char="8067" id="token-109-19" morph="none" pos="word" start_char="8060">wherever</TOKEN>
<TOKEN end_char="8068" id="token-109-20" morph="none" pos="punct" start_char="8068">)</TOKEN>
<TOKEN end_char="8070" id="token-109-21" morph="none" pos="punct" start_char="8070">-</TOKEN>
<TOKEN end_char="8076" id="token-109-22" morph="none" pos="word" start_char="8072">where</TOKEN>
<TOKEN end_char="8079" id="token-109-23" morph="none" pos="word" start_char="8078">it</TOKEN>
<TOKEN end_char="8083" id="token-109-24" morph="none" pos="word" start_char="8081">was</TOKEN>
<TOKEN end_char="8088" id="token-109-25" morph="none" pos="word" start_char="8085">also</TOKEN>
<TOKEN end_char="8098" id="token-109-26" morph="none" pos="word" start_char="8090">magically</TOKEN>
<TOKEN end_char="8106" id="token-109-27" morph="none" pos="word" start_char="8100">showing</TOKEN>
<TOKEN end_char="8116" id="token-109-28" morph="none" pos="word" start_char="8108">restraint</TOKEN>
<TOKEN end_char="8120" id="token-109-29" morph="none" pos="word" start_char="8118">and</TOKEN>
<TOKEN end_char="8124" id="token-109-30" morph="none" pos="word" start_char="8122">not</TOKEN>
<TOKEN end_char="8132" id="token-109-31" morph="none" pos="word" start_char="8126">killing</TOKEN>
<TOKEN end_char="8139" id="token-109-32" morph="none" pos="word" start_char="8134">anyone</TOKEN>
<TOKEN end_char="8140" id="token-109-33" morph="none" pos="punct" start_char="8140">.</TOKEN>
</SEG>
<SEG end_char="8168" id="segment-110" start_char="8143">
<ORIGINAL_TEXT>Your suggestion is absurd.</ORIGINAL_TEXT>
<TOKEN end_char="8146" id="token-110-0" morph="none" pos="word" start_char="8143">Your</TOKEN>
<TOKEN end_char="8157" id="token-110-1" morph="none" pos="word" start_char="8148">suggestion</TOKEN>
<TOKEN end_char="8160" id="token-110-2" morph="none" pos="word" start_char="8159">is</TOKEN>
<TOKEN end_char="8167" id="token-110-3" morph="none" pos="word" start_char="8162">absurd</TOKEN>
<TOKEN end_char="8168" id="token-110-4" morph="none" pos="punct" start_char="8168">.</TOKEN>
</SEG>
<SEG end_char="8202" id="segment-111" start_char="8170">
<ORIGINAL_TEXT>Why are you still clinging to it?</ORIGINAL_TEXT>
<TOKEN end_char="8172" id="token-111-0" morph="none" pos="word" start_char="8170">Why</TOKEN>
<TOKEN end_char="8176" id="token-111-1" morph="none" pos="word" start_char="8174">are</TOKEN>
<TOKEN end_char="8180" id="token-111-2" morph="none" pos="word" start_char="8178">you</TOKEN>
<TOKEN end_char="8186" id="token-111-3" morph="none" pos="word" start_char="8182">still</TOKEN>
<TOKEN end_char="8195" id="token-111-4" morph="none" pos="word" start_char="8188">clinging</TOKEN>
<TOKEN end_char="8198" id="token-111-5" morph="none" pos="word" start_char="8197">to</TOKEN>
<TOKEN end_char="8201" id="token-111-6" morph="none" pos="word" start_char="8200">it</TOKEN>
<TOKEN end_char="8202" id="token-111-7" morph="none" pos="punct" start_char="8202">?</TOKEN>
</SEG>
<SEG end_char="8253" id="segment-112" start_char="8206">
<ORIGINAL_TEXT>Quote from: Bored chemist on 07/02/2021 10:24:14</ORIGINAL_TEXT>
<TOKEN end_char="8210" id="token-112-0" morph="none" pos="word" start_char="8206">Quote</TOKEN>
<TOKEN end_char="8215" id="token-112-1" morph="none" pos="word" start_char="8212">from</TOKEN>
<TOKEN end_char="8216" id="token-112-2" morph="none" pos="punct" start_char="8216">:</TOKEN>
<TOKEN end_char="8222" id="token-112-3" morph="none" pos="word" start_char="8218">Bored</TOKEN>
<TOKEN end_char="8230" id="token-112-4" morph="none" pos="word" start_char="8224">chemist</TOKEN>
<TOKEN end_char="8233" id="token-112-5" morph="none" pos="word" start_char="8232">on</TOKEN>
<TOKEN end_char="8244" id="token-112-6" morph="none" pos="unknown" start_char="8235">07/02/2021</TOKEN>
<TOKEN end_char="8253" id="token-112-7" morph="none" pos="unknown" start_char="8246">10:24:14</TOKEN>
</SEG>
<SEG end_char="8296" id="segment-113" start_char="8256">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 04:17:23</ORIGINAL_TEXT>
<TOKEN end_char="8260" id="token-113-0" morph="none" pos="word" start_char="8256">Quote</TOKEN>
<TOKEN end_char="8265" id="token-113-1" morph="none" pos="word" start_char="8262">from</TOKEN>
<TOKEN end_char="8266" id="token-113-2" morph="none" pos="punct" start_char="8266">:</TOKEN>
<TOKEN end_char="8273" id="token-113-3" morph="none" pos="word" start_char="8268">Jolly2</TOKEN>
<TOKEN end_char="8276" id="token-113-4" morph="none" pos="word" start_char="8275">on</TOKEN>
<TOKEN end_char="8287" id="token-113-5" morph="none" pos="unknown" start_char="8278">07/02/2021</TOKEN>
<TOKEN end_char="8296" id="token-113-6" morph="none" pos="unknown" start_char="8289">04:17:23</TOKEN>
</SEG>
<SEG end_char="8299" id="segment-114" start_char="8299">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN end_char="8299" id="token-114-0" morph="none" pos="punct" start_char="8299">.</TOKEN>
</SEG>
<SEG end_char="8327" id="segment-115" start_char="8301">
<ORIGINAL_TEXT>Means it spread undetected,</ORIGINAL_TEXT>
<TOKEN end_char="8305" id="token-115-0" morph="none" pos="word" start_char="8301">Means</TOKEN>
<TOKEN end_char="8308" id="token-115-1" morph="none" pos="word" start_char="8307">it</TOKEN>
<TOKEN end_char="8315" id="token-115-2" morph="none" pos="word" start_char="8310">spread</TOKEN>
<TOKEN end_char="8326" id="token-115-3" morph="none" pos="word" start_char="8317">undetected</TOKEN>
<TOKEN end_char="8327" id="token-115-4" morph="none" pos="punct" start_char="8327">,</TOKEN>
</SEG>
<SEG end_char="8692" id="segment-116" start_char="8330">
<ORIGINAL_TEXT>You may be stunned to realise this, but Chines people notice if all the grannies and granddads are dying.So it's essentially impossible that it spread undetected through China but somehow magically got there from Spain (or wherever) - where it was also magically showing restraint and not killing anyone.Your suggestion is absurd.Why are you still clinging to it?</ORIGINAL_TEXT>
<TOKEN end_char="8332" id="token-116-0" morph="none" pos="word" start_char="8330">You</TOKEN>
<TOKEN end_char="8336" id="token-116-1" morph="none" pos="word" start_char="8334">may</TOKEN>
<TOKEN end_char="8339" id="token-116-2" morph="none" pos="word" start_char="8338">be</TOKEN>
<TOKEN end_char="8347" id="token-116-3" morph="none" pos="word" start_char="8341">stunned</TOKEN>
<TOKEN end_char="8350" id="token-116-4" morph="none" pos="word" start_char="8349">to</TOKEN>
<TOKEN end_char="8358" id="token-116-5" morph="none" pos="word" start_char="8352">realise</TOKEN>
<TOKEN end_char="8363" id="token-116-6" morph="none" pos="word" start_char="8360">this</TOKEN>
<TOKEN end_char="8364" id="token-116-7" morph="none" pos="punct" start_char="8364">,</TOKEN>
<TOKEN end_char="8368" id="token-116-8" morph="none" pos="word" start_char="8366">but</TOKEN>
<TOKEN end_char="8375" id="token-116-9" morph="none" pos="word" start_char="8370">Chines</TOKEN>
<TOKEN end_char="8382" id="token-116-10" morph="none" pos="word" start_char="8377">people</TOKEN>
<TOKEN end_char="8389" id="token-116-11" morph="none" pos="word" start_char="8384">notice</TOKEN>
<TOKEN end_char="8392" id="token-116-12" morph="none" pos="word" start_char="8391">if</TOKEN>
<TOKEN end_char="8396" id="token-116-13" morph="none" pos="word" start_char="8394">all</TOKEN>
<TOKEN end_char="8400" id="token-116-14" morph="none" pos="word" start_char="8398">the</TOKEN>
<TOKEN end_char="8409" id="token-116-15" morph="none" pos="word" start_char="8402">grannies</TOKEN>
<TOKEN end_char="8413" id="token-116-16" morph="none" pos="word" start_char="8411">and</TOKEN>
<TOKEN end_char="8423" id="token-116-17" morph="none" pos="word" start_char="8415">granddads</TOKEN>
<TOKEN end_char="8427" id="token-116-18" morph="none" pos="word" start_char="8425">are</TOKEN>
<TOKEN end_char="8436" id="token-116-19" morph="none" pos="unknown" start_char="8429">dying.So</TOKEN>
<TOKEN end_char="8441" id="token-116-20" morph="none" pos="word" start_char="8438">it's</TOKEN>
<TOKEN end_char="8453" id="token-116-21" morph="none" pos="word" start_char="8443">essentially</TOKEN>
<TOKEN end_char="8464" id="token-116-22" morph="none" pos="word" start_char="8455">impossible</TOKEN>
<TOKEN end_char="8469" id="token-116-23" morph="none" pos="word" start_char="8466">that</TOKEN>
<TOKEN end_char="8472" id="token-116-24" morph="none" pos="word" start_char="8471">it</TOKEN>
<TOKEN end_char="8479" id="token-116-25" morph="none" pos="word" start_char="8474">spread</TOKEN>
<TOKEN end_char="8490" id="token-116-26" morph="none" pos="word" start_char="8481">undetected</TOKEN>
<TOKEN end_char="8498" id="token-116-27" morph="none" pos="word" start_char="8492">through</TOKEN>
<TOKEN end_char="8504" id="token-116-28" morph="none" pos="word" start_char="8500">China</TOKEN>
<TOKEN end_char="8508" id="token-116-29" morph="none" pos="word" start_char="8506">but</TOKEN>
<TOKEN end_char="8516" id="token-116-30" morph="none" pos="word" start_char="8510">somehow</TOKEN>
<TOKEN end_char="8526" id="token-116-31" morph="none" pos="word" start_char="8518">magically</TOKEN>
<TOKEN end_char="8530" id="token-116-32" morph="none" pos="word" start_char="8528">got</TOKEN>
<TOKEN end_char="8536" id="token-116-33" morph="none" pos="word" start_char="8532">there</TOKEN>
<TOKEN end_char="8541" id="token-116-34" morph="none" pos="word" start_char="8538">from</TOKEN>
<TOKEN end_char="8547" id="token-116-35" morph="none" pos="word" start_char="8543">Spain</TOKEN>
<TOKEN end_char="8549" id="token-116-36" morph="none" pos="punct" start_char="8549">(</TOKEN>
<TOKEN end_char="8551" id="token-116-37" morph="none" pos="word" start_char="8550">or</TOKEN>
<TOKEN end_char="8560" id="token-116-38" morph="none" pos="word" start_char="8553">wherever</TOKEN>
<TOKEN end_char="8561" id="token-116-39" morph="none" pos="punct" start_char="8561">)</TOKEN>
<TOKEN end_char="8563" id="token-116-40" morph="none" pos="punct" start_char="8563">-</TOKEN>
<TOKEN end_char="8569" id="token-116-41" morph="none" pos="word" start_char="8565">where</TOKEN>
<TOKEN end_char="8572" id="token-116-42" morph="none" pos="word" start_char="8571">it</TOKEN>
<TOKEN end_char="8576" id="token-116-43" morph="none" pos="word" start_char="8574">was</TOKEN>
<TOKEN end_char="8581" id="token-116-44" morph="none" pos="word" start_char="8578">also</TOKEN>
<TOKEN end_char="8591" id="token-116-45" morph="none" pos="word" start_char="8583">magically</TOKEN>
<TOKEN end_char="8599" id="token-116-46" morph="none" pos="word" start_char="8593">showing</TOKEN>
<TOKEN end_char="8609" id="token-116-47" morph="none" pos="word" start_char="8601">restraint</TOKEN>
<TOKEN end_char="8613" id="token-116-48" morph="none" pos="word" start_char="8611">and</TOKEN>
<TOKEN end_char="8617" id="token-116-49" morph="none" pos="word" start_char="8615">not</TOKEN>
<TOKEN end_char="8625" id="token-116-50" morph="none" pos="word" start_char="8619">killing</TOKEN>
<TOKEN end_char="8637" id="token-116-51" morph="none" pos="unknown" start_char="8627">anyone.Your</TOKEN>
<TOKEN end_char="8648" id="token-116-52" morph="none" pos="word" start_char="8639">suggestion</TOKEN>
<TOKEN end_char="8651" id="token-116-53" morph="none" pos="word" start_char="8650">is</TOKEN>
<TOKEN end_char="8662" id="token-116-54" morph="none" pos="unknown" start_char="8653">absurd.Why</TOKEN>
<TOKEN end_char="8666" id="token-116-55" morph="none" pos="word" start_char="8664">are</TOKEN>
<TOKEN end_char="8670" id="token-116-56" morph="none" pos="word" start_char="8668">you</TOKEN>
<TOKEN end_char="8676" id="token-116-57" morph="none" pos="word" start_char="8672">still</TOKEN>
<TOKEN end_char="8685" id="token-116-58" morph="none" pos="word" start_char="8678">clinging</TOKEN>
<TOKEN end_char="8688" id="token-116-59" morph="none" pos="word" start_char="8687">to</TOKEN>
<TOKEN end_char="8691" id="token-116-60" morph="none" pos="word" start_char="8690">it</TOKEN>
<TOKEN end_char="8692" id="token-116-61" morph="none" pos="punct" start_char="8692">?</TOKEN>
</SEG>
<SEG end_char="8847" id="segment-117" start_char="8695">
<ORIGINAL_TEXT>The death rate is low for covid, the symptoms Express themselves like flu, something like 80% of people infected have mild symptoms or are A symptomatic.</ORIGINAL_TEXT>
<TOKEN end_char="8697" id="token-117-0" morph="none" pos="word" start_char="8695">The</TOKEN>
<TOKEN end_char="8703" id="token-117-1" morph="none" pos="word" start_char="8699">death</TOKEN>
<TOKEN end_char="8708" id="token-117-2" morph="none" pos="word" start_char="8705">rate</TOKEN>
<TOKEN end_char="8711" id="token-117-3" morph="none" pos="word" start_char="8710">is</TOKEN>
<TOKEN end_char="8715" id="token-117-4" morph="none" pos="word" start_char="8713">low</TOKEN>
<TOKEN end_char="8719" id="token-117-5" morph="none" pos="word" start_char="8717">for</TOKEN>
<TOKEN end_char="8725" id="token-117-6" morph="none" pos="word" start_char="8721">covid</TOKEN>
<TOKEN end_char="8726" id="token-117-7" morph="none" pos="punct" start_char="8726">,</TOKEN>
<TOKEN end_char="8730" id="token-117-8" morph="none" pos="word" start_char="8728">the</TOKEN>
<TOKEN end_char="8739" id="token-117-9" morph="none" pos="word" start_char="8732">symptoms</TOKEN>
<TOKEN end_char="8747" id="token-117-10" morph="none" pos="word" start_char="8741">Express</TOKEN>
<TOKEN end_char="8758" id="token-117-11" morph="none" pos="word" start_char="8749">themselves</TOKEN>
<TOKEN end_char="8763" id="token-117-12" morph="none" pos="word" start_char="8760">like</TOKEN>
<TOKEN end_char="8767" id="token-117-13" morph="none" pos="word" start_char="8765">flu</TOKEN>
<TOKEN end_char="8768" id="token-117-14" morph="none" pos="punct" start_char="8768">,</TOKEN>
<TOKEN end_char="8778" id="token-117-15" morph="none" pos="word" start_char="8770">something</TOKEN>
<TOKEN end_char="8783" id="token-117-16" morph="none" pos="word" start_char="8780">like</TOKEN>
<TOKEN end_char="8786" id="token-117-17" morph="none" pos="word" start_char="8785">80</TOKEN>
<TOKEN end_char="8787" id="token-117-18" morph="none" pos="punct" start_char="8787">%</TOKEN>
<TOKEN end_char="8790" id="token-117-19" morph="none" pos="word" start_char="8789">of</TOKEN>
<TOKEN end_char="8797" id="token-117-20" morph="none" pos="word" start_char="8792">people</TOKEN>
<TOKEN end_char="8806" id="token-117-21" morph="none" pos="word" start_char="8799">infected</TOKEN>
<TOKEN end_char="8811" id="token-117-22" morph="none" pos="word" start_char="8808">have</TOKEN>
<TOKEN end_char="8816" id="token-117-23" morph="none" pos="word" start_char="8813">mild</TOKEN>
<TOKEN end_char="8825" id="token-117-24" morph="none" pos="word" start_char="8818">symptoms</TOKEN>
<TOKEN end_char="8828" id="token-117-25" morph="none" pos="word" start_char="8827">or</TOKEN>
<TOKEN end_char="8832" id="token-117-26" morph="none" pos="word" start_char="8830">are</TOKEN>
<TOKEN end_char="8834" id="token-117-27" morph="none" pos="word" start_char="8834">A</TOKEN>
<TOKEN end_char="8846" id="token-117-28" morph="none" pos="word" start_char="8836">symptomatic</TOKEN>
<TOKEN end_char="8847" id="token-117-29" morph="none" pos="punct" start_char="8847">.</TOKEN>
</SEG>
<SEG end_char="8920" id="segment-118" start_char="8849">
<ORIGINAL_TEXT>Deaths only occur with the elderly or people with underlying conditions.</ORIGINAL_TEXT>
<TOKEN end_char="8854" id="token-118-0" morph="none" pos="word" start_char="8849">Deaths</TOKEN>
<TOKEN end_char="8859" id="token-118-1" morph="none" pos="word" start_char="8856">only</TOKEN>
<TOKEN end_char="8865" id="token-118-2" morph="none" pos="word" start_char="8861">occur</TOKEN>
<TOKEN end_char="8870" id="token-118-3" morph="none" pos="word" start_char="8867">with</TOKEN>
<TOKEN end_char="8874" id="token-118-4" morph="none" pos="word" start_char="8872">the</TOKEN>
<TOKEN end_char="8882" id="token-118-5" morph="none" pos="word" start_char="8876">elderly</TOKEN>
<TOKEN end_char="8885" id="token-118-6" morph="none" pos="word" start_char="8884">or</TOKEN>
<TOKEN end_char="8892" id="token-118-7" morph="none" pos="word" start_char="8887">people</TOKEN>
<TOKEN end_char="8897" id="token-118-8" morph="none" pos="word" start_char="8894">with</TOKEN>
<TOKEN end_char="8908" id="token-118-9" morph="none" pos="word" start_char="8899">underlying</TOKEN>
<TOKEN end_char="8919" id="token-118-10" morph="none" pos="word" start_char="8910">conditions</TOKEN>
<TOKEN end_char="8920" id="token-118-11" morph="none" pos="punct" start_char="8920">.</TOKEN>
</SEG>
<SEG end_char="9059" id="segment-119" start_char="8923">
<ORIGINAL_TEXT>With up to a month incubation time, inherently means the virus will spread, as it has, arround the population atleast a month undetected.</ORIGINAL_TEXT>
<TOKEN end_char="8926" id="token-119-0" morph="none" pos="word" start_char="8923">With</TOKEN>
<TOKEN end_char="8929" id="token-119-1" morph="none" pos="word" start_char="8928">up</TOKEN>
<TOKEN end_char="8932" id="token-119-2" morph="none" pos="word" start_char="8931">to</TOKEN>
<TOKEN end_char="8934" id="token-119-3" morph="none" pos="word" start_char="8934">a</TOKEN>
<TOKEN end_char="8940" id="token-119-4" morph="none" pos="word" start_char="8936">month</TOKEN>
<TOKEN end_char="8951" id="token-119-5" morph="none" pos="word" start_char="8942">incubation</TOKEN>
<TOKEN end_char="8956" id="token-119-6" morph="none" pos="word" start_char="8953">time</TOKEN>
<TOKEN end_char="8957" id="token-119-7" morph="none" pos="punct" start_char="8957">,</TOKEN>
<TOKEN end_char="8968" id="token-119-8" morph="none" pos="word" start_char="8959">inherently</TOKEN>
<TOKEN end_char="8974" id="token-119-9" morph="none" pos="word" start_char="8970">means</TOKEN>
<TOKEN end_char="8978" id="token-119-10" morph="none" pos="word" start_char="8976">the</TOKEN>
<TOKEN end_char="8984" id="token-119-11" morph="none" pos="word" start_char="8980">virus</TOKEN>
<TOKEN end_char="8989" id="token-119-12" morph="none" pos="word" start_char="8986">will</TOKEN>
<TOKEN end_char="8996" id="token-119-13" morph="none" pos="word" start_char="8991">spread</TOKEN>
<TOKEN end_char="8997" id="token-119-14" morph="none" pos="punct" start_char="8997">,</TOKEN>
<TOKEN end_char="9000" id="token-119-15" morph="none" pos="word" start_char="8999">as</TOKEN>
<TOKEN end_char="9003" id="token-119-16" morph="none" pos="word" start_char="9002">it</TOKEN>
<TOKEN end_char="9007" id="token-119-17" morph="none" pos="word" start_char="9005">has</TOKEN>
<TOKEN end_char="9008" id="token-119-18" morph="none" pos="punct" start_char="9008">,</TOKEN>
<TOKEN end_char="9016" id="token-119-19" morph="none" pos="word" start_char="9010">arround</TOKEN>
<TOKEN end_char="9020" id="token-119-20" morph="none" pos="word" start_char="9018">the</TOKEN>
<TOKEN end_char="9031" id="token-119-21" morph="none" pos="word" start_char="9022">population</TOKEN>
<TOKEN end_char="9039" id="token-119-22" morph="none" pos="word" start_char="9033">atleast</TOKEN>
<TOKEN end_char="9041" id="token-119-23" morph="none" pos="word" start_char="9041">a</TOKEN>
<TOKEN end_char="9047" id="token-119-24" morph="none" pos="word" start_char="9043">month</TOKEN>
<TOKEN end_char="9058" id="token-119-25" morph="none" pos="word" start_char="9049">undetected</TOKEN>
<TOKEN end_char="9059" id="token-119-26" morph="none" pos="punct" start_char="9059">.</TOKEN>
</SEG>
<SEG end_char="9112" id="segment-120" start_char="9061">
<ORIGINAL_TEXT>With 80% presenting flu like symptoms if any at all.</ORIGINAL_TEXT>
<TOKEN end_char="9064" id="token-120-0" morph="none" pos="word" start_char="9061">With</TOKEN>
<TOKEN end_char="9067" id="token-120-1" morph="none" pos="word" start_char="9066">80</TOKEN>
<TOKEN end_char="9068" id="token-120-2" morph="none" pos="punct" start_char="9068">%</TOKEN>
<TOKEN end_char="9079" id="token-120-3" morph="none" pos="word" start_char="9070">presenting</TOKEN>
<TOKEN end_char="9083" id="token-120-4" morph="none" pos="word" start_char="9081">flu</TOKEN>
<TOKEN end_char="9088" id="token-120-5" morph="none" pos="word" start_char="9085">like</TOKEN>
<TOKEN end_char="9097" id="token-120-6" morph="none" pos="word" start_char="9090">symptoms</TOKEN>
<TOKEN end_char="9100" id="token-120-7" morph="none" pos="word" start_char="9099">if</TOKEN>
<TOKEN end_char="9104" id="token-120-8" morph="none" pos="word" start_char="9102">any</TOKEN>
<TOKEN end_char="9107" id="token-120-9" morph="none" pos="word" start_char="9106">at</TOKEN>
<TOKEN end_char="9111" id="token-120-10" morph="none" pos="word" start_char="9109">all</TOKEN>
<TOKEN end_char="9112" id="token-120-11" morph="none" pos="punct" start_char="9112">.</TOKEN>
</SEG>
<SEG end_char="9155" id="segment-121" start_char="9115">
<ORIGINAL_TEXT>So your position doesn't match the stats.</ORIGINAL_TEXT>
<TOKEN end_char="9116" id="token-121-0" morph="none" pos="word" start_char="9115">So</TOKEN>
<TOKEN end_char="9121" id="token-121-1" morph="none" pos="word" start_char="9118">your</TOKEN>
<TOKEN end_char="9130" id="token-121-2" morph="none" pos="word" start_char="9123">position</TOKEN>
<TOKEN end_char="9138" id="token-121-3" morph="none" pos="word" start_char="9132">doesn't</TOKEN>
<TOKEN end_char="9144" id="token-121-4" morph="none" pos="word" start_char="9140">match</TOKEN>
<TOKEN end_char="9148" id="token-121-5" morph="none" pos="word" start_char="9146">the</TOKEN>
<TOKEN end_char="9154" id="token-121-6" morph="none" pos="word" start_char="9150">stats</TOKEN>
<TOKEN end_char="9155" id="token-121-7" morph="none" pos="punct" start_char="9155">.</TOKEN>
</SEG>
<SEG end_char="9204" id="segment-122" start_char="9159">
<ORIGINAL_TEXT>Quote from: alancalverd on 07/02/2021 01:11:23</ORIGINAL_TEXT>
<TOKEN end_char="9163" id="token-122-0" morph="none" pos="word" start_char="9159">Quote</TOKEN>
<TOKEN end_char="9168" id="token-122-1" morph="none" pos="word" start_char="9165">from</TOKEN>
<TOKEN end_char="9169" id="token-122-2" morph="none" pos="punct" start_char="9169">:</TOKEN>
<TOKEN end_char="9181" id="token-122-3" morph="none" pos="word" start_char="9171">alancalverd</TOKEN>
<TOKEN end_char="9184" id="token-122-4" morph="none" pos="word" start_char="9183">on</TOKEN>
<TOKEN end_char="9195" id="token-122-5" morph="none" pos="unknown" start_char="9186">07/02/2021</TOKEN>
<TOKEN end_char="9204" id="token-122-6" morph="none" pos="unknown" start_char="9197">01:11:23</TOKEN>
</SEG>
<SEG end_char="9309" id="segment-123" start_char="9207">
<ORIGINAL_TEXT>Quote from: Bored chemist on Yesterday at 20:19:01Quote from: Jolly2 on Yesterday at 20:16:16they found</ORIGINAL_TEXT>
<TOKEN end_char="9211" id="token-123-0" morph="none" pos="word" start_char="9207">Quote</TOKEN>
<TOKEN end_char="9216" id="token-123-1" morph="none" pos="word" start_char="9213">from</TOKEN>
<TOKEN end_char="9217" id="token-123-2" morph="none" pos="punct" start_char="9217">:</TOKEN>
<TOKEN end_char="9223" id="token-123-3" morph="none" pos="word" start_char="9219">Bored</TOKEN>
<TOKEN end_char="9231" id="token-123-4" morph="none" pos="word" start_char="9225">chemist</TOKEN>
<TOKEN end_char="9234" id="token-123-5" morph="none" pos="word" start_char="9233">on</TOKEN>
<TOKEN end_char="9244" id="token-123-6" morph="none" pos="word" start_char="9236">Yesterday</TOKEN>
<TOKEN end_char="9247" id="token-123-7" morph="none" pos="word" start_char="9246">at</TOKEN>
<TOKEN end_char="9261" id="token-123-8" morph="none" pos="unknown" start_char="9249">20:19:01Quote</TOKEN>
<TOKEN end_char="9266" id="token-123-9" morph="none" pos="word" start_char="9263">from</TOKEN>
<TOKEN end_char="9267" id="token-123-10" morph="none" pos="punct" start_char="9267">:</TOKEN>
<TOKEN end_char="9274" id="token-123-11" morph="none" pos="word" start_char="9269">Jolly2</TOKEN>
<TOKEN end_char="9277" id="token-123-12" morph="none" pos="word" start_char="9276">on</TOKEN>
<TOKEN end_char="9287" id="token-123-13" morph="none" pos="word" start_char="9279">Yesterday</TOKEN>
<TOKEN end_char="9290" id="token-123-14" morph="none" pos="word" start_char="9289">at</TOKEN>
<TOKEN end_char="9303" id="token-123-15" morph="none" pos="unknown" start_char="9292">20:16:16they</TOKEN>
<TOKEN end_char="9309" id="token-123-16" morph="none" pos="word" start_char="9305">found</TOKEN>
</SEG>
<SEG end_char="9425" id="segment-124" start_char="9312">
<ORIGINAL_TEXT>The issue here is that we don't know how long fort Detrick was releasing materials or even what materials escaped.</ORIGINAL_TEXT>
<TOKEN end_char="9314" id="token-124-0" morph="none" pos="word" start_char="9312">The</TOKEN>
<TOKEN end_char="9320" id="token-124-1" morph="none" pos="word" start_char="9316">issue</TOKEN>
<TOKEN end_char="9325" id="token-124-2" morph="none" pos="word" start_char="9322">here</TOKEN>
<TOKEN end_char="9328" id="token-124-3" morph="none" pos="word" start_char="9327">is</TOKEN>
<TOKEN end_char="9333" id="token-124-4" morph="none" pos="word" start_char="9330">that</TOKEN>
<TOKEN end_char="9336" id="token-124-5" morph="none" pos="word" start_char="9335">we</TOKEN>
<TOKEN end_char="9342" id="token-124-6" morph="none" pos="word" start_char="9338">don't</TOKEN>
<TOKEN end_char="9347" id="token-124-7" morph="none" pos="word" start_char="9344">know</TOKEN>
<TOKEN end_char="9351" id="token-124-8" morph="none" pos="word" start_char="9349">how</TOKEN>
<TOKEN end_char="9356" id="token-124-9" morph="none" pos="word" start_char="9353">long</TOKEN>
<TOKEN end_char="9361" id="token-124-10" morph="none" pos="word" start_char="9358">fort</TOKEN>
<TOKEN end_char="9369" id="token-124-11" morph="none" pos="word" start_char="9363">Detrick</TOKEN>
<TOKEN end_char="9373" id="token-124-12" morph="none" pos="word" start_char="9371">was</TOKEN>
<TOKEN end_char="9383" id="token-124-13" morph="none" pos="word" start_char="9375">releasing</TOKEN>
<TOKEN end_char="9393" id="token-124-14" morph="none" pos="word" start_char="9385">materials</TOKEN>
<TOKEN end_char="9396" id="token-124-15" morph="none" pos="word" start_char="9395">or</TOKEN>
<TOKEN end_char="9401" id="token-124-16" morph="none" pos="word" start_char="9398">even</TOKEN>
<TOKEN end_char="9406" id="token-124-17" morph="none" pos="word" start_char="9403">what</TOKEN>
<TOKEN end_char="9416" id="token-124-18" morph="none" pos="word" start_char="9408">materials</TOKEN>
<TOKEN end_char="9424" id="token-124-19" morph="none" pos="word" start_char="9418">escaped</TOKEN>
<TOKEN end_char="9425" id="token-124-20" morph="none" pos="punct" start_char="9425">.</TOKEN>
</SEG>
<SEG end_char="9486" id="segment-125" start_char="9427">
<ORIGINAL_TEXT>How long would it take for a virus to travel a 4 hour drive?</ORIGINAL_TEXT>
<TOKEN end_char="9429" id="token-125-0" morph="none" pos="word" start_char="9427">How</TOKEN>
<TOKEN end_char="9434" id="token-125-1" morph="none" pos="word" start_char="9431">long</TOKEN>
<TOKEN end_char="9440" id="token-125-2" morph="none" pos="word" start_char="9436">would</TOKEN>
<TOKEN end_char="9443" id="token-125-3" morph="none" pos="word" start_char="9442">it</TOKEN>
<TOKEN end_char="9448" id="token-125-4" morph="none" pos="word" start_char="9445">take</TOKEN>
<TOKEN end_char="9452" id="token-125-5" morph="none" pos="word" start_char="9450">for</TOKEN>
<TOKEN end_char="9454" id="token-125-6" morph="none" pos="word" start_char="9454">a</TOKEN>
<TOKEN end_char="9460" id="token-125-7" morph="none" pos="word" start_char="9456">virus</TOKEN>
<TOKEN end_char="9463" id="token-125-8" morph="none" pos="word" start_char="9462">to</TOKEN>
<TOKEN end_char="9470" id="token-125-9" morph="none" pos="word" start_char="9465">travel</TOKEN>
<TOKEN end_char="9472" id="token-125-10" morph="none" pos="word" start_char="9472">a</TOKEN>
<TOKEN end_char="9474" id="token-125-11" morph="none" pos="word" start_char="9474">4</TOKEN>
<TOKEN end_char="9479" id="token-125-12" morph="none" pos="word" start_char="9476">hour</TOKEN>
<TOKEN end_char="9485" id="token-125-13" morph="none" pos="word" start_char="9481">drive</TOKEN>
<TOKEN end_char="9486" id="token-125-14" morph="none" pos="punct" start_char="9486">?</TOKEN>
</SEG>
<SEG end_char="9633" id="segment-126" start_char="9488">
<ORIGINAL_TEXT>Hard to say, but considering covid spread rather rapidly across borders, of 1000s of miles in weeks, a four hour drive is a rather short distance.</ORIGINAL_TEXT>
<TOKEN end_char="9491" id="token-126-0" morph="none" pos="word" start_char="9488">Hard</TOKEN>
<TOKEN end_char="9494" id="token-126-1" morph="none" pos="word" start_char="9493">to</TOKEN>
<TOKEN end_char="9498" id="token-126-2" morph="none" pos="word" start_char="9496">say</TOKEN>
<TOKEN end_char="9499" id="token-126-3" morph="none" pos="punct" start_char="9499">,</TOKEN>
<TOKEN end_char="9503" id="token-126-4" morph="none" pos="word" start_char="9501">but</TOKEN>
<TOKEN end_char="9515" id="token-126-5" morph="none" pos="word" start_char="9505">considering</TOKEN>
<TOKEN end_char="9521" id="token-126-6" morph="none" pos="word" start_char="9517">covid</TOKEN>
<TOKEN end_char="9528" id="token-126-7" morph="none" pos="word" start_char="9523">spread</TOKEN>
<TOKEN end_char="9535" id="token-126-8" morph="none" pos="word" start_char="9530">rather</TOKEN>
<TOKEN end_char="9543" id="token-126-9" morph="none" pos="word" start_char="9537">rapidly</TOKEN>
<TOKEN end_char="9550" id="token-126-10" morph="none" pos="word" start_char="9545">across</TOKEN>
<TOKEN end_char="9558" id="token-126-11" morph="none" pos="word" start_char="9552">borders</TOKEN>
<TOKEN end_char="9559" id="token-126-12" morph="none" pos="punct" start_char="9559">,</TOKEN>
<TOKEN end_char="9562" id="token-126-13" morph="none" pos="word" start_char="9561">of</TOKEN>
<TOKEN end_char="9568" id="token-126-14" morph="none" pos="word" start_char="9564">1000s</TOKEN>
<TOKEN end_char="9571" id="token-126-15" morph="none" pos="word" start_char="9570">of</TOKEN>
<TOKEN end_char="9577" id="token-126-16" morph="none" pos="word" start_char="9573">miles</TOKEN>
<TOKEN end_char="9580" id="token-126-17" morph="none" pos="word" start_char="9579">in</TOKEN>
<TOKEN end_char="9586" id="token-126-18" morph="none" pos="word" start_char="9582">weeks</TOKEN>
<TOKEN end_char="9587" id="token-126-19" morph="none" pos="punct" start_char="9587">,</TOKEN>
<TOKEN end_char="9589" id="token-126-20" morph="none" pos="word" start_char="9589">a</TOKEN>
<TOKEN end_char="9594" id="token-126-21" morph="none" pos="word" start_char="9591">four</TOKEN>
<TOKEN end_char="9599" id="token-126-22" morph="none" pos="word" start_char="9596">hour</TOKEN>
<TOKEN end_char="9605" id="token-126-23" morph="none" pos="word" start_char="9601">drive</TOKEN>
<TOKEN end_char="9608" id="token-126-24" morph="none" pos="word" start_char="9607">is</TOKEN>
<TOKEN end_char="9610" id="token-126-25" morph="none" pos="word" start_char="9610">a</TOKEN>
<TOKEN end_char="9617" id="token-126-26" morph="none" pos="word" start_char="9612">rather</TOKEN>
<TOKEN end_char="9623" id="token-126-27" morph="none" pos="word" start_char="9619">short</TOKEN>
<TOKEN end_char="9632" id="token-126-28" morph="none" pos="word" start_char="9625">distance</TOKEN>
<TOKEN end_char="9633" id="token-126-29" morph="none" pos="punct" start_char="9633">.</TOKEN>
</SEG>
<SEG end_char="9733" id="segment-127" start_char="9636">
<ORIGINAL_TEXT>We know the out break happened 2 weeks before fort Detrick was closed for miss managing materials.</ORIGINAL_TEXT>
<TOKEN end_char="9637" id="token-127-0" morph="none" pos="word" start_char="9636">We</TOKEN>
<TOKEN end_char="9642" id="token-127-1" morph="none" pos="word" start_char="9639">know</TOKEN>
<TOKEN end_char="9646" id="token-127-2" morph="none" pos="word" start_char="9644">the</TOKEN>
<TOKEN end_char="9650" id="token-127-3" morph="none" pos="word" start_char="9648">out</TOKEN>
<TOKEN end_char="9656" id="token-127-4" morph="none" pos="word" start_char="9652">break</TOKEN>
<TOKEN end_char="9665" id="token-127-5" morph="none" pos="word" start_char="9658">happened</TOKEN>
<TOKEN end_char="9667" id="token-127-6" morph="none" pos="word" start_char="9667">2</TOKEN>
<TOKEN end_char="9673" id="token-127-7" morph="none" pos="word" start_char="9669">weeks</TOKEN>
<TOKEN end_char="9680" id="token-127-8" morph="none" pos="word" start_char="9675">before</TOKEN>
<TOKEN end_char="9685" id="token-127-9" morph="none" pos="word" start_char="9682">fort</TOKEN>
<TOKEN end_char="9693" id="token-127-10" morph="none" pos="word" start_char="9687">Detrick</TOKEN>
<TOKEN end_char="9697" id="token-127-11" morph="none" pos="word" start_char="9695">was</TOKEN>
<TOKEN end_char="9704" id="token-127-12" morph="none" pos="word" start_char="9699">closed</TOKEN>
<TOKEN end_char="9708" id="token-127-13" morph="none" pos="word" start_char="9706">for</TOKEN>
<TOKEN end_char="9713" id="token-127-14" morph="none" pos="word" start_char="9710">miss</TOKEN>
<TOKEN end_char="9722" id="token-127-15" morph="none" pos="word" start_char="9715">managing</TOKEN>
<TOKEN end_char="9732" id="token-127-16" morph="none" pos="word" start_char="9724">materials</TOKEN>
<TOKEN end_char="9733" id="token-127-17" morph="none" pos="punct" start_char="9733">.</TOKEN>
</SEG>
<SEG end_char="9875" id="segment-128" start_char="9735">
<ORIGINAL_TEXT>The out break could have been the sign that lead to fort Detricks closing, but under national security, America isn't revealing what happened</ORIGINAL_TEXT>
<TOKEN end_char="9737" id="token-128-0" morph="none" pos="word" start_char="9735">The</TOKEN>
<TOKEN end_char="9741" id="token-128-1" morph="none" pos="word" start_char="9739">out</TOKEN>
<TOKEN end_char="9747" id="token-128-2" morph="none" pos="word" start_char="9743">break</TOKEN>
<TOKEN end_char="9753" id="token-128-3" morph="none" pos="word" start_char="9749">could</TOKEN>
<TOKEN end_char="9758" id="token-128-4" morph="none" pos="word" start_char="9755">have</TOKEN>
<TOKEN end_char="9763" id="token-128-5" morph="none" pos="word" start_char="9760">been</TOKEN>
<TOKEN end_char="9767" id="token-128-6" morph="none" pos="word" start_char="9765">the</TOKEN>
<TOKEN end_char="9772" id="token-128-7" morph="none" pos="word" start_char="9769">sign</TOKEN>
<TOKEN end_char="9777" id="token-128-8" morph="none" pos="word" start_char="9774">that</TOKEN>
<TOKEN end_char="9782" id="token-128-9" morph="none" pos="word" start_char="9779">lead</TOKEN>
<TOKEN end_char="9785" id="token-128-10" morph="none" pos="word" start_char="9784">to</TOKEN>
<TOKEN end_char="9790" id="token-128-11" morph="none" pos="word" start_char="9787">fort</TOKEN>
<TOKEN end_char="9799" id="token-128-12" morph="none" pos="word" start_char="9792">Detricks</TOKEN>
<TOKEN end_char="9807" id="token-128-13" morph="none" pos="word" start_char="9801">closing</TOKEN>
<TOKEN end_char="9808" id="token-128-14" morph="none" pos="punct" start_char="9808">,</TOKEN>
<TOKEN end_char="9812" id="token-128-15" morph="none" pos="word" start_char="9810">but</TOKEN>
<TOKEN end_char="9818" id="token-128-16" morph="none" pos="word" start_char="9814">under</TOKEN>
<TOKEN end_char="9827" id="token-128-17" morph="none" pos="word" start_char="9820">national</TOKEN>
<TOKEN end_char="9836" id="token-128-18" morph="none" pos="word" start_char="9829">security</TOKEN>
<TOKEN end_char="9837" id="token-128-19" morph="none" pos="punct" start_char="9837">,</TOKEN>
<TOKEN end_char="9845" id="token-128-20" morph="none" pos="word" start_char="9839">America</TOKEN>
<TOKEN end_char="9851" id="token-128-21" morph="none" pos="word" start_char="9847">isn't</TOKEN>
<TOKEN end_char="9861" id="token-128-22" morph="none" pos="word" start_char="9853">revealing</TOKEN>
<TOKEN end_char="9866" id="token-128-23" morph="none" pos="word" start_char="9863">what</TOKEN>
<TOKEN end_char="9875" id="token-128-24" morph="none" pos="word" start_char="9868">happened</TOKEN>
</SEG>
<SEG end_char="9920" id="segment-129" start_char="9879">
<ORIGINAL_TEXT>Quote from: evan_au on 07/02/2021 04:19:18</ORIGINAL_TEXT>
<TOKEN end_char="9883" id="token-129-0" morph="none" pos="word" start_char="9879">Quote</TOKEN>
<TOKEN end_char="9888" id="token-129-1" morph="none" pos="word" start_char="9885">from</TOKEN>
<TOKEN end_char="9889" id="token-129-2" morph="none" pos="punct" start_char="9889">:</TOKEN>
<TOKEN end_char="9897" id="token-129-3" morph="none" pos="word" start_char="9891">evan_au</TOKEN>
<TOKEN end_char="9900" id="token-129-4" morph="none" pos="word" start_char="9899">on</TOKEN>
<TOKEN end_char="9911" id="token-129-5" morph="none" pos="unknown" start_char="9902">07/02/2021</TOKEN>
<TOKEN end_char="9920" id="token-129-6" morph="none" pos="unknown" start_char="9913">04:19:18</TOKEN>
<TRANSLATED_TEXT>Quote from: luv on 07 / 02 / 2016 12: 19: 18</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="9940" id="segment-130" start_char="9923">
<ORIGINAL_TEXT>Quote from: Jolly2</ORIGINAL_TEXT>
<TOKEN end_char="9927" id="token-130-0" morph="none" pos="word" start_char="9923">Quote</TOKEN>
<TOKEN end_char="9932" id="token-130-1" morph="none" pos="word" start_char="9929">from</TOKEN>
<TOKEN end_char="9933" id="token-130-2" morph="none" pos="punct" start_char="9933">:</TOKEN>
<TOKEN end_char="9940" id="token-130-3" morph="none" pos="word" start_char="9935">Jolly2</TOKEN>
</SEG>
<SEG end_char="10001" id="segment-131" start_char="9943">
<ORIGINAL_TEXT>spain as early as March 2019...they found samples in sewage</ORIGINAL_TEXT>
<TOKEN end_char="9947" id="token-131-0" morph="none" pos="word" start_char="9943">spain</TOKEN>
<TOKEN end_char="9950" id="token-131-1" morph="none" pos="word" start_char="9949">as</TOKEN>
<TOKEN end_char="9956" id="token-131-2" morph="none" pos="word" start_char="9952">early</TOKEN>
<TOKEN end_char="9959" id="token-131-3" morph="none" pos="word" start_char="9958">as</TOKEN>
<TOKEN end_char="9965" id="token-131-4" morph="none" pos="word" start_char="9961">March</TOKEN>
<TOKEN end_char="9977" id="token-131-5" morph="none" pos="unknown" start_char="9967">2019...they</TOKEN>
<TOKEN end_char="9983" id="token-131-6" morph="none" pos="word" start_char="9979">found</TOKEN>
<TOKEN end_char="9991" id="token-131-7" morph="none" pos="word" start_char="9985">samples</TOKEN>
<TOKEN end_char="9994" id="token-131-8" morph="none" pos="word" start_char="9993">in</TOKEN>
<TOKEN end_char="10001" id="token-131-9" morph="none" pos="word" start_char="9996">sewage</TOKEN>
</SEG>
<SEG end_char="10928" id="segment-132" start_char="10004">
<ORIGINAL_TEXT>The quality of RNA in sewage is very variable.- We know that the fatty coat of the SARS-COV2 virus is broken down by soaps and detergents- Most people flush soap down the sewer when they take a bath or shower- Most people flush detergents down the sewer when they wash the dishes (and use even more destructive chemicals in the dishwasher)- Industrial processes can also flush destructive chemicals into sewers- So RNA is badly degraded when it is collected (within 1 week)- And, depending on how it is stored, may continue degrading if it is stored for months afterwards.So we have degraded RNA from stored sewage samples, which is compared to a new viral sequence (SARS-COV2).- This comparison must exclude the 4 common coronaviruses that are responsible for something like 30% of the "common cold" infections- And residual infections of MERS and SARS, which have 50% (MERS) or 80% (SARS) genetic similarities to SARS-COV2.</ORIGINAL_TEXT>
<TOKEN end_char="10006" id="token-132-0" morph="none" pos="word" start_char="10004">The</TOKEN>
<TOKEN end_char="10014" id="token-132-1" morph="none" pos="word" start_char="10008">quality</TOKEN>
<TOKEN end_char="10017" id="token-132-2" morph="none" pos="word" start_char="10016">of</TOKEN>
<TOKEN end_char="10021" id="token-132-3" morph="none" pos="word" start_char="10019">RNA</TOKEN>
<TOKEN end_char="10024" id="token-132-4" morph="none" pos="word" start_char="10023">in</TOKEN>
<TOKEN end_char="10031" id="token-132-5" morph="none" pos="word" start_char="10026">sewage</TOKEN>
<TOKEN end_char="10034" id="token-132-6" morph="none" pos="word" start_char="10033">is</TOKEN>
<TOKEN end_char="10039" id="token-132-7" morph="none" pos="word" start_char="10036">very</TOKEN>
<TOKEN end_char="10048" id="token-132-8" morph="none" pos="word" start_char="10041">variable</TOKEN>
<TOKEN end_char="10050" id="token-132-9" morph="none" pos="punct" start_char="10049">.-</TOKEN>
<TOKEN end_char="10053" id="token-132-10" morph="none" pos="word" start_char="10052">We</TOKEN>
<TOKEN end_char="10058" id="token-132-11" morph="none" pos="word" start_char="10055">know</TOKEN>
<TOKEN end_char="10063" id="token-132-12" morph="none" pos="word" start_char="10060">that</TOKEN>
<TOKEN end_char="10067" id="token-132-13" morph="none" pos="word" start_char="10065">the</TOKEN>
<TOKEN end_char="10073" id="token-132-14" morph="none" pos="word" start_char="10069">fatty</TOKEN>
<TOKEN end_char="10078" id="token-132-15" morph="none" pos="word" start_char="10075">coat</TOKEN>
<TOKEN end_char="10081" id="token-132-16" morph="none" pos="word" start_char="10080">of</TOKEN>
<TOKEN end_char="10085" id="token-132-17" morph="none" pos="word" start_char="10083">the</TOKEN>
<TOKEN end_char="10095" id="token-132-18" morph="none" pos="unknown" start_char="10087">SARS-COV2</TOKEN>
<TOKEN end_char="10101" id="token-132-19" morph="none" pos="word" start_char="10097">virus</TOKEN>
<TOKEN end_char="10104" id="token-132-20" morph="none" pos="word" start_char="10103">is</TOKEN>
<TOKEN end_char="10111" id="token-132-21" morph="none" pos="word" start_char="10106">broken</TOKEN>
<TOKEN end_char="10116" id="token-132-22" morph="none" pos="word" start_char="10113">down</TOKEN>
<TOKEN end_char="10119" id="token-132-23" morph="none" pos="word" start_char="10118">by</TOKEN>
<TOKEN end_char="10125" id="token-132-24" morph="none" pos="word" start_char="10121">soaps</TOKEN>
<TOKEN end_char="10129" id="token-132-25" morph="none" pos="word" start_char="10127">and</TOKEN>
<TOKEN end_char="10140" id="token-132-26" morph="none" pos="word" start_char="10131">detergents</TOKEN>
<TOKEN end_char="10141" id="token-132-27" morph="none" pos="punct" start_char="10141">-</TOKEN>
<TOKEN end_char="10146" id="token-132-28" morph="none" pos="word" start_char="10143">Most</TOKEN>
<TOKEN end_char="10153" id="token-132-29" morph="none" pos="word" start_char="10148">people</TOKEN>
<TOKEN end_char="10159" id="token-132-30" morph="none" pos="word" start_char="10155">flush</TOKEN>
<TOKEN end_char="10164" id="token-132-31" morph="none" pos="word" start_char="10161">soap</TOKEN>
<TOKEN end_char="10169" id="token-132-32" morph="none" pos="word" start_char="10166">down</TOKEN>
<TOKEN end_char="10173" id="token-132-33" morph="none" pos="word" start_char="10171">the</TOKEN>
<TOKEN end_char="10179" id="token-132-34" morph="none" pos="word" start_char="10175">sewer</TOKEN>
<TOKEN end_char="10184" id="token-132-35" morph="none" pos="word" start_char="10181">when</TOKEN>
<TOKEN end_char="10189" id="token-132-36" morph="none" pos="word" start_char="10186">they</TOKEN>
<TOKEN end_char="10194" id="token-132-37" morph="none" pos="word" start_char="10191">take</TOKEN>
<TOKEN end_char="10196" id="token-132-38" morph="none" pos="word" start_char="10196">a</TOKEN>
<TOKEN end_char="10201" id="token-132-39" morph="none" pos="word" start_char="10198">bath</TOKEN>
<TOKEN end_char="10204" id="token-132-40" morph="none" pos="word" start_char="10203">or</TOKEN>
<TOKEN end_char="10211" id="token-132-41" morph="none" pos="word" start_char="10206">shower</TOKEN>
<TOKEN end_char="10212" id="token-132-42" morph="none" pos="punct" start_char="10212">-</TOKEN>
<TOKEN end_char="10217" id="token-132-43" morph="none" pos="word" start_char="10214">Most</TOKEN>
<TOKEN end_char="10224" id="token-132-44" morph="none" pos="word" start_char="10219">people</TOKEN>
<TOKEN end_char="10230" id="token-132-45" morph="none" pos="word" start_char="10226">flush</TOKEN>
<TOKEN end_char="10241" id="token-132-46" morph="none" pos="word" start_char="10232">detergents</TOKEN>
<TOKEN end_char="10246" id="token-132-47" morph="none" pos="word" start_char="10243">down</TOKEN>
<TOKEN end_char="10250" id="token-132-48" morph="none" pos="word" start_char="10248">the</TOKEN>
<TOKEN end_char="10256" id="token-132-49" morph="none" pos="word" start_char="10252">sewer</TOKEN>
<TOKEN end_char="10261" id="token-132-50" morph="none" pos="word" start_char="10258">when</TOKEN>
<TOKEN end_char="10266" id="token-132-51" morph="none" pos="word" start_char="10263">they</TOKEN>
<TOKEN end_char="10271" id="token-132-52" morph="none" pos="word" start_char="10268">wash</TOKEN>
<TOKEN end_char="10275" id="token-132-53" morph="none" pos="word" start_char="10273">the</TOKEN>
<TOKEN end_char="10282" id="token-132-54" morph="none" pos="word" start_char="10277">dishes</TOKEN>
<TOKEN end_char="10284" id="token-132-55" morph="none" pos="punct" start_char="10284">(</TOKEN>
<TOKEN end_char="10287" id="token-132-56" morph="none" pos="word" start_char="10285">and</TOKEN>
<TOKEN end_char="10291" id="token-132-57" morph="none" pos="word" start_char="10289">use</TOKEN>
<TOKEN end_char="10296" id="token-132-58" morph="none" pos="word" start_char="10293">even</TOKEN>
<TOKEN end_char="10301" id="token-132-59" morph="none" pos="word" start_char="10298">more</TOKEN>
<TOKEN end_char="10313" id="token-132-60" morph="none" pos="word" start_char="10303">destructive</TOKEN>
<TOKEN end_char="10323" id="token-132-61" morph="none" pos="word" start_char="10315">chemicals</TOKEN>
<TOKEN end_char="10326" id="token-132-62" morph="none" pos="word" start_char="10325">in</TOKEN>
<TOKEN end_char="10330" id="token-132-63" morph="none" pos="word" start_char="10328">the</TOKEN>
<TOKEN end_char="10341" id="token-132-64" morph="none" pos="word" start_char="10332">dishwasher</TOKEN>
<TOKEN end_char="10343" id="token-132-65" morph="none" pos="punct" start_char="10342">)-</TOKEN>
<TOKEN end_char="10354" id="token-132-66" morph="none" pos="word" start_char="10345">Industrial</TOKEN>
<TOKEN end_char="10364" id="token-132-67" morph="none" pos="word" start_char="10356">processes</TOKEN>
<TOKEN end_char="10368" id="token-132-68" morph="none" pos="word" start_char="10366">can</TOKEN>
<TOKEN end_char="10373" id="token-132-69" morph="none" pos="word" start_char="10370">also</TOKEN>
<TOKEN end_char="10379" id="token-132-70" morph="none" pos="word" start_char="10375">flush</TOKEN>
<TOKEN end_char="10391" id="token-132-71" morph="none" pos="word" start_char="10381">destructive</TOKEN>
<TOKEN end_char="10401" id="token-132-72" morph="none" pos="word" start_char="10393">chemicals</TOKEN>
<TOKEN end_char="10406" id="token-132-73" morph="none" pos="word" start_char="10403">into</TOKEN>
<TOKEN end_char="10413" id="token-132-74" morph="none" pos="word" start_char="10408">sewers</TOKEN>
<TOKEN end_char="10414" id="token-132-75" morph="none" pos="punct" start_char="10414">-</TOKEN>
<TOKEN end_char="10417" id="token-132-76" morph="none" pos="word" start_char="10416">So</TOKEN>
<TOKEN end_char="10421" id="token-132-77" morph="none" pos="word" start_char="10419">RNA</TOKEN>
<TOKEN end_char="10424" id="token-132-78" morph="none" pos="word" start_char="10423">is</TOKEN>
<TOKEN end_char="10430" id="token-132-79" morph="none" pos="word" start_char="10426">badly</TOKEN>
<TOKEN end_char="10439" id="token-132-80" morph="none" pos="word" start_char="10432">degraded</TOKEN>
<TOKEN end_char="10444" id="token-132-81" morph="none" pos="word" start_char="10441">when</TOKEN>
<TOKEN end_char="10447" id="token-132-82" morph="none" pos="word" start_char="10446">it</TOKEN>
<TOKEN end_char="10450" id="token-132-83" morph="none" pos="word" start_char="10449">is</TOKEN>
<TOKEN end_char="10460" id="token-132-84" morph="none" pos="word" start_char="10452">collected</TOKEN>
<TOKEN end_char="10462" id="token-132-85" morph="none" pos="punct" start_char="10462">(</TOKEN>
<TOKEN end_char="10468" id="token-132-86" morph="none" pos="word" start_char="10463">within</TOKEN>
<TOKEN end_char="10470" id="token-132-87" morph="none" pos="word" start_char="10470">1</TOKEN>
<TOKEN end_char="10475" id="token-132-88" morph="none" pos="word" start_char="10472">week</TOKEN>
<TOKEN end_char="10477" id="token-132-89" morph="none" pos="punct" start_char="10476">)-</TOKEN>
<TOKEN end_char="10481" id="token-132-90" morph="none" pos="word" start_char="10479">And</TOKEN>
<TOKEN end_char="10482" id="token-132-91" morph="none" pos="punct" start_char="10482">,</TOKEN>
<TOKEN end_char="10492" id="token-132-92" morph="none" pos="word" start_char="10484">depending</TOKEN>
<TOKEN end_char="10495" id="token-132-93" morph="none" pos="word" start_char="10494">on</TOKEN>
<TOKEN end_char="10499" id="token-132-94" morph="none" pos="word" start_char="10497">how</TOKEN>
<TOKEN end_char="10502" id="token-132-95" morph="none" pos="word" start_char="10501">it</TOKEN>
<TOKEN end_char="10505" id="token-132-96" morph="none" pos="word" start_char="10504">is</TOKEN>
<TOKEN end_char="10512" id="token-132-97" morph="none" pos="word" start_char="10507">stored</TOKEN>
<TOKEN end_char="10513" id="token-132-98" morph="none" pos="punct" start_char="10513">,</TOKEN>
<TOKEN end_char="10517" id="token-132-99" morph="none" pos="word" start_char="10515">may</TOKEN>
<TOKEN end_char="10526" id="token-132-100" morph="none" pos="word" start_char="10519">continue</TOKEN>
<TOKEN end_char="10536" id="token-132-101" morph="none" pos="word" start_char="10528">degrading</TOKEN>
<TOKEN end_char="10539" id="token-132-102" morph="none" pos="word" start_char="10538">if</TOKEN>
<TOKEN end_char="10542" id="token-132-103" morph="none" pos="word" start_char="10541">it</TOKEN>
<TOKEN end_char="10545" id="token-132-104" morph="none" pos="word" start_char="10544">is</TOKEN>
<TOKEN end_char="10552" id="token-132-105" morph="none" pos="word" start_char="10547">stored</TOKEN>
<TOKEN end_char="10556" id="token-132-106" morph="none" pos="word" start_char="10554">for</TOKEN>
<TOKEN end_char="10563" id="token-132-107" morph="none" pos="word" start_char="10558">months</TOKEN>
<TOKEN end_char="10577" id="token-132-108" morph="none" pos="unknown" start_char="10565">afterwards.So</TOKEN>
<TOKEN end_char="10580" id="token-132-109" morph="none" pos="word" start_char="10579">we</TOKEN>
<TOKEN end_char="10585" id="token-132-110" morph="none" pos="word" start_char="10582">have</TOKEN>
<TOKEN end_char="10594" id="token-132-111" morph="none" pos="word" start_char="10587">degraded</TOKEN>
<TOKEN end_char="10598" id="token-132-112" morph="none" pos="word" start_char="10596">RNA</TOKEN>
<TOKEN end_char="10603" id="token-132-113" morph="none" pos="word" start_char="10600">from</TOKEN>
<TOKEN end_char="10610" id="token-132-114" morph="none" pos="word" start_char="10605">stored</TOKEN>
<TOKEN end_char="10617" id="token-132-115" morph="none" pos="word" start_char="10612">sewage</TOKEN>
<TOKEN end_char="10625" id="token-132-116" morph="none" pos="word" start_char="10619">samples</TOKEN>
<TOKEN end_char="10626" id="token-132-117" morph="none" pos="punct" start_char="10626">,</TOKEN>
<TOKEN end_char="10632" id="token-132-118" morph="none" pos="word" start_char="10628">which</TOKEN>
<TOKEN end_char="10635" id="token-132-119" morph="none" pos="word" start_char="10634">is</TOKEN>
<TOKEN end_char="10644" id="token-132-120" morph="none" pos="word" start_char="10637">compared</TOKEN>
<TOKEN end_char="10647" id="token-132-121" morph="none" pos="word" start_char="10646">to</TOKEN>
<TOKEN end_char="10649" id="token-132-122" morph="none" pos="word" start_char="10649">a</TOKEN>
<TOKEN end_char="10653" id="token-132-123" morph="none" pos="word" start_char="10651">new</TOKEN>
<TOKEN end_char="10659" id="token-132-124" morph="none" pos="word" start_char="10655">viral</TOKEN>
<TOKEN end_char="10668" id="token-132-125" morph="none" pos="word" start_char="10661">sequence</TOKEN>
<TOKEN end_char="10670" id="token-132-126" morph="none" pos="punct" start_char="10670">(</TOKEN>
<TOKEN end_char="10679" id="token-132-127" morph="none" pos="unknown" start_char="10671">SARS-COV2</TOKEN>
<TOKEN end_char="10682" id="token-132-128" morph="none" pos="punct" start_char="10680">).-</TOKEN>
<TOKEN end_char="10687" id="token-132-129" morph="none" pos="word" start_char="10684">This</TOKEN>
<TOKEN end_char="10698" id="token-132-130" morph="none" pos="word" start_char="10689">comparison</TOKEN>
<TOKEN end_char="10703" id="token-132-131" morph="none" pos="word" start_char="10700">must</TOKEN>
<TOKEN end_char="10711" id="token-132-132" morph="none" pos="word" start_char="10705">exclude</TOKEN>
<TOKEN end_char="10715" id="token-132-133" morph="none" pos="word" start_char="10713">the</TOKEN>
<TOKEN end_char="10717" id="token-132-134" morph="none" pos="word" start_char="10717">4</TOKEN>
<TOKEN end_char="10724" id="token-132-135" morph="none" pos="word" start_char="10719">common</TOKEN>
<TOKEN end_char="10738" id="token-132-136" morph="none" pos="word" start_char="10726">coronaviruses</TOKEN>
<TOKEN end_char="10743" id="token-132-137" morph="none" pos="word" start_char="10740">that</TOKEN>
<TOKEN end_char="10747" id="token-132-138" morph="none" pos="word" start_char="10745">are</TOKEN>
<TOKEN end_char="10759" id="token-132-139" morph="none" pos="word" start_char="10749">responsible</TOKEN>
<TOKEN end_char="10763" id="token-132-140" morph="none" pos="word" start_char="10761">for</TOKEN>
<TOKEN end_char="10773" id="token-132-141" morph="none" pos="word" start_char="10765">something</TOKEN>
<TOKEN end_char="10778" id="token-132-142" morph="none" pos="word" start_char="10775">like</TOKEN>
<TOKEN end_char="10781" id="token-132-143" morph="none" pos="word" start_char="10780">30</TOKEN>
<TOKEN end_char="10782" id="token-132-144" morph="none" pos="punct" start_char="10782">%</TOKEN>
<TOKEN end_char="10785" id="token-132-145" morph="none" pos="word" start_char="10784">of</TOKEN>
<TOKEN end_char="10789" id="token-132-146" morph="none" pos="word" start_char="10787">the</TOKEN>
<TOKEN end_char="10791" id="token-132-147" morph="none" pos="punct" start_char="10791">"</TOKEN>
<TOKEN end_char="10797" id="token-132-148" morph="none" pos="word" start_char="10792">common</TOKEN>
<TOKEN end_char="10802" id="token-132-149" morph="none" pos="word" start_char="10799">cold</TOKEN>
<TOKEN end_char="10803" id="token-132-150" morph="none" pos="punct" start_char="10803">"</TOKEN>
<TOKEN end_char="10814" id="token-132-151" morph="none" pos="word" start_char="10805">infections</TOKEN>
<TOKEN end_char="10815" id="token-132-152" morph="none" pos="punct" start_char="10815">-</TOKEN>
<TOKEN end_char="10819" id="token-132-153" morph="none" pos="word" start_char="10817">And</TOKEN>
<TOKEN end_char="10828" id="token-132-154" morph="none" pos="word" start_char="10821">residual</TOKEN>
<TOKEN end_char="10839" id="token-132-155" morph="none" pos="word" start_char="10830">infections</TOKEN>
<TOKEN end_char="10842" id="token-132-156" morph="none" pos="word" start_char="10841">of</TOKEN>
<TOKEN end_char="10847" id="token-132-157" morph="none" pos="word" start_char="10844">MERS</TOKEN>
<TOKEN end_char="10851" id="token-132-158" morph="none" pos="word" start_char="10849">and</TOKEN>
<TOKEN end_char="10856" id="token-132-159" morph="none" pos="word" start_char="10853">SARS</TOKEN>
<TOKEN end_char="10857" id="token-132-160" morph="none" pos="punct" start_char="10857">,</TOKEN>
<TOKEN end_char="10863" id="token-132-161" morph="none" pos="word" start_char="10859">which</TOKEN>
<TOKEN end_char="10868" id="token-132-162" morph="none" pos="word" start_char="10865">have</TOKEN>
<TOKEN end_char="10871" id="token-132-163" morph="none" pos="word" start_char="10870">50</TOKEN>
<TOKEN end_char="10872" id="token-132-164" morph="none" pos="punct" start_char="10872">%</TOKEN>
<TOKEN end_char="10874" id="token-132-165" morph="none" pos="punct" start_char="10874">(</TOKEN>
<TOKEN end_char="10878" id="token-132-166" morph="none" pos="word" start_char="10875">MERS</TOKEN>
<TOKEN end_char="10879" id="token-132-167" morph="none" pos="punct" start_char="10879">)</TOKEN>
<TOKEN end_char="10882" id="token-132-168" morph="none" pos="word" start_char="10881">or</TOKEN>
<TOKEN end_char="10885" id="token-132-169" morph="none" pos="word" start_char="10884">80</TOKEN>
<TOKEN end_char="10886" id="token-132-170" morph="none" pos="punct" start_char="10886">%</TOKEN>
<TOKEN end_char="10888" id="token-132-171" morph="none" pos="punct" start_char="10888">(</TOKEN>
<TOKEN end_char="10892" id="token-132-172" morph="none" pos="word" start_char="10889">SARS</TOKEN>
<TOKEN end_char="10893" id="token-132-173" morph="none" pos="punct" start_char="10893">)</TOKEN>
<TOKEN end_char="10901" id="token-132-174" morph="none" pos="word" start_char="10895">genetic</TOKEN>
<TOKEN end_char="10914" id="token-132-175" morph="none" pos="word" start_char="10903">similarities</TOKEN>
<TOKEN end_char="10917" id="token-132-176" morph="none" pos="word" start_char="10916">to</TOKEN>
<TOKEN end_char="10927" id="token-132-177" morph="none" pos="unknown" start_char="10919">SARS-COV2</TOKEN>
<TOKEN end_char="10928" id="token-132-178" morph="none" pos="punct" start_char="10928">.</TOKEN>
</SEG>
<SEG end_char="11137" id="segment-133" start_char="10930">
<ORIGINAL_TEXT>These viruses were never eliminated, but since R 1, the occasional local outbreak will occur, and then die out.- There are other coronaviruses circulating in bats that may occasionally spill over into humans.</ORIGINAL_TEXT>
<TOKEN end_char="10934" id="token-133-0" morph="none" pos="word" start_char="10930">These</TOKEN>
<TOKEN end_char="10942" id="token-133-1" morph="none" pos="word" start_char="10936">viruses</TOKEN>
<TOKEN end_char="10947" id="token-133-2" morph="none" pos="word" start_char="10944">were</TOKEN>
<TOKEN end_char="10953" id="token-133-3" morph="none" pos="word" start_char="10949">never</TOKEN>
<TOKEN end_char="10964" id="token-133-4" morph="none" pos="word" start_char="10955">eliminated</TOKEN>
<TOKEN end_char="10965" id="token-133-5" morph="none" pos="punct" start_char="10965">,</TOKEN>
<TOKEN end_char="10969" id="token-133-6" morph="none" pos="word" start_char="10967">but</TOKEN>
<TOKEN end_char="10975" id="token-133-7" morph="none" pos="word" start_char="10971">since</TOKEN>
<TOKEN end_char="10977" id="token-133-8" morph="none" pos="word" start_char="10977">R</TOKEN>
<TOKEN end_char="10979" id="token-133-9" morph="none" pos="word" start_char="10979">1</TOKEN>
<TOKEN end_char="10980" id="token-133-10" morph="none" pos="punct" start_char="10980">,</TOKEN>
<TOKEN end_char="10984" id="token-133-11" morph="none" pos="word" start_char="10982">the</TOKEN>
<TOKEN end_char="10995" id="token-133-12" morph="none" pos="word" start_char="10986">occasional</TOKEN>
<TOKEN end_char="11001" id="token-133-13" morph="none" pos="word" start_char="10997">local</TOKEN>
<TOKEN end_char="11010" id="token-133-14" morph="none" pos="word" start_char="11003">outbreak</TOKEN>
<TOKEN end_char="11015" id="token-133-15" morph="none" pos="word" start_char="11012">will</TOKEN>
<TOKEN end_char="11021" id="token-133-16" morph="none" pos="word" start_char="11017">occur</TOKEN>
<TOKEN end_char="11022" id="token-133-17" morph="none" pos="punct" start_char="11022">,</TOKEN>
<TOKEN end_char="11026" id="token-133-18" morph="none" pos="word" start_char="11024">and</TOKEN>
<TOKEN end_char="11031" id="token-133-19" morph="none" pos="word" start_char="11028">then</TOKEN>
<TOKEN end_char="11035" id="token-133-20" morph="none" pos="word" start_char="11033">die</TOKEN>
<TOKEN end_char="11039" id="token-133-21" morph="none" pos="word" start_char="11037">out</TOKEN>
<TOKEN end_char="11041" id="token-133-22" morph="none" pos="punct" start_char="11040">.-</TOKEN>
<TOKEN end_char="11047" id="token-133-23" morph="none" pos="word" start_char="11043">There</TOKEN>
<TOKEN end_char="11051" id="token-133-24" morph="none" pos="word" start_char="11049">are</TOKEN>
<TOKEN end_char="11057" id="token-133-25" morph="none" pos="word" start_char="11053">other</TOKEN>
<TOKEN end_char="11071" id="token-133-26" morph="none" pos="word" start_char="11059">coronaviruses</TOKEN>
<TOKEN end_char="11083" id="token-133-27" morph="none" pos="word" start_char="11073">circulating</TOKEN>
<TOKEN end_char="11086" id="token-133-28" morph="none" pos="word" start_char="11085">in</TOKEN>
<TOKEN end_char="11091" id="token-133-29" morph="none" pos="word" start_char="11088">bats</TOKEN>
<TOKEN end_char="11096" id="token-133-30" morph="none" pos="word" start_char="11093">that</TOKEN>
<TOKEN end_char="11100" id="token-133-31" morph="none" pos="word" start_char="11098">may</TOKEN>
<TOKEN end_char="11113" id="token-133-32" morph="none" pos="word" start_char="11102">occasionally</TOKEN>
<TOKEN end_char="11119" id="token-133-33" morph="none" pos="word" start_char="11115">spill</TOKEN>
<TOKEN end_char="11124" id="token-133-34" morph="none" pos="word" start_char="11121">over</TOKEN>
<TOKEN end_char="11129" id="token-133-35" morph="none" pos="word" start_char="11126">into</TOKEN>
<TOKEN end_char="11136" id="token-133-36" morph="none" pos="word" start_char="11131">humans</TOKEN>
<TOKEN end_char="11137" id="token-133-37" morph="none" pos="punct" start_char="11137">.</TOKEN>
</SEG>
<SEG end_char="11674" id="segment-134" start_char="11139">
<ORIGINAL_TEXT>But provided they aren't spread by human-to-human contact, these will show up as sporadic positive samples for coronavirus, without going on to create a pandemic, an epidemic, or even a local outbreak.It is pretty clear that the pandemic form of the coronavirus spread out from Wuhan (although previous coronavirus samples suggest it may have been carried to Wuhan from bats in a warmer province of China, further south).The very first COVID-19 case detected in Australia was a man who flew from Wuhan to visit his parents in Melbourne.</ORIGINAL_TEXT>
<TOKEN end_char="11141" id="token-134-0" morph="none" pos="word" start_char="11139">But</TOKEN>
<TOKEN end_char="11150" id="token-134-1" morph="none" pos="word" start_char="11143">provided</TOKEN>
<TOKEN end_char="11155" id="token-134-2" morph="none" pos="word" start_char="11152">they</TOKEN>
<TOKEN end_char="11162" id="token-134-3" morph="none" pos="word" start_char="11157">aren't</TOKEN>
<TOKEN end_char="11169" id="token-134-4" morph="none" pos="word" start_char="11164">spread</TOKEN>
<TOKEN end_char="11172" id="token-134-5" morph="none" pos="word" start_char="11171">by</TOKEN>
<TOKEN end_char="11187" id="token-134-6" morph="none" pos="unknown" start_char="11174">human-to-human</TOKEN>
<TOKEN end_char="11195" id="token-134-7" morph="none" pos="word" start_char="11189">contact</TOKEN>
<TOKEN end_char="11196" id="token-134-8" morph="none" pos="punct" start_char="11196">,</TOKEN>
<TOKEN end_char="11202" id="token-134-9" morph="none" pos="word" start_char="11198">these</TOKEN>
<TOKEN end_char="11207" id="token-134-10" morph="none" pos="word" start_char="11204">will</TOKEN>
<TOKEN end_char="11212" id="token-134-11" morph="none" pos="word" start_char="11209">show</TOKEN>
<TOKEN end_char="11215" id="token-134-12" morph="none" pos="word" start_char="11214">up</TOKEN>
<TOKEN end_char="11218" id="token-134-13" morph="none" pos="word" start_char="11217">as</TOKEN>
<TOKEN end_char="11227" id="token-134-14" morph="none" pos="word" start_char="11220">sporadic</TOKEN>
<TOKEN end_char="11236" id="token-134-15" morph="none" pos="word" start_char="11229">positive</TOKEN>
<TOKEN end_char="11244" id="token-134-16" morph="none" pos="word" start_char="11238">samples</TOKEN>
<TOKEN end_char="11248" id="token-134-17" morph="none" pos="word" start_char="11246">for</TOKEN>
<TOKEN end_char="11260" id="token-134-18" morph="none" pos="word" start_char="11250">coronavirus</TOKEN>
<TOKEN end_char="11261" id="token-134-19" morph="none" pos="punct" start_char="11261">,</TOKEN>
<TOKEN end_char="11269" id="token-134-20" morph="none" pos="word" start_char="11263">without</TOKEN>
<TOKEN end_char="11275" id="token-134-21" morph="none" pos="word" start_char="11271">going</TOKEN>
<TOKEN end_char="11278" id="token-134-22" morph="none" pos="word" start_char="11277">on</TOKEN>
<TOKEN end_char="11281" id="token-134-23" morph="none" pos="word" start_char="11280">to</TOKEN>
<TOKEN end_char="11288" id="token-134-24" morph="none" pos="word" start_char="11283">create</TOKEN>
<TOKEN end_char="11290" id="token-134-25" morph="none" pos="word" start_char="11290">a</TOKEN>
<TOKEN end_char="11299" id="token-134-26" morph="none" pos="word" start_char="11292">pandemic</TOKEN>
<TOKEN end_char="11300" id="token-134-27" morph="none" pos="punct" start_char="11300">,</TOKEN>
<TOKEN end_char="11303" id="token-134-28" morph="none" pos="word" start_char="11302">an</TOKEN>
<TOKEN end_char="11312" id="token-134-29" morph="none" pos="word" start_char="11305">epidemic</TOKEN>
<TOKEN end_char="11313" id="token-134-30" morph="none" pos="punct" start_char="11313">,</TOKEN>
<TOKEN end_char="11316" id="token-134-31" morph="none" pos="word" start_char="11315">or</TOKEN>
<TOKEN end_char="11321" id="token-134-32" morph="none" pos="word" start_char="11318">even</TOKEN>
<TOKEN end_char="11323" id="token-134-33" morph="none" pos="word" start_char="11323">a</TOKEN>
<TOKEN end_char="11329" id="token-134-34" morph="none" pos="word" start_char="11325">local</TOKEN>
<TOKEN end_char="11341" id="token-134-35" morph="none" pos="unknown" start_char="11331">outbreak.It</TOKEN>
<TOKEN end_char="11344" id="token-134-36" morph="none" pos="word" start_char="11343">is</TOKEN>
<TOKEN end_char="11351" id="token-134-37" morph="none" pos="word" start_char="11346">pretty</TOKEN>
<TOKEN end_char="11357" id="token-134-38" morph="none" pos="word" start_char="11353">clear</TOKEN>
<TOKEN end_char="11362" id="token-134-39" morph="none" pos="word" start_char="11359">that</TOKEN>
<TOKEN end_char="11366" id="token-134-40" morph="none" pos="word" start_char="11364">the</TOKEN>
<TOKEN end_char="11375" id="token-134-41" morph="none" pos="word" start_char="11368">pandemic</TOKEN>
<TOKEN end_char="11380" id="token-134-42" morph="none" pos="word" start_char="11377">form</TOKEN>
<TOKEN end_char="11383" id="token-134-43" morph="none" pos="word" start_char="11382">of</TOKEN>
<TOKEN end_char="11387" id="token-134-44" morph="none" pos="word" start_char="11385">the</TOKEN>
<TOKEN end_char="11399" id="token-134-45" morph="none" pos="word" start_char="11389">coronavirus</TOKEN>
<TOKEN end_char="11406" id="token-134-46" morph="none" pos="word" start_char="11401">spread</TOKEN>
<TOKEN end_char="11410" id="token-134-47" morph="none" pos="word" start_char="11408">out</TOKEN>
<TOKEN end_char="11415" id="token-134-48" morph="none" pos="word" start_char="11412">from</TOKEN>
<TOKEN end_char="11421" id="token-134-49" morph="none" pos="word" start_char="11417">Wuhan</TOKEN>
<TOKEN end_char="11423" id="token-134-50" morph="none" pos="punct" start_char="11423">(</TOKEN>
<TOKEN end_char="11431" id="token-134-51" morph="none" pos="word" start_char="11424">although</TOKEN>
<TOKEN end_char="11440" id="token-134-52" morph="none" pos="word" start_char="11433">previous</TOKEN>
<TOKEN end_char="11452" id="token-134-53" morph="none" pos="word" start_char="11442">coronavirus</TOKEN>
<TOKEN end_char="11460" id="token-134-54" morph="none" pos="word" start_char="11454">samples</TOKEN>
<TOKEN end_char="11468" id="token-134-55" morph="none" pos="word" start_char="11462">suggest</TOKEN>
<TOKEN end_char="11471" id="token-134-56" morph="none" pos="word" start_char="11470">it</TOKEN>
<TOKEN end_char="11475" id="token-134-57" morph="none" pos="word" start_char="11473">may</TOKEN>
<TOKEN end_char="11480" id="token-134-58" morph="none" pos="word" start_char="11477">have</TOKEN>
<TOKEN end_char="11485" id="token-134-59" morph="none" pos="word" start_char="11482">been</TOKEN>
<TOKEN end_char="11493" id="token-134-60" morph="none" pos="word" start_char="11487">carried</TOKEN>
<TOKEN end_char="11496" id="token-134-61" morph="none" pos="word" start_char="11495">to</TOKEN>
<TOKEN end_char="11502" id="token-134-62" morph="none" pos="word" start_char="11498">Wuhan</TOKEN>
<TOKEN end_char="11507" id="token-134-63" morph="none" pos="word" start_char="11504">from</TOKEN>
<TOKEN end_char="11512" id="token-134-64" morph="none" pos="word" start_char="11509">bats</TOKEN>
<TOKEN end_char="11515" id="token-134-65" morph="none" pos="word" start_char="11514">in</TOKEN>
<TOKEN end_char="11517" id="token-134-66" morph="none" pos="word" start_char="11517">a</TOKEN>
<TOKEN end_char="11524" id="token-134-67" morph="none" pos="word" start_char="11519">warmer</TOKEN>
<TOKEN end_char="11533" id="token-134-68" morph="none" pos="word" start_char="11526">province</TOKEN>
<TOKEN end_char="11536" id="token-134-69" morph="none" pos="word" start_char="11535">of</TOKEN>
<TOKEN end_char="11542" id="token-134-70" morph="none" pos="word" start_char="11538">China</TOKEN>
<TOKEN end_char="11543" id="token-134-71" morph="none" pos="punct" start_char="11543">,</TOKEN>
<TOKEN end_char="11551" id="token-134-72" morph="none" pos="word" start_char="11545">further</TOKEN>
<TOKEN end_char="11562" id="token-134-73" morph="none" pos="unknown" start_char="11553">south).The</TOKEN>
<TOKEN end_char="11567" id="token-134-74" morph="none" pos="word" start_char="11564">very</TOKEN>
<TOKEN end_char="11573" id="token-134-75" morph="none" pos="word" start_char="11569">first</TOKEN>
<TOKEN end_char="11582" id="token-134-76" morph="none" pos="unknown" start_char="11575">COVID-19</TOKEN>
<TOKEN end_char="11587" id="token-134-77" morph="none" pos="word" start_char="11584">case</TOKEN>
<TOKEN end_char="11596" id="token-134-78" morph="none" pos="word" start_char="11589">detected</TOKEN>
<TOKEN end_char="11599" id="token-134-79" morph="none" pos="word" start_char="11598">in</TOKEN>
<TOKEN end_char="11609" id="token-134-80" morph="none" pos="word" start_char="11601">Australia</TOKEN>
<TOKEN end_char="11613" id="token-134-81" morph="none" pos="word" start_char="11611">was</TOKEN>
<TOKEN end_char="11615" id="token-134-82" morph="none" pos="word" start_char="11615">a</TOKEN>
<TOKEN end_char="11619" id="token-134-83" morph="none" pos="word" start_char="11617">man</TOKEN>
<TOKEN end_char="11623" id="token-134-84" morph="none" pos="word" start_char="11621">who</TOKEN>
<TOKEN end_char="11628" id="token-134-85" morph="none" pos="word" start_char="11625">flew</TOKEN>
<TOKEN end_char="11633" id="token-134-86" morph="none" pos="word" start_char="11630">from</TOKEN>
<TOKEN end_char="11639" id="token-134-87" morph="none" pos="word" start_char="11635">Wuhan</TOKEN>
<TOKEN end_char="11642" id="token-134-88" morph="none" pos="word" start_char="11641">to</TOKEN>
<TOKEN end_char="11648" id="token-134-89" morph="none" pos="word" start_char="11644">visit</TOKEN>
<TOKEN end_char="11652" id="token-134-90" morph="none" pos="word" start_char="11650">his</TOKEN>
<TOKEN end_char="11660" id="token-134-91" morph="none" pos="word" start_char="11654">parents</TOKEN>
<TOKEN end_char="11663" id="token-134-92" morph="none" pos="word" start_char="11662">in</TOKEN>
<TOKEN end_char="11673" id="token-134-93" morph="none" pos="word" start_char="11665">Melbourne</TOKEN>
<TOKEN end_char="11674" id="token-134-94" morph="none" pos="punct" start_char="11674">.</TOKEN>
</SEG>
<SEG end_char="11844" id="segment-135" start_char="11676">
<ORIGINAL_TEXT>- Fortunately, he knew of the pandemic in Wuhan (he had just come from there), self-isolated, wore a mask, and was hospitalized, all without spreading it to anyone else.</ORIGINAL_TEXT>
<TOKEN end_char="11676" id="token-135-0" morph="none" pos="punct" start_char="11676">-</TOKEN>
<TOKEN end_char="11688" id="token-135-1" morph="none" pos="word" start_char="11678">Fortunately</TOKEN>
<TOKEN end_char="11689" id="token-135-2" morph="none" pos="punct" start_char="11689">,</TOKEN>
<TOKEN end_char="11692" id="token-135-3" morph="none" pos="word" start_char="11691">he</TOKEN>
<TOKEN end_char="11697" id="token-135-4" morph="none" pos="word" start_char="11694">knew</TOKEN>
<TOKEN end_char="11700" id="token-135-5" morph="none" pos="word" start_char="11699">of</TOKEN>
<TOKEN end_char="11704" id="token-135-6" morph="none" pos="word" start_char="11702">the</TOKEN>
<TOKEN end_char="11713" id="token-135-7" morph="none" pos="word" start_char="11706">pandemic</TOKEN>
<TOKEN end_char="11716" id="token-135-8" morph="none" pos="word" start_char="11715">in</TOKEN>
<TOKEN end_char="11722" id="token-135-9" morph="none" pos="word" start_char="11718">Wuhan</TOKEN>
<TOKEN end_char="11724" id="token-135-10" morph="none" pos="punct" start_char="11724">(</TOKEN>
<TOKEN end_char="11726" id="token-135-11" morph="none" pos="word" start_char="11725">he</TOKEN>
<TOKEN end_char="11730" id="token-135-12" morph="none" pos="word" start_char="11728">had</TOKEN>
<TOKEN end_char="11735" id="token-135-13" morph="none" pos="word" start_char="11732">just</TOKEN>
<TOKEN end_char="11740" id="token-135-14" morph="none" pos="word" start_char="11737">come</TOKEN>
<TOKEN end_char="11745" id="token-135-15" morph="none" pos="word" start_char="11742">from</TOKEN>
<TOKEN end_char="11751" id="token-135-16" morph="none" pos="word" start_char="11747">there</TOKEN>
<TOKEN end_char="11753" id="token-135-17" morph="none" pos="punct" start_char="11752">),</TOKEN>
<TOKEN end_char="11767" id="token-135-18" morph="none" pos="unknown" start_char="11755">self-isolated</TOKEN>
<TOKEN end_char="11768" id="token-135-19" morph="none" pos="punct" start_char="11768">,</TOKEN>
<TOKEN end_char="11773" id="token-135-20" morph="none" pos="word" start_char="11770">wore</TOKEN>
<TOKEN end_char="11775" id="token-135-21" morph="none" pos="word" start_char="11775">a</TOKEN>
<TOKEN end_char="11780" id="token-135-22" morph="none" pos="word" start_char="11777">mask</TOKEN>
<TOKEN end_char="11781" id="token-135-23" morph="none" pos="punct" start_char="11781">,</TOKEN>
<TOKEN end_char="11785" id="token-135-24" morph="none" pos="word" start_char="11783">and</TOKEN>
<TOKEN end_char="11789" id="token-135-25" morph="none" pos="word" start_char="11787">was</TOKEN>
<TOKEN end_char="11802" id="token-135-26" morph="none" pos="word" start_char="11791">hospitalized</TOKEN>
<TOKEN end_char="11803" id="token-135-27" morph="none" pos="punct" start_char="11803">,</TOKEN>
<TOKEN end_char="11807" id="token-135-28" morph="none" pos="word" start_char="11805">all</TOKEN>
<TOKEN end_char="11815" id="token-135-29" morph="none" pos="word" start_char="11809">without</TOKEN>
<TOKEN end_char="11825" id="token-135-30" morph="none" pos="word" start_char="11817">spreading</TOKEN>
<TOKEN end_char="11828" id="token-135-31" morph="none" pos="word" start_char="11827">it</TOKEN>
<TOKEN end_char="11831" id="token-135-32" morph="none" pos="word" start_char="11830">to</TOKEN>
<TOKEN end_char="11838" id="token-135-33" morph="none" pos="word" start_char="11833">anyone</TOKEN>
<TOKEN end_char="11843" id="token-135-34" morph="none" pos="word" start_char="11840">else</TOKEN>
<TOKEN end_char="11844" id="token-135-35" morph="none" pos="punct" start_char="11844">.</TOKEN>
</SEG>
<SEG end_char="11961" id="segment-136" start_char="11846">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Germany was a woman who flew from Wuhan to run a training course in Munich.</ORIGINAL_TEXT>
<TOKEN end_char="11848" id="token-136-0" morph="none" pos="word" start_char="11846">The</TOKEN>
<TOKEN end_char="11853" id="token-136-1" morph="none" pos="word" start_char="11850">very</TOKEN>
<TOKEN end_char="11859" id="token-136-2" morph="none" pos="word" start_char="11855">first</TOKEN>
<TOKEN end_char="11868" id="token-136-3" morph="none" pos="unknown" start_char="11861">COVID-19</TOKEN>
<TOKEN end_char="11873" id="token-136-4" morph="none" pos="word" start_char="11870">case</TOKEN>
<TOKEN end_char="11882" id="token-136-5" morph="none" pos="word" start_char="11875">detected</TOKEN>
<TOKEN end_char="11885" id="token-136-6" morph="none" pos="word" start_char="11884">in</TOKEN>
<TOKEN end_char="11893" id="token-136-7" morph="none" pos="word" start_char="11887">Germany</TOKEN>
<TOKEN end_char="11897" id="token-136-8" morph="none" pos="word" start_char="11895">was</TOKEN>
<TOKEN end_char="11899" id="token-136-9" morph="none" pos="word" start_char="11899">a</TOKEN>
<TOKEN end_char="11905" id="token-136-10" morph="none" pos="word" start_char="11901">woman</TOKEN>
<TOKEN end_char="11909" id="token-136-11" morph="none" pos="word" start_char="11907">who</TOKEN>
<TOKEN end_char="11914" id="token-136-12" morph="none" pos="word" start_char="11911">flew</TOKEN>
<TOKEN end_char="11919" id="token-136-13" morph="none" pos="word" start_char="11916">from</TOKEN>
<TOKEN end_char="11925" id="token-136-14" morph="none" pos="word" start_char="11921">Wuhan</TOKEN>
<TOKEN end_char="11928" id="token-136-15" morph="none" pos="word" start_char="11927">to</TOKEN>
<TOKEN end_char="11932" id="token-136-16" morph="none" pos="word" start_char="11930">run</TOKEN>
<TOKEN end_char="11934" id="token-136-17" morph="none" pos="word" start_char="11934">a</TOKEN>
<TOKEN end_char="11943" id="token-136-18" morph="none" pos="word" start_char="11936">training</TOKEN>
<TOKEN end_char="11950" id="token-136-19" morph="none" pos="word" start_char="11945">course</TOKEN>
<TOKEN end_char="11953" id="token-136-20" morph="none" pos="word" start_char="11952">in</TOKEN>
<TOKEN end_char="11960" id="token-136-21" morph="none" pos="word" start_char="11955">Munich</TOKEN>
<TOKEN end_char="11961" id="token-136-22" morph="none" pos="punct" start_char="11961">.</TOKEN>
</SEG>
<SEG end_char="12170" id="segment-137" start_char="11963">
<ORIGINAL_TEXT>- She only developed symptoms on the return flight to China, but several people on the course were infected.Listen (43 minutes): https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</ORIGINAL_TEXT>
<TOKEN end_char="11963" id="token-137-0" morph="none" pos="punct" start_char="11963">-</TOKEN>
<TOKEN end_char="11967" id="token-137-1" morph="none" pos="word" start_char="11965">She</TOKEN>
<TOKEN end_char="11972" id="token-137-2" morph="none" pos="word" start_char="11969">only</TOKEN>
<TOKEN end_char="11982" id="token-137-3" morph="none" pos="word" start_char="11974">developed</TOKEN>
<TOKEN end_char="11991" id="token-137-4" morph="none" pos="word" start_char="11984">symptoms</TOKEN>
<TOKEN end_char="11994" id="token-137-5" morph="none" pos="word" start_char="11993">on</TOKEN>
<TOKEN end_char="11998" id="token-137-6" morph="none" pos="word" start_char="11996">the</TOKEN>
<TOKEN end_char="12005" id="token-137-7" morph="none" pos="word" start_char="12000">return</TOKEN>
<TOKEN end_char="12012" id="token-137-8" morph="none" pos="word" start_char="12007">flight</TOKEN>
<TOKEN end_char="12015" id="token-137-9" morph="none" pos="word" start_char="12014">to</TOKEN>
<TOKEN end_char="12021" id="token-137-10" morph="none" pos="word" start_char="12017">China</TOKEN>
<TOKEN end_char="12022" id="token-137-11" morph="none" pos="punct" start_char="12022">,</TOKEN>
<TOKEN end_char="12026" id="token-137-12" morph="none" pos="word" start_char="12024">but</TOKEN>
<TOKEN end_char="12034" id="token-137-13" morph="none" pos="word" start_char="12028">several</TOKEN>
<TOKEN end_char="12041" id="token-137-14" morph="none" pos="word" start_char="12036">people</TOKEN>
<TOKEN end_char="12044" id="token-137-15" morph="none" pos="word" start_char="12043">on</TOKEN>
<TOKEN end_char="12048" id="token-137-16" morph="none" pos="word" start_char="12046">the</TOKEN>
<TOKEN end_char="12055" id="token-137-17" morph="none" pos="word" start_char="12050">course</TOKEN>
<TOKEN end_char="12060" id="token-137-18" morph="none" pos="word" start_char="12057">were</TOKEN>
<TOKEN end_char="12076" id="token-137-19" morph="none" pos="unknown" start_char="12062">infected.Listen</TOKEN>
<TOKEN end_char="12078" id="token-137-20" morph="none" pos="punct" start_char="12078">(</TOKEN>
<TOKEN end_char="12080" id="token-137-21" morph="none" pos="word" start_char="12079">43</TOKEN>
<TOKEN end_char="12088" id="token-137-22" morph="none" pos="word" start_char="12082">minutes</TOKEN>
<TOKEN end_char="12090" id="token-137-23" morph="none" pos="punct" start_char="12089">):</TOKEN>
<TOKEN end_char="12170" id="token-137-24" morph="none" pos="url" start_char="12092">https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</TOKEN>
</SEG>
<SEG end_char="12442" id="segment-138" start_char="12173">
<ORIGINAL_TEXT>This is all speculation, if the virus is embeded in the excrement then the pooh package could very well offer a protective barrier to soaps and detergents, virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN end_char="12176" id="token-138-0" morph="none" pos="word" start_char="12173">This</TOKEN>
<TOKEN end_char="12179" id="token-138-1" morph="none" pos="word" start_char="12178">is</TOKEN>
<TOKEN end_char="12183" id="token-138-2" morph="none" pos="word" start_char="12181">all</TOKEN>
<TOKEN end_char="12195" id="token-138-3" morph="none" pos="word" start_char="12185">speculation</TOKEN>
<TOKEN end_char="12196" id="token-138-4" morph="none" pos="punct" start_char="12196">,</TOKEN>
<TOKEN end_char="12199" id="token-138-5" morph="none" pos="word" start_char="12198">if</TOKEN>
<TOKEN end_char="12203" id="token-138-6" morph="none" pos="word" start_char="12201">the</TOKEN>
<TOKEN end_char="12209" id="token-138-7" morph="none" pos="word" start_char="12205">virus</TOKEN>
<TOKEN end_char="12212" id="token-138-8" morph="none" pos="word" start_char="12211">is</TOKEN>
<TOKEN end_char="12220" id="token-138-9" morph="none" pos="word" start_char="12214">embeded</TOKEN>
<TOKEN end_char="12223" id="token-138-10" morph="none" pos="word" start_char="12222">in</TOKEN>
<TOKEN end_char="12227" id="token-138-11" morph="none" pos="word" start_char="12225">the</TOKEN>
<TOKEN end_char="12237" id="token-138-12" morph="none" pos="word" start_char="12229">excrement</TOKEN>
<TOKEN end_char="12242" id="token-138-13" morph="none" pos="word" start_char="12239">then</TOKEN>
<TOKEN end_char="12246" id="token-138-14" morph="none" pos="word" start_char="12244">the</TOKEN>
<TOKEN end_char="12251" id="token-138-15" morph="none" pos="word" start_char="12248">pooh</TOKEN>
<TOKEN end_char="12259" id="token-138-16" morph="none" pos="word" start_char="12253">package</TOKEN>
<TOKEN end_char="12265" id="token-138-17" morph="none" pos="word" start_char="12261">could</TOKEN>
<TOKEN end_char="12270" id="token-138-18" morph="none" pos="word" start_char="12267">very</TOKEN>
<TOKEN end_char="12275" id="token-138-19" morph="none" pos="word" start_char="12272">well</TOKEN>
<TOKEN end_char="12281" id="token-138-20" morph="none" pos="word" start_char="12277">offer</TOKEN>
<TOKEN end_char="12283" id="token-138-21" morph="none" pos="word" start_char="12283">a</TOKEN>
<TOKEN end_char="12294" id="token-138-22" morph="none" pos="word" start_char="12285">protective</TOKEN>
<TOKEN end_char="12302" id="token-138-23" morph="none" pos="word" start_char="12296">barrier</TOKEN>
<TOKEN end_char="12305" id="token-138-24" morph="none" pos="word" start_char="12304">to</TOKEN>
<TOKEN end_char="12311" id="token-138-25" morph="none" pos="word" start_char="12307">soaps</TOKEN>
<TOKEN end_char="12315" id="token-138-26" morph="none" pos="word" start_char="12313">and</TOKEN>
<TOKEN end_char="12326" id="token-138-27" morph="none" pos="word" start_char="12317">detergents</TOKEN>
<TOKEN end_char="12327" id="token-138-28" morph="none" pos="punct" start_char="12327">,</TOKEN>
<TOKEN end_char="12333" id="token-138-29" morph="none" pos="word" start_char="12329">virus</TOKEN>
<TOKEN end_char="12343" id="token-138-30" morph="none" pos="word" start_char="12335">particles</TOKEN>
<TOKEN end_char="12346" id="token-138-31" morph="none" pos="word" start_char="12345">on</TOKEN>
<TOKEN end_char="12350" id="token-138-32" morph="none" pos="word" start_char="12348">the</TOKEN>
<TOKEN end_char="12358" id="token-138-33" morph="none" pos="word" start_char="12352">outside</TOKEN>
<TOKEN end_char="12361" id="token-138-34" morph="none" pos="word" start_char="12360">of</TOKEN>
<TOKEN end_char="12363" id="token-138-35" morph="none" pos="word" start_char="12363">a</TOKEN>
<TOKEN end_char="12368" id="token-138-36" morph="none" pos="word" start_char="12365">pooh</TOKEN>
<TOKEN end_char="12372" id="token-138-37" morph="none" pos="word" start_char="12370">may</TOKEN>
<TOKEN end_char="12377" id="token-138-38" morph="none" pos="word" start_char="12374">well</TOKEN>
<TOKEN end_char="12380" id="token-138-39" morph="none" pos="word" start_char="12379">be</TOKEN>
<TOKEN end_char="12390" id="token-138-40" morph="none" pos="word" start_char="12382">destroyed</TOKEN>
<TOKEN end_char="12394" id="token-138-41" morph="none" pos="word" start_char="12392">but</TOKEN>
<TOKEN end_char="12400" id="token-138-42" morph="none" pos="word" start_char="12396">those</TOKEN>
<TOKEN end_char="12409" id="token-138-43" morph="none" pos="word" start_char="12402">embedded</TOKEN>
<TOKEN end_char="12416" id="token-138-44" morph="none" pos="word" start_char="12411">inside</TOKEN>
<TOKEN end_char="12422" id="token-138-45" morph="none" pos="word" start_char="12418">could</TOKEN>
<TOKEN end_char="12427" id="token-138-46" morph="none" pos="word" start_char="12424">have</TOKEN>
<TOKEN end_char="12432" id="token-138-47" morph="none" pos="word" start_char="12429">been</TOKEN>
<TOKEN end_char="12442" id="token-138-48" morph="none" pos="word" start_char="12434">protected</TOKEN>
</SEG>
<SEG end_char="12486" id="segment-139" start_char="12446">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN end_char="12450" id="token-139-0" morph="none" pos="word" start_char="12446">Quote</TOKEN>
<TOKEN end_char="12455" id="token-139-1" morph="none" pos="word" start_char="12452">from</TOKEN>
<TOKEN end_char="12456" id="token-139-2" morph="none" pos="punct" start_char="12456">:</TOKEN>
<TOKEN end_char="12463" id="token-139-3" morph="none" pos="word" start_char="12458">Jolly2</TOKEN>
<TOKEN end_char="12466" id="token-139-4" morph="none" pos="word" start_char="12465">on</TOKEN>
<TOKEN end_char="12477" id="token-139-5" morph="none" pos="unknown" start_char="12468">07/02/2021</TOKEN>
<TOKEN end_char="12486" id="token-139-6" morph="none" pos="unknown" start_char="12479">16:56:51</TOKEN>
</SEG>
<SEG end_char="12512" id="segment-140" start_char="12489">
<ORIGINAL_TEXT>This is all speculation,</ORIGINAL_TEXT>
<TOKEN end_char="12492" id="token-140-0" morph="none" pos="word" start_char="12489">This</TOKEN>
<TOKEN end_char="12495" id="token-140-1" morph="none" pos="word" start_char="12494">is</TOKEN>
<TOKEN end_char="12499" id="token-140-2" morph="none" pos="word" start_char="12497">all</TOKEN>
<TOKEN end_char="12511" id="token-140-3" morph="none" pos="word" start_char="12501">speculation</TOKEN>
<TOKEN end_char="12512" id="token-140-4" morph="none" pos="punct" start_char="12512">,</TOKEN>
</SEG>
<SEG end_char="12525" id="segment-141" start_char="12516">
<ORIGINAL_TEXT>A bit like</ORIGINAL_TEXT>
<TOKEN end_char="12516" id="token-141-0" morph="none" pos="word" start_char="12516">A</TOKEN>
<TOKEN end_char="12520" id="token-141-1" morph="none" pos="word" start_char="12518">bit</TOKEN>
<TOKEN end_char="12525" id="token-141-2" morph="none" pos="word" start_char="12522">like</TOKEN>
</SEG>
<SEG end_char="12575" id="segment-142" start_char="12528">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN end_char="12532" id="token-142-0" morph="none" pos="word" start_char="12528">Quote</TOKEN>
<TOKEN end_char="12537" id="token-142-1" morph="none" pos="word" start_char="12534">from</TOKEN>
<TOKEN end_char="12538" id="token-142-2" morph="none" pos="punct" start_char="12538">:</TOKEN>
<TOKEN end_char="12544" id="token-142-3" morph="none" pos="word" start_char="12540">Bored</TOKEN>
<TOKEN end_char="12552" id="token-142-4" morph="none" pos="word" start_char="12546">chemist</TOKEN>
<TOKEN end_char="12555" id="token-142-5" morph="none" pos="word" start_char="12554">on</TOKEN>
<TOKEN end_char="12566" id="token-142-6" morph="none" pos="unknown" start_char="12557">06/02/2021</TOKEN>
<TOKEN end_char="12575" id="token-142-7" morph="none" pos="unknown" start_char="12568">20:13:01</TOKEN>
</SEG>
<SEG end_char="12924" id="segment-143" start_char="12578">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36covid was in Brazil in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36France in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36And spain as early as March 2019Quote from: Jolly2 on 05/02/2021 18:15:55We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="12582" id="token-143-0" morph="none" pos="word" start_char="12578">Quote</TOKEN>
<TOKEN end_char="12587" id="token-143-1" morph="none" pos="word" start_char="12584">from</TOKEN>
<TOKEN end_char="12588" id="token-143-2" morph="none" pos="punct" start_char="12588">:</TOKEN>
<TOKEN end_char="12595" id="token-143-3" morph="none" pos="word" start_char="12590">Jolly2</TOKEN>
<TOKEN end_char="12598" id="token-143-4" morph="none" pos="word" start_char="12597">on</TOKEN>
<TOKEN end_char="12609" id="token-143-5" morph="none" pos="unknown" start_char="12600">05/02/2021</TOKEN>
<TOKEN end_char="12623" id="token-143-6" morph="none" pos="unknown" start_char="12611">18:12:36covid</TOKEN>
<TOKEN end_char="12627" id="token-143-7" morph="none" pos="word" start_char="12625">was</TOKEN>
<TOKEN end_char="12630" id="token-143-8" morph="none" pos="word" start_char="12629">in</TOKEN>
<TOKEN end_char="12637" id="token-143-9" morph="none" pos="word" start_char="12632">Brazil</TOKEN>
<TOKEN end_char="12640" id="token-143-10" morph="none" pos="word" start_char="12639">in</TOKEN>
<TOKEN end_char="12649" id="token-143-11" morph="none" pos="word" start_char="12642">November</TOKEN>
<TOKEN end_char="12659" id="token-143-12" morph="none" pos="word" start_char="12651">2019Quote</TOKEN>
<TOKEN end_char="12664" id="token-143-13" morph="none" pos="word" start_char="12661">from</TOKEN>
<TOKEN end_char="12665" id="token-143-14" morph="none" pos="punct" start_char="12665">:</TOKEN>
<TOKEN end_char="12672" id="token-143-15" morph="none" pos="word" start_char="12667">Jolly2</TOKEN>
<TOKEN end_char="12675" id="token-143-16" morph="none" pos="word" start_char="12674">on</TOKEN>
<TOKEN end_char="12686" id="token-143-17" morph="none" pos="unknown" start_char="12677">05/02/2021</TOKEN>
<TOKEN end_char="12701" id="token-143-18" morph="none" pos="unknown" start_char="12688">18:12:36France</TOKEN>
<TOKEN end_char="12704" id="token-143-19" morph="none" pos="word" start_char="12703">in</TOKEN>
<TOKEN end_char="12713" id="token-143-20" morph="none" pos="word" start_char="12706">November</TOKEN>
<TOKEN end_char="12723" id="token-143-21" morph="none" pos="word" start_char="12715">2019Quote</TOKEN>
<TOKEN end_char="12728" id="token-143-22" morph="none" pos="word" start_char="12725">from</TOKEN>
<TOKEN end_char="12729" id="token-143-23" morph="none" pos="punct" start_char="12729">:</TOKEN>
<TOKEN end_char="12736" id="token-143-24" morph="none" pos="word" start_char="12731">Jolly2</TOKEN>
<TOKEN end_char="12739" id="token-143-25" morph="none" pos="word" start_char="12738">on</TOKEN>
<TOKEN end_char="12750" id="token-143-26" morph="none" pos="unknown" start_char="12741">05/02/2021</TOKEN>
<TOKEN end_char="12762" id="token-143-27" morph="none" pos="unknown" start_char="12752">18:12:36And</TOKEN>
<TOKEN end_char="12768" id="token-143-28" morph="none" pos="word" start_char="12764">spain</TOKEN>
<TOKEN end_char="12771" id="token-143-29" morph="none" pos="word" start_char="12770">as</TOKEN>
<TOKEN end_char="12777" id="token-143-30" morph="none" pos="word" start_char="12773">early</TOKEN>
<TOKEN end_char="12780" id="token-143-31" morph="none" pos="word" start_char="12779">as</TOKEN>
<TOKEN end_char="12786" id="token-143-32" morph="none" pos="word" start_char="12782">March</TOKEN>
<TOKEN end_char="12796" id="token-143-33" morph="none" pos="word" start_char="12788">2019Quote</TOKEN>
<TOKEN end_char="12801" id="token-143-34" morph="none" pos="word" start_char="12798">from</TOKEN>
<TOKEN end_char="12802" id="token-143-35" morph="none" pos="punct" start_char="12802">:</TOKEN>
<TOKEN end_char="12809" id="token-143-36" morph="none" pos="word" start_char="12804">Jolly2</TOKEN>
<TOKEN end_char="12812" id="token-143-37" morph="none" pos="word" start_char="12811">on</TOKEN>
<TOKEN end_char="12823" id="token-143-38" morph="none" pos="unknown" start_char="12814">05/02/2021</TOKEN>
<TOKEN end_char="12834" id="token-143-39" morph="none" pos="unknown" start_char="12825">18:15:55We</TOKEN>
<TOKEN end_char="12839" id="token-143-40" morph="none" pos="word" start_char="12836">know</TOKEN>
<TOKEN end_char="12846" id="token-143-41" morph="none" pos="word" start_char="12841">Donald</TOKEN>
<TOKEN end_char="12852" id="token-143-42" morph="none" pos="word" start_char="12848">Trump</TOKEN>
<TOKEN end_char="12860" id="token-143-43" morph="none" pos="word" start_char="12854">claimed</TOKEN>
<TOKEN end_char="12863" id="token-143-44" morph="none" pos="word" start_char="12862">to</TOKEN>
<TOKEN end_char="12868" id="token-143-45" morph="none" pos="word" start_char="12865">have</TOKEN>
<TOKEN end_char="12873" id="token-143-46" morph="none" pos="word" start_char="12870">seen</TOKEN>
<TOKEN end_char="12882" id="token-143-47" morph="none" pos="word" start_char="12875">evidence</TOKEN>
<TOKEN end_char="12887" id="token-143-48" morph="none" pos="word" start_char="12884">that</TOKEN>
<TOKEN end_char="12893" id="token-143-49" morph="none" pos="word" start_char="12889">covid</TOKEN>
<TOKEN end_char="12896" id="token-143-50" morph="none" pos="word" start_char="12895">19</TOKEN>
<TOKEN end_char="12900" id="token-143-51" morph="none" pos="word" start_char="12898">had</TOKEN>
<TOKEN end_char="12905" id="token-143-52" morph="none" pos="word" start_char="12902">come</TOKEN>
<TOKEN end_char="12910" id="token-143-53" morph="none" pos="word" start_char="12907">from</TOKEN>
<TOKEN end_char="12912" id="token-143-54" morph="none" pos="word" start_char="12912">a</TOKEN>
<TOKEN end_char="12923" id="token-143-55" morph="none" pos="word" start_char="12914">laboratory</TOKEN>
<TOKEN end_char="12924" id="token-143-56" morph="none" pos="punct" start_char="12924">.</TOKEN>
</SEG>
<SEG end_char="12984" id="segment-144" start_char="12926">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN end_char="12930" id="token-144-0" morph="none" pos="word" start_char="12926">Could</TOKEN>
<TOKEN end_char="12933" id="token-144-1" morph="none" pos="word" start_char="12932">it</TOKEN>
<TOKEN end_char="12936" id="token-144-2" morph="none" pos="word" start_char="12935">be</TOKEN>
<TOKEN end_char="12941" id="token-144-3" morph="none" pos="word" start_char="12938">that</TOKEN>
<TOKEN end_char="12945" id="token-144-4" morph="none" pos="word" start_char="12943">the</TOKEN>
<TOKEN end_char="12954" id="token-144-5" morph="none" pos="word" start_char="12947">evidence</TOKEN>
<TOKEN end_char="12957" id="token-144-6" morph="none" pos="word" start_char="12956">he</TOKEN>
<TOKEN end_char="12961" id="token-144-7" morph="none" pos="word" start_char="12959">saw</TOKEN>
<TOKEN end_char="12965" id="token-144-8" morph="none" pos="word" start_char="12963">was</TOKEN>
<TOKEN end_char="12970" id="token-144-9" morph="none" pos="word" start_char="12967">from</TOKEN>
<TOKEN end_char="12975" id="token-144-10" morph="none" pos="word" start_char="12972">fort</TOKEN>
<TOKEN end_char="12983" id="token-144-11" morph="none" pos="word" start_char="12977">Detrick</TOKEN>
<TOKEN end_char="12984" id="token-144-12" morph="none" pos="punct" start_char="12984">?</TOKEN>
</SEG>
<SEG end_char="13027" id="segment-145" start_char="12987">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN end_char="12991" id="token-145-0" morph="none" pos="word" start_char="12987">Quote</TOKEN>
<TOKEN end_char="12996" id="token-145-1" morph="none" pos="word" start_char="12993">from</TOKEN>
<TOKEN end_char="12997" id="token-145-2" morph="none" pos="punct" start_char="12997">:</TOKEN>
<TOKEN end_char="13004" id="token-145-3" morph="none" pos="word" start_char="12999">Jolly2</TOKEN>
<TOKEN end_char="13007" id="token-145-4" morph="none" pos="word" start_char="13006">on</TOKEN>
<TOKEN end_char="13018" id="token-145-5" morph="none" pos="unknown" start_char="13009">07/02/2021</TOKEN>
<TOKEN end_char="13027" id="token-145-6" morph="none" pos="unknown" start_char="13020">16:56:51</TOKEN>
</SEG>
<SEG end_char="13143" id="segment-146" start_char="13030">
<ORIGINAL_TEXT>virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN end_char="13034" id="token-146-0" morph="none" pos="word" start_char="13030">virus</TOKEN>
<TOKEN end_char="13044" id="token-146-1" morph="none" pos="word" start_char="13036">particles</TOKEN>
<TOKEN end_char="13047" id="token-146-2" morph="none" pos="word" start_char="13046">on</TOKEN>
<TOKEN end_char="13051" id="token-146-3" morph="none" pos="word" start_char="13049">the</TOKEN>
<TOKEN end_char="13059" id="token-146-4" morph="none" pos="word" start_char="13053">outside</TOKEN>
<TOKEN end_char="13062" id="token-146-5" morph="none" pos="word" start_char="13061">of</TOKEN>
<TOKEN end_char="13064" id="token-146-6" morph="none" pos="word" start_char="13064">a</TOKEN>
<TOKEN end_char="13069" id="token-146-7" morph="none" pos="word" start_char="13066">pooh</TOKEN>
<TOKEN end_char="13073" id="token-146-8" morph="none" pos="word" start_char="13071">may</TOKEN>
<TOKEN end_char="13078" id="token-146-9" morph="none" pos="word" start_char="13075">well</TOKEN>
<TOKEN end_char="13081" id="token-146-10" morph="none" pos="word" start_char="13080">be</TOKEN>
<TOKEN end_char="13091" id="token-146-11" morph="none" pos="word" start_char="13083">destroyed</TOKEN>
<TOKEN end_char="13095" id="token-146-12" morph="none" pos="word" start_char="13093">but</TOKEN>
<TOKEN end_char="13101" id="token-146-13" morph="none" pos="word" start_char="13097">those</TOKEN>
<TOKEN end_char="13110" id="token-146-14" morph="none" pos="word" start_char="13103">embedded</TOKEN>
<TOKEN end_char="13117" id="token-146-15" morph="none" pos="word" start_char="13112">inside</TOKEN>
<TOKEN end_char="13123" id="token-146-16" morph="none" pos="word" start_char="13119">could</TOKEN>
<TOKEN end_char="13128" id="token-146-17" morph="none" pos="word" start_char="13125">have</TOKEN>
<TOKEN end_char="13133" id="token-146-18" morph="none" pos="word" start_char="13130">been</TOKEN>
<TOKEN end_char="13143" id="token-146-19" morph="none" pos="word" start_char="13135">protected</TOKEN>
</SEG>
<SEG end_char="13229" id="segment-147" start_char="13147">
<ORIGINAL_TEXT>Among the things they would have been protected from is getting into a sample vial.</ORIGINAL_TEXT>
<TOKEN end_char="13151" id="token-147-0" morph="none" pos="word" start_char="13147">Among</TOKEN>
<TOKEN end_char="13155" id="token-147-1" morph="none" pos="word" start_char="13153">the</TOKEN>
<TOKEN end_char="13162" id="token-147-2" morph="none" pos="word" start_char="13157">things</TOKEN>
<TOKEN end_char="13167" id="token-147-3" morph="none" pos="word" start_char="13164">they</TOKEN>
<TOKEN end_char="13173" id="token-147-4" morph="none" pos="word" start_char="13169">would</TOKEN>
<TOKEN end_char="13178" id="token-147-5" morph="none" pos="word" start_char="13175">have</TOKEN>
<TOKEN end_char="13183" id="token-147-6" morph="none" pos="word" start_char="13180">been</TOKEN>
<TOKEN end_char="13193" id="token-147-7" morph="none" pos="word" start_char="13185">protected</TOKEN>
<TOKEN end_char="13198" id="token-147-8" morph="none" pos="word" start_char="13195">from</TOKEN>
<TOKEN end_char="13201" id="token-147-9" morph="none" pos="word" start_char="13200">is</TOKEN>
<TOKEN end_char="13209" id="token-147-10" morph="none" pos="word" start_char="13203">getting</TOKEN>
<TOKEN end_char="13214" id="token-147-11" morph="none" pos="word" start_char="13211">into</TOKEN>
<TOKEN end_char="13216" id="token-147-12" morph="none" pos="word" start_char="13216">a</TOKEN>
<TOKEN end_char="13223" id="token-147-13" morph="none" pos="word" start_char="13218">sample</TOKEN>
<TOKEN end_char="13228" id="token-147-14" morph="none" pos="word" start_char="13225">vial</TOKEN>
<TOKEN end_char="13229" id="token-147-15" morph="none" pos="punct" start_char="13229">.</TOKEN>
</SEG>
<SEG end_char="13287" id="segment-148" start_char="13231">
<ORIGINAL_TEXT>So they would never have made it to a lab to be detected.</ORIGINAL_TEXT>
<TOKEN end_char="13232" id="token-148-0" morph="none" pos="word" start_char="13231">So</TOKEN>
<TOKEN end_char="13237" id="token-148-1" morph="none" pos="word" start_char="13234">they</TOKEN>
<TOKEN end_char="13243" id="token-148-2" morph="none" pos="word" start_char="13239">would</TOKEN>
<TOKEN end_char="13249" id="token-148-3" morph="none" pos="word" start_char="13245">never</TOKEN>
<TOKEN end_char="13254" id="token-148-4" morph="none" pos="word" start_char="13251">have</TOKEN>
<TOKEN end_char="13259" id="token-148-5" morph="none" pos="word" start_char="13256">made</TOKEN>
<TOKEN end_char="13262" id="token-148-6" morph="none" pos="word" start_char="13261">it</TOKEN>
<TOKEN end_char="13265" id="token-148-7" morph="none" pos="word" start_char="13264">to</TOKEN>
<TOKEN end_char="13267" id="token-148-8" morph="none" pos="word" start_char="13267">a</TOKEN>
<TOKEN end_char="13271" id="token-148-9" morph="none" pos="word" start_char="13269">lab</TOKEN>
<TOKEN end_char="13274" id="token-148-10" morph="none" pos="word" start_char="13273">to</TOKEN>
<TOKEN end_char="13277" id="token-148-11" morph="none" pos="word" start_char="13276">be</TOKEN>
<TOKEN end_char="13286" id="token-148-12" morph="none" pos="word" start_char="13279">detected</TOKEN>
<TOKEN end_char="13287" id="token-148-13" morph="none" pos="punct" start_char="13287">.</TOKEN>
</SEG>
<SEG end_char="13291" id="segment-149" start_char="13291">
<ORIGINAL_TEXT>Ο</ORIGINAL_TEXT>
<TOKEN end_char="13291" id="token-149-0" morph="none" pos="word" start_char="13291">Ο</TOKEN>
</SEG>
<SEG end_char="13341" id="segment-150" start_char="13294">
<ORIGINAL_TEXT>Quote from: Bored chemist on 07/02/2021 17:25:40</ORIGINAL_TEXT>
<TOKEN end_char="13298" id="token-150-0" morph="none" pos="word" start_char="13294">Quote</TOKEN>
<TOKEN end_char="13303" id="token-150-1" morph="none" pos="word" start_char="13300">from</TOKEN>
<TOKEN end_char="13304" id="token-150-2" morph="none" pos="punct" start_char="13304">:</TOKEN>
<TOKEN end_char="13310" id="token-150-3" morph="none" pos="word" start_char="13306">Bored</TOKEN>
<TOKEN end_char="13318" id="token-150-4" morph="none" pos="word" start_char="13312">chemist</TOKEN>
<TOKEN end_char="13321" id="token-150-5" morph="none" pos="word" start_char="13320">on</TOKEN>
<TOKEN end_char="13332" id="token-150-6" morph="none" pos="unknown" start_char="13323">07/02/2021</TOKEN>
<TOKEN end_char="13341" id="token-150-7" morph="none" pos="unknown" start_char="13334">17:25:40</TOKEN>
</SEG>
<SEG end_char="13384" id="segment-151" start_char="13344">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN end_char="13348" id="token-151-0" morph="none" pos="word" start_char="13344">Quote</TOKEN>
<TOKEN end_char="13353" id="token-151-1" morph="none" pos="word" start_char="13350">from</TOKEN>
<TOKEN end_char="13354" id="token-151-2" morph="none" pos="punct" start_char="13354">:</TOKEN>
<TOKEN end_char="13361" id="token-151-3" morph="none" pos="word" start_char="13356">Jolly2</TOKEN>
<TOKEN end_char="13364" id="token-151-4" morph="none" pos="word" start_char="13363">on</TOKEN>
<TOKEN end_char="13375" id="token-151-5" morph="none" pos="unknown" start_char="13366">07/02/2021</TOKEN>
<TOKEN end_char="13384" id="token-151-6" morph="none" pos="unknown" start_char="13377">16:56:51</TOKEN>
</SEG>
<SEG end_char="13410" id="segment-152" start_char="13387">
<ORIGINAL_TEXT>This is all speculation,</ORIGINAL_TEXT>
<TOKEN end_char="13390" id="token-152-0" morph="none" pos="word" start_char="13387">This</TOKEN>
<TOKEN end_char="13393" id="token-152-1" morph="none" pos="word" start_char="13392">is</TOKEN>
<TOKEN end_char="13397" id="token-152-2" morph="none" pos="word" start_char="13395">all</TOKEN>
<TOKEN end_char="13409" id="token-152-3" morph="none" pos="word" start_char="13399">speculation</TOKEN>
<TOKEN end_char="13410" id="token-152-4" morph="none" pos="punct" start_char="13410">,</TOKEN>
</SEG>
<SEG end_char="13470" id="segment-153" start_char="13413">
<ORIGINAL_TEXT>A bit likeQuote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN end_char="13413" id="token-153-0" morph="none" pos="word" start_char="13413">A</TOKEN>
<TOKEN end_char="13417" id="token-153-1" morph="none" pos="word" start_char="13415">bit</TOKEN>
<TOKEN end_char="13427" id="token-153-2" morph="none" pos="word" start_char="13419">likeQuote</TOKEN>
<TOKEN end_char="13432" id="token-153-3" morph="none" pos="word" start_char="13429">from</TOKEN>
<TOKEN end_char="13433" id="token-153-4" morph="none" pos="punct" start_char="13433">:</TOKEN>
<TOKEN end_char="13439" id="token-153-5" morph="none" pos="word" start_char="13435">Bored</TOKEN>
<TOKEN end_char="13447" id="token-153-6" morph="none" pos="word" start_char="13441">chemist</TOKEN>
<TOKEN end_char="13450" id="token-153-7" morph="none" pos="word" start_char="13449">on</TOKEN>
<TOKEN end_char="13461" id="token-153-8" morph="none" pos="unknown" start_char="13452">06/02/2021</TOKEN>
<TOKEN end_char="13470" id="token-153-9" morph="none" pos="unknown" start_char="13463">20:13:01</TOKEN>
</SEG>
<SEG end_char="13819" id="segment-154" start_char="13473">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36covid was in Brazil in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36France in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36And spain as early as March 2019Quote from: Jolly2 on 05/02/2021 18:15:55We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="13477" id="token-154-0" morph="none" pos="word" start_char="13473">Quote</TOKEN>
<TOKEN end_char="13482" id="token-154-1" morph="none" pos="word" start_char="13479">from</TOKEN>
<TOKEN end_char="13483" id="token-154-2" morph="none" pos="punct" start_char="13483">:</TOKEN>
<TOKEN end_char="13490" id="token-154-3" morph="none" pos="word" start_char="13485">Jolly2</TOKEN>
<TOKEN end_char="13493" id="token-154-4" morph="none" pos="word" start_char="13492">on</TOKEN>
<TOKEN end_char="13504" id="token-154-5" morph="none" pos="unknown" start_char="13495">05/02/2021</TOKEN>
<TOKEN end_char="13518" id="token-154-6" morph="none" pos="unknown" start_char="13506">18:12:36covid</TOKEN>
<TOKEN end_char="13522" id="token-154-7" morph="none" pos="word" start_char="13520">was</TOKEN>
<TOKEN end_char="13525" id="token-154-8" morph="none" pos="word" start_char="13524">in</TOKEN>
<TOKEN end_char="13532" id="token-154-9" morph="none" pos="word" start_char="13527">Brazil</TOKEN>
<TOKEN end_char="13535" id="token-154-10" morph="none" pos="word" start_char="13534">in</TOKEN>
<TOKEN end_char="13544" id="token-154-11" morph="none" pos="word" start_char="13537">November</TOKEN>
<TOKEN end_char="13554" id="token-154-12" morph="none" pos="word" start_char="13546">2019Quote</TOKEN>
<TOKEN end_char="13559" id="token-154-13" morph="none" pos="word" start_char="13556">from</TOKEN>
<TOKEN end_char="13560" id="token-154-14" morph="none" pos="punct" start_char="13560">:</TOKEN>
<TOKEN end_char="13567" id="token-154-15" morph="none" pos="word" start_char="13562">Jolly2</TOKEN>
<TOKEN end_char="13570" id="token-154-16" morph="none" pos="word" start_char="13569">on</TOKEN>
<TOKEN end_char="13581" id="token-154-17" morph="none" pos="unknown" start_char="13572">05/02/2021</TOKEN>
<TOKEN end_char="13596" id="token-154-18" morph="none" pos="unknown" start_char="13583">18:12:36France</TOKEN>
<TOKEN end_char="13599" id="token-154-19" morph="none" pos="word" start_char="13598">in</TOKEN>
<TOKEN end_char="13608" id="token-154-20" morph="none" pos="word" start_char="13601">November</TOKEN>
<TOKEN end_char="13618" id="token-154-21" morph="none" pos="word" start_char="13610">2019Quote</TOKEN>
<TOKEN end_char="13623" id="token-154-22" morph="none" pos="word" start_char="13620">from</TOKEN>
<TOKEN end_char="13624" id="token-154-23" morph="none" pos="punct" start_char="13624">:</TOKEN>
<TOKEN end_char="13631" id="token-154-24" morph="none" pos="word" start_char="13626">Jolly2</TOKEN>
<TOKEN end_char="13634" id="token-154-25" morph="none" pos="word" start_char="13633">on</TOKEN>
<TOKEN end_char="13645" id="token-154-26" morph="none" pos="unknown" start_char="13636">05/02/2021</TOKEN>
<TOKEN end_char="13657" id="token-154-27" morph="none" pos="unknown" start_char="13647">18:12:36And</TOKEN>
<TOKEN end_char="13663" id="token-154-28" morph="none" pos="word" start_char="13659">spain</TOKEN>
<TOKEN end_char="13666" id="token-154-29" morph="none" pos="word" start_char="13665">as</TOKEN>
<TOKEN end_char="13672" id="token-154-30" morph="none" pos="word" start_char="13668">early</TOKEN>
<TOKEN end_char="13675" id="token-154-31" morph="none" pos="word" start_char="13674">as</TOKEN>
<TOKEN end_char="13681" id="token-154-32" morph="none" pos="word" start_char="13677">March</TOKEN>
<TOKEN end_char="13691" id="token-154-33" morph="none" pos="word" start_char="13683">2019Quote</TOKEN>
<TOKEN end_char="13696" id="token-154-34" morph="none" pos="word" start_char="13693">from</TOKEN>
<TOKEN end_char="13697" id="token-154-35" morph="none" pos="punct" start_char="13697">:</TOKEN>
<TOKEN end_char="13704" id="token-154-36" morph="none" pos="word" start_char="13699">Jolly2</TOKEN>
<TOKEN end_char="13707" id="token-154-37" morph="none" pos="word" start_char="13706">on</TOKEN>
<TOKEN end_char="13718" id="token-154-38" morph="none" pos="unknown" start_char="13709">05/02/2021</TOKEN>
<TOKEN end_char="13729" id="token-154-39" morph="none" pos="unknown" start_char="13720">18:15:55We</TOKEN>
<TOKEN end_char="13734" id="token-154-40" morph="none" pos="word" start_char="13731">know</TOKEN>
<TOKEN end_char="13741" id="token-154-41" morph="none" pos="word" start_char="13736">Donald</TOKEN>
<TOKEN end_char="13747" id="token-154-42" morph="none" pos="word" start_char="13743">Trump</TOKEN>
<TOKEN end_char="13755" id="token-154-43" morph="none" pos="word" start_char="13749">claimed</TOKEN>
<TOKEN end_char="13758" id="token-154-44" morph="none" pos="word" start_char="13757">to</TOKEN>
<TOKEN end_char="13763" id="token-154-45" morph="none" pos="word" start_char="13760">have</TOKEN>
<TOKEN end_char="13768" id="token-154-46" morph="none" pos="word" start_char="13765">seen</TOKEN>
<TOKEN end_char="13777" id="token-154-47" morph="none" pos="word" start_char="13770">evidence</TOKEN>
<TOKEN end_char="13782" id="token-154-48" morph="none" pos="word" start_char="13779">that</TOKEN>
<TOKEN end_char="13788" id="token-154-49" morph="none" pos="word" start_char="13784">covid</TOKEN>
<TOKEN end_char="13791" id="token-154-50" morph="none" pos="word" start_char="13790">19</TOKEN>
<TOKEN end_char="13795" id="token-154-51" morph="none" pos="word" start_char="13793">had</TOKEN>
<TOKEN end_char="13800" id="token-154-52" morph="none" pos="word" start_char="13797">come</TOKEN>
<TOKEN end_char="13805" id="token-154-53" morph="none" pos="word" start_char="13802">from</TOKEN>
<TOKEN end_char="13807" id="token-154-54" morph="none" pos="word" start_char="13807">a</TOKEN>
<TOKEN end_char="13818" id="token-154-55" morph="none" pos="word" start_char="13809">laboratory</TOKEN>
<TOKEN end_char="13819" id="token-154-56" morph="none" pos="punct" start_char="13819">.</TOKEN>
</SEG>
<SEG end_char="13920" id="segment-155" start_char="13821">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN end_char="13825" id="token-155-0" morph="none" pos="word" start_char="13821">Could</TOKEN>
<TOKEN end_char="13828" id="token-155-1" morph="none" pos="word" start_char="13827">it</TOKEN>
<TOKEN end_char="13831" id="token-155-2" morph="none" pos="word" start_char="13830">be</TOKEN>
<TOKEN end_char="13836" id="token-155-3" morph="none" pos="word" start_char="13833">that</TOKEN>
<TOKEN end_char="13840" id="token-155-4" morph="none" pos="word" start_char="13838">the</TOKEN>
<TOKEN end_char="13849" id="token-155-5" morph="none" pos="word" start_char="13842">evidence</TOKEN>
<TOKEN end_char="13852" id="token-155-6" morph="none" pos="word" start_char="13851">he</TOKEN>
<TOKEN end_char="13856" id="token-155-7" morph="none" pos="word" start_char="13854">saw</TOKEN>
<TOKEN end_char="13860" id="token-155-8" morph="none" pos="word" start_char="13858">was</TOKEN>
<TOKEN end_char="13865" id="token-155-9" morph="none" pos="word" start_char="13862">from</TOKEN>
<TOKEN end_char="13870" id="token-155-10" morph="none" pos="word" start_char="13867">fort</TOKEN>
<TOKEN end_char="13884" id="token-155-11" morph="none" pos="unknown" start_char="13872">Detrick?Quote</TOKEN>
<TOKEN end_char="13889" id="token-155-12" morph="none" pos="word" start_char="13886">from</TOKEN>
<TOKEN end_char="13890" id="token-155-13" morph="none" pos="punct" start_char="13890">:</TOKEN>
<TOKEN end_char="13897" id="token-155-14" morph="none" pos="word" start_char="13892">Jolly2</TOKEN>
<TOKEN end_char="13900" id="token-155-15" morph="none" pos="word" start_char="13899">on</TOKEN>
<TOKEN end_char="13911" id="token-155-16" morph="none" pos="unknown" start_char="13902">07/02/2021</TOKEN>
<TOKEN end_char="13920" id="token-155-17" morph="none" pos="unknown" start_char="13913">16:56:51</TOKEN>
</SEG>
<SEG end_char="14036" id="segment-156" start_char="13923">
<ORIGINAL_TEXT>virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN end_char="13927" id="token-156-0" morph="none" pos="word" start_char="13923">virus</TOKEN>
<TOKEN end_char="13937" id="token-156-1" morph="none" pos="word" start_char="13929">particles</TOKEN>
<TOKEN end_char="13940" id="token-156-2" morph="none" pos="word" start_char="13939">on</TOKEN>
<TOKEN end_char="13944" id="token-156-3" morph="none" pos="word" start_char="13942">the</TOKEN>
<TOKEN end_char="13952" id="token-156-4" morph="none" pos="word" start_char="13946">outside</TOKEN>
<TOKEN end_char="13955" id="token-156-5" morph="none" pos="word" start_char="13954">of</TOKEN>
<TOKEN end_char="13957" id="token-156-6" morph="none" pos="word" start_char="13957">a</TOKEN>
<TOKEN end_char="13962" id="token-156-7" morph="none" pos="word" start_char="13959">pooh</TOKEN>
<TOKEN end_char="13966" id="token-156-8" morph="none" pos="word" start_char="13964">may</TOKEN>
<TOKEN end_char="13971" id="token-156-9" morph="none" pos="word" start_char="13968">well</TOKEN>
<TOKEN end_char="13974" id="token-156-10" morph="none" pos="word" start_char="13973">be</TOKEN>
<TOKEN end_char="13984" id="token-156-11" morph="none" pos="word" start_char="13976">destroyed</TOKEN>
<TOKEN end_char="13988" id="token-156-12" morph="none" pos="word" start_char="13986">but</TOKEN>
<TOKEN end_char="13994" id="token-156-13" morph="none" pos="word" start_char="13990">those</TOKEN>
<TOKEN end_char="14003" id="token-156-14" morph="none" pos="word" start_char="13996">embedded</TOKEN>
<TOKEN end_char="14010" id="token-156-15" morph="none" pos="word" start_char="14005">inside</TOKEN>
<TOKEN end_char="14016" id="token-156-16" morph="none" pos="word" start_char="14012">could</TOKEN>
<TOKEN end_char="14021" id="token-156-17" morph="none" pos="word" start_char="14018">have</TOKEN>
<TOKEN end_char="14026" id="token-156-18" morph="none" pos="word" start_char="14023">been</TOKEN>
<TOKEN end_char="14036" id="token-156-19" morph="none" pos="word" start_char="14028">protected</TOKEN>
</SEG>
<SEG end_char="14178" id="segment-157" start_char="14039">
<ORIGINAL_TEXT>Among the things they would have been protected from is getting into a sample vial.So they would never have made it to a lab to be detected.</ORIGINAL_TEXT>
<TOKEN end_char="14043" id="token-157-0" morph="none" pos="word" start_char="14039">Among</TOKEN>
<TOKEN end_char="14047" id="token-157-1" morph="none" pos="word" start_char="14045">the</TOKEN>
<TOKEN end_char="14054" id="token-157-2" morph="none" pos="word" start_char="14049">things</TOKEN>
<TOKEN end_char="14059" id="token-157-3" morph="none" pos="word" start_char="14056">they</TOKEN>
<TOKEN end_char="14065" id="token-157-4" morph="none" pos="word" start_char="14061">would</TOKEN>
<TOKEN end_char="14070" id="token-157-5" morph="none" pos="word" start_char="14067">have</TOKEN>
<TOKEN end_char="14075" id="token-157-6" morph="none" pos="word" start_char="14072">been</TOKEN>
<TOKEN end_char="14085" id="token-157-7" morph="none" pos="word" start_char="14077">protected</TOKEN>
<TOKEN end_char="14090" id="token-157-8" morph="none" pos="word" start_char="14087">from</TOKEN>
<TOKEN end_char="14093" id="token-157-9" morph="none" pos="word" start_char="14092">is</TOKEN>
<TOKEN end_char="14101" id="token-157-10" morph="none" pos="word" start_char="14095">getting</TOKEN>
<TOKEN end_char="14106" id="token-157-11" morph="none" pos="word" start_char="14103">into</TOKEN>
<TOKEN end_char="14108" id="token-157-12" morph="none" pos="word" start_char="14108">a</TOKEN>
<TOKEN end_char="14115" id="token-157-13" morph="none" pos="word" start_char="14110">sample</TOKEN>
<TOKEN end_char="14123" id="token-157-14" morph="none" pos="unknown" start_char="14117">vial.So</TOKEN>
<TOKEN end_char="14128" id="token-157-15" morph="none" pos="word" start_char="14125">they</TOKEN>
<TOKEN end_char="14134" id="token-157-16" morph="none" pos="word" start_char="14130">would</TOKEN>
<TOKEN end_char="14140" id="token-157-17" morph="none" pos="word" start_char="14136">never</TOKEN>
<TOKEN end_char="14145" id="token-157-18" morph="none" pos="word" start_char="14142">have</TOKEN>
<TOKEN end_char="14150" id="token-157-19" morph="none" pos="word" start_char="14147">made</TOKEN>
<TOKEN end_char="14153" id="token-157-20" morph="none" pos="word" start_char="14152">it</TOKEN>
<TOKEN end_char="14156" id="token-157-21" morph="none" pos="word" start_char="14155">to</TOKEN>
<TOKEN end_char="14158" id="token-157-22" morph="none" pos="word" start_char="14158">a</TOKEN>
<TOKEN end_char="14162" id="token-157-23" morph="none" pos="word" start_char="14160">lab</TOKEN>
<TOKEN end_char="14165" id="token-157-24" morph="none" pos="word" start_char="14164">to</TOKEN>
<TOKEN end_char="14168" id="token-157-25" morph="none" pos="word" start_char="14167">be</TOKEN>
<TOKEN end_char="14177" id="token-157-26" morph="none" pos="word" start_char="14170">detected</TOKEN>
<TOKEN end_char="14178" id="token-157-27" morph="none" pos="punct" start_char="14178">.</TOKEN>
</SEG>
<SEG end_char="14197" id="segment-158" start_char="14181">
<ORIGINAL_TEXT>Speculative also.</ORIGINAL_TEXT>
<TOKEN end_char="14191" id="token-158-0" morph="none" pos="word" start_char="14181">Speculative</TOKEN>
<TOKEN end_char="14196" id="token-158-1" morph="none" pos="word" start_char="14193">also</TOKEN>
<TOKEN end_char="14197" id="token-158-2" morph="none" pos="punct" start_char="14197">.</TOKEN>
</SEG>
<SEG end_char="14341" id="segment-159" start_char="14199">
<ORIGINAL_TEXT>Whole point of scientific investigation you speculate, calculate implications of the speculation and then test against experiment and evidence.</ORIGINAL_TEXT>
<TOKEN end_char="14203" id="token-159-0" morph="none" pos="word" start_char="14199">Whole</TOKEN>
<TOKEN end_char="14209" id="token-159-1" morph="none" pos="word" start_char="14205">point</TOKEN>
<TOKEN end_char="14212" id="token-159-2" morph="none" pos="word" start_char="14211">of</TOKEN>
<TOKEN end_char="14223" id="token-159-3" morph="none" pos="word" start_char="14214">scientific</TOKEN>
<TOKEN end_char="14237" id="token-159-4" morph="none" pos="word" start_char="14225">investigation</TOKEN>
<TOKEN end_char="14241" id="token-159-5" morph="none" pos="word" start_char="14239">you</TOKEN>
<TOKEN end_char="14251" id="token-159-6" morph="none" pos="word" start_char="14243">speculate</TOKEN>
<TOKEN end_char="14252" id="token-159-7" morph="none" pos="punct" start_char="14252">,</TOKEN>
<TOKEN end_char="14262" id="token-159-8" morph="none" pos="word" start_char="14254">calculate</TOKEN>
<TOKEN end_char="14275" id="token-159-9" morph="none" pos="word" start_char="14264">implications</TOKEN>
<TOKEN end_char="14278" id="token-159-10" morph="none" pos="word" start_char="14277">of</TOKEN>
<TOKEN end_char="14282" id="token-159-11" morph="none" pos="word" start_char="14280">the</TOKEN>
<TOKEN end_char="14294" id="token-159-12" morph="none" pos="word" start_char="14284">speculation</TOKEN>
<TOKEN end_char="14298" id="token-159-13" morph="none" pos="word" start_char="14296">and</TOKEN>
<TOKEN end_char="14303" id="token-159-14" morph="none" pos="word" start_char="14300">then</TOKEN>
<TOKEN end_char="14308" id="token-159-15" morph="none" pos="word" start_char="14305">test</TOKEN>
<TOKEN end_char="14316" id="token-159-16" morph="none" pos="word" start_char="14310">against</TOKEN>
<TOKEN end_char="14327" id="token-159-17" morph="none" pos="word" start_char="14318">experiment</TOKEN>
<TOKEN end_char="14331" id="token-159-18" morph="none" pos="word" start_char="14329">and</TOKEN>
<TOKEN end_char="14340" id="token-159-19" morph="none" pos="word" start_char="14333">evidence</TOKEN>
<TOKEN end_char="14341" id="token-159-20" morph="none" pos="punct" start_char="14341">.</TOKEN>
</SEG>
<SEG end_char="14386" id="segment-160" start_char="14345">
<ORIGINAL_TEXT>Quote from: Kryptid on 07/02/2021 04:14:41</ORIGINAL_TEXT>
<TOKEN end_char="14349" id="token-160-0" morph="none" pos="word" start_char="14345">Quote</TOKEN>
<TOKEN end_char="14354" id="token-160-1" morph="none" pos="word" start_char="14351">from</TOKEN>
<TOKEN end_char="14355" id="token-160-2" morph="none" pos="punct" start_char="14355">:</TOKEN>
<TOKEN end_char="14363" id="token-160-3" morph="none" pos="word" start_char="14357">Kryptid</TOKEN>
<TOKEN end_char="14366" id="token-160-4" morph="none" pos="word" start_char="14365">on</TOKEN>
<TOKEN end_char="14377" id="token-160-5" morph="none" pos="unknown" start_char="14368">07/02/2021</TOKEN>
<TOKEN end_char="14386" id="token-160-6" morph="none" pos="unknown" start_char="14379">04:14:41</TOKEN>
</SEG>
<SEG end_char="14469" id="segment-161" start_char="14389">
<ORIGINAL_TEXT>https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</ORIGINAL_TEXT>
<TOKEN end_char="14469" id="token-161-0" morph="none" pos="url" start_char="14389">https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</TOKEN>
<TRANSLATED_TEXT>https: / / theconversation.com / was-coronavirus-really-in-europe-in-march-2019-141582</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="14500" id="segment-162" start_char="14472">
<ORIGINAL_TEXT>I ponder what the hold up is.</ORIGINAL_TEXT>
<TOKEN end_char="14472" id="token-162-0" morph="none" pos="word" start_char="14472">I</TOKEN>
<TOKEN end_char="14479" id="token-162-1" morph="none" pos="word" start_char="14474">ponder</TOKEN>
<TOKEN end_char="14484" id="token-162-2" morph="none" pos="word" start_char="14481">what</TOKEN>
<TOKEN end_char="14488" id="token-162-3" morph="none" pos="word" start_char="14486">the</TOKEN>
<TOKEN end_char="14493" id="token-162-4" morph="none" pos="word" start_char="14490">hold</TOKEN>
<TOKEN end_char="14496" id="token-162-5" morph="none" pos="word" start_char="14495">up</TOKEN>
<TOKEN end_char="14499" id="token-162-6" morph="none" pos="word" start_char="14498">is</TOKEN>
<TOKEN end_char="14500" id="token-162-7" morph="none" pos="punct" start_char="14500">.</TOKEN>
</SEG>
<SEG end_char="14543" id="segment-163" start_char="14502">
<ORIGINAL_TEXT>Shouldn't take a year to do a peer review.</ORIGINAL_TEXT>
<TOKEN end_char="14510" id="token-163-0" morph="none" pos="word" start_char="14502">Shouldn't</TOKEN>
<TOKEN end_char="14515" id="token-163-1" morph="none" pos="word" start_char="14512">take</TOKEN>
<TOKEN end_char="14517" id="token-163-2" morph="none" pos="word" start_char="14517">a</TOKEN>
<TOKEN end_char="14522" id="token-163-3" morph="none" pos="word" start_char="14519">year</TOKEN>
<TOKEN end_char="14525" id="token-163-4" morph="none" pos="word" start_char="14524">to</TOKEN>
<TOKEN end_char="14528" id="token-163-5" morph="none" pos="word" start_char="14527">do</TOKEN>
<TOKEN end_char="14530" id="token-163-6" morph="none" pos="word" start_char="14530">a</TOKEN>
<TOKEN end_char="14535" id="token-163-7" morph="none" pos="word" start_char="14532">peer</TOKEN>
<TOKEN end_char="14542" id="token-163-8" morph="none" pos="word" start_char="14537">review</TOKEN>
<TOKEN end_char="14543" id="token-163-9" morph="none" pos="punct" start_char="14543">.</TOKEN>
</SEG>
<SEG end_char="14583" id="segment-164" start_char="14545">
<ORIGINAL_TEXT>The hold up is in-of-itself suspicious.</ORIGINAL_TEXT>
<TOKEN end_char="14547" id="token-164-0" morph="none" pos="word" start_char="14545">The</TOKEN>
<TOKEN end_char="14552" id="token-164-1" morph="none" pos="word" start_char="14549">hold</TOKEN>
<TOKEN end_char="14555" id="token-164-2" morph="none" pos="word" start_char="14554">up</TOKEN>
<TOKEN end_char="14558" id="token-164-3" morph="none" pos="word" start_char="14557">is</TOKEN>
<TOKEN end_char="14571" id="token-164-4" morph="none" pos="unknown" start_char="14560">in-of-itself</TOKEN>
<TOKEN end_char="14582" id="token-164-5" morph="none" pos="word" start_char="14573">suspicious</TOKEN>
<TOKEN end_char="14583" id="token-164-6" morph="none" pos="punct" start_char="14583">.</TOKEN>
</SEG>
<SEG end_char="14734" id="segment-165" start_char="14586">
<ORIGINAL_TEXT>Still returning to Trump he clearly claimed repeatedly that he had seen evidence covid had come from a laboratory and repeatedly sort to blame China.</ORIGINAL_TEXT>
<TOKEN end_char="14590" id="token-165-0" morph="none" pos="word" start_char="14586">Still</TOKEN>
<TOKEN end_char="14600" id="token-165-1" morph="none" pos="word" start_char="14592">returning</TOKEN>
<TOKEN end_char="14603" id="token-165-2" morph="none" pos="word" start_char="14602">to</TOKEN>
<TOKEN end_char="14609" id="token-165-3" morph="none" pos="word" start_char="14605">Trump</TOKEN>
<TOKEN end_char="14612" id="token-165-4" morph="none" pos="word" start_char="14611">he</TOKEN>
<TOKEN end_char="14620" id="token-165-5" morph="none" pos="word" start_char="14614">clearly</TOKEN>
<TOKEN end_char="14628" id="token-165-6" morph="none" pos="word" start_char="14622">claimed</TOKEN>
<TOKEN end_char="14639" id="token-165-7" morph="none" pos="word" start_char="14630">repeatedly</TOKEN>
<TOKEN end_char="14644" id="token-165-8" morph="none" pos="word" start_char="14641">that</TOKEN>
<TOKEN end_char="14647" id="token-165-9" morph="none" pos="word" start_char="14646">he</TOKEN>
<TOKEN end_char="14651" id="token-165-10" morph="none" pos="word" start_char="14649">had</TOKEN>
<TOKEN end_char="14656" id="token-165-11" morph="none" pos="word" start_char="14653">seen</TOKEN>
<TOKEN end_char="14665" id="token-165-12" morph="none" pos="word" start_char="14658">evidence</TOKEN>
<TOKEN end_char="14671" id="token-165-13" morph="none" pos="word" start_char="14667">covid</TOKEN>
<TOKEN end_char="14675" id="token-165-14" morph="none" pos="word" start_char="14673">had</TOKEN>
<TOKEN end_char="14680" id="token-165-15" morph="none" pos="word" start_char="14677">come</TOKEN>
<TOKEN end_char="14685" id="token-165-16" morph="none" pos="word" start_char="14682">from</TOKEN>
<TOKEN end_char="14687" id="token-165-17" morph="none" pos="word" start_char="14687">a</TOKEN>
<TOKEN end_char="14698" id="token-165-18" morph="none" pos="word" start_char="14689">laboratory</TOKEN>
<TOKEN end_char="14702" id="token-165-19" morph="none" pos="word" start_char="14700">and</TOKEN>
<TOKEN end_char="14713" id="token-165-20" morph="none" pos="word" start_char="14704">repeatedly</TOKEN>
<TOKEN end_char="14718" id="token-165-21" morph="none" pos="word" start_char="14715">sort</TOKEN>
<TOKEN end_char="14721" id="token-165-22" morph="none" pos="word" start_char="14720">to</TOKEN>
<TOKEN end_char="14727" id="token-165-23" morph="none" pos="word" start_char="14723">blame</TOKEN>
<TOKEN end_char="14733" id="token-165-24" morph="none" pos="word" start_char="14729">China</TOKEN>
<TOKEN end_char="14734" id="token-165-25" morph="none" pos="punct" start_char="14734">.</TOKEN>
</SEG>
<SEG end_char="14873" id="segment-166" start_char="14737">
<ORIGINAL_TEXT>Hillary Clinton before the 2016 election had a group investigate her greatest weakness when it came to challenging Trump in the election.</ORIGINAL_TEXT>
<TOKEN end_char="14743" id="token-166-0" morph="none" pos="word" start_char="14737">Hillary</TOKEN>
<TOKEN end_char="14751" id="token-166-1" morph="none" pos="word" start_char="14745">Clinton</TOKEN>
<TOKEN end_char="14758" id="token-166-2" morph="none" pos="word" start_char="14753">before</TOKEN>
<TOKEN end_char="14762" id="token-166-3" morph="none" pos="word" start_char="14760">the</TOKEN>
<TOKEN end_char="14767" id="token-166-4" morph="none" pos="word" start_char="14764">2016</TOKEN>
<TOKEN end_char="14776" id="token-166-5" morph="none" pos="word" start_char="14769">election</TOKEN>
<TOKEN end_char="14780" id="token-166-6" morph="none" pos="word" start_char="14778">had</TOKEN>
<TOKEN end_char="14782" id="token-166-7" morph="none" pos="word" start_char="14782">a</TOKEN>
<TOKEN end_char="14788" id="token-166-8" morph="none" pos="word" start_char="14784">group</TOKEN>
<TOKEN end_char="14800" id="token-166-9" morph="none" pos="word" start_char="14790">investigate</TOKEN>
<TOKEN end_char="14804" id="token-166-10" morph="none" pos="word" start_char="14802">her</TOKEN>
<TOKEN end_char="14813" id="token-166-11" morph="none" pos="word" start_char="14806">greatest</TOKEN>
<TOKEN end_char="14822" id="token-166-12" morph="none" pos="word" start_char="14815">weakness</TOKEN>
<TOKEN end_char="14827" id="token-166-13" morph="none" pos="word" start_char="14824">when</TOKEN>
<TOKEN end_char="14830" id="token-166-14" morph="none" pos="word" start_char="14829">it</TOKEN>
<TOKEN end_char="14835" id="token-166-15" morph="none" pos="word" start_char="14832">came</TOKEN>
<TOKEN end_char="14838" id="token-166-16" morph="none" pos="word" start_char="14837">to</TOKEN>
<TOKEN end_char="14850" id="token-166-17" morph="none" pos="word" start_char="14840">challenging</TOKEN>
<TOKEN end_char="14856" id="token-166-18" morph="none" pos="word" start_char="14852">Trump</TOKEN>
<TOKEN end_char="14859" id="token-166-19" morph="none" pos="word" start_char="14858">in</TOKEN>
<TOKEN end_char="14863" id="token-166-20" morph="none" pos="word" start_char="14861">the</TOKEN>
<TOKEN end_char="14872" id="token-166-21" morph="none" pos="word" start_char="14865">election</TOKEN>
<TOKEN end_char="14873" id="token-166-22" morph="none" pos="punct" start_char="14873">.</TOKEN>
</SEG>
<SEG end_char="15135" id="segment-167" start_char="14875">
<ORIGINAL_TEXT>The report concluded that her tries to Russia and her assistance in allowing Russia to gain a position in the company 'uranium 1' and so a substantial part of Americas uranium production would be seen as her greatest weakness if known by the general population.</ORIGINAL_TEXT>
<TOKEN end_char="14877" id="token-167-0" morph="none" pos="word" start_char="14875">The</TOKEN>
<TOKEN end_char="14884" id="token-167-1" morph="none" pos="word" start_char="14879">report</TOKEN>
<TOKEN end_char="14894" id="token-167-2" morph="none" pos="word" start_char="14886">concluded</TOKEN>
<TOKEN end_char="14899" id="token-167-3" morph="none" pos="word" start_char="14896">that</TOKEN>
<TOKEN end_char="14903" id="token-167-4" morph="none" pos="word" start_char="14901">her</TOKEN>
<TOKEN end_char="14909" id="token-167-5" morph="none" pos="word" start_char="14905">tries</TOKEN>
<TOKEN end_char="14912" id="token-167-6" morph="none" pos="word" start_char="14911">to</TOKEN>
<TOKEN end_char="14919" id="token-167-7" morph="none" pos="word" start_char="14914">Russia</TOKEN>
<TOKEN end_char="14923" id="token-167-8" morph="none" pos="word" start_char="14921">and</TOKEN>
<TOKEN end_char="14927" id="token-167-9" morph="none" pos="word" start_char="14925">her</TOKEN>
<TOKEN end_char="14938" id="token-167-10" morph="none" pos="word" start_char="14929">assistance</TOKEN>
<TOKEN end_char="14941" id="token-167-11" morph="none" pos="word" start_char="14940">in</TOKEN>
<TOKEN end_char="14950" id="token-167-12" morph="none" pos="word" start_char="14943">allowing</TOKEN>
<TOKEN end_char="14957" id="token-167-13" morph="none" pos="word" start_char="14952">Russia</TOKEN>
<TOKEN end_char="14960" id="token-167-14" morph="none" pos="word" start_char="14959">to</TOKEN>
<TOKEN end_char="14965" id="token-167-15" morph="none" pos="word" start_char="14962">gain</TOKEN>
<TOKEN end_char="14967" id="token-167-16" morph="none" pos="word" start_char="14967">a</TOKEN>
<TOKEN end_char="14976" id="token-167-17" morph="none" pos="word" start_char="14969">position</TOKEN>
<TOKEN end_char="14979" id="token-167-18" morph="none" pos="word" start_char="14978">in</TOKEN>
<TOKEN end_char="14983" id="token-167-19" morph="none" pos="word" start_char="14981">the</TOKEN>
<TOKEN end_char="14991" id="token-167-20" morph="none" pos="word" start_char="14985">company</TOKEN>
<TOKEN end_char="14993" id="token-167-21" morph="none" pos="punct" start_char="14993">'</TOKEN>
<TOKEN end_char="15000" id="token-167-22" morph="none" pos="word" start_char="14994">uranium</TOKEN>
<TOKEN end_char="15002" id="token-167-23" morph="none" pos="word" start_char="15002">1</TOKEN>
<TOKEN end_char="15003" id="token-167-24" morph="none" pos="punct" start_char="15003">'</TOKEN>
<TOKEN end_char="15007" id="token-167-25" morph="none" pos="word" start_char="15005">and</TOKEN>
<TOKEN end_char="15010" id="token-167-26" morph="none" pos="word" start_char="15009">so</TOKEN>
<TOKEN end_char="15012" id="token-167-27" morph="none" pos="word" start_char="15012">a</TOKEN>
<TOKEN end_char="15024" id="token-167-28" morph="none" pos="word" start_char="15014">substantial</TOKEN>
<TOKEN end_char="15029" id="token-167-29" morph="none" pos="word" start_char="15026">part</TOKEN>
<TOKEN end_char="15032" id="token-167-30" morph="none" pos="word" start_char="15031">of</TOKEN>
<TOKEN end_char="15041" id="token-167-31" morph="none" pos="word" start_char="15034">Americas</TOKEN>
<TOKEN end_char="15049" id="token-167-32" morph="none" pos="word" start_char="15043">uranium</TOKEN>
<TOKEN end_char="15060" id="token-167-33" morph="none" pos="word" start_char="15051">production</TOKEN>
<TOKEN end_char="15066" id="token-167-34" morph="none" pos="word" start_char="15062">would</TOKEN>
<TOKEN end_char="15069" id="token-167-35" morph="none" pos="word" start_char="15068">be</TOKEN>
<TOKEN end_char="15074" id="token-167-36" morph="none" pos="word" start_char="15071">seen</TOKEN>
<TOKEN end_char="15077" id="token-167-37" morph="none" pos="word" start_char="15076">as</TOKEN>
<TOKEN end_char="15081" id="token-167-38" morph="none" pos="word" start_char="15079">her</TOKEN>
<TOKEN end_char="15090" id="token-167-39" morph="none" pos="word" start_char="15083">greatest</TOKEN>
<TOKEN end_char="15099" id="token-167-40" morph="none" pos="word" start_char="15092">weakness</TOKEN>
<TOKEN end_char="15102" id="token-167-41" morph="none" pos="word" start_char="15101">if</TOKEN>
<TOKEN end_char="15108" id="token-167-42" morph="none" pos="word" start_char="15104">known</TOKEN>
<TOKEN end_char="15111" id="token-167-43" morph="none" pos="word" start_char="15110">by</TOKEN>
<TOKEN end_char="15115" id="token-167-44" morph="none" pos="word" start_char="15113">the</TOKEN>
<TOKEN end_char="15123" id="token-167-45" morph="none" pos="word" start_char="15117">general</TOKEN>
<TOKEN end_char="15134" id="token-167-46" morph="none" pos="word" start_char="15125">population</TOKEN>
<TOKEN end_char="15135" id="token-167-47" morph="none" pos="punct" start_char="15135">.</TOKEN>
</SEG>
<SEG end_char="15316" id="segment-168" start_char="15137">
<ORIGINAL_TEXT>So Mrs Clinton took her greatest weakness and started accusing Trump of being a Russian asset as a means of deflecting attention from herself and her own ties to country of Russia.</ORIGINAL_TEXT>
<TOKEN end_char="15138" id="token-168-0" morph="none" pos="word" start_char="15137">So</TOKEN>
<TOKEN end_char="15142" id="token-168-1" morph="none" pos="word" start_char="15140">Mrs</TOKEN>
<TOKEN end_char="15150" id="token-168-2" morph="none" pos="word" start_char="15144">Clinton</TOKEN>
<TOKEN end_char="15155" id="token-168-3" morph="none" pos="word" start_char="15152">took</TOKEN>
<TOKEN end_char="15159" id="token-168-4" morph="none" pos="word" start_char="15157">her</TOKEN>
<TOKEN end_char="15168" id="token-168-5" morph="none" pos="word" start_char="15161">greatest</TOKEN>
<TOKEN end_char="15177" id="token-168-6" morph="none" pos="word" start_char="15170">weakness</TOKEN>
<TOKEN end_char="15181" id="token-168-7" morph="none" pos="word" start_char="15179">and</TOKEN>
<TOKEN end_char="15189" id="token-168-8" morph="none" pos="word" start_char="15183">started</TOKEN>
<TOKEN end_char="15198" id="token-168-9" morph="none" pos="word" start_char="15191">accusing</TOKEN>
<TOKEN end_char="15204" id="token-168-10" morph="none" pos="word" start_char="15200">Trump</TOKEN>
<TOKEN end_char="15207" id="token-168-11" morph="none" pos="word" start_char="15206">of</TOKEN>
<TOKEN end_char="15213" id="token-168-12" morph="none" pos="word" start_char="15209">being</TOKEN>
<TOKEN end_char="15215" id="token-168-13" morph="none" pos="word" start_char="15215">a</TOKEN>
<TOKEN end_char="15223" id="token-168-14" morph="none" pos="word" start_char="15217">Russian</TOKEN>
<TOKEN end_char="15229" id="token-168-15" morph="none" pos="word" start_char="15225">asset</TOKEN>
<TOKEN end_char="15232" id="token-168-16" morph="none" pos="word" start_char="15231">as</TOKEN>
<TOKEN end_char="15234" id="token-168-17" morph="none" pos="word" start_char="15234">a</TOKEN>
<TOKEN end_char="15240" id="token-168-18" morph="none" pos="word" start_char="15236">means</TOKEN>
<TOKEN end_char="15243" id="token-168-19" morph="none" pos="word" start_char="15242">of</TOKEN>
<TOKEN end_char="15254" id="token-168-20" morph="none" pos="word" start_char="15245">deflecting</TOKEN>
<TOKEN end_char="15264" id="token-168-21" morph="none" pos="word" start_char="15256">attention</TOKEN>
<TOKEN end_char="15269" id="token-168-22" morph="none" pos="word" start_char="15266">from</TOKEN>
<TOKEN end_char="15277" id="token-168-23" morph="none" pos="word" start_char="15271">herself</TOKEN>
<TOKEN end_char="15281" id="token-168-24" morph="none" pos="word" start_char="15279">and</TOKEN>
<TOKEN end_char="15285" id="token-168-25" morph="none" pos="word" start_char="15283">her</TOKEN>
<TOKEN end_char="15289" id="token-168-26" morph="none" pos="word" start_char="15287">own</TOKEN>
<TOKEN end_char="15294" id="token-168-27" morph="none" pos="word" start_char="15291">ties</TOKEN>
<TOKEN end_char="15297" id="token-168-28" morph="none" pos="word" start_char="15296">to</TOKEN>
<TOKEN end_char="15305" id="token-168-29" morph="none" pos="word" start_char="15299">country</TOKEN>
<TOKEN end_char="15308" id="token-168-30" morph="none" pos="word" start_char="15307">of</TOKEN>
<TOKEN end_char="15315" id="token-168-31" morph="none" pos="word" start_char="15310">Russia</TOKEN>
<TOKEN end_char="15316" id="token-168-32" morph="none" pos="punct" start_char="15316">.</TOKEN>
</SEG>
<SEG end_char="15505" id="segment-169" start_char="15319">
<ORIGINAL_TEXT>The reason I tell this story is because of Trumps continual allegation that China released the virus; To speak with such certianly implies a desire much like Clinton to deflect attention.</ORIGINAL_TEXT>
<TOKEN end_char="15321" id="token-169-0" morph="none" pos="word" start_char="15319">The</TOKEN>
<TOKEN end_char="15328" id="token-169-1" morph="none" pos="word" start_char="15323">reason</TOKEN>
<TOKEN end_char="15330" id="token-169-2" morph="none" pos="word" start_char="15330">I</TOKEN>
<TOKEN end_char="15335" id="token-169-3" morph="none" pos="word" start_char="15332">tell</TOKEN>
<TOKEN end_char="15340" id="token-169-4" morph="none" pos="word" start_char="15337">this</TOKEN>
<TOKEN end_char="15346" id="token-169-5" morph="none" pos="word" start_char="15342">story</TOKEN>
<TOKEN end_char="15349" id="token-169-6" morph="none" pos="word" start_char="15348">is</TOKEN>
<TOKEN end_char="15357" id="token-169-7" morph="none" pos="word" start_char="15351">because</TOKEN>
<TOKEN end_char="15360" id="token-169-8" morph="none" pos="word" start_char="15359">of</TOKEN>
<TOKEN end_char="15367" id="token-169-9" morph="none" pos="word" start_char="15362">Trumps</TOKEN>
<TOKEN end_char="15377" id="token-169-10" morph="none" pos="word" start_char="15369">continual</TOKEN>
<TOKEN end_char="15388" id="token-169-11" morph="none" pos="word" start_char="15379">allegation</TOKEN>
<TOKEN end_char="15393" id="token-169-12" morph="none" pos="word" start_char="15390">that</TOKEN>
<TOKEN end_char="15399" id="token-169-13" morph="none" pos="word" start_char="15395">China</TOKEN>
<TOKEN end_char="15408" id="token-169-14" morph="none" pos="word" start_char="15401">released</TOKEN>
<TOKEN end_char="15412" id="token-169-15" morph="none" pos="word" start_char="15410">the</TOKEN>
<TOKEN end_char="15418" id="token-169-16" morph="none" pos="word" start_char="15414">virus</TOKEN>
<TOKEN end_char="15419" id="token-169-17" morph="none" pos="punct" start_char="15419">;</TOKEN>
<TOKEN end_char="15422" id="token-169-18" morph="none" pos="word" start_char="15421">To</TOKEN>
<TOKEN end_char="15428" id="token-169-19" morph="none" pos="word" start_char="15424">speak</TOKEN>
<TOKEN end_char="15433" id="token-169-20" morph="none" pos="word" start_char="15430">with</TOKEN>
<TOKEN end_char="15438" id="token-169-21" morph="none" pos="word" start_char="15435">such</TOKEN>
<TOKEN end_char="15448" id="token-169-22" morph="none" pos="word" start_char="15440">certianly</TOKEN>
<TOKEN end_char="15456" id="token-169-23" morph="none" pos="word" start_char="15450">implies</TOKEN>
<TOKEN end_char="15458" id="token-169-24" morph="none" pos="word" start_char="15458">a</TOKEN>
<TOKEN end_char="15465" id="token-169-25" morph="none" pos="word" start_char="15460">desire</TOKEN>
<TOKEN end_char="15470" id="token-169-26" morph="none" pos="word" start_char="15467">much</TOKEN>
<TOKEN end_char="15475" id="token-169-27" morph="none" pos="word" start_char="15472">like</TOKEN>
<TOKEN end_char="15483" id="token-169-28" morph="none" pos="word" start_char="15477">Clinton</TOKEN>
<TOKEN end_char="15486" id="token-169-29" morph="none" pos="word" start_char="15485">to</TOKEN>
<TOKEN end_char="15494" id="token-169-30" morph="none" pos="word" start_char="15488">deflect</TOKEN>
<TOKEN end_char="15504" id="token-169-31" morph="none" pos="word" start_char="15496">attention</TOKEN>
<TOKEN end_char="15505" id="token-169-32" morph="none" pos="punct" start_char="15505">.</TOKEN>
</SEG>
<SEG end_char="15746" id="segment-170" start_char="15508">
<ORIGINAL_TEXT>Trumps cliams he saw evidence covid came from a laboratory, I feel looking at this, if he did actually see evidence the evidence was from fort Detrick and so as a means of deflection he blamed China, but also as a means to wage a cold war.</ORIGINAL_TEXT>
<TOKEN end_char="15513" id="token-170-0" morph="none" pos="word" start_char="15508">Trumps</TOKEN>
<TOKEN end_char="15520" id="token-170-1" morph="none" pos="word" start_char="15515">cliams</TOKEN>
<TOKEN end_char="15523" id="token-170-2" morph="none" pos="word" start_char="15522">he</TOKEN>
<TOKEN end_char="15527" id="token-170-3" morph="none" pos="word" start_char="15525">saw</TOKEN>
<TOKEN end_char="15536" id="token-170-4" morph="none" pos="word" start_char="15529">evidence</TOKEN>
<TOKEN end_char="15542" id="token-170-5" morph="none" pos="word" start_char="15538">covid</TOKEN>
<TOKEN end_char="15547" id="token-170-6" morph="none" pos="word" start_char="15544">came</TOKEN>
<TOKEN end_char="15552" id="token-170-7" morph="none" pos="word" start_char="15549">from</TOKEN>
<TOKEN end_char="15554" id="token-170-8" morph="none" pos="word" start_char="15554">a</TOKEN>
<TOKEN end_char="15565" id="token-170-9" morph="none" pos="word" start_char="15556">laboratory</TOKEN>
<TOKEN end_char="15566" id="token-170-10" morph="none" pos="punct" start_char="15566">,</TOKEN>
<TOKEN end_char="15568" id="token-170-11" morph="none" pos="word" start_char="15568">I</TOKEN>
<TOKEN end_char="15573" id="token-170-12" morph="none" pos="word" start_char="15570">feel</TOKEN>
<TOKEN end_char="15581" id="token-170-13" morph="none" pos="word" start_char="15575">looking</TOKEN>
<TOKEN end_char="15584" id="token-170-14" morph="none" pos="word" start_char="15583">at</TOKEN>
<TOKEN end_char="15589" id="token-170-15" morph="none" pos="word" start_char="15586">this</TOKEN>
<TOKEN end_char="15590" id="token-170-16" morph="none" pos="punct" start_char="15590">,</TOKEN>
<TOKEN end_char="15593" id="token-170-17" morph="none" pos="word" start_char="15592">if</TOKEN>
<TOKEN end_char="15596" id="token-170-18" morph="none" pos="word" start_char="15595">he</TOKEN>
<TOKEN end_char="15600" id="token-170-19" morph="none" pos="word" start_char="15598">did</TOKEN>
<TOKEN end_char="15609" id="token-170-20" morph="none" pos="word" start_char="15602">actually</TOKEN>
<TOKEN end_char="15613" id="token-170-21" morph="none" pos="word" start_char="15611">see</TOKEN>
<TOKEN end_char="15622" id="token-170-22" morph="none" pos="word" start_char="15615">evidence</TOKEN>
<TOKEN end_char="15626" id="token-170-23" morph="none" pos="word" start_char="15624">the</TOKEN>
<TOKEN end_char="15635" id="token-170-24" morph="none" pos="word" start_char="15628">evidence</TOKEN>
<TOKEN end_char="15639" id="token-170-25" morph="none" pos="word" start_char="15637">was</TOKEN>
<TOKEN end_char="15644" id="token-170-26" morph="none" pos="word" start_char="15641">from</TOKEN>
<TOKEN end_char="15649" id="token-170-27" morph="none" pos="word" start_char="15646">fort</TOKEN>
<TOKEN end_char="15657" id="token-170-28" morph="none" pos="word" start_char="15651">Detrick</TOKEN>
<TOKEN end_char="15661" id="token-170-29" morph="none" pos="word" start_char="15659">and</TOKEN>
<TOKEN end_char="15664" id="token-170-30" morph="none" pos="word" start_char="15663">so</TOKEN>
<TOKEN end_char="15667" id="token-170-31" morph="none" pos="word" start_char="15666">as</TOKEN>
<TOKEN end_char="15669" id="token-170-32" morph="none" pos="word" start_char="15669">a</TOKEN>
<TOKEN end_char="15675" id="token-170-33" morph="none" pos="word" start_char="15671">means</TOKEN>
<TOKEN end_char="15678" id="token-170-34" morph="none" pos="word" start_char="15677">of</TOKEN>
<TOKEN end_char="15689" id="token-170-35" morph="none" pos="word" start_char="15680">deflection</TOKEN>
<TOKEN end_char="15692" id="token-170-36" morph="none" pos="word" start_char="15691">he</TOKEN>
<TOKEN end_char="15699" id="token-170-37" morph="none" pos="word" start_char="15694">blamed</TOKEN>
<TOKEN end_char="15705" id="token-170-38" morph="none" pos="word" start_char="15701">China</TOKEN>
<TOKEN end_char="15706" id="token-170-39" morph="none" pos="punct" start_char="15706">,</TOKEN>
<TOKEN end_char="15710" id="token-170-40" morph="none" pos="word" start_char="15708">but</TOKEN>
<TOKEN end_char="15715" id="token-170-41" morph="none" pos="word" start_char="15712">also</TOKEN>
<TOKEN end_char="15718" id="token-170-42" morph="none" pos="word" start_char="15717">as</TOKEN>
<TOKEN end_char="15720" id="token-170-43" morph="none" pos="word" start_char="15720">a</TOKEN>
<TOKEN end_char="15726" id="token-170-44" morph="none" pos="word" start_char="15722">means</TOKEN>
<TOKEN end_char="15729" id="token-170-45" morph="none" pos="word" start_char="15728">to</TOKEN>
<TOKEN end_char="15734" id="token-170-46" morph="none" pos="word" start_char="15731">wage</TOKEN>
<TOKEN end_char="15736" id="token-170-47" morph="none" pos="word" start_char="15736">a</TOKEN>
<TOKEN end_char="15741" id="token-170-48" morph="none" pos="word" start_char="15738">cold</TOKEN>
<TOKEN end_char="15745" id="token-170-49" morph="none" pos="word" start_char="15743">war</TOKEN>
<TOKEN end_char="15746" id="token-170-50" morph="none" pos="punct" start_char="15746">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>