<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CABY" lang="spa" raw_text_char_length="1643" raw_text_md5="202eec721025c26be98d5a2223560aaf" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="30" id="segment-0" start_char="1">
<ORIGINAL_TEXT>When Did Covid Actually Begin?</ORIGINAL_TEXT>
<TOKEN end_char="4" id="token-0-0" morph="none" pos="word" start_char="1">When</TOKEN>
<TOKEN end_char="8" id="token-0-1" morph="none" pos="word" start_char="6">Did</TOKEN>
<TOKEN end_char="14" id="token-0-2" morph="none" pos="word" start_char="10">Covid</TOKEN>
<TOKEN end_char="23" id="token-0-3" morph="none" pos="word" start_char="16">Actually</TOKEN>
<TOKEN end_char="29" id="token-0-4" morph="none" pos="word" start_char="25">Begin</TOKEN>
<TOKEN end_char="30" id="token-0-5" morph="none" pos="punct" start_char="30">?</TOKEN>
</SEG>
<SEG end_char="124" id="segment-1" start_char="34">
<ORIGINAL_TEXT>Scientists have been trying to piece together a timeline for the Covid outbreak for months.</ORIGINAL_TEXT>
<TOKEN end_char="43" id="token-1-0" morph="none" pos="word" start_char="34">Scientists</TOKEN>
<TOKEN end_char="48" id="token-1-1" morph="none" pos="word" start_char="45">have</TOKEN>
<TOKEN end_char="53" id="token-1-2" morph="none" pos="word" start_char="50">been</TOKEN>
<TOKEN end_char="60" id="token-1-3" morph="none" pos="word" start_char="55">trying</TOKEN>
<TOKEN end_char="63" id="token-1-4" morph="none" pos="word" start_char="62">to</TOKEN>
<TOKEN end_char="69" id="token-1-5" morph="none" pos="word" start_char="65">piece</TOKEN>
<TOKEN end_char="78" id="token-1-6" morph="none" pos="word" start_char="71">together</TOKEN>
<TOKEN end_char="80" id="token-1-7" morph="none" pos="word" start_char="80">a</TOKEN>
<TOKEN end_char="89" id="token-1-8" morph="none" pos="word" start_char="82">timeline</TOKEN>
<TOKEN end_char="93" id="token-1-9" morph="none" pos="word" start_char="91">for</TOKEN>
<TOKEN end_char="97" id="token-1-10" morph="none" pos="word" start_char="95">the</TOKEN>
<TOKEN end_char="103" id="token-1-11" morph="none" pos="word" start_char="99">Covid</TOKEN>
<TOKEN end_char="112" id="token-1-12" morph="none" pos="word" start_char="105">outbreak</TOKEN>
<TOKEN end_char="116" id="token-1-13" morph="none" pos="word" start_char="114">for</TOKEN>
<TOKEN end_char="123" id="token-1-14" morph="none" pos="word" start_char="118">months</TOKEN>
<TOKEN end_char="124" id="token-1-15" morph="none" pos="punct" start_char="124">.</TOKEN>
</SEG>
<SEG end_char="300" id="segment-2" start_char="126">
<ORIGINAL_TEXT>The first official warning about the danger the infection posed was in mid February, but evidence is surfacing that Covid had expanded the the western hemisphere much earlier.</ORIGINAL_TEXT>
<TOKEN end_char="128" id="token-2-0" morph="none" pos="word" start_char="126">The</TOKEN>
<TOKEN end_char="134" id="token-2-1" morph="none" pos="word" start_char="130">first</TOKEN>
<TOKEN end_char="143" id="token-2-2" morph="none" pos="word" start_char="136">official</TOKEN>
<TOKEN end_char="151" id="token-2-3" morph="none" pos="word" start_char="145">warning</TOKEN>
<TOKEN end_char="157" id="token-2-4" morph="none" pos="word" start_char="153">about</TOKEN>
<TOKEN end_char="161" id="token-2-5" morph="none" pos="word" start_char="159">the</TOKEN>
<TOKEN end_char="168" id="token-2-6" morph="none" pos="word" start_char="163">danger</TOKEN>
<TOKEN end_char="172" id="token-2-7" morph="none" pos="word" start_char="170">the</TOKEN>
<TOKEN end_char="182" id="token-2-8" morph="none" pos="word" start_char="174">infection</TOKEN>
<TOKEN end_char="188" id="token-2-9" morph="none" pos="word" start_char="184">posed</TOKEN>
<TOKEN end_char="192" id="token-2-10" morph="none" pos="word" start_char="190">was</TOKEN>
<TOKEN end_char="195" id="token-2-11" morph="none" pos="word" start_char="194">in</TOKEN>
<TOKEN end_char="199" id="token-2-12" morph="none" pos="word" start_char="197">mid</TOKEN>
<TOKEN end_char="208" id="token-2-13" morph="none" pos="word" start_char="201">February</TOKEN>
<TOKEN end_char="209" id="token-2-14" morph="none" pos="punct" start_char="209">,</TOKEN>
<TOKEN end_char="213" id="token-2-15" morph="none" pos="word" start_char="211">but</TOKEN>
<TOKEN end_char="222" id="token-2-16" morph="none" pos="word" start_char="215">evidence</TOKEN>
<TOKEN end_char="225" id="token-2-17" morph="none" pos="word" start_char="224">is</TOKEN>
<TOKEN end_char="235" id="token-2-18" morph="none" pos="word" start_char="227">surfacing</TOKEN>
<TOKEN end_char="240" id="token-2-19" morph="none" pos="word" start_char="237">that</TOKEN>
<TOKEN end_char="246" id="token-2-20" morph="none" pos="word" start_char="242">Covid</TOKEN>
<TOKEN end_char="250" id="token-2-21" morph="none" pos="word" start_char="248">had</TOKEN>
<TOKEN end_char="259" id="token-2-22" morph="none" pos="word" start_char="252">expanded</TOKEN>
<TOKEN end_char="263" id="token-2-23" morph="none" pos="word" start_char="261">the</TOKEN>
<TOKEN end_char="267" id="token-2-24" morph="none" pos="word" start_char="265">the</TOKEN>
<TOKEN end_char="275" id="token-2-25" morph="none" pos="word" start_char="269">western</TOKEN>
<TOKEN end_char="286" id="token-2-26" morph="none" pos="word" start_char="277">hemisphere</TOKEN>
<TOKEN end_char="291" id="token-2-27" morph="none" pos="word" start_char="288">much</TOKEN>
<TOKEN end_char="299" id="token-2-28" morph="none" pos="word" start_char="293">earlier</TOKEN>
<TOKEN end_char="300" id="token-2-29" morph="none" pos="punct" start_char="300">.</TOKEN>
</SEG>
<SEG end_char="427" id="segment-3" start_char="303">
<ORIGINAL_TEXT>Blood test analyses have found infections in the US as early as December in California, Oregon, Washington, and other states.</ORIGINAL_TEXT>
<TOKEN end_char="307" id="token-3-0" morph="none" pos="word" start_char="303">Blood</TOKEN>
<TOKEN end_char="312" id="token-3-1" morph="none" pos="word" start_char="309">test</TOKEN>
<TOKEN end_char="321" id="token-3-2" morph="none" pos="word" start_char="314">analyses</TOKEN>
<TOKEN end_char="326" id="token-3-3" morph="none" pos="word" start_char="323">have</TOKEN>
<TOKEN end_char="332" id="token-3-4" morph="none" pos="word" start_char="328">found</TOKEN>
<TOKEN end_char="343" id="token-3-5" morph="none" pos="word" start_char="334">infections</TOKEN>
<TOKEN end_char="346" id="token-3-6" morph="none" pos="word" start_char="345">in</TOKEN>
<TOKEN end_char="350" id="token-3-7" morph="none" pos="word" start_char="348">the</TOKEN>
<TOKEN end_char="353" id="token-3-8" morph="none" pos="word" start_char="352">US</TOKEN>
<TOKEN end_char="356" id="token-3-9" morph="none" pos="word" start_char="355">as</TOKEN>
<TOKEN end_char="362" id="token-3-10" morph="none" pos="word" start_char="358">early</TOKEN>
<TOKEN end_char="365" id="token-3-11" morph="none" pos="word" start_char="364">as</TOKEN>
<TOKEN end_char="374" id="token-3-12" morph="none" pos="word" start_char="367">December</TOKEN>
<TOKEN end_char="377" id="token-3-13" morph="none" pos="word" start_char="376">in</TOKEN>
<TOKEN end_char="388" id="token-3-14" morph="none" pos="word" start_char="379">California</TOKEN>
<TOKEN end_char="389" id="token-3-15" morph="none" pos="punct" start_char="389">,</TOKEN>
<TOKEN end_char="396" id="token-3-16" morph="none" pos="word" start_char="391">Oregon</TOKEN>
<TOKEN end_char="397" id="token-3-17" morph="none" pos="punct" start_char="397">,</TOKEN>
<TOKEN end_char="408" id="token-3-18" morph="none" pos="word" start_char="399">Washington</TOKEN>
<TOKEN end_char="409" id="token-3-19" morph="none" pos="punct" start_char="409">,</TOKEN>
<TOKEN end_char="413" id="token-3-20" morph="none" pos="word" start_char="411">and</TOKEN>
<TOKEN end_char="419" id="token-3-21" morph="none" pos="word" start_char="415">other</TOKEN>
<TOKEN end_char="426" id="token-3-22" morph="none" pos="word" start_char="421">states</TOKEN>
<TOKEN end_char="427" id="token-3-23" morph="none" pos="punct" start_char="427">.</TOKEN>
</SEG>
<SEG end_char="551" id="segment-4" start_char="429">
<ORIGINAL_TEXT>Similar analyses conducted in Europe show similar findings, claiming that Italy had Covid cases as early as September 2019.</ORIGINAL_TEXT>
<TOKEN end_char="435" id="token-4-0" morph="none" pos="word" start_char="429">Similar</TOKEN>
<TOKEN end_char="444" id="token-4-1" morph="none" pos="word" start_char="437">analyses</TOKEN>
<TOKEN end_char="454" id="token-4-2" morph="none" pos="word" start_char="446">conducted</TOKEN>
<TOKEN end_char="457" id="token-4-3" morph="none" pos="word" start_char="456">in</TOKEN>
<TOKEN end_char="464" id="token-4-4" morph="none" pos="word" start_char="459">Europe</TOKEN>
<TOKEN end_char="469" id="token-4-5" morph="none" pos="word" start_char="466">show</TOKEN>
<TOKEN end_char="477" id="token-4-6" morph="none" pos="word" start_char="471">similar</TOKEN>
<TOKEN end_char="486" id="token-4-7" morph="none" pos="word" start_char="479">findings</TOKEN>
<TOKEN end_char="487" id="token-4-8" morph="none" pos="punct" start_char="487">,</TOKEN>
<TOKEN end_char="496" id="token-4-9" morph="none" pos="word" start_char="489">claiming</TOKEN>
<TOKEN end_char="501" id="token-4-10" morph="none" pos="word" start_char="498">that</TOKEN>
<TOKEN end_char="507" id="token-4-11" morph="none" pos="word" start_char="503">Italy</TOKEN>
<TOKEN end_char="511" id="token-4-12" morph="none" pos="word" start_char="509">had</TOKEN>
<TOKEN end_char="517" id="token-4-13" morph="none" pos="word" start_char="513">Covid</TOKEN>
<TOKEN end_char="523" id="token-4-14" morph="none" pos="word" start_char="519">cases</TOKEN>
<TOKEN end_char="526" id="token-4-15" morph="none" pos="word" start_char="525">as</TOKEN>
<TOKEN end_char="532" id="token-4-16" morph="none" pos="word" start_char="528">early</TOKEN>
<TOKEN end_char="535" id="token-4-17" morph="none" pos="word" start_char="534">as</TOKEN>
<TOKEN end_char="545" id="token-4-18" morph="none" pos="word" start_char="537">September</TOKEN>
<TOKEN end_char="550" id="token-4-19" morph="none" pos="word" start_char="547">2019</TOKEN>
<TOKEN end_char="551" id="token-4-20" morph="none" pos="punct" start_char="551">.</TOKEN>
</SEG>
<SEG end_char="617" id="segment-5" start_char="553">
<ORIGINAL_TEXT>France had a cases in December, and Spain found traces in sewage.</ORIGINAL_TEXT>
<TOKEN end_char="558" id="token-5-0" morph="none" pos="word" start_char="553">France</TOKEN>
<TOKEN end_char="562" id="token-5-1" morph="none" pos="word" start_char="560">had</TOKEN>
<TOKEN end_char="564" id="token-5-2" morph="none" pos="word" start_char="564">a</TOKEN>
<TOKEN end_char="570" id="token-5-3" morph="none" pos="word" start_char="566">cases</TOKEN>
<TOKEN end_char="573" id="token-5-4" morph="none" pos="word" start_char="572">in</TOKEN>
<TOKEN end_char="582" id="token-5-5" morph="none" pos="word" start_char="575">December</TOKEN>
<TOKEN end_char="583" id="token-5-6" morph="none" pos="punct" start_char="583">,</TOKEN>
<TOKEN end_char="587" id="token-5-7" morph="none" pos="word" start_char="585">and</TOKEN>
<TOKEN end_char="593" id="token-5-8" morph="none" pos="word" start_char="589">Spain</TOKEN>
<TOKEN end_char="599" id="token-5-9" morph="none" pos="word" start_char="595">found</TOKEN>
<TOKEN end_char="606" id="token-5-10" morph="none" pos="word" start_char="601">traces</TOKEN>
<TOKEN end_char="609" id="token-5-11" morph="none" pos="word" start_char="608">in</TOKEN>
<TOKEN end_char="616" id="token-5-12" morph="none" pos="word" start_char="611">sewage</TOKEN>
<TOKEN end_char="617" id="token-5-13" morph="none" pos="punct" start_char="617">.</TOKEN>
</SEG>
<SEG end_char="763" id="segment-6" start_char="620">
<ORIGINAL_TEXT>To make all of this worse, leaked documents to CNN prove that the Chinese covered up thousands of cases at the beginning of the global outbreak.</ORIGINAL_TEXT>
<TOKEN end_char="621" id="token-6-0" morph="none" pos="word" start_char="620">To</TOKEN>
<TOKEN end_char="626" id="token-6-1" morph="none" pos="word" start_char="623">make</TOKEN>
<TOKEN end_char="630" id="token-6-2" morph="none" pos="word" start_char="628">all</TOKEN>
<TOKEN end_char="633" id="token-6-3" morph="none" pos="word" start_char="632">of</TOKEN>
<TOKEN end_char="638" id="token-6-4" morph="none" pos="word" start_char="635">this</TOKEN>
<TOKEN end_char="644" id="token-6-5" morph="none" pos="word" start_char="640">worse</TOKEN>
<TOKEN end_char="645" id="token-6-6" morph="none" pos="punct" start_char="645">,</TOKEN>
<TOKEN end_char="652" id="token-6-7" morph="none" pos="word" start_char="647">leaked</TOKEN>
<TOKEN end_char="662" id="token-6-8" morph="none" pos="word" start_char="654">documents</TOKEN>
<TOKEN end_char="665" id="token-6-9" morph="none" pos="word" start_char="664">to</TOKEN>
<TOKEN end_char="669" id="token-6-10" morph="none" pos="word" start_char="667">CNN</TOKEN>
<TOKEN end_char="675" id="token-6-11" morph="none" pos="word" start_char="671">prove</TOKEN>
<TOKEN end_char="680" id="token-6-12" morph="none" pos="word" start_char="677">that</TOKEN>
<TOKEN end_char="684" id="token-6-13" morph="none" pos="word" start_char="682">the</TOKEN>
<TOKEN end_char="692" id="token-6-14" morph="none" pos="word" start_char="686">Chinese</TOKEN>
<TOKEN end_char="700" id="token-6-15" morph="none" pos="word" start_char="694">covered</TOKEN>
<TOKEN end_char="703" id="token-6-16" morph="none" pos="word" start_char="702">up</TOKEN>
<TOKEN end_char="713" id="token-6-17" morph="none" pos="word" start_char="705">thousands</TOKEN>
<TOKEN end_char="716" id="token-6-18" morph="none" pos="word" start_char="715">of</TOKEN>
<TOKEN end_char="722" id="token-6-19" morph="none" pos="word" start_char="718">cases</TOKEN>
<TOKEN end_char="725" id="token-6-20" morph="none" pos="word" start_char="724">at</TOKEN>
<TOKEN end_char="729" id="token-6-21" morph="none" pos="word" start_char="727">the</TOKEN>
<TOKEN end_char="739" id="token-6-22" morph="none" pos="word" start_char="731">beginning</TOKEN>
<TOKEN end_char="742" id="token-6-23" morph="none" pos="word" start_char="741">of</TOKEN>
<TOKEN end_char="746" id="token-6-24" morph="none" pos="word" start_char="744">the</TOKEN>
<TOKEN end_char="753" id="token-6-25" morph="none" pos="word" start_char="748">global</TOKEN>
<TOKEN end_char="762" id="token-6-26" morph="none" pos="word" start_char="755">outbreak</TOKEN>
<TOKEN end_char="763" id="token-6-27" morph="none" pos="punct" start_char="763">.</TOKEN>
</SEG>
<SEG end_char="832" id="segment-7" start_char="765">
<ORIGINAL_TEXT>This information could have been vital to saving thousands of lives.</ORIGINAL_TEXT>
<TOKEN end_char="768" id="token-7-0" morph="none" pos="word" start_char="765">This</TOKEN>
<TOKEN end_char="780" id="token-7-1" morph="none" pos="word" start_char="770">information</TOKEN>
<TOKEN end_char="786" id="token-7-2" morph="none" pos="word" start_char="782">could</TOKEN>
<TOKEN end_char="791" id="token-7-3" morph="none" pos="word" start_char="788">have</TOKEN>
<TOKEN end_char="796" id="token-7-4" morph="none" pos="word" start_char="793">been</TOKEN>
<TOKEN end_char="802" id="token-7-5" morph="none" pos="word" start_char="798">vital</TOKEN>
<TOKEN end_char="805" id="token-7-6" morph="none" pos="word" start_char="804">to</TOKEN>
<TOKEN end_char="812" id="token-7-7" morph="none" pos="word" start_char="807">saving</TOKEN>
<TOKEN end_char="822" id="token-7-8" morph="none" pos="word" start_char="814">thousands</TOKEN>
<TOKEN end_char="825" id="token-7-9" morph="none" pos="word" start_char="824">of</TOKEN>
<TOKEN end_char="831" id="token-7-10" morph="none" pos="word" start_char="827">lives</TOKEN>
<TOKEN end_char="832" id="token-7-11" morph="none" pos="punct" start_char="832">.</TOKEN>
</SEG>
<SEG end_char="920" id="segment-8" start_char="834">
<ORIGINAL_TEXT>China claimed to have almost one hundred thousand cases and almost half that in deaths.</ORIGINAL_TEXT>
<TOKEN end_char="838" id="token-8-0" morph="none" pos="word" start_char="834">China</TOKEN>
<TOKEN end_char="846" id="token-8-1" morph="none" pos="word" start_char="840">claimed</TOKEN>
<TOKEN end_char="849" id="token-8-2" morph="none" pos="word" start_char="848">to</TOKEN>
<TOKEN end_char="854" id="token-8-3" morph="none" pos="word" start_char="851">have</TOKEN>
<TOKEN end_char="861" id="token-8-4" morph="none" pos="word" start_char="856">almost</TOKEN>
<TOKEN end_char="865" id="token-8-5" morph="none" pos="word" start_char="863">one</TOKEN>
<TOKEN end_char="873" id="token-8-6" morph="none" pos="word" start_char="867">hundred</TOKEN>
<TOKEN end_char="882" id="token-8-7" morph="none" pos="word" start_char="875">thousand</TOKEN>
<TOKEN end_char="888" id="token-8-8" morph="none" pos="word" start_char="884">cases</TOKEN>
<TOKEN end_char="892" id="token-8-9" morph="none" pos="word" start_char="890">and</TOKEN>
<TOKEN end_char="899" id="token-8-10" morph="none" pos="word" start_char="894">almost</TOKEN>
<TOKEN end_char="904" id="token-8-11" morph="none" pos="word" start_char="901">half</TOKEN>
<TOKEN end_char="909" id="token-8-12" morph="none" pos="word" start_char="906">that</TOKEN>
<TOKEN end_char="912" id="token-8-13" morph="none" pos="word" start_char="911">in</TOKEN>
<TOKEN end_char="919" id="token-8-14" morph="none" pos="word" start_char="914">deaths</TOKEN>
<TOKEN end_char="920" id="token-8-15" morph="none" pos="punct" start_char="920">.</TOKEN>
</SEG>
<SEG end_char="1088" id="segment-9" start_char="922">
<ORIGINAL_TEXT>This, compared to a much better ratio of cases to deaths in all of the western world, should indicate that China has been less than truthful about their Covid numbers.</ORIGINAL_TEXT>
<TOKEN end_char="925" id="token-9-0" morph="none" pos="word" start_char="922">This</TOKEN>
<TOKEN end_char="926" id="token-9-1" morph="none" pos="punct" start_char="926">,</TOKEN>
<TOKEN end_char="935" id="token-9-2" morph="none" pos="word" start_char="928">compared</TOKEN>
<TOKEN end_char="938" id="token-9-3" morph="none" pos="word" start_char="937">to</TOKEN>
<TOKEN end_char="940" id="token-9-4" morph="none" pos="word" start_char="940">a</TOKEN>
<TOKEN end_char="945" id="token-9-5" morph="none" pos="word" start_char="942">much</TOKEN>
<TOKEN end_char="952" id="token-9-6" morph="none" pos="word" start_char="947">better</TOKEN>
<TOKEN end_char="958" id="token-9-7" morph="none" pos="word" start_char="954">ratio</TOKEN>
<TOKEN end_char="961" id="token-9-8" morph="none" pos="word" start_char="960">of</TOKEN>
<TOKEN end_char="967" id="token-9-9" morph="none" pos="word" start_char="963">cases</TOKEN>
<TOKEN end_char="970" id="token-9-10" morph="none" pos="word" start_char="969">to</TOKEN>
<TOKEN end_char="977" id="token-9-11" morph="none" pos="word" start_char="972">deaths</TOKEN>
<TOKEN end_char="980" id="token-9-12" morph="none" pos="word" start_char="979">in</TOKEN>
<TOKEN end_char="984" id="token-9-13" morph="none" pos="word" start_char="982">all</TOKEN>
<TOKEN end_char="987" id="token-9-14" morph="none" pos="word" start_char="986">of</TOKEN>
<TOKEN end_char="991" id="token-9-15" morph="none" pos="word" start_char="989">the</TOKEN>
<TOKEN end_char="999" id="token-9-16" morph="none" pos="word" start_char="993">western</TOKEN>
<TOKEN end_char="1005" id="token-9-17" morph="none" pos="word" start_char="1001">world</TOKEN>
<TOKEN end_char="1006" id="token-9-18" morph="none" pos="punct" start_char="1006">,</TOKEN>
<TOKEN end_char="1013" id="token-9-19" morph="none" pos="word" start_char="1008">should</TOKEN>
<TOKEN end_char="1022" id="token-9-20" morph="none" pos="word" start_char="1015">indicate</TOKEN>
<TOKEN end_char="1027" id="token-9-21" morph="none" pos="word" start_char="1024">that</TOKEN>
<TOKEN end_char="1033" id="token-9-22" morph="none" pos="word" start_char="1029">China</TOKEN>
<TOKEN end_char="1037" id="token-9-23" morph="none" pos="word" start_char="1035">has</TOKEN>
<TOKEN end_char="1042" id="token-9-24" morph="none" pos="word" start_char="1039">been</TOKEN>
<TOKEN end_char="1047" id="token-9-25" morph="none" pos="word" start_char="1044">less</TOKEN>
<TOKEN end_char="1052" id="token-9-26" morph="none" pos="word" start_char="1049">than</TOKEN>
<TOKEN end_char="1061" id="token-9-27" morph="none" pos="word" start_char="1054">truthful</TOKEN>
<TOKEN end_char="1067" id="token-9-28" morph="none" pos="word" start_char="1063">about</TOKEN>
<TOKEN end_char="1073" id="token-9-29" morph="none" pos="word" start_char="1069">their</TOKEN>
<TOKEN end_char="1079" id="token-9-30" morph="none" pos="word" start_char="1075">Covid</TOKEN>
<TOKEN end_char="1087" id="token-9-31" morph="none" pos="word" start_char="1081">numbers</TOKEN>
<TOKEN end_char="1088" id="token-9-32" morph="none" pos="punct" start_char="1088">.</TOKEN>
</SEG>
<SEG end_char="1189" id="segment-10" start_char="1091">
<ORIGINAL_TEXT>What this should indicate to the rest of the world is that China is a bad actor on the world stage.</ORIGINAL_TEXT>
<TOKEN end_char="1094" id="token-10-0" morph="none" pos="word" start_char="1091">What</TOKEN>
<TOKEN end_char="1099" id="token-10-1" morph="none" pos="word" start_char="1096">this</TOKEN>
<TOKEN end_char="1106" id="token-10-2" morph="none" pos="word" start_char="1101">should</TOKEN>
<TOKEN end_char="1115" id="token-10-3" morph="none" pos="word" start_char="1108">indicate</TOKEN>
<TOKEN end_char="1118" id="token-10-4" morph="none" pos="word" start_char="1117">to</TOKEN>
<TOKEN end_char="1122" id="token-10-5" morph="none" pos="word" start_char="1120">the</TOKEN>
<TOKEN end_char="1127" id="token-10-6" morph="none" pos="word" start_char="1124">rest</TOKEN>
<TOKEN end_char="1130" id="token-10-7" morph="none" pos="word" start_char="1129">of</TOKEN>
<TOKEN end_char="1134" id="token-10-8" morph="none" pos="word" start_char="1132">the</TOKEN>
<TOKEN end_char="1140" id="token-10-9" morph="none" pos="word" start_char="1136">world</TOKEN>
<TOKEN end_char="1143" id="token-10-10" morph="none" pos="word" start_char="1142">is</TOKEN>
<TOKEN end_char="1148" id="token-10-11" morph="none" pos="word" start_char="1145">that</TOKEN>
<TOKEN end_char="1154" id="token-10-12" morph="none" pos="word" start_char="1150">China</TOKEN>
<TOKEN end_char="1157" id="token-10-13" morph="none" pos="word" start_char="1156">is</TOKEN>
<TOKEN end_char="1159" id="token-10-14" morph="none" pos="word" start_char="1159">a</TOKEN>
<TOKEN end_char="1163" id="token-10-15" morph="none" pos="word" start_char="1161">bad</TOKEN>
<TOKEN end_char="1169" id="token-10-16" morph="none" pos="word" start_char="1165">actor</TOKEN>
<TOKEN end_char="1172" id="token-10-17" morph="none" pos="word" start_char="1171">on</TOKEN>
<TOKEN end_char="1176" id="token-10-18" morph="none" pos="word" start_char="1174">the</TOKEN>
<TOKEN end_char="1182" id="token-10-19" morph="none" pos="word" start_char="1178">world</TOKEN>
<TOKEN end_char="1188" id="token-10-20" morph="none" pos="word" start_char="1184">stage</TOKEN>
<TOKEN end_char="1189" id="token-10-21" morph="none" pos="punct" start_char="1189">.</TOKEN>
</SEG>
<SEG end_char="1347" id="segment-11" start_char="1191">
<ORIGINAL_TEXT>It is clear, based only on Covid response, that China can not be trusted and should show that China is not ready to be recognized as an ally in any capacity.</ORIGINAL_TEXT>
<TOKEN end_char="1192" id="token-11-0" morph="none" pos="word" start_char="1191">It</TOKEN>
<TOKEN end_char="1195" id="token-11-1" morph="none" pos="word" start_char="1194">is</TOKEN>
<TOKEN end_char="1201" id="token-11-2" morph="none" pos="word" start_char="1197">clear</TOKEN>
<TOKEN end_char="1202" id="token-11-3" morph="none" pos="punct" start_char="1202">,</TOKEN>
<TOKEN end_char="1208" id="token-11-4" morph="none" pos="word" start_char="1204">based</TOKEN>
<TOKEN end_char="1213" id="token-11-5" morph="none" pos="word" start_char="1210">only</TOKEN>
<TOKEN end_char="1216" id="token-11-6" morph="none" pos="word" start_char="1215">on</TOKEN>
<TOKEN end_char="1222" id="token-11-7" morph="none" pos="word" start_char="1218">Covid</TOKEN>
<TOKEN end_char="1231" id="token-11-8" morph="none" pos="word" start_char="1224">response</TOKEN>
<TOKEN end_char="1232" id="token-11-9" morph="none" pos="punct" start_char="1232">,</TOKEN>
<TOKEN end_char="1237" id="token-11-10" morph="none" pos="word" start_char="1234">that</TOKEN>
<TOKEN end_char="1243" id="token-11-11" morph="none" pos="word" start_char="1239">China</TOKEN>
<TOKEN end_char="1247" id="token-11-12" morph="none" pos="word" start_char="1245">can</TOKEN>
<TOKEN end_char="1251" id="token-11-13" morph="none" pos="word" start_char="1249">not</TOKEN>
<TOKEN end_char="1254" id="token-11-14" morph="none" pos="word" start_char="1253">be</TOKEN>
<TOKEN end_char="1262" id="token-11-15" morph="none" pos="word" start_char="1256">trusted</TOKEN>
<TOKEN end_char="1266" id="token-11-16" morph="none" pos="word" start_char="1264">and</TOKEN>
<TOKEN end_char="1273" id="token-11-17" morph="none" pos="word" start_char="1268">should</TOKEN>
<TOKEN end_char="1278" id="token-11-18" morph="none" pos="word" start_char="1275">show</TOKEN>
<TOKEN end_char="1283" id="token-11-19" morph="none" pos="word" start_char="1280">that</TOKEN>
<TOKEN end_char="1289" id="token-11-20" morph="none" pos="word" start_char="1285">China</TOKEN>
<TOKEN end_char="1292" id="token-11-21" morph="none" pos="word" start_char="1291">is</TOKEN>
<TOKEN end_char="1296" id="token-11-22" morph="none" pos="word" start_char="1294">not</TOKEN>
<TOKEN end_char="1302" id="token-11-23" morph="none" pos="word" start_char="1298">ready</TOKEN>
<TOKEN end_char="1305" id="token-11-24" morph="none" pos="word" start_char="1304">to</TOKEN>
<TOKEN end_char="1308" id="token-11-25" morph="none" pos="word" start_char="1307">be</TOKEN>
<TOKEN end_char="1319" id="token-11-26" morph="none" pos="word" start_char="1310">recognized</TOKEN>
<TOKEN end_char="1322" id="token-11-27" morph="none" pos="word" start_char="1321">as</TOKEN>
<TOKEN end_char="1325" id="token-11-28" morph="none" pos="word" start_char="1324">an</TOKEN>
<TOKEN end_char="1330" id="token-11-29" morph="none" pos="word" start_char="1327">ally</TOKEN>
<TOKEN end_char="1333" id="token-11-30" morph="none" pos="word" start_char="1332">in</TOKEN>
<TOKEN end_char="1337" id="token-11-31" morph="none" pos="word" start_char="1335">any</TOKEN>
<TOKEN end_char="1346" id="token-11-32" morph="none" pos="word" start_char="1339">capacity</TOKEN>
<TOKEN end_char="1347" id="token-11-33" morph="none" pos="punct" start_char="1347">.</TOKEN>
</SEG>
<SEG end_char="1377" id="segment-12" start_char="1350">
<ORIGINAL_TEXT>But there is some good news.</ORIGINAL_TEXT>
<TOKEN end_char="1352" id="token-12-0" morph="none" pos="word" start_char="1350">But</TOKEN>
<TOKEN end_char="1358" id="token-12-1" morph="none" pos="word" start_char="1354">there</TOKEN>
<TOKEN end_char="1361" id="token-12-2" morph="none" pos="word" start_char="1360">is</TOKEN>
<TOKEN end_char="1366" id="token-12-3" morph="none" pos="word" start_char="1363">some</TOKEN>
<TOKEN end_char="1371" id="token-12-4" morph="none" pos="word" start_char="1368">good</TOKEN>
<TOKEN end_char="1376" id="token-12-5" morph="none" pos="word" start_char="1373">news</TOKEN>
<TOKEN end_char="1377" id="token-12-6" morph="none" pos="punct" start_char="1377">.</TOKEN>
</SEG>
<SEG end_char="1488" id="segment-13" start_char="1379">
<ORIGINAL_TEXT>Multiple companies have applied for emergency use of their vaccines which should be available early next year.</ORIGINAL_TEXT>
<TOKEN end_char="1386" id="token-13-0" morph="none" pos="word" start_char="1379">Multiple</TOKEN>
<TOKEN end_char="1396" id="token-13-1" morph="none" pos="word" start_char="1388">companies</TOKEN>
<TOKEN end_char="1401" id="token-13-2" morph="none" pos="word" start_char="1398">have</TOKEN>
<TOKEN end_char="1409" id="token-13-3" morph="none" pos="word" start_char="1403">applied</TOKEN>
<TOKEN end_char="1413" id="token-13-4" morph="none" pos="word" start_char="1411">for</TOKEN>
<TOKEN end_char="1423" id="token-13-5" morph="none" pos="word" start_char="1415">emergency</TOKEN>
<TOKEN end_char="1427" id="token-13-6" morph="none" pos="word" start_char="1425">use</TOKEN>
<TOKEN end_char="1430" id="token-13-7" morph="none" pos="word" start_char="1429">of</TOKEN>
<TOKEN end_char="1436" id="token-13-8" morph="none" pos="word" start_char="1432">their</TOKEN>
<TOKEN end_char="1445" id="token-13-9" morph="none" pos="word" start_char="1438">vaccines</TOKEN>
<TOKEN end_char="1451" id="token-13-10" morph="none" pos="word" start_char="1447">which</TOKEN>
<TOKEN end_char="1458" id="token-13-11" morph="none" pos="word" start_char="1453">should</TOKEN>
<TOKEN end_char="1461" id="token-13-12" morph="none" pos="word" start_char="1460">be</TOKEN>
<TOKEN end_char="1471" id="token-13-13" morph="none" pos="word" start_char="1463">available</TOKEN>
<TOKEN end_char="1477" id="token-13-14" morph="none" pos="word" start_char="1473">early</TOKEN>
<TOKEN end_char="1482" id="token-13-15" morph="none" pos="word" start_char="1479">next</TOKEN>
<TOKEN end_char="1487" id="token-13-16" morph="none" pos="word" start_char="1484">year</TOKEN>
<TOKEN end_char="1488" id="token-13-17" morph="none" pos="punct" start_char="1488">.</TOKEN>
</SEG>
<SEG end_char="1559" id="segment-14" start_char="1490">
<ORIGINAL_TEXT>These vaccines are shown to be over 90% affective in preventing Covid.</ORIGINAL_TEXT>
<TOKEN end_char="1494" id="token-14-0" morph="none" pos="word" start_char="1490">These</TOKEN>
<TOKEN end_char="1503" id="token-14-1" morph="none" pos="word" start_char="1496">vaccines</TOKEN>
<TOKEN end_char="1507" id="token-14-2" morph="none" pos="word" start_char="1505">are</TOKEN>
<TOKEN end_char="1513" id="token-14-3" morph="none" pos="word" start_char="1509">shown</TOKEN>
<TOKEN end_char="1516" id="token-14-4" morph="none" pos="word" start_char="1515">to</TOKEN>
<TOKEN end_char="1519" id="token-14-5" morph="none" pos="word" start_char="1518">be</TOKEN>
<TOKEN end_char="1524" id="token-14-6" morph="none" pos="word" start_char="1521">over</TOKEN>
<TOKEN end_char="1527" id="token-14-7" morph="none" pos="word" start_char="1526">90</TOKEN>
<TOKEN end_char="1528" id="token-14-8" morph="none" pos="punct" start_char="1528">%</TOKEN>
<TOKEN end_char="1538" id="token-14-9" morph="none" pos="word" start_char="1530">affective</TOKEN>
<TOKEN end_char="1541" id="token-14-10" morph="none" pos="word" start_char="1540">in</TOKEN>
<TOKEN end_char="1552" id="token-14-11" morph="none" pos="word" start_char="1543">preventing</TOKEN>
<TOKEN end_char="1558" id="token-14-12" morph="none" pos="word" start_char="1554">Covid</TOKEN>
<TOKEN end_char="1559" id="token-14-13" morph="none" pos="punct" start_char="1559">.</TOKEN>
</SEG>
<SEG end_char="1639" id="segment-15" start_char="1561">
<ORIGINAL_TEXT>After these vaccines get distributed, hopefully we can get back to normal life.</ORIGINAL_TEXT>
<TOKEN end_char="1565" id="token-15-0" morph="none" pos="word" start_char="1561">After</TOKEN>
<TOKEN end_char="1571" id="token-15-1" morph="none" pos="word" start_char="1567">these</TOKEN>
<TOKEN end_char="1580" id="token-15-2" morph="none" pos="word" start_char="1573">vaccines</TOKEN>
<TOKEN end_char="1584" id="token-15-3" morph="none" pos="word" start_char="1582">get</TOKEN>
<TOKEN end_char="1596" id="token-15-4" morph="none" pos="word" start_char="1586">distributed</TOKEN>
<TOKEN end_char="1597" id="token-15-5" morph="none" pos="punct" start_char="1597">,</TOKEN>
<TOKEN end_char="1607" id="token-15-6" morph="none" pos="word" start_char="1599">hopefully</TOKEN>
<TOKEN end_char="1610" id="token-15-7" morph="none" pos="word" start_char="1609">we</TOKEN>
<TOKEN end_char="1614" id="token-15-8" morph="none" pos="word" start_char="1612">can</TOKEN>
<TOKEN end_char="1618" id="token-15-9" morph="none" pos="word" start_char="1616">get</TOKEN>
<TOKEN end_char="1623" id="token-15-10" morph="none" pos="word" start_char="1620">back</TOKEN>
<TOKEN end_char="1626" id="token-15-11" morph="none" pos="word" start_char="1625">to</TOKEN>
<TOKEN end_char="1633" id="token-15-12" morph="none" pos="word" start_char="1628">normal</TOKEN>
<TOKEN end_char="1638" id="token-15-13" morph="none" pos="word" start_char="1635">life</TOKEN>
<TOKEN end_char="1639" id="token-15-14" morph="none" pos="punct" start_char="1639">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>