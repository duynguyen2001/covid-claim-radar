<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04ATBT" lang="spa" raw_text_char_length="3172" raw_text_md5="109586661602811217b9ed7b9f6847aa" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="68" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Análisis genéticos apuntan al murciélago como origen del coronavirus</ORIGINAL_TEXT>
<TOKEN end_char="8" id="token-0-0" morph="none" pos="word" start_char="1">Análisis</TOKEN>
<TOKEN end_char="18" id="token-0-1" morph="none" pos="word" start_char="10">genéticos</TOKEN>
<TOKEN end_char="26" id="token-0-2" morph="none" pos="word" start_char="20">apuntan</TOKEN>
<TOKEN end_char="29" id="token-0-3" morph="none" pos="word" start_char="28">al</TOKEN>
<TOKEN end_char="40" id="token-0-4" morph="none" pos="word" start_char="31">murciélago</TOKEN>
<TOKEN end_char="45" id="token-0-5" morph="none" pos="word" start_char="42">como</TOKEN>
<TOKEN end_char="52" id="token-0-6" morph="none" pos="word" start_char="47">origen</TOKEN>
<TOKEN end_char="56" id="token-0-7" morph="none" pos="word" start_char="54">del</TOKEN>
<TOKEN end_char="68" id="token-0-8" morph="none" pos="word" start_char="58">coronavirus</TOKEN>
<TRANSLATED_TEXT>Genetic analysis points to bat as the origin of coronavirus</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="352" id="segment-1" start_char="73">
<ORIGINAL_TEXT>Los murciélagos de la especie Rhinolophus Affinis, muy común en China y el sureste asiático, se han vuelto los principales sospechosos del origen del coronavirus, debido a un análisis comparativo del genoma del virus, hecho en Italia y publicado en el Journal of Medical Virology.</ORIGINAL_TEXT>
<TOKEN end_char="75" id="token-1-0" morph="none" pos="word" start_char="73">Los</TOKEN>
<TOKEN end_char="87" id="token-1-1" morph="none" pos="word" start_char="77">murciélagos</TOKEN>
<TOKEN end_char="90" id="token-1-2" morph="none" pos="word" start_char="89">de</TOKEN>
<TOKEN end_char="93" id="token-1-3" morph="none" pos="word" start_char="92">la</TOKEN>
<TOKEN end_char="101" id="token-1-4" morph="none" pos="word" start_char="95">especie</TOKEN>
<TOKEN end_char="113" id="token-1-5" morph="none" pos="word" start_char="103">Rhinolophus</TOKEN>
<TOKEN end_char="121" id="token-1-6" morph="none" pos="word" start_char="115">Affinis</TOKEN>
<TOKEN end_char="122" id="token-1-7" morph="none" pos="punct" start_char="122">,</TOKEN>
<TOKEN end_char="126" id="token-1-8" morph="none" pos="word" start_char="124">muy</TOKEN>
<TOKEN end_char="132" id="token-1-9" morph="none" pos="word" start_char="128">común</TOKEN>
<TOKEN end_char="135" id="token-1-10" morph="none" pos="word" start_char="134">en</TOKEN>
<TOKEN end_char="141" id="token-1-11" morph="none" pos="word" start_char="137">China</TOKEN>
<TOKEN end_char="143" id="token-1-12" morph="none" pos="word" start_char="143">y</TOKEN>
<TOKEN end_char="146" id="token-1-13" morph="none" pos="word" start_char="145">el</TOKEN>
<TOKEN end_char="154" id="token-1-14" morph="none" pos="word" start_char="148">sureste</TOKEN>
<TOKEN end_char="163" id="token-1-15" morph="none" pos="word" start_char="156">asiático</TOKEN>
<TOKEN end_char="164" id="token-1-16" morph="none" pos="punct" start_char="164">,</TOKEN>
<TOKEN end_char="167" id="token-1-17" morph="none" pos="word" start_char="166">se</TOKEN>
<TOKEN end_char="171" id="token-1-18" morph="none" pos="word" start_char="169">han</TOKEN>
<TOKEN end_char="178" id="token-1-19" morph="none" pos="word" start_char="173">vuelto</TOKEN>
<TOKEN end_char="182" id="token-1-20" morph="none" pos="word" start_char="180">los</TOKEN>
<TOKEN end_char="194" id="token-1-21" morph="none" pos="word" start_char="184">principales</TOKEN>
<TOKEN end_char="206" id="token-1-22" morph="none" pos="word" start_char="196">sospechosos</TOKEN>
<TOKEN end_char="210" id="token-1-23" morph="none" pos="word" start_char="208">del</TOKEN>
<TOKEN end_char="217" id="token-1-24" morph="none" pos="word" start_char="212">origen</TOKEN>
<TOKEN end_char="221" id="token-1-25" morph="none" pos="word" start_char="219">del</TOKEN>
<TOKEN end_char="233" id="token-1-26" morph="none" pos="word" start_char="223">coronavirus</TOKEN>
<TOKEN end_char="234" id="token-1-27" morph="none" pos="punct" start_char="234">,</TOKEN>
<TOKEN end_char="241" id="token-1-28" morph="none" pos="word" start_char="236">debido</TOKEN>
<TOKEN end_char="243" id="token-1-29" morph="none" pos="word" start_char="243">a</TOKEN>
<TOKEN end_char="246" id="token-1-30" morph="none" pos="word" start_char="245">un</TOKEN>
<TOKEN end_char="255" id="token-1-31" morph="none" pos="word" start_char="248">análisis</TOKEN>
<TOKEN end_char="267" id="token-1-32" morph="none" pos="word" start_char="257">comparativo</TOKEN>
<TOKEN end_char="271" id="token-1-33" morph="none" pos="word" start_char="269">del</TOKEN>
<TOKEN end_char="278" id="token-1-34" morph="none" pos="word" start_char="273">genoma</TOKEN>
<TOKEN end_char="282" id="token-1-35" morph="none" pos="word" start_char="280">del</TOKEN>
<TOKEN end_char="288" id="token-1-36" morph="none" pos="word" start_char="284">virus</TOKEN>
<TOKEN end_char="289" id="token-1-37" morph="none" pos="punct" start_char="289">,</TOKEN>
<TOKEN end_char="295" id="token-1-38" morph="none" pos="word" start_char="291">hecho</TOKEN>
<TOKEN end_char="298" id="token-1-39" morph="none" pos="word" start_char="297">en</TOKEN>
<TOKEN end_char="305" id="token-1-40" morph="none" pos="word" start_char="300">Italia</TOKEN>
<TOKEN end_char="307" id="token-1-41" morph="none" pos="word" start_char="307">y</TOKEN>
<TOKEN end_char="317" id="token-1-42" morph="none" pos="word" start_char="309">publicado</TOKEN>
<TOKEN end_char="320" id="token-1-43" morph="none" pos="word" start_char="319">en</TOKEN>
<TOKEN end_char="323" id="token-1-44" morph="none" pos="word" start_char="322">el</TOKEN>
<TOKEN end_char="331" id="token-1-45" morph="none" pos="word" start_char="325">Journal</TOKEN>
<TOKEN end_char="334" id="token-1-46" morph="none" pos="word" start_char="333">of</TOKEN>
<TOKEN end_char="342" id="token-1-47" morph="none" pos="word" start_char="336">Medical</TOKEN>
<TOKEN end_char="351" id="token-1-48" morph="none" pos="word" start_char="344">Virology</TOKEN>
<TOKEN end_char="352" id="token-1-49" morph="none" pos="punct" start_char="352">.</TOKEN>
<TRANSLATED_TEXT>Bats of the species Rhinolophus Affinis, very common in China and Southeast Asia, have become the main suspects of the origin of coronavirus, due to a comparative analysis of the virus genome, done in Italy and published in the Journal of Medical Virology.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="521" id="segment-2" start_char="355">
<ORIGINAL_TEXT>Los Rhinolophus Affini ya eran sospechosos desde el 23 de enero, cuando el Instituto de Virología de Wuhan lanzó el primer identikit genético de coronavirus 2019-nCoV.</ORIGINAL_TEXT>
<TOKEN end_char="357" id="token-2-0" morph="none" pos="word" start_char="355">Los</TOKEN>
<TOKEN end_char="369" id="token-2-1" morph="none" pos="word" start_char="359">Rhinolophus</TOKEN>
<TOKEN end_char="376" id="token-2-2" morph="none" pos="word" start_char="371">Affini</TOKEN>
<TOKEN end_char="379" id="token-2-3" morph="none" pos="word" start_char="378">ya</TOKEN>
<TOKEN end_char="384" id="token-2-4" morph="none" pos="word" start_char="381">eran</TOKEN>
<TOKEN end_char="396" id="token-2-5" morph="none" pos="word" start_char="386">sospechosos</TOKEN>
<TOKEN end_char="402" id="token-2-6" morph="none" pos="word" start_char="398">desde</TOKEN>
<TOKEN end_char="405" id="token-2-7" morph="none" pos="word" start_char="404">el</TOKEN>
<TOKEN end_char="408" id="token-2-8" morph="none" pos="word" start_char="407">23</TOKEN>
<TOKEN end_char="411" id="token-2-9" morph="none" pos="word" start_char="410">de</TOKEN>
<TOKEN end_char="417" id="token-2-10" morph="none" pos="word" start_char="413">enero</TOKEN>
<TOKEN end_char="418" id="token-2-11" morph="none" pos="punct" start_char="418">,</TOKEN>
<TOKEN end_char="425" id="token-2-12" morph="none" pos="word" start_char="420">cuando</TOKEN>
<TOKEN end_char="428" id="token-2-13" morph="none" pos="word" start_char="427">el</TOKEN>
<TOKEN end_char="438" id="token-2-14" morph="none" pos="word" start_char="430">Instituto</TOKEN>
<TOKEN end_char="441" id="token-2-15" morph="none" pos="word" start_char="440">de</TOKEN>
<TOKEN end_char="451" id="token-2-16" morph="none" pos="word" start_char="443">Virología</TOKEN>
<TOKEN end_char="454" id="token-2-17" morph="none" pos="word" start_char="453">de</TOKEN>
<TOKEN end_char="460" id="token-2-18" morph="none" pos="word" start_char="456">Wuhan</TOKEN>
<TOKEN end_char="466" id="token-2-19" morph="none" pos="word" start_char="462">lanzó</TOKEN>
<TOKEN end_char="469" id="token-2-20" morph="none" pos="word" start_char="468">el</TOKEN>
<TOKEN end_char="476" id="token-2-21" morph="none" pos="word" start_char="471">primer</TOKEN>
<TOKEN end_char="486" id="token-2-22" morph="none" pos="word" start_char="478">identikit</TOKEN>
<TOKEN end_char="495" id="token-2-23" morph="none" pos="word" start_char="488">genético</TOKEN>
<TOKEN end_char="498" id="token-2-24" morph="none" pos="word" start_char="497">de</TOKEN>
<TOKEN end_char="510" id="token-2-25" morph="none" pos="word" start_char="500">coronavirus</TOKEN>
<TOKEN end_char="520" id="token-2-26" morph="none" pos="unknown" start_char="512">2019-nCoV</TOKEN>
<TOKEN end_char="521" id="token-2-27" morph="none" pos="punct" start_char="521">.</TOKEN>
<TRANSLATED_TEXT>The Rhinolophus Affini have been suspected since January 23, when the Wuhan Institute of Virology launched the first coronavirus genetic identikit 2019-nCoV.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="867" id="segment-3" start_char="524">
<ORIGINAL_TEXT>"El domingo pasado descargamos los seis genomas de coronavirus contenido en las bases de datos Gisaid y Genbank y buscamos secuencias similares en bases a datos públicos", dijo el coordinador de la investigación, el experto en bioinformática Federico Giorgi, del Departamento de Farmacia y Biotecnología de Universidad de Bologna, informó ANSA.</ORIGINAL_TEXT>
<TOKEN end_char="524" id="token-3-0" morph="none" pos="punct" start_char="524">"</TOKEN>
<TOKEN end_char="526" id="token-3-1" morph="none" pos="word" start_char="525">El</TOKEN>
<TOKEN end_char="534" id="token-3-2" morph="none" pos="word" start_char="528">domingo</TOKEN>
<TOKEN end_char="541" id="token-3-3" morph="none" pos="word" start_char="536">pasado</TOKEN>
<TOKEN end_char="553" id="token-3-4" morph="none" pos="word" start_char="543">descargamos</TOKEN>
<TOKEN end_char="557" id="token-3-5" morph="none" pos="word" start_char="555">los</TOKEN>
<TOKEN end_char="562" id="token-3-6" morph="none" pos="word" start_char="559">seis</TOKEN>
<TOKEN end_char="570" id="token-3-7" morph="none" pos="word" start_char="564">genomas</TOKEN>
<TOKEN end_char="573" id="token-3-8" morph="none" pos="word" start_char="572">de</TOKEN>
<TOKEN end_char="585" id="token-3-9" morph="none" pos="word" start_char="575">coronavirus</TOKEN>
<TOKEN end_char="595" id="token-3-10" morph="none" pos="word" start_char="587">contenido</TOKEN>
<TOKEN end_char="598" id="token-3-11" morph="none" pos="word" start_char="597">en</TOKEN>
<TOKEN end_char="602" id="token-3-12" morph="none" pos="word" start_char="600">las</TOKEN>
<TOKEN end_char="608" id="token-3-13" morph="none" pos="word" start_char="604">bases</TOKEN>
<TOKEN end_char="611" id="token-3-14" morph="none" pos="word" start_char="610">de</TOKEN>
<TOKEN end_char="617" id="token-3-15" morph="none" pos="word" start_char="613">datos</TOKEN>
<TOKEN end_char="624" id="token-3-16" morph="none" pos="word" start_char="619">Gisaid</TOKEN>
<TOKEN end_char="626" id="token-3-17" morph="none" pos="word" start_char="626">y</TOKEN>
<TOKEN end_char="634" id="token-3-18" morph="none" pos="word" start_char="628">Genbank</TOKEN>
<TOKEN end_char="636" id="token-3-19" morph="none" pos="word" start_char="636">y</TOKEN>
<TOKEN end_char="645" id="token-3-20" morph="none" pos="word" start_char="638">buscamos</TOKEN>
<TOKEN end_char="656" id="token-3-21" morph="none" pos="word" start_char="647">secuencias</TOKEN>
<TOKEN end_char="666" id="token-3-22" morph="none" pos="word" start_char="658">similares</TOKEN>
<TOKEN end_char="669" id="token-3-23" morph="none" pos="word" start_char="668">en</TOKEN>
<TOKEN end_char="675" id="token-3-24" morph="none" pos="word" start_char="671">bases</TOKEN>
<TOKEN end_char="677" id="token-3-25" morph="none" pos="word" start_char="677">a</TOKEN>
<TOKEN end_char="683" id="token-3-26" morph="none" pos="word" start_char="679">datos</TOKEN>
<TOKEN end_char="692" id="token-3-27" morph="none" pos="word" start_char="685">públicos</TOKEN>
<TOKEN end_char="694" id="token-3-28" morph="none" pos="punct" start_char="693">",</TOKEN>
<TOKEN end_char="699" id="token-3-29" morph="none" pos="word" start_char="696">dijo</TOKEN>
<TOKEN end_char="702" id="token-3-30" morph="none" pos="word" start_char="701">el</TOKEN>
<TOKEN end_char="714" id="token-3-31" morph="none" pos="word" start_char="704">coordinador</TOKEN>
<TOKEN end_char="717" id="token-3-32" morph="none" pos="word" start_char="716">de</TOKEN>
<TOKEN end_char="720" id="token-3-33" morph="none" pos="word" start_char="719">la</TOKEN>
<TOKEN end_char="734" id="token-3-34" morph="none" pos="word" start_char="722">investigación</TOKEN>
<TOKEN end_char="735" id="token-3-35" morph="none" pos="punct" start_char="735">,</TOKEN>
<TOKEN end_char="738" id="token-3-36" morph="none" pos="word" start_char="737">el</TOKEN>
<TOKEN end_char="746" id="token-3-37" morph="none" pos="word" start_char="740">experto</TOKEN>
<TOKEN end_char="749" id="token-3-38" morph="none" pos="word" start_char="748">en</TOKEN>
<TOKEN end_char="764" id="token-3-39" morph="none" pos="word" start_char="751">bioinformática</TOKEN>
<TOKEN end_char="773" id="token-3-40" morph="none" pos="word" start_char="766">Federico</TOKEN>
<TOKEN end_char="780" id="token-3-41" morph="none" pos="word" start_char="775">Giorgi</TOKEN>
<TOKEN end_char="781" id="token-3-42" morph="none" pos="punct" start_char="781">,</TOKEN>
<TOKEN end_char="785" id="token-3-43" morph="none" pos="word" start_char="783">del</TOKEN>
<TOKEN end_char="798" id="token-3-44" morph="none" pos="word" start_char="787">Departamento</TOKEN>
<TOKEN end_char="801" id="token-3-45" morph="none" pos="word" start_char="800">de</TOKEN>
<TOKEN end_char="810" id="token-3-46" morph="none" pos="word" start_char="803">Farmacia</TOKEN>
<TOKEN end_char="812" id="token-3-47" morph="none" pos="word" start_char="812">y</TOKEN>
<TOKEN end_char="826" id="token-3-48" morph="none" pos="word" start_char="814">Biotecnología</TOKEN>
<TOKEN end_char="829" id="token-3-49" morph="none" pos="word" start_char="828">de</TOKEN>
<TOKEN end_char="841" id="token-3-50" morph="none" pos="word" start_char="831">Universidad</TOKEN>
<TOKEN end_char="844" id="token-3-51" morph="none" pos="word" start_char="843">de</TOKEN>
<TOKEN end_char="852" id="token-3-52" morph="none" pos="word" start_char="846">Bologna</TOKEN>
<TOKEN end_char="853" id="token-3-53" morph="none" pos="punct" start_char="853">,</TOKEN>
<TOKEN end_char="861" id="token-3-54" morph="none" pos="word" start_char="855">informó</TOKEN>
<TOKEN end_char="866" id="token-3-55" morph="none" pos="word" start_char="863">ANSA</TOKEN>
<TOKEN end_char="867" id="token-3-56" morph="none" pos="punct" start_char="867">.</TOKEN>
<TRANSLATED_TEXT>"Last Sunday we downloaded the six coronavirus genomes contained in the Gisaid and Genbank databases and looked for similar sequences in public databases," said the research coordinator, bioinformatics expert Federico Giorgi, of the Department of Pharmacy and Biotechnology of the University of Bologna, ANSA reported.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1003" id="segment-4" start_char="870">
<ORIGINAL_TEXT>"El virus no es muy heterogéneo y cambiante" y eso implica que "es posible una terapia farmacológica, que debería funcionar en todos".</ORIGINAL_TEXT>
<TOKEN end_char="870" id="token-4-0" morph="none" pos="punct" start_char="870">"</TOKEN>
<TOKEN end_char="872" id="token-4-1" morph="none" pos="word" start_char="871">El</TOKEN>
<TOKEN end_char="878" id="token-4-2" morph="none" pos="word" start_char="874">virus</TOKEN>
<TOKEN end_char="881" id="token-4-3" morph="none" pos="word" start_char="880">no</TOKEN>
<TOKEN end_char="884" id="token-4-4" morph="none" pos="word" start_char="883">es</TOKEN>
<TOKEN end_char="888" id="token-4-5" morph="none" pos="word" start_char="886">muy</TOKEN>
<TOKEN end_char="900" id="token-4-6" morph="none" pos="word" start_char="890">heterogéneo</TOKEN>
<TOKEN end_char="902" id="token-4-7" morph="none" pos="word" start_char="902">y</TOKEN>
<TOKEN end_char="912" id="token-4-8" morph="none" pos="word" start_char="904">cambiante</TOKEN>
<TOKEN end_char="913" id="token-4-9" morph="none" pos="punct" start_char="913">"</TOKEN>
<TOKEN end_char="915" id="token-4-10" morph="none" pos="word" start_char="915">y</TOKEN>
<TOKEN end_char="919" id="token-4-11" morph="none" pos="word" start_char="917">eso</TOKEN>
<TOKEN end_char="927" id="token-4-12" morph="none" pos="word" start_char="921">implica</TOKEN>
<TOKEN end_char="931" id="token-4-13" morph="none" pos="word" start_char="929">que</TOKEN>
<TOKEN end_char="933" id="token-4-14" morph="none" pos="punct" start_char="933">"</TOKEN>
<TOKEN end_char="935" id="token-4-15" morph="none" pos="word" start_char="934">es</TOKEN>
<TOKEN end_char="943" id="token-4-16" morph="none" pos="word" start_char="937">posible</TOKEN>
<TOKEN end_char="947" id="token-4-17" morph="none" pos="word" start_char="945">una</TOKEN>
<TOKEN end_char="955" id="token-4-18" morph="none" pos="word" start_char="949">terapia</TOKEN>
<TOKEN end_char="969" id="token-4-19" morph="none" pos="word" start_char="957">farmacológica</TOKEN>
<TOKEN end_char="970" id="token-4-20" morph="none" pos="punct" start_char="970">,</TOKEN>
<TOKEN end_char="974" id="token-4-21" morph="none" pos="word" start_char="972">que</TOKEN>
<TOKEN end_char="982" id="token-4-22" morph="none" pos="word" start_char="976">debería</TOKEN>
<TOKEN end_char="992" id="token-4-23" morph="none" pos="word" start_char="984">funcionar</TOKEN>
<TOKEN end_char="995" id="token-4-24" morph="none" pos="word" start_char="994">en</TOKEN>
<TOKEN end_char="1001" id="token-4-25" morph="none" pos="word" start_char="997">todos</TOKEN>
<TOKEN end_char="1003" id="token-4-26" morph="none" pos="punct" start_char="1002">".</TOKEN>
<TRANSLATED_TEXT>"The virus is not very heterogeneous and changeable" and that implies that "a pharmacological therapy is possible, which should work in everyone."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1285" id="segment-5" start_char="1007">
<ORIGINAL_TEXT>El análisis también indica que el virus está cambiando muy lentamente porque todos los coronavirus humanos secuenciados hasta ahora "son muchos, similares entre sí, incluso si provienen de diferentes regiones de China y del mundo", hasta el punto de superponerse a más de un 99%.</ORIGINAL_TEXT>
<TOKEN end_char="1008" id="token-5-0" morph="none" pos="word" start_char="1007">El</TOKEN>
<TOKEN end_char="1017" id="token-5-1" morph="none" pos="word" start_char="1010">análisis</TOKEN>
<TOKEN end_char="1025" id="token-5-2" morph="none" pos="word" start_char="1019">también</TOKEN>
<TOKEN end_char="1032" id="token-5-3" morph="none" pos="word" start_char="1027">indica</TOKEN>
<TOKEN end_char="1036" id="token-5-4" morph="none" pos="word" start_char="1034">que</TOKEN>
<TOKEN end_char="1039" id="token-5-5" morph="none" pos="word" start_char="1038">el</TOKEN>
<TOKEN end_char="1045" id="token-5-6" morph="none" pos="word" start_char="1041">virus</TOKEN>
<TOKEN end_char="1050" id="token-5-7" morph="none" pos="word" start_char="1047">está</TOKEN>
<TOKEN end_char="1060" id="token-5-8" morph="none" pos="word" start_char="1052">cambiando</TOKEN>
<TOKEN end_char="1064" id="token-5-9" morph="none" pos="word" start_char="1062">muy</TOKEN>
<TOKEN end_char="1075" id="token-5-10" morph="none" pos="word" start_char="1066">lentamente</TOKEN>
<TOKEN end_char="1082" id="token-5-11" morph="none" pos="word" start_char="1077">porque</TOKEN>
<TOKEN end_char="1088" id="token-5-12" morph="none" pos="word" start_char="1084">todos</TOKEN>
<TOKEN end_char="1092" id="token-5-13" morph="none" pos="word" start_char="1090">los</TOKEN>
<TOKEN end_char="1104" id="token-5-14" morph="none" pos="word" start_char="1094">coronavirus</TOKEN>
<TOKEN end_char="1112" id="token-5-15" morph="none" pos="word" start_char="1106">humanos</TOKEN>
<TOKEN end_char="1125" id="token-5-16" morph="none" pos="word" start_char="1114">secuenciados</TOKEN>
<TOKEN end_char="1131" id="token-5-17" morph="none" pos="word" start_char="1127">hasta</TOKEN>
<TOKEN end_char="1137" id="token-5-18" morph="none" pos="word" start_char="1133">ahora</TOKEN>
<TOKEN end_char="1139" id="token-5-19" morph="none" pos="punct" start_char="1139">"</TOKEN>
<TOKEN end_char="1142" id="token-5-20" morph="none" pos="word" start_char="1140">son</TOKEN>
<TOKEN end_char="1149" id="token-5-21" morph="none" pos="word" start_char="1144">muchos</TOKEN>
<TOKEN end_char="1150" id="token-5-22" morph="none" pos="punct" start_char="1150">,</TOKEN>
<TOKEN end_char="1160" id="token-5-23" morph="none" pos="word" start_char="1152">similares</TOKEN>
<TOKEN end_char="1166" id="token-5-24" morph="none" pos="word" start_char="1162">entre</TOKEN>
<TOKEN end_char="1169" id="token-5-25" morph="none" pos="word" start_char="1168">sí</TOKEN>
<TOKEN end_char="1170" id="token-5-26" morph="none" pos="punct" start_char="1170">,</TOKEN>
<TOKEN end_char="1178" id="token-5-27" morph="none" pos="word" start_char="1172">incluso</TOKEN>
<TOKEN end_char="1181" id="token-5-28" morph="none" pos="word" start_char="1180">si</TOKEN>
<TOKEN end_char="1191" id="token-5-29" morph="none" pos="word" start_char="1183">provienen</TOKEN>
<TOKEN end_char="1194" id="token-5-30" morph="none" pos="word" start_char="1193">de</TOKEN>
<TOKEN end_char="1205" id="token-5-31" morph="none" pos="word" start_char="1196">diferentes</TOKEN>
<TOKEN end_char="1214" id="token-5-32" morph="none" pos="word" start_char="1207">regiones</TOKEN>
<TOKEN end_char="1217" id="token-5-33" morph="none" pos="word" start_char="1216">de</TOKEN>
<TOKEN end_char="1223" id="token-5-34" morph="none" pos="word" start_char="1219">China</TOKEN>
<TOKEN end_char="1225" id="token-5-35" morph="none" pos="word" start_char="1225">y</TOKEN>
<TOKEN end_char="1229" id="token-5-36" morph="none" pos="word" start_char="1227">del</TOKEN>
<TOKEN end_char="1235" id="token-5-37" morph="none" pos="word" start_char="1231">mundo</TOKEN>
<TOKEN end_char="1237" id="token-5-38" morph="none" pos="punct" start_char="1236">",</TOKEN>
<TOKEN end_char="1243" id="token-5-39" morph="none" pos="word" start_char="1239">hasta</TOKEN>
<TOKEN end_char="1246" id="token-5-40" morph="none" pos="word" start_char="1245">el</TOKEN>
<TOKEN end_char="1252" id="token-5-41" morph="none" pos="word" start_char="1248">punto</TOKEN>
<TOKEN end_char="1255" id="token-5-42" morph="none" pos="word" start_char="1254">de</TOKEN>
<TOKEN end_char="1268" id="token-5-43" morph="none" pos="word" start_char="1257">superponerse</TOKEN>
<TOKEN end_char="1270" id="token-5-44" morph="none" pos="word" start_char="1270">a</TOKEN>
<TOKEN end_char="1274" id="token-5-45" morph="none" pos="word" start_char="1272">más</TOKEN>
<TOKEN end_char="1277" id="token-5-46" morph="none" pos="word" start_char="1276">de</TOKEN>
<TOKEN end_char="1280" id="token-5-47" morph="none" pos="word" start_char="1279">un</TOKEN>
<TOKEN end_char="1283" id="token-5-48" morph="none" pos="word" start_char="1282">99</TOKEN>
<TOKEN end_char="1285" id="token-5-49" morph="none" pos="punct" start_char="1284">%.</TOKEN>
<TRANSLATED_TEXT>The analysis also indicates that the virus is changing very slowly because all human coronaviruses sequenced so far "are many, similar to each other, even if they come from different regions of China and the world," to the point of exceeding 99%.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1456" id="segment-6" start_char="1288">
<ORIGINAL_TEXT>Esto significa, señaló Giorgi, que "el virus no es muy heterogéneo y cambiante" y eso implica que "es posible una terapia farmacológica, que debería funcionar en todos".</ORIGINAL_TEXT>
<TOKEN end_char="1291" id="token-6-0" morph="none" pos="word" start_char="1288">Esto</TOKEN>
<TOKEN end_char="1301" id="token-6-1" morph="none" pos="word" start_char="1293">significa</TOKEN>
<TOKEN end_char="1302" id="token-6-2" morph="none" pos="punct" start_char="1302">,</TOKEN>
<TOKEN end_char="1309" id="token-6-3" morph="none" pos="word" start_char="1304">señaló</TOKEN>
<TOKEN end_char="1316" id="token-6-4" morph="none" pos="word" start_char="1311">Giorgi</TOKEN>
<TOKEN end_char="1317" id="token-6-5" morph="none" pos="punct" start_char="1317">,</TOKEN>
<TOKEN end_char="1321" id="token-6-6" morph="none" pos="word" start_char="1319">que</TOKEN>
<TOKEN end_char="1323" id="token-6-7" morph="none" pos="punct" start_char="1323">"</TOKEN>
<TOKEN end_char="1325" id="token-6-8" morph="none" pos="word" start_char="1324">el</TOKEN>
<TOKEN end_char="1331" id="token-6-9" morph="none" pos="word" start_char="1327">virus</TOKEN>
<TOKEN end_char="1334" id="token-6-10" morph="none" pos="word" start_char="1333">no</TOKEN>
<TOKEN end_char="1337" id="token-6-11" morph="none" pos="word" start_char="1336">es</TOKEN>
<TOKEN end_char="1341" id="token-6-12" morph="none" pos="word" start_char="1339">muy</TOKEN>
<TOKEN end_char="1353" id="token-6-13" morph="none" pos="word" start_char="1343">heterogéneo</TOKEN>
<TOKEN end_char="1355" id="token-6-14" morph="none" pos="word" start_char="1355">y</TOKEN>
<TOKEN end_char="1365" id="token-6-15" morph="none" pos="word" start_char="1357">cambiante</TOKEN>
<TOKEN end_char="1366" id="token-6-16" morph="none" pos="punct" start_char="1366">"</TOKEN>
<TOKEN end_char="1368" id="token-6-17" morph="none" pos="word" start_char="1368">y</TOKEN>
<TOKEN end_char="1372" id="token-6-18" morph="none" pos="word" start_char="1370">eso</TOKEN>
<TOKEN end_char="1380" id="token-6-19" morph="none" pos="word" start_char="1374">implica</TOKEN>
<TOKEN end_char="1384" id="token-6-20" morph="none" pos="word" start_char="1382">que</TOKEN>
<TOKEN end_char="1386" id="token-6-21" morph="none" pos="punct" start_char="1386">"</TOKEN>
<TOKEN end_char="1388" id="token-6-22" morph="none" pos="word" start_char="1387">es</TOKEN>
<TOKEN end_char="1396" id="token-6-23" morph="none" pos="word" start_char="1390">posible</TOKEN>
<TOKEN end_char="1400" id="token-6-24" morph="none" pos="word" start_char="1398">una</TOKEN>
<TOKEN end_char="1408" id="token-6-25" morph="none" pos="word" start_char="1402">terapia</TOKEN>
<TOKEN end_char="1422" id="token-6-26" morph="none" pos="word" start_char="1410">farmacológica</TOKEN>
<TOKEN end_char="1423" id="token-6-27" morph="none" pos="punct" start_char="1423">,</TOKEN>
<TOKEN end_char="1427" id="token-6-28" morph="none" pos="word" start_char="1425">que</TOKEN>
<TOKEN end_char="1435" id="token-6-29" morph="none" pos="word" start_char="1429">debería</TOKEN>
<TOKEN end_char="1445" id="token-6-30" morph="none" pos="word" start_char="1437">funcionar</TOKEN>
<TOKEN end_char="1448" id="token-6-31" morph="none" pos="word" start_char="1447">en</TOKEN>
<TOKEN end_char="1454" id="token-6-32" morph="none" pos="word" start_char="1450">todos</TOKEN>
<TOKEN end_char="1456" id="token-6-33" morph="none" pos="punct" start_char="1455">".</TOKEN>
<TRANSLATED_TEXT>This means, Giorgi noted, that "the virus is not very heterogeneous and changing" and that implies that "a pharmacological therapy is possible, which should work in all."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1547" id="segment-7" start_char="1459">
<ORIGINAL_TEXT>Igualmente, el material genético del coronavirus muestra "un alto grado de variabilidad".</ORIGINAL_TEXT>
<TOKEN end_char="1468" id="token-7-0" morph="none" pos="word" start_char="1459">Igualmente</TOKEN>
<TOKEN end_char="1469" id="token-7-1" morph="none" pos="punct" start_char="1469">,</TOKEN>
<TOKEN end_char="1472" id="token-7-2" morph="none" pos="word" start_char="1471">el</TOKEN>
<TOKEN end_char="1481" id="token-7-3" morph="none" pos="word" start_char="1474">material</TOKEN>
<TOKEN end_char="1490" id="token-7-4" morph="none" pos="word" start_char="1483">genético</TOKEN>
<TOKEN end_char="1494" id="token-7-5" morph="none" pos="word" start_char="1492">del</TOKEN>
<TOKEN end_char="1506" id="token-7-6" morph="none" pos="word" start_char="1496">coronavirus</TOKEN>
<TOKEN end_char="1514" id="token-7-7" morph="none" pos="word" start_char="1508">muestra</TOKEN>
<TOKEN end_char="1516" id="token-7-8" morph="none" pos="punct" start_char="1516">"</TOKEN>
<TOKEN end_char="1518" id="token-7-9" morph="none" pos="word" start_char="1517">un</TOKEN>
<TOKEN end_char="1523" id="token-7-10" morph="none" pos="word" start_char="1520">alto</TOKEN>
<TOKEN end_char="1529" id="token-7-11" morph="none" pos="word" start_char="1525">grado</TOKEN>
<TOKEN end_char="1532" id="token-7-12" morph="none" pos="word" start_char="1531">de</TOKEN>
<TOKEN end_char="1545" id="token-7-13" morph="none" pos="word" start_char="1534">variabilidad</TOKEN>
<TOKEN end_char="1547" id="token-7-14" morph="none" pos="punct" start_char="1546">".</TOKEN>
<TRANSLATED_TEXT>Similarly, the genetic material of coronavirus shows "a high degree of variability."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1592" id="segment-8" start_char="1549">
<ORIGINAL_TEXT>Aprender más es el próximo objetivo, agregó.</ORIGINAL_TEXT>
<TOKEN end_char="1556" id="token-8-0" morph="none" pos="word" start_char="1549">Aprender</TOKEN>
<TOKEN end_char="1560" id="token-8-1" morph="none" pos="word" start_char="1558">más</TOKEN>
<TOKEN end_char="1563" id="token-8-2" morph="none" pos="word" start_char="1562">es</TOKEN>
<TOKEN end_char="1566" id="token-8-3" morph="none" pos="word" start_char="1565">el</TOKEN>
<TOKEN end_char="1574" id="token-8-4" morph="none" pos="word" start_char="1568">próximo</TOKEN>
<TOKEN end_char="1583" id="token-8-5" morph="none" pos="word" start_char="1576">objetivo</TOKEN>
<TOKEN end_char="1584" id="token-8-6" morph="none" pos="punct" start_char="1584">,</TOKEN>
<TOKEN end_char="1591" id="token-8-7" morph="none" pos="word" start_char="1586">agregó</TOKEN>
<TOKEN end_char="1592" id="token-8-8" morph="none" pos="punct" start_char="1592">.</TOKEN>
<TRANSLATED_TEXT>Learning more is the next goal, he added.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1661" id="segment-9" start_char="1595">
<ORIGINAL_TEXT>No hay idea de como el animal podría haberlo transmitido al hombre.</ORIGINAL_TEXT>
<TOKEN end_char="1596" id="token-9-0" morph="none" pos="word" start_char="1595">No</TOKEN>
<TOKEN end_char="1600" id="token-9-1" morph="none" pos="word" start_char="1598">hay</TOKEN>
<TOKEN end_char="1605" id="token-9-2" morph="none" pos="word" start_char="1602">idea</TOKEN>
<TOKEN end_char="1608" id="token-9-3" morph="none" pos="word" start_char="1607">de</TOKEN>
<TOKEN end_char="1613" id="token-9-4" morph="none" pos="word" start_char="1610">como</TOKEN>
<TOKEN end_char="1616" id="token-9-5" morph="none" pos="word" start_char="1615">el</TOKEN>
<TOKEN end_char="1623" id="token-9-6" morph="none" pos="word" start_char="1618">animal</TOKEN>
<TOKEN end_char="1630" id="token-9-7" morph="none" pos="word" start_char="1625">podría</TOKEN>
<TOKEN end_char="1638" id="token-9-8" morph="none" pos="word" start_char="1632">haberlo</TOKEN>
<TOKEN end_char="1650" id="token-9-9" morph="none" pos="word" start_char="1640">transmitido</TOKEN>
<TOKEN end_char="1653" id="token-9-10" morph="none" pos="word" start_char="1652">al</TOKEN>
<TOKEN end_char="1660" id="token-9-11" morph="none" pos="word" start_char="1655">hombre</TOKEN>
<TOKEN end_char="1661" id="token-9-12" morph="none" pos="punct" start_char="1661">.</TOKEN>
<TRANSLATED_TEXT>There is no idea how the animal could have transmitted it to man.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1728" id="segment-10" start_char="1663">
<ORIGINAL_TEXT>Acaso, por intermedio de otro animal, o por una mordedura directa.</ORIGINAL_TEXT>
<TOKEN end_char="1667" id="token-10-0" morph="none" pos="word" start_char="1663">Acaso</TOKEN>
<TOKEN end_char="1668" id="token-10-1" morph="none" pos="punct" start_char="1668">,</TOKEN>
<TOKEN end_char="1672" id="token-10-2" morph="none" pos="word" start_char="1670">por</TOKEN>
<TOKEN end_char="1683" id="token-10-3" morph="none" pos="word" start_char="1674">intermedio</TOKEN>
<TOKEN end_char="1686" id="token-10-4" morph="none" pos="word" start_char="1685">de</TOKEN>
<TOKEN end_char="1691" id="token-10-5" morph="none" pos="word" start_char="1688">otro</TOKEN>
<TOKEN end_char="1698" id="token-10-6" morph="none" pos="word" start_char="1693">animal</TOKEN>
<TOKEN end_char="1699" id="token-10-7" morph="none" pos="punct" start_char="1699">,</TOKEN>
<TOKEN end_char="1701" id="token-10-8" morph="none" pos="word" start_char="1701">o</TOKEN>
<TOKEN end_char="1705" id="token-10-9" morph="none" pos="word" start_char="1703">por</TOKEN>
<TOKEN end_char="1709" id="token-10-10" morph="none" pos="word" start_char="1707">una</TOKEN>
<TOKEN end_char="1719" id="token-10-11" morph="none" pos="word" start_char="1711">mordedura</TOKEN>
<TOKEN end_char="1727" id="token-10-12" morph="none" pos="word" start_char="1721">directa</TOKEN>
<TOKEN end_char="1728" id="token-10-13" morph="none" pos="punct" start_char="1728">.</TOKEN>
<TRANSLATED_TEXT>Here, by another animal, or by a direct bite.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1970" id="segment-11" start_char="1732">
<ORIGINAL_TEXT>El equipo de Bologna también descubrió que el genoma del coronavirus en humanos comparte el 96,2% de sus activos genéticos con el del murciélago Rhinolophus Affinis, cuya secuencia se había obtenido en 2013 en la provincia china de Yunnan.</ORIGINAL_TEXT>
<TOKEN end_char="1733" id="token-11-0" morph="none" pos="word" start_char="1732">El</TOKEN>
<TOKEN end_char="1740" id="token-11-1" morph="none" pos="word" start_char="1735">equipo</TOKEN>
<TOKEN end_char="1743" id="token-11-2" morph="none" pos="word" start_char="1742">de</TOKEN>
<TOKEN end_char="1751" id="token-11-3" morph="none" pos="word" start_char="1745">Bologna</TOKEN>
<TOKEN end_char="1759" id="token-11-4" morph="none" pos="word" start_char="1753">también</TOKEN>
<TOKEN end_char="1769" id="token-11-5" morph="none" pos="word" start_char="1761">descubrió</TOKEN>
<TOKEN end_char="1773" id="token-11-6" morph="none" pos="word" start_char="1771">que</TOKEN>
<TOKEN end_char="1776" id="token-11-7" morph="none" pos="word" start_char="1775">el</TOKEN>
<TOKEN end_char="1783" id="token-11-8" morph="none" pos="word" start_char="1778">genoma</TOKEN>
<TOKEN end_char="1787" id="token-11-9" morph="none" pos="word" start_char="1785">del</TOKEN>
<TOKEN end_char="1799" id="token-11-10" morph="none" pos="word" start_char="1789">coronavirus</TOKEN>
<TOKEN end_char="1802" id="token-11-11" morph="none" pos="word" start_char="1801">en</TOKEN>
<TOKEN end_char="1810" id="token-11-12" morph="none" pos="word" start_char="1804">humanos</TOKEN>
<TOKEN end_char="1819" id="token-11-13" morph="none" pos="word" start_char="1812">comparte</TOKEN>
<TOKEN end_char="1822" id="token-11-14" morph="none" pos="word" start_char="1821">el</TOKEN>
<TOKEN end_char="1827" id="token-11-15" morph="none" pos="unknown" start_char="1824">96,2</TOKEN>
<TOKEN end_char="1828" id="token-11-16" morph="none" pos="punct" start_char="1828">%</TOKEN>
<TOKEN end_char="1831" id="token-11-17" morph="none" pos="word" start_char="1830">de</TOKEN>
<TOKEN end_char="1835" id="token-11-18" morph="none" pos="word" start_char="1833">sus</TOKEN>
<TOKEN end_char="1843" id="token-11-19" morph="none" pos="word" start_char="1837">activos</TOKEN>
<TOKEN end_char="1853" id="token-11-20" morph="none" pos="word" start_char="1845">genéticos</TOKEN>
<TOKEN end_char="1857" id="token-11-21" morph="none" pos="word" start_char="1855">con</TOKEN>
<TOKEN end_char="1860" id="token-11-22" morph="none" pos="word" start_char="1859">el</TOKEN>
<TOKEN end_char="1864" id="token-11-23" morph="none" pos="word" start_char="1862">del</TOKEN>
<TOKEN end_char="1875" id="token-11-24" morph="none" pos="word" start_char="1866">murciélago</TOKEN>
<TOKEN end_char="1887" id="token-11-25" morph="none" pos="word" start_char="1877">Rhinolophus</TOKEN>
<TOKEN end_char="1895" id="token-11-26" morph="none" pos="word" start_char="1889">Affinis</TOKEN>
<TOKEN end_char="1896" id="token-11-27" morph="none" pos="punct" start_char="1896">,</TOKEN>
<TOKEN end_char="1901" id="token-11-28" morph="none" pos="word" start_char="1898">cuya</TOKEN>
<TOKEN end_char="1911" id="token-11-29" morph="none" pos="word" start_char="1903">secuencia</TOKEN>
<TOKEN end_char="1914" id="token-11-30" morph="none" pos="word" start_char="1913">se</TOKEN>
<TOKEN end_char="1920" id="token-11-31" morph="none" pos="word" start_char="1916">había</TOKEN>
<TOKEN end_char="1929" id="token-11-32" morph="none" pos="word" start_char="1922">obtenido</TOKEN>
<TOKEN end_char="1932" id="token-11-33" morph="none" pos="word" start_char="1931">en</TOKEN>
<TOKEN end_char="1937" id="token-11-34" morph="none" pos="word" start_char="1934">2013</TOKEN>
<TOKEN end_char="1940" id="token-11-35" morph="none" pos="word" start_char="1939">en</TOKEN>
<TOKEN end_char="1943" id="token-11-36" morph="none" pos="word" start_char="1942">la</TOKEN>
<TOKEN end_char="1953" id="token-11-37" morph="none" pos="word" start_char="1945">provincia</TOKEN>
<TOKEN end_char="1959" id="token-11-38" morph="none" pos="word" start_char="1955">china</TOKEN>
<TOKEN end_char="1962" id="token-11-39" morph="none" pos="word" start_char="1961">de</TOKEN>
<TOKEN end_char="1969" id="token-11-40" morph="none" pos="word" start_char="1964">Yunnan</TOKEN>
<TOKEN end_char="1970" id="token-11-41" morph="none" pos="punct" start_char="1970">.</TOKEN>
<TRANSLATED_TEXT>The Bologna team also discovered that the human coronavirus genome shares 96.2% of its genetic assets with that of the bat Rhinolophus Affinis, whose sequence had been obtained in 2013 in the Chinese province of Yunnan.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2147" id="segment-12" start_char="1974">
<ORIGINAL_TEXT>Realizzata all’Università di Bologna, la ricerca conferma l’origine nei pipistrelli e mostra che il virus è poco mutabile, ma individua anche un punto di elevata variabilità.</ORIGINAL_TEXT>
<TOKEN end_char="1983" id="token-12-0" morph="none" pos="word" start_char="1974">Realizzata</TOKEN>
<TOKEN end_char="1998" id="token-12-1" morph="none" pos="word" start_char="1985">all’Università</TOKEN>
<TOKEN end_char="2001" id="token-12-2" morph="none" pos="word" start_char="2000">di</TOKEN>
<TOKEN end_char="2009" id="token-12-3" morph="none" pos="word" start_char="2003">Bologna</TOKEN>
<TOKEN end_char="2010" id="token-12-4" morph="none" pos="punct" start_char="2010">,</TOKEN>
<TOKEN end_char="2013" id="token-12-5" morph="none" pos="word" start_char="2012">la</TOKEN>
<TOKEN end_char="2021" id="token-12-6" morph="none" pos="word" start_char="2015">ricerca</TOKEN>
<TOKEN end_char="2030" id="token-12-7" morph="none" pos="word" start_char="2023">conferma</TOKEN>
<TOKEN end_char="2040" id="token-12-8" morph="none" pos="word" start_char="2032">l’origine</TOKEN>
<TOKEN end_char="2044" id="token-12-9" morph="none" pos="word" start_char="2042">nei</TOKEN>
<TOKEN end_char="2056" id="token-12-10" morph="none" pos="word" start_char="2046">pipistrelli</TOKEN>
<TOKEN end_char="2058" id="token-12-11" morph="none" pos="word" start_char="2058">e</TOKEN>
<TOKEN end_char="2065" id="token-12-12" morph="none" pos="word" start_char="2060">mostra</TOKEN>
<TOKEN end_char="2069" id="token-12-13" morph="none" pos="word" start_char="2067">che</TOKEN>
<TOKEN end_char="2072" id="token-12-14" morph="none" pos="word" start_char="2071">il</TOKEN>
<TOKEN end_char="2078" id="token-12-15" morph="none" pos="word" start_char="2074">virus</TOKEN>
<TOKEN end_char="2080" id="token-12-16" morph="none" pos="word" start_char="2080">è</TOKEN>
<TOKEN end_char="2085" id="token-12-17" morph="none" pos="word" start_char="2082">poco</TOKEN>
<TOKEN end_char="2094" id="token-12-18" morph="none" pos="word" start_char="2087">mutabile</TOKEN>
<TOKEN end_char="2095" id="token-12-19" morph="none" pos="punct" start_char="2095">,</TOKEN>
<TOKEN end_char="2098" id="token-12-20" morph="none" pos="word" start_char="2097">ma</TOKEN>
<TOKEN end_char="2108" id="token-12-21" morph="none" pos="word" start_char="2100">individua</TOKEN>
<TOKEN end_char="2114" id="token-12-22" morph="none" pos="word" start_char="2110">anche</TOKEN>
<TOKEN end_char="2117" id="token-12-23" morph="none" pos="word" start_char="2116">un</TOKEN>
<TOKEN end_char="2123" id="token-12-24" morph="none" pos="word" start_char="2119">punto</TOKEN>
<TOKEN end_char="2126" id="token-12-25" morph="none" pos="word" start_char="2125">di</TOKEN>
<TOKEN end_char="2134" id="token-12-26" morph="none" pos="word" start_char="2128">elevata</TOKEN>
<TOKEN end_char="2146" id="token-12-27" morph="none" pos="word" start_char="2136">variabilità</TOKEN>
<TOKEN end_char="2147" id="token-12-28" morph="none" pos="punct" start_char="2147">.</TOKEN>
<TRANSLATED_TEXT>Research conducted at the University of Bologna confirms the origin in bats and shows that the virus is not mutable, but also identifies a point of high variability.</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="2249" id="segment-13" start_char="2149">
<ORIGINAL_TEXT>https://t.co/CsXLG3vkn8 #Unibo #ricerca #coronavirus— UniboMagazine (@UniboMagazine) February 7, 2020</ORIGINAL_TEXT>
<TOKEN end_char="2171" id="token-13-0" morph="none" pos="url" start_char="2149">https://t.co/CsXLG3vkn8</TOKEN>
<TOKEN end_char="2178" id="token-13-1" morph="none" pos="tag" start_char="2173">#Unibo</TOKEN>
<TOKEN end_char="2187" id="token-13-2" morph="none" pos="tag" start_char="2180">#ricerca</TOKEN>
<TOKEN end_char="2201" id="token-13-3" morph="none" pos="tag" start_char="2189">#coronavirus—</TOKEN>
<TOKEN end_char="2215" id="token-13-4" morph="none" pos="word" start_char="2203">UniboMagazine</TOKEN>
<TOKEN end_char="2218" id="token-13-5" morph="none" pos="punct" start_char="2217">(@</TOKEN>
<TOKEN end_char="2231" id="token-13-6" morph="none" pos="word" start_char="2219">UniboMagazine</TOKEN>
<TOKEN end_char="2232" id="token-13-7" morph="none" pos="punct" start_char="2232">)</TOKEN>
<TOKEN end_char="2241" id="token-13-8" morph="none" pos="word" start_char="2234">February</TOKEN>
<TOKEN end_char="2243" id="token-13-9" morph="none" pos="word" start_char="2243">7</TOKEN>
<TOKEN end_char="2244" id="token-13-10" morph="none" pos="punct" start_char="2244">,</TOKEN>
<TOKEN end_char="2249" id="token-13-11" morph="none" pos="word" start_char="2246">2020</TOKEN>
<TRANSLATED_TEXT>https: / / t.co / CsXLG3vkn8 # Unibo # research # coronavirus - UniboMagazine (@ UniboMagazine) February 7, 2020</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="2458" id="segment-14" start_char="2253">
<ORIGINAL_TEXT>Definitivamente, sostiene el equipo científico, es más bajo al 80,3% del parecido al virus Sars (Síndrome Respiratorio Agudo Severo), la enfermedad también causada por coronavirus que apareció en 2002-2003.</ORIGINAL_TEXT>
<TOKEN end_char="2267" id="token-14-0" morph="none" pos="word" start_char="2253">Definitivamente</TOKEN>
<TOKEN end_char="2268" id="token-14-1" morph="none" pos="punct" start_char="2268">,</TOKEN>
<TOKEN end_char="2277" id="token-14-2" morph="none" pos="word" start_char="2270">sostiene</TOKEN>
<TOKEN end_char="2280" id="token-14-3" morph="none" pos="word" start_char="2279">el</TOKEN>
<TOKEN end_char="2287" id="token-14-4" morph="none" pos="word" start_char="2282">equipo</TOKEN>
<TOKEN end_char="2298" id="token-14-5" morph="none" pos="word" start_char="2289">científico</TOKEN>
<TOKEN end_char="2299" id="token-14-6" morph="none" pos="punct" start_char="2299">,</TOKEN>
<TOKEN end_char="2302" id="token-14-7" morph="none" pos="word" start_char="2301">es</TOKEN>
<TOKEN end_char="2306" id="token-14-8" morph="none" pos="word" start_char="2304">más</TOKEN>
<TOKEN end_char="2311" id="token-14-9" morph="none" pos="word" start_char="2308">bajo</TOKEN>
<TOKEN end_char="2314" id="token-14-10" morph="none" pos="word" start_char="2313">al</TOKEN>
<TOKEN end_char="2319" id="token-14-11" morph="none" pos="unknown" start_char="2316">80,3</TOKEN>
<TOKEN end_char="2320" id="token-14-12" morph="none" pos="punct" start_char="2320">%</TOKEN>
<TOKEN end_char="2324" id="token-14-13" morph="none" pos="word" start_char="2322">del</TOKEN>
<TOKEN end_char="2333" id="token-14-14" morph="none" pos="word" start_char="2326">parecido</TOKEN>
<TOKEN end_char="2336" id="token-14-15" morph="none" pos="word" start_char="2335">al</TOKEN>
<TOKEN end_char="2342" id="token-14-16" morph="none" pos="word" start_char="2338">virus</TOKEN>
<TOKEN end_char="2347" id="token-14-17" morph="none" pos="word" start_char="2344">Sars</TOKEN>
<TOKEN end_char="2349" id="token-14-18" morph="none" pos="punct" start_char="2349">(</TOKEN>
<TOKEN end_char="2357" id="token-14-19" morph="none" pos="word" start_char="2350">Síndrome</TOKEN>
<TOKEN end_char="2370" id="token-14-20" morph="none" pos="word" start_char="2359">Respiratorio</TOKEN>
<TOKEN end_char="2376" id="token-14-21" morph="none" pos="word" start_char="2372">Agudo</TOKEN>
<TOKEN end_char="2383" id="token-14-22" morph="none" pos="word" start_char="2378">Severo</TOKEN>
<TOKEN end_char="2385" id="token-14-23" morph="none" pos="punct" start_char="2384">),</TOKEN>
<TOKEN end_char="2388" id="token-14-24" morph="none" pos="word" start_char="2387">la</TOKEN>
<TOKEN end_char="2399" id="token-14-25" morph="none" pos="word" start_char="2390">enfermedad</TOKEN>
<TOKEN end_char="2407" id="token-14-26" morph="none" pos="word" start_char="2401">también</TOKEN>
<TOKEN end_char="2415" id="token-14-27" morph="none" pos="word" start_char="2409">causada</TOKEN>
<TOKEN end_char="2419" id="token-14-28" morph="none" pos="word" start_char="2417">por</TOKEN>
<TOKEN end_char="2431" id="token-14-29" morph="none" pos="word" start_char="2421">coronavirus</TOKEN>
<TOKEN end_char="2435" id="token-14-30" morph="none" pos="word" start_char="2433">que</TOKEN>
<TOKEN end_char="2444" id="token-14-31" morph="none" pos="word" start_char="2437">apareció</TOKEN>
<TOKEN end_char="2447" id="token-14-32" morph="none" pos="word" start_char="2446">en</TOKEN>
<TOKEN end_char="2457" id="token-14-33" morph="none" pos="unknown" start_char="2449">2002-2003</TOKEN>
<TOKEN end_char="2458" id="token-14-34" morph="none" pos="punct" start_char="2458">.</TOKEN>
<TRANSLATED_TEXT>Definitely, according to the scientific team, it is lower than 80.3% of the Sars virus (Severe Acute Respiratory Syndrome), the disease also caused by coronavirus which appeared in 2002-2003.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2618" id="segment-15" start_char="2461">
<ORIGINAL_TEXT>Es el segundo análisis genético más grande publicado hasta ahora, tras un primer árbol genealógico de coronavirus que en la revista Lancet comparó 10 genomas.</ORIGINAL_TEXT>
<TOKEN end_char="2462" id="token-15-0" morph="none" pos="word" start_char="2461">Es</TOKEN>
<TOKEN end_char="2465" id="token-15-1" morph="none" pos="word" start_char="2464">el</TOKEN>
<TOKEN end_char="2473" id="token-15-2" morph="none" pos="word" start_char="2467">segundo</TOKEN>
<TOKEN end_char="2482" id="token-15-3" morph="none" pos="word" start_char="2475">análisis</TOKEN>
<TOKEN end_char="2491" id="token-15-4" morph="none" pos="word" start_char="2484">genético</TOKEN>
<TOKEN end_char="2495" id="token-15-5" morph="none" pos="word" start_char="2493">más</TOKEN>
<TOKEN end_char="2502" id="token-15-6" morph="none" pos="word" start_char="2497">grande</TOKEN>
<TOKEN end_char="2512" id="token-15-7" morph="none" pos="word" start_char="2504">publicado</TOKEN>
<TOKEN end_char="2518" id="token-15-8" morph="none" pos="word" start_char="2514">hasta</TOKEN>
<TOKEN end_char="2524" id="token-15-9" morph="none" pos="word" start_char="2520">ahora</TOKEN>
<TOKEN end_char="2525" id="token-15-10" morph="none" pos="punct" start_char="2525">,</TOKEN>
<TOKEN end_char="2530" id="token-15-11" morph="none" pos="word" start_char="2527">tras</TOKEN>
<TOKEN end_char="2533" id="token-15-12" morph="none" pos="word" start_char="2532">un</TOKEN>
<TOKEN end_char="2540" id="token-15-13" morph="none" pos="word" start_char="2535">primer</TOKEN>
<TOKEN end_char="2546" id="token-15-14" morph="none" pos="word" start_char="2542">árbol</TOKEN>
<TOKEN end_char="2558" id="token-15-15" morph="none" pos="word" start_char="2548">genealógico</TOKEN>
<TOKEN end_char="2561" id="token-15-16" morph="none" pos="word" start_char="2560">de</TOKEN>
<TOKEN end_char="2573" id="token-15-17" morph="none" pos="word" start_char="2563">coronavirus</TOKEN>
<TOKEN end_char="2577" id="token-15-18" morph="none" pos="word" start_char="2575">que</TOKEN>
<TOKEN end_char="2580" id="token-15-19" morph="none" pos="word" start_char="2579">en</TOKEN>
<TOKEN end_char="2583" id="token-15-20" morph="none" pos="word" start_char="2582">la</TOKEN>
<TOKEN end_char="2591" id="token-15-21" morph="none" pos="word" start_char="2585">revista</TOKEN>
<TOKEN end_char="2598" id="token-15-22" morph="none" pos="word" start_char="2593">Lancet</TOKEN>
<TOKEN end_char="2606" id="token-15-23" morph="none" pos="word" start_char="2600">comparó</TOKEN>
<TOKEN end_char="2609" id="token-15-24" morph="none" pos="word" start_char="2608">10</TOKEN>
<TOKEN end_char="2617" id="token-15-25" morph="none" pos="word" start_char="2611">genomas</TOKEN>
<TOKEN end_char="2618" id="token-15-26" morph="none" pos="punct" start_char="2618">.</TOKEN>
<TRANSLATED_TEXT>It is the second largest genetic analysis published so far, after the first coronavirus family tree that the Lancet magazine compared 10 genomes.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2754" id="segment-16" start_char="2621">
<ORIGINAL_TEXT>En el trabajo ya han aumentado a 56 y desde el domingo 2 de febrero hasta la fecha, las secuencias publicadas se han convertido en 74.</ORIGINAL_TEXT>
<TOKEN end_char="2622" id="token-16-0" morph="none" pos="word" start_char="2621">En</TOKEN>
<TOKEN end_char="2625" id="token-16-1" morph="none" pos="word" start_char="2624">el</TOKEN>
<TOKEN end_char="2633" id="token-16-2" morph="none" pos="word" start_char="2627">trabajo</TOKEN>
<TOKEN end_char="2636" id="token-16-3" morph="none" pos="word" start_char="2635">ya</TOKEN>
<TOKEN end_char="2640" id="token-16-4" morph="none" pos="word" start_char="2638">han</TOKEN>
<TOKEN end_char="2650" id="token-16-5" morph="none" pos="word" start_char="2642">aumentado</TOKEN>
<TOKEN end_char="2652" id="token-16-6" morph="none" pos="word" start_char="2652">a</TOKEN>
<TOKEN end_char="2655" id="token-16-7" morph="none" pos="word" start_char="2654">56</TOKEN>
<TOKEN end_char="2657" id="token-16-8" morph="none" pos="word" start_char="2657">y</TOKEN>
<TOKEN end_char="2663" id="token-16-9" morph="none" pos="word" start_char="2659">desde</TOKEN>
<TOKEN end_char="2666" id="token-16-10" morph="none" pos="word" start_char="2665">el</TOKEN>
<TOKEN end_char="2674" id="token-16-11" morph="none" pos="word" start_char="2668">domingo</TOKEN>
<TOKEN end_char="2676" id="token-16-12" morph="none" pos="word" start_char="2676">2</TOKEN>
<TOKEN end_char="2679" id="token-16-13" morph="none" pos="word" start_char="2678">de</TOKEN>
<TOKEN end_char="2687" id="token-16-14" morph="none" pos="word" start_char="2681">febrero</TOKEN>
<TOKEN end_char="2693" id="token-16-15" morph="none" pos="word" start_char="2689">hasta</TOKEN>
<TOKEN end_char="2696" id="token-16-16" morph="none" pos="word" start_char="2695">la</TOKEN>
<TOKEN end_char="2702" id="token-16-17" morph="none" pos="word" start_char="2698">fecha</TOKEN>
<TOKEN end_char="2703" id="token-16-18" morph="none" pos="punct" start_char="2703">,</TOKEN>
<TOKEN end_char="2707" id="token-16-19" morph="none" pos="word" start_char="2705">las</TOKEN>
<TOKEN end_char="2718" id="token-16-20" morph="none" pos="word" start_char="2709">secuencias</TOKEN>
<TOKEN end_char="2729" id="token-16-21" morph="none" pos="word" start_char="2720">publicadas</TOKEN>
<TOKEN end_char="2732" id="token-16-22" morph="none" pos="word" start_char="2731">se</TOKEN>
<TOKEN end_char="2736" id="token-16-23" morph="none" pos="word" start_char="2734">han</TOKEN>
<TOKEN end_char="2747" id="token-16-24" morph="none" pos="word" start_char="2738">convertido</TOKEN>
<TOKEN end_char="2750" id="token-16-25" morph="none" pos="word" start_char="2749">en</TOKEN>
<TOKEN end_char="2753" id="token-16-26" morph="none" pos="word" start_char="2752">74</TOKEN>
<TOKEN end_char="2754" id="token-16-27" morph="none" pos="punct" start_char="2754">.</TOKEN>
<TRANSLATED_TEXT>The work has already increased to 56 and from Sunday 2 February to date, the published sequences have become 74.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2873" id="segment-17" start_char="2756">
<ORIGINAL_TEXT>También están los del coronavirus aislados de dos turistas chinos hospitalizados en Roma, en el Instituto Spallanzani.</ORIGINAL_TEXT>
<TOKEN end_char="2762" id="token-17-0" morph="none" pos="word" start_char="2756">También</TOKEN>
<TOKEN end_char="2768" id="token-17-1" morph="none" pos="word" start_char="2764">están</TOKEN>
<TOKEN end_char="2772" id="token-17-2" morph="none" pos="word" start_char="2770">los</TOKEN>
<TOKEN end_char="2776" id="token-17-3" morph="none" pos="word" start_char="2774">del</TOKEN>
<TOKEN end_char="2788" id="token-17-4" morph="none" pos="word" start_char="2778">coronavirus</TOKEN>
<TOKEN end_char="2797" id="token-17-5" morph="none" pos="word" start_char="2790">aislados</TOKEN>
<TOKEN end_char="2800" id="token-17-6" morph="none" pos="word" start_char="2799">de</TOKEN>
<TOKEN end_char="2804" id="token-17-7" morph="none" pos="word" start_char="2802">dos</TOKEN>
<TOKEN end_char="2813" id="token-17-8" morph="none" pos="word" start_char="2806">turistas</TOKEN>
<TOKEN end_char="2820" id="token-17-9" morph="none" pos="word" start_char="2815">chinos</TOKEN>
<TOKEN end_char="2835" id="token-17-10" morph="none" pos="word" start_char="2822">hospitalizados</TOKEN>
<TOKEN end_char="2838" id="token-17-11" morph="none" pos="word" start_char="2837">en</TOKEN>
<TOKEN end_char="2843" id="token-17-12" morph="none" pos="word" start_char="2840">Roma</TOKEN>
<TOKEN end_char="2844" id="token-17-13" morph="none" pos="punct" start_char="2844">,</TOKEN>
<TOKEN end_char="2847" id="token-17-14" morph="none" pos="word" start_char="2846">en</TOKEN>
<TOKEN end_char="2850" id="token-17-15" morph="none" pos="word" start_char="2849">el</TOKEN>
<TOKEN end_char="2860" id="token-17-16" morph="none" pos="word" start_char="2852">Instituto</TOKEN>
<TOKEN end_char="2872" id="token-17-17" morph="none" pos="word" start_char="2862">Spallanzani</TOKEN>
<TOKEN end_char="2873" id="token-17-18" morph="none" pos="punct" start_char="2873">.</TOKEN>
<TRANSLATED_TEXT>There are also coronavirus isolates from two Chinese tourists hospitalized in Rome, at the Spallanzani Institute.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2946" id="segment-18" start_char="2876">
<ORIGINAL_TEXT>Ninguna secuencia, en cambio, hasta ahora llegó de Africa y Sudamérica.</ORIGINAL_TEXT>
<TOKEN end_char="2882" id="token-18-0" morph="none" pos="word" start_char="2876">Ninguna</TOKEN>
<TOKEN end_char="2892" id="token-18-1" morph="none" pos="word" start_char="2884">secuencia</TOKEN>
<TOKEN end_char="2893" id="token-18-2" morph="none" pos="punct" start_char="2893">,</TOKEN>
<TOKEN end_char="2896" id="token-18-3" morph="none" pos="word" start_char="2895">en</TOKEN>
<TOKEN end_char="2903" id="token-18-4" morph="none" pos="word" start_char="2898">cambio</TOKEN>
<TOKEN end_char="2904" id="token-18-5" morph="none" pos="punct" start_char="2904">,</TOKEN>
<TOKEN end_char="2910" id="token-18-6" morph="none" pos="word" start_char="2906">hasta</TOKEN>
<TOKEN end_char="2916" id="token-18-7" morph="none" pos="word" start_char="2912">ahora</TOKEN>
<TOKEN end_char="2922" id="token-18-8" morph="none" pos="word" start_char="2918">llegó</TOKEN>
<TOKEN end_char="2925" id="token-18-9" morph="none" pos="word" start_char="2924">de</TOKEN>
<TOKEN end_char="2932" id="token-18-10" morph="none" pos="word" start_char="2927">Africa</TOKEN>
<TOKEN end_char="2934" id="token-18-11" morph="none" pos="word" start_char="2934">y</TOKEN>
<TOKEN end_char="2945" id="token-18-12" morph="none" pos="word" start_char="2936">Sudamérica</TOKEN>
<TOKEN end_char="2946" id="token-18-13" morph="none" pos="punct" start_char="2946">.</TOKEN>
<TRANSLATED_TEXT>No sequence, however, has so far come from Africa and South America.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3015" id="segment-19" start_char="2949">
<ORIGINAL_TEXT>No hay idea de como el animal podría haberlo transmitido al hombre.</ORIGINAL_TEXT>
<TOKEN end_char="2950" id="token-19-0" morph="none" pos="word" start_char="2949">No</TOKEN>
<TOKEN end_char="2954" id="token-19-1" morph="none" pos="word" start_char="2952">hay</TOKEN>
<TOKEN end_char="2959" id="token-19-2" morph="none" pos="word" start_char="2956">idea</TOKEN>
<TOKEN end_char="2962" id="token-19-3" morph="none" pos="word" start_char="2961">de</TOKEN>
<TOKEN end_char="2967" id="token-19-4" morph="none" pos="word" start_char="2964">como</TOKEN>
<TOKEN end_char="2970" id="token-19-5" morph="none" pos="word" start_char="2969">el</TOKEN>
<TOKEN end_char="2977" id="token-19-6" morph="none" pos="word" start_char="2972">animal</TOKEN>
<TOKEN end_char="2984" id="token-19-7" morph="none" pos="word" start_char="2979">podría</TOKEN>
<TOKEN end_char="2992" id="token-19-8" morph="none" pos="word" start_char="2986">haberlo</TOKEN>
<TOKEN end_char="3004" id="token-19-9" morph="none" pos="word" start_char="2994">transmitido</TOKEN>
<TOKEN end_char="3007" id="token-19-10" morph="none" pos="word" start_char="3006">al</TOKEN>
<TOKEN end_char="3014" id="token-19-11" morph="none" pos="word" start_char="3009">hombre</TOKEN>
<TOKEN end_char="3015" id="token-19-12" morph="none" pos="punct" start_char="3015">.</TOKEN>
<TRANSLATED_TEXT>There is no idea how the animal could have transmitted it to man.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3082" id="segment-20" start_char="3017">
<ORIGINAL_TEXT>Acaso, por intermedio de otro animal, o por una mordedura directa.</ORIGINAL_TEXT>
<TOKEN end_char="3021" id="token-20-0" morph="none" pos="word" start_char="3017">Acaso</TOKEN>
<TOKEN end_char="3022" id="token-20-1" morph="none" pos="punct" start_char="3022">,</TOKEN>
<TOKEN end_char="3026" id="token-20-2" morph="none" pos="word" start_char="3024">por</TOKEN>
<TOKEN end_char="3037" id="token-20-3" morph="none" pos="word" start_char="3028">intermedio</TOKEN>
<TOKEN end_char="3040" id="token-20-4" morph="none" pos="word" start_char="3039">de</TOKEN>
<TOKEN end_char="3045" id="token-20-5" morph="none" pos="word" start_char="3042">otro</TOKEN>
<TOKEN end_char="3052" id="token-20-6" morph="none" pos="word" start_char="3047">animal</TOKEN>
<TOKEN end_char="3053" id="token-20-7" morph="none" pos="punct" start_char="3053">,</TOKEN>
<TOKEN end_char="3055" id="token-20-8" morph="none" pos="word" start_char="3055">o</TOKEN>
<TOKEN end_char="3059" id="token-20-9" morph="none" pos="word" start_char="3057">por</TOKEN>
<TOKEN end_char="3063" id="token-20-10" morph="none" pos="word" start_char="3061">una</TOKEN>
<TOKEN end_char="3073" id="token-20-11" morph="none" pos="word" start_char="3065">mordedura</TOKEN>
<TOKEN end_char="3081" id="token-20-12" morph="none" pos="word" start_char="3075">directa</TOKEN>
<TOKEN end_char="3082" id="token-20-13" morph="none" pos="punct" start_char="3082">.</TOKEN>
<TRANSLATED_TEXT>Here, by another animal, or by a direct bite.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3168" id="segment-21" start_char="3085">
<ORIGINAL_TEXT>Según Giorgi, por el momento "no hay evidencia de un posible invitado en la cadena".</ORIGINAL_TEXT>
<TOKEN end_char="3089" id="token-21-0" morph="none" pos="word" start_char="3085">Según</TOKEN>
<TOKEN end_char="3096" id="token-21-1" morph="none" pos="word" start_char="3091">Giorgi</TOKEN>
<TOKEN end_char="3097" id="token-21-2" morph="none" pos="punct" start_char="3097">,</TOKEN>
<TOKEN end_char="3101" id="token-21-3" morph="none" pos="word" start_char="3099">por</TOKEN>
<TOKEN end_char="3104" id="token-21-4" morph="none" pos="word" start_char="3103">el</TOKEN>
<TOKEN end_char="3112" id="token-21-5" morph="none" pos="word" start_char="3106">momento</TOKEN>
<TOKEN end_char="3114" id="token-21-6" morph="none" pos="punct" start_char="3114">"</TOKEN>
<TOKEN end_char="3116" id="token-21-7" morph="none" pos="word" start_char="3115">no</TOKEN>
<TOKEN end_char="3120" id="token-21-8" morph="none" pos="word" start_char="3118">hay</TOKEN>
<TOKEN end_char="3130" id="token-21-9" morph="none" pos="word" start_char="3122">evidencia</TOKEN>
<TOKEN end_char="3133" id="token-21-10" morph="none" pos="word" start_char="3132">de</TOKEN>
<TOKEN end_char="3136" id="token-21-11" morph="none" pos="word" start_char="3135">un</TOKEN>
<TOKEN end_char="3144" id="token-21-12" morph="none" pos="word" start_char="3138">posible</TOKEN>
<TOKEN end_char="3153" id="token-21-13" morph="none" pos="word" start_char="3146">invitado</TOKEN>
<TOKEN end_char="3156" id="token-21-14" morph="none" pos="word" start_char="3155">en</TOKEN>
<TOKEN end_char="3159" id="token-21-15" morph="none" pos="word" start_char="3158">la</TOKEN>
<TOKEN end_char="3166" id="token-21-16" morph="none" pos="word" start_char="3161">cadena</TOKEN>
<TOKEN end_char="3168" id="token-21-17" morph="none" pos="punct" start_char="3167">".</TOKEN>
<TRANSLATED_TEXT>According to Giorgi, for the time being "there is no evidence of a possible guest on the chain."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>