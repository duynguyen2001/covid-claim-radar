<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CA6T" lang="spa" raw_text_char_length="300" raw_text_md5="7972ba4fbf64fa7de3678c75d06db18b" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="79" id="segment-0" start_char="1">
<ORIGINAL_TEXT>WHO experts believe badgers and rabbits could have spread coronavirus to humans</ORIGINAL_TEXT>
<TOKEN end_char="3" id="token-0-0" morph="none" pos="word" start_char="1">WHO</TOKEN>
<TOKEN end_char="11" id="token-0-1" morph="none" pos="word" start_char="5">experts</TOKEN>
<TOKEN end_char="19" id="token-0-2" morph="none" pos="word" start_char="13">believe</TOKEN>
<TOKEN end_char="27" id="token-0-3" morph="none" pos="word" start_char="21">badgers</TOKEN>
<TOKEN end_char="31" id="token-0-4" morph="none" pos="word" start_char="29">and</TOKEN>
<TOKEN end_char="39" id="token-0-5" morph="none" pos="word" start_char="33">rabbits</TOKEN>
<TOKEN end_char="45" id="token-0-6" morph="none" pos="word" start_char="41">could</TOKEN>
<TOKEN end_char="50" id="token-0-7" morph="none" pos="word" start_char="47">have</TOKEN>
<TOKEN end_char="57" id="token-0-8" morph="none" pos="word" start_char="52">spread</TOKEN>
<TOKEN end_char="69" id="token-0-9" morph="none" pos="word" start_char="59">coronavirus</TOKEN>
<TOKEN end_char="72" id="token-0-10" morph="none" pos="word" start_char="71">to</TOKEN>
<TOKEN end_char="79" id="token-0-11" morph="none" pos="word" start_char="74">humans</TOKEN>
</SEG>
<SEG end_char="152" id="segment-1" start_char="85">
<ORIGINAL_TEXT>The WHO expert team finished their four-week trip to China last week</ORIGINAL_TEXT>
<TOKEN end_char="87" id="token-1-0" morph="none" pos="word" start_char="85">The</TOKEN>
<TOKEN end_char="91" id="token-1-1" morph="none" pos="word" start_char="89">WHO</TOKEN>
<TOKEN end_char="98" id="token-1-2" morph="none" pos="word" start_char="93">expert</TOKEN>
<TOKEN end_char="103" id="token-1-3" morph="none" pos="word" start_char="100">team</TOKEN>
<TOKEN end_char="112" id="token-1-4" morph="none" pos="word" start_char="105">finished</TOKEN>
<TOKEN end_char="118" id="token-1-5" morph="none" pos="word" start_char="114">their</TOKEN>
<TOKEN end_char="128" id="token-1-6" morph="none" pos="unknown" start_char="120">four-week</TOKEN>
<TOKEN end_char="133" id="token-1-7" morph="none" pos="word" start_char="130">trip</TOKEN>
<TOKEN end_char="136" id="token-1-8" morph="none" pos="word" start_char="135">to</TOKEN>
<TOKEN end_char="142" id="token-1-9" morph="none" pos="word" start_char="138">China</TOKEN>
<TOKEN end_char="147" id="token-1-10" morph="none" pos="word" start_char="144">last</TOKEN>
<TOKEN end_char="152" id="token-1-11" morph="none" pos="word" start_char="149">week</TOKEN>
</SEG>
<SEG end_char="294" id="segment-2" start_char="156">
<ORIGINAL_TEXT>At a press conference, they announced that any leak of the virus from a lab was unlikely, while the Wuhan wet market's role remains unclear</ORIGINAL_TEXT>
<TOKEN end_char="157" id="token-2-0" morph="none" pos="word" start_char="156">At</TOKEN>
<TOKEN end_char="159" id="token-2-1" morph="none" pos="word" start_char="159">a</TOKEN>
<TOKEN end_char="165" id="token-2-2" morph="none" pos="word" start_char="161">press</TOKEN>
<TOKEN end_char="176" id="token-2-3" morph="none" pos="word" start_char="167">conference</TOKEN>
<TOKEN end_char="177" id="token-2-4" morph="none" pos="punct" start_char="177">,</TOKEN>
<TOKEN end_char="182" id="token-2-5" morph="none" pos="word" start_char="179">they</TOKEN>
<TOKEN end_char="192" id="token-2-6" morph="none" pos="word" start_char="184">announced</TOKEN>
<TOKEN end_char="197" id="token-2-7" morph="none" pos="word" start_char="194">that</TOKEN>
<TOKEN end_char="201" id="token-2-8" morph="none" pos="word" start_char="199">any</TOKEN>
<TOKEN end_char="206" id="token-2-9" morph="none" pos="word" start_char="203">leak</TOKEN>
<TOKEN end_char="209" id="token-2-10" morph="none" pos="word" start_char="208">of</TOKEN>
<TOKEN end_char="213" id="token-2-11" morph="none" pos="word" start_char="211">the</TOKEN>
<TOKEN end_char="219" id="token-2-12" morph="none" pos="word" start_char="215">virus</TOKEN>
<TOKEN end_char="224" id="token-2-13" morph="none" pos="word" start_char="221">from</TOKEN>
<TOKEN end_char="226" id="token-2-14" morph="none" pos="word" start_char="226">a</TOKEN>
<TOKEN end_char="230" id="token-2-15" morph="none" pos="word" start_char="228">lab</TOKEN>
<TOKEN end_char="234" id="token-2-16" morph="none" pos="word" start_char="232">was</TOKEN>
<TOKEN end_char="243" id="token-2-17" morph="none" pos="word" start_char="236">unlikely</TOKEN>
<TOKEN end_char="244" id="token-2-18" morph="none" pos="punct" start_char="244">,</TOKEN>
<TOKEN end_char="250" id="token-2-19" morph="none" pos="word" start_char="246">while</TOKEN>
<TOKEN end_char="254" id="token-2-20" morph="none" pos="word" start_char="252">the</TOKEN>
<TOKEN end_char="260" id="token-2-21" morph="none" pos="word" start_char="256">Wuhan</TOKEN>
<TOKEN end_char="264" id="token-2-22" morph="none" pos="word" start_char="262">wet</TOKEN>
<TOKEN end_char="273" id="token-2-23" morph="none" pos="word" start_char="266">market's</TOKEN>
<TOKEN end_char="278" id="token-2-24" morph="none" pos="word" start_char="275">role</TOKEN>
<TOKEN end_char="286" id="token-2-25" morph="none" pos="word" start_char="280">remains</TOKEN>
<TOKEN end_char="294" id="token-2-26" morph="none" pos="word" start_char="288">unclear</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>