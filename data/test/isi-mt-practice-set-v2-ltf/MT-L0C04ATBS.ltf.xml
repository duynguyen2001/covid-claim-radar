<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04ATBS" lang="spa" raw_text_char_length="1169" raw_text_md5="e57891dab9df288052bb5b6e14177ec2" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="19" id="segment-0" start_char="1">
<ORIGINAL_TEXT>¡Pangolín inocente!</ORIGINAL_TEXT>
<TOKEN end_char="1" id="token-0-0" morph="none" pos="punct" start_char="1">¡</TOKEN>
<TOKEN end_char="9" id="token-0-1" morph="none" pos="word" start_char="2">Pangolín</TOKEN>
<TOKEN end_char="18" id="token-0-2" morph="none" pos="word" start_char="11">inocente</TOKEN>
<TOKEN end_char="19" id="token-0-3" morph="none" pos="punct" start_char="19">!</TOKEN>
<TRANSLATED_TEXT>They're innocent!</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="55" id="segment-1" start_char="21">
<ORIGINAL_TEXT>Revelan que no esparció coronavirus</ORIGINAL_TEXT>
<TOKEN end_char="27" id="token-1-0" morph="none" pos="word" start_char="21">Revelan</TOKEN>
<TOKEN end_char="31" id="token-1-1" morph="none" pos="word" start_char="29">que</TOKEN>
<TOKEN end_char="34" id="token-1-2" morph="none" pos="word" start_char="33">no</TOKEN>
<TOKEN end_char="43" id="token-1-3" morph="none" pos="word" start_char="36">esparció</TOKEN>
<TOKEN end_char="55" id="token-1-4" morph="none" pos="word" start_char="45">coronavirus</TOKEN>
<TRANSLATED_TEXT>They reveal no coronavirus spread.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="406" id="segment-2" start_char="60">
<ORIGINAL_TEXT>Su hipótesis inicial, sostenía que un coronavirus hallado en un pangolín tenía un genoma igual en un 99% al del que causa el Covid-19 en los humanos, resultó ser errónea, pues un nuevo análisis reveló que la equivalencia entre genomas se sitúa en torno al 90%, lo que resulta insuficiente para afirmar que este animal fue el origen de la pandemia.</ORIGINAL_TEXT>
<TOKEN end_char="61" id="token-2-0" morph="none" pos="word" start_char="60">Su</TOKEN>
<TOKEN end_char="71" id="token-2-1" morph="none" pos="word" start_char="63">hipótesis</TOKEN>
<TOKEN end_char="79" id="token-2-2" morph="none" pos="word" start_char="73">inicial</TOKEN>
<TOKEN end_char="80" id="token-2-3" morph="none" pos="punct" start_char="80">,</TOKEN>
<TOKEN end_char="89" id="token-2-4" morph="none" pos="word" start_char="82">sostenía</TOKEN>
<TOKEN end_char="93" id="token-2-5" morph="none" pos="word" start_char="91">que</TOKEN>
<TOKEN end_char="96" id="token-2-6" morph="none" pos="word" start_char="95">un</TOKEN>
<TOKEN end_char="108" id="token-2-7" morph="none" pos="word" start_char="98">coronavirus</TOKEN>
<TOKEN end_char="116" id="token-2-8" morph="none" pos="word" start_char="110">hallado</TOKEN>
<TOKEN end_char="119" id="token-2-9" morph="none" pos="word" start_char="118">en</TOKEN>
<TOKEN end_char="122" id="token-2-10" morph="none" pos="word" start_char="121">un</TOKEN>
<TOKEN end_char="131" id="token-2-11" morph="none" pos="word" start_char="124">pangolín</TOKEN>
<TOKEN end_char="137" id="token-2-12" morph="none" pos="word" start_char="133">tenía</TOKEN>
<TOKEN end_char="140" id="token-2-13" morph="none" pos="word" start_char="139">un</TOKEN>
<TOKEN end_char="147" id="token-2-14" morph="none" pos="word" start_char="142">genoma</TOKEN>
<TOKEN end_char="153" id="token-2-15" morph="none" pos="word" start_char="149">igual</TOKEN>
<TOKEN end_char="156" id="token-2-16" morph="none" pos="word" start_char="155">en</TOKEN>
<TOKEN end_char="159" id="token-2-17" morph="none" pos="word" start_char="158">un</TOKEN>
<TOKEN end_char="162" id="token-2-18" morph="none" pos="word" start_char="161">99</TOKEN>
<TOKEN end_char="163" id="token-2-19" morph="none" pos="punct" start_char="163">%</TOKEN>
<TOKEN end_char="166" id="token-2-20" morph="none" pos="word" start_char="165">al</TOKEN>
<TOKEN end_char="170" id="token-2-21" morph="none" pos="word" start_char="168">del</TOKEN>
<TOKEN end_char="174" id="token-2-22" morph="none" pos="word" start_char="172">que</TOKEN>
<TOKEN end_char="180" id="token-2-23" morph="none" pos="word" start_char="176">causa</TOKEN>
<TOKEN end_char="183" id="token-2-24" morph="none" pos="word" start_char="182">el</TOKEN>
<TOKEN end_char="192" id="token-2-25" morph="none" pos="unknown" start_char="185">Covid-19</TOKEN>
<TOKEN end_char="195" id="token-2-26" morph="none" pos="word" start_char="194">en</TOKEN>
<TOKEN end_char="199" id="token-2-27" morph="none" pos="word" start_char="197">los</TOKEN>
<TOKEN end_char="207" id="token-2-28" morph="none" pos="word" start_char="201">humanos</TOKEN>
<TOKEN end_char="208" id="token-2-29" morph="none" pos="punct" start_char="208">,</TOKEN>
<TOKEN end_char="216" id="token-2-30" morph="none" pos="word" start_char="210">resultó</TOKEN>
<TOKEN end_char="220" id="token-2-31" morph="none" pos="word" start_char="218">ser</TOKEN>
<TOKEN end_char="228" id="token-2-32" morph="none" pos="word" start_char="222">errónea</TOKEN>
<TOKEN end_char="229" id="token-2-33" morph="none" pos="punct" start_char="229">,</TOKEN>
<TOKEN end_char="234" id="token-2-34" morph="none" pos="word" start_char="231">pues</TOKEN>
<TOKEN end_char="237" id="token-2-35" morph="none" pos="word" start_char="236">un</TOKEN>
<TOKEN end_char="243" id="token-2-36" morph="none" pos="word" start_char="239">nuevo</TOKEN>
<TOKEN end_char="252" id="token-2-37" morph="none" pos="word" start_char="245">análisis</TOKEN>
<TOKEN end_char="259" id="token-2-38" morph="none" pos="word" start_char="254">reveló</TOKEN>
<TOKEN end_char="263" id="token-2-39" morph="none" pos="word" start_char="261">que</TOKEN>
<TOKEN end_char="266" id="token-2-40" morph="none" pos="word" start_char="265">la</TOKEN>
<TOKEN end_char="279" id="token-2-41" morph="none" pos="word" start_char="268">equivalencia</TOKEN>
<TOKEN end_char="285" id="token-2-42" morph="none" pos="word" start_char="281">entre</TOKEN>
<TOKEN end_char="293" id="token-2-43" morph="none" pos="word" start_char="287">genomas</TOKEN>
<TOKEN end_char="296" id="token-2-44" morph="none" pos="word" start_char="295">se</TOKEN>
<TOKEN end_char="302" id="token-2-45" morph="none" pos="word" start_char="298">sitúa</TOKEN>
<TOKEN end_char="305" id="token-2-46" morph="none" pos="word" start_char="304">en</TOKEN>
<TOKEN end_char="311" id="token-2-47" morph="none" pos="word" start_char="307">torno</TOKEN>
<TOKEN end_char="314" id="token-2-48" morph="none" pos="word" start_char="313">al</TOKEN>
<TOKEN end_char="317" id="token-2-49" morph="none" pos="word" start_char="316">90</TOKEN>
<TOKEN end_char="319" id="token-2-50" morph="none" pos="punct" start_char="318">%,</TOKEN>
<TOKEN end_char="322" id="token-2-51" morph="none" pos="word" start_char="321">lo</TOKEN>
<TOKEN end_char="326" id="token-2-52" morph="none" pos="word" start_char="324">que</TOKEN>
<TOKEN end_char="334" id="token-2-53" morph="none" pos="word" start_char="328">resulta</TOKEN>
<TOKEN end_char="347" id="token-2-54" morph="none" pos="word" start_char="336">insuficiente</TOKEN>
<TOKEN end_char="352" id="token-2-55" morph="none" pos="word" start_char="349">para</TOKEN>
<TOKEN end_char="360" id="token-2-56" morph="none" pos="word" start_char="354">afirmar</TOKEN>
<TOKEN end_char="364" id="token-2-57" morph="none" pos="word" start_char="362">que</TOKEN>
<TOKEN end_char="369" id="token-2-58" morph="none" pos="word" start_char="366">este</TOKEN>
<TOKEN end_char="376" id="token-2-59" morph="none" pos="word" start_char="371">animal</TOKEN>
<TOKEN end_char="380" id="token-2-60" morph="none" pos="word" start_char="378">fue</TOKEN>
<TOKEN end_char="383" id="token-2-61" morph="none" pos="word" start_char="382">el</TOKEN>
<TOKEN end_char="390" id="token-2-62" morph="none" pos="word" start_char="385">origen</TOKEN>
<TOKEN end_char="393" id="token-2-63" morph="none" pos="word" start_char="392">de</TOKEN>
<TOKEN end_char="396" id="token-2-64" morph="none" pos="word" start_char="395">la</TOKEN>
<TOKEN end_char="405" id="token-2-65" morph="none" pos="word" start_char="398">pandemia</TOKEN>
<TOKEN end_char="406" id="token-2-66" morph="none" pos="punct" start_char="406">.</TOKEN>
<TRANSLATED_TEXT>His initial hypothesis, that a coronavirus found in a pangolin had a genome 99% the same as that caused by Covid-19 in humans, proved to be erroneous, as a new analysis revealed that genome equivalence is around 90%, which is insufficient to claim that this animal was the origin of the pandemic.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="684" id="segment-3" start_char="409">
<ORIGINAL_TEXT>Según sostenían los científicos de la Universidad de Agricultura del sur del país asiático, los pangolines fueron los animales intermediarios entre los murciélagos y el ser humano, sin embargo, la institución advirtió desde un principio que los resultados no eran definitivos.</ORIGINAL_TEXT>
<TOKEN end_char="413" id="token-3-0" morph="none" pos="word" start_char="409">Según</TOKEN>
<TOKEN end_char="423" id="token-3-1" morph="none" pos="word" start_char="415">sostenían</TOKEN>
<TOKEN end_char="427" id="token-3-2" morph="none" pos="word" start_char="425">los</TOKEN>
<TOKEN end_char="439" id="token-3-3" morph="none" pos="word" start_char="429">científicos</TOKEN>
<TOKEN end_char="442" id="token-3-4" morph="none" pos="word" start_char="441">de</TOKEN>
<TOKEN end_char="445" id="token-3-5" morph="none" pos="word" start_char="444">la</TOKEN>
<TOKEN end_char="457" id="token-3-6" morph="none" pos="word" start_char="447">Universidad</TOKEN>
<TOKEN end_char="460" id="token-3-7" morph="none" pos="word" start_char="459">de</TOKEN>
<TOKEN end_char="472" id="token-3-8" morph="none" pos="word" start_char="462">Agricultura</TOKEN>
<TOKEN end_char="476" id="token-3-9" morph="none" pos="word" start_char="474">del</TOKEN>
<TOKEN end_char="480" id="token-3-10" morph="none" pos="word" start_char="478">sur</TOKEN>
<TOKEN end_char="484" id="token-3-11" morph="none" pos="word" start_char="482">del</TOKEN>
<TOKEN end_char="489" id="token-3-12" morph="none" pos="word" start_char="486">país</TOKEN>
<TOKEN end_char="498" id="token-3-13" morph="none" pos="word" start_char="491">asiático</TOKEN>
<TOKEN end_char="499" id="token-3-14" morph="none" pos="punct" start_char="499">,</TOKEN>
<TOKEN end_char="503" id="token-3-15" morph="none" pos="word" start_char="501">los</TOKEN>
<TOKEN end_char="514" id="token-3-16" morph="none" pos="word" start_char="505">pangolines</TOKEN>
<TOKEN end_char="521" id="token-3-17" morph="none" pos="word" start_char="516">fueron</TOKEN>
<TOKEN end_char="525" id="token-3-18" morph="none" pos="word" start_char="523">los</TOKEN>
<TOKEN end_char="534" id="token-3-19" morph="none" pos="word" start_char="527">animales</TOKEN>
<TOKEN end_char="549" id="token-3-20" morph="none" pos="word" start_char="536">intermediarios</TOKEN>
<TOKEN end_char="555" id="token-3-21" morph="none" pos="word" start_char="551">entre</TOKEN>
<TOKEN end_char="559" id="token-3-22" morph="none" pos="word" start_char="557">los</TOKEN>
<TOKEN end_char="571" id="token-3-23" morph="none" pos="word" start_char="561">murciélagos</TOKEN>
<TOKEN end_char="573" id="token-3-24" morph="none" pos="word" start_char="573">y</TOKEN>
<TOKEN end_char="576" id="token-3-25" morph="none" pos="word" start_char="575">el</TOKEN>
<TOKEN end_char="580" id="token-3-26" morph="none" pos="word" start_char="578">ser</TOKEN>
<TOKEN end_char="587" id="token-3-27" morph="none" pos="word" start_char="582">humano</TOKEN>
<TOKEN end_char="588" id="token-3-28" morph="none" pos="punct" start_char="588">,</TOKEN>
<TOKEN end_char="592" id="token-3-29" morph="none" pos="word" start_char="590">sin</TOKEN>
<TOKEN end_char="600" id="token-3-30" morph="none" pos="word" start_char="594">embargo</TOKEN>
<TOKEN end_char="601" id="token-3-31" morph="none" pos="punct" start_char="601">,</TOKEN>
<TOKEN end_char="604" id="token-3-32" morph="none" pos="word" start_char="603">la</TOKEN>
<TOKEN end_char="616" id="token-3-33" morph="none" pos="word" start_char="606">institución</TOKEN>
<TOKEN end_char="625" id="token-3-34" morph="none" pos="word" start_char="618">advirtió</TOKEN>
<TOKEN end_char="631" id="token-3-35" morph="none" pos="word" start_char="627">desde</TOKEN>
<TOKEN end_char="634" id="token-3-36" morph="none" pos="word" start_char="633">un</TOKEN>
<TOKEN end_char="644" id="token-3-37" morph="none" pos="word" start_char="636">principio</TOKEN>
<TOKEN end_char="648" id="token-3-38" morph="none" pos="word" start_char="646">que</TOKEN>
<TOKEN end_char="652" id="token-3-39" morph="none" pos="word" start_char="650">los</TOKEN>
<TOKEN end_char="663" id="token-3-40" morph="none" pos="word" start_char="654">resultados</TOKEN>
<TOKEN end_char="666" id="token-3-41" morph="none" pos="word" start_char="665">no</TOKEN>
<TOKEN end_char="671" id="token-3-42" morph="none" pos="word" start_char="668">eran</TOKEN>
<TOKEN end_char="683" id="token-3-43" morph="none" pos="word" start_char="673">definitivos</TOKEN>
<TOKEN end_char="684" id="token-3-44" morph="none" pos="punct" start_char="684">.</TOKEN>
<TRANSLATED_TEXT>According to scientists at the South Asian Agricultural University, pangolins were the intermediate animals between bats and humans, however, the institution warned from the outset that the results were not definitive.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1059" id="segment-4" start_char="687">
<ORIGINAL_TEXT>El error, explicó Xiao Linhua, coautor del estudio, a la revista Nature, se debió a un error de comunicación entre el grupo de bioinformática y el grupo de laboratorio, pues la equivalencia del 99% del genoma se refería únicamente a una pequeña parte del genoma del coronavirus, es decir, sólo a la de una proteína que el virus utiliza para entrar a las células que afecta.</ORIGINAL_TEXT>
<TOKEN end_char="688" id="token-4-0" morph="none" pos="word" start_char="687">El</TOKEN>
<TOKEN end_char="694" id="token-4-1" morph="none" pos="word" start_char="690">error</TOKEN>
<TOKEN end_char="695" id="token-4-2" morph="none" pos="punct" start_char="695">,</TOKEN>
<TOKEN end_char="703" id="token-4-3" morph="none" pos="word" start_char="697">explicó</TOKEN>
<TOKEN end_char="708" id="token-4-4" morph="none" pos="word" start_char="705">Xiao</TOKEN>
<TOKEN end_char="715" id="token-4-5" morph="none" pos="word" start_char="710">Linhua</TOKEN>
<TOKEN end_char="716" id="token-4-6" morph="none" pos="punct" start_char="716">,</TOKEN>
<TOKEN end_char="724" id="token-4-7" morph="none" pos="word" start_char="718">coautor</TOKEN>
<TOKEN end_char="728" id="token-4-8" morph="none" pos="word" start_char="726">del</TOKEN>
<TOKEN end_char="736" id="token-4-9" morph="none" pos="word" start_char="730">estudio</TOKEN>
<TOKEN end_char="737" id="token-4-10" morph="none" pos="punct" start_char="737">,</TOKEN>
<TOKEN end_char="739" id="token-4-11" morph="none" pos="word" start_char="739">a</TOKEN>
<TOKEN end_char="742" id="token-4-12" morph="none" pos="word" start_char="741">la</TOKEN>
<TOKEN end_char="750" id="token-4-13" morph="none" pos="word" start_char="744">revista</TOKEN>
<TOKEN end_char="757" id="token-4-14" morph="none" pos="word" start_char="752">Nature</TOKEN>
<TOKEN end_char="758" id="token-4-15" morph="none" pos="punct" start_char="758">,</TOKEN>
<TOKEN end_char="761" id="token-4-16" morph="none" pos="word" start_char="760">se</TOKEN>
<TOKEN end_char="767" id="token-4-17" morph="none" pos="word" start_char="763">debió</TOKEN>
<TOKEN end_char="769" id="token-4-18" morph="none" pos="word" start_char="769">a</TOKEN>
<TOKEN end_char="772" id="token-4-19" morph="none" pos="word" start_char="771">un</TOKEN>
<TOKEN end_char="778" id="token-4-20" morph="none" pos="word" start_char="774">error</TOKEN>
<TOKEN end_char="781" id="token-4-21" morph="none" pos="word" start_char="780">de</TOKEN>
<TOKEN end_char="794" id="token-4-22" morph="none" pos="word" start_char="783">comunicación</TOKEN>
<TOKEN end_char="800" id="token-4-23" morph="none" pos="word" start_char="796">entre</TOKEN>
<TOKEN end_char="803" id="token-4-24" morph="none" pos="word" start_char="802">el</TOKEN>
<TOKEN end_char="809" id="token-4-25" morph="none" pos="word" start_char="805">grupo</TOKEN>
<TOKEN end_char="812" id="token-4-26" morph="none" pos="word" start_char="811">de</TOKEN>
<TOKEN end_char="827" id="token-4-27" morph="none" pos="word" start_char="814">bioinformática</TOKEN>
<TOKEN end_char="829" id="token-4-28" morph="none" pos="word" start_char="829">y</TOKEN>
<TOKEN end_char="832" id="token-4-29" morph="none" pos="word" start_char="831">el</TOKEN>
<TOKEN end_char="838" id="token-4-30" morph="none" pos="word" start_char="834">grupo</TOKEN>
<TOKEN end_char="841" id="token-4-31" morph="none" pos="word" start_char="840">de</TOKEN>
<TOKEN end_char="853" id="token-4-32" morph="none" pos="word" start_char="843">laboratorio</TOKEN>
<TOKEN end_char="854" id="token-4-33" morph="none" pos="punct" start_char="854">,</TOKEN>
<TOKEN end_char="859" id="token-4-34" morph="none" pos="word" start_char="856">pues</TOKEN>
<TOKEN end_char="862" id="token-4-35" morph="none" pos="word" start_char="861">la</TOKEN>
<TOKEN end_char="875" id="token-4-36" morph="none" pos="word" start_char="864">equivalencia</TOKEN>
<TOKEN end_char="879" id="token-4-37" morph="none" pos="word" start_char="877">del</TOKEN>
<TOKEN end_char="882" id="token-4-38" morph="none" pos="word" start_char="881">99</TOKEN>
<TOKEN end_char="883" id="token-4-39" morph="none" pos="punct" start_char="883">%</TOKEN>
<TOKEN end_char="887" id="token-4-40" morph="none" pos="word" start_char="885">del</TOKEN>
<TOKEN end_char="894" id="token-4-41" morph="none" pos="word" start_char="889">genoma</TOKEN>
<TOKEN end_char="897" id="token-4-42" morph="none" pos="word" start_char="896">se</TOKEN>
<TOKEN end_char="905" id="token-4-43" morph="none" pos="word" start_char="899">refería</TOKEN>
<TOKEN end_char="916" id="token-4-44" morph="none" pos="word" start_char="907">únicamente</TOKEN>
<TOKEN end_char="918" id="token-4-45" morph="none" pos="word" start_char="918">a</TOKEN>
<TOKEN end_char="922" id="token-4-46" morph="none" pos="word" start_char="920">una</TOKEN>
<TOKEN end_char="930" id="token-4-47" morph="none" pos="word" start_char="924">pequeña</TOKEN>
<TOKEN end_char="936" id="token-4-48" morph="none" pos="word" start_char="932">parte</TOKEN>
<TOKEN end_char="940" id="token-4-49" morph="none" pos="word" start_char="938">del</TOKEN>
<TOKEN end_char="947" id="token-4-50" morph="none" pos="word" start_char="942">genoma</TOKEN>
<TOKEN end_char="951" id="token-4-51" morph="none" pos="word" start_char="949">del</TOKEN>
<TOKEN end_char="963" id="token-4-52" morph="none" pos="word" start_char="953">coronavirus</TOKEN>
<TOKEN end_char="964" id="token-4-53" morph="none" pos="punct" start_char="964">,</TOKEN>
<TOKEN end_char="967" id="token-4-54" morph="none" pos="word" start_char="966">es</TOKEN>
<TOKEN end_char="973" id="token-4-55" morph="none" pos="word" start_char="969">decir</TOKEN>
<TOKEN end_char="974" id="token-4-56" morph="none" pos="punct" start_char="974">,</TOKEN>
<TOKEN end_char="979" id="token-4-57" morph="none" pos="word" start_char="976">sólo</TOKEN>
<TOKEN end_char="981" id="token-4-58" morph="none" pos="word" start_char="981">a</TOKEN>
<TOKEN end_char="984" id="token-4-59" morph="none" pos="word" start_char="983">la</TOKEN>
<TOKEN end_char="987" id="token-4-60" morph="none" pos="word" start_char="986">de</TOKEN>
<TOKEN end_char="991" id="token-4-61" morph="none" pos="word" start_char="989">una</TOKEN>
<TOKEN end_char="1000" id="token-4-62" morph="none" pos="word" start_char="993">proteína</TOKEN>
<TOKEN end_char="1004" id="token-4-63" morph="none" pos="word" start_char="1002">que</TOKEN>
<TOKEN end_char="1007" id="token-4-64" morph="none" pos="word" start_char="1006">el</TOKEN>
<TOKEN end_char="1013" id="token-4-65" morph="none" pos="word" start_char="1009">virus</TOKEN>
<TOKEN end_char="1021" id="token-4-66" morph="none" pos="word" start_char="1015">utiliza</TOKEN>
<TOKEN end_char="1026" id="token-4-67" morph="none" pos="word" start_char="1023">para</TOKEN>
<TOKEN end_char="1033" id="token-4-68" morph="none" pos="word" start_char="1028">entrar</TOKEN>
<TOKEN end_char="1035" id="token-4-69" morph="none" pos="word" start_char="1035">a</TOKEN>
<TOKEN end_char="1039" id="token-4-70" morph="none" pos="word" start_char="1037">las</TOKEN>
<TOKEN end_char="1047" id="token-4-71" morph="none" pos="word" start_char="1041">células</TOKEN>
<TOKEN end_char="1051" id="token-4-72" morph="none" pos="word" start_char="1049">que</TOKEN>
<TOKEN end_char="1058" id="token-4-73" morph="none" pos="word" start_char="1053">afecta</TOKEN>
<TOKEN end_char="1059" id="token-4-74" morph="none" pos="punct" start_char="1059">.</TOKEN>
<TRANSLATED_TEXT>The error, Xiao Linhua, co-author of the study, explained to Nature, was due to a communication error between the bioinformatics group and the laboratory group, as the 99% equivalence of the genome referred to only a small part of the coronavirus genome, i.e. only that of a protein that the virus uses to enter the cells it affects.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1165" id="segment-5" start_char="1062">
<ORIGINAL_TEXT>Sin embargo, si se tiene en cuenta la secuencia completa del genoma, la equivalencia se reduce al 90.3%.</ORIGINAL_TEXT>
<TOKEN end_char="1064" id="token-5-0" morph="none" pos="word" start_char="1062">Sin</TOKEN>
<TOKEN end_char="1072" id="token-5-1" morph="none" pos="word" start_char="1066">embargo</TOKEN>
<TOKEN end_char="1073" id="token-5-2" morph="none" pos="punct" start_char="1073">,</TOKEN>
<TOKEN end_char="1076" id="token-5-3" morph="none" pos="word" start_char="1075">si</TOKEN>
<TOKEN end_char="1079" id="token-5-4" morph="none" pos="word" start_char="1078">se</TOKEN>
<TOKEN end_char="1085" id="token-5-5" morph="none" pos="word" start_char="1081">tiene</TOKEN>
<TOKEN end_char="1088" id="token-5-6" morph="none" pos="word" start_char="1087">en</TOKEN>
<TOKEN end_char="1095" id="token-5-7" morph="none" pos="word" start_char="1090">cuenta</TOKEN>
<TOKEN end_char="1098" id="token-5-8" morph="none" pos="word" start_char="1097">la</TOKEN>
<TOKEN end_char="1108" id="token-5-9" morph="none" pos="word" start_char="1100">secuencia</TOKEN>
<TOKEN end_char="1117" id="token-5-10" morph="none" pos="word" start_char="1110">completa</TOKEN>
<TOKEN end_char="1121" id="token-5-11" morph="none" pos="word" start_char="1119">del</TOKEN>
<TOKEN end_char="1128" id="token-5-12" morph="none" pos="word" start_char="1123">genoma</TOKEN>
<TOKEN end_char="1129" id="token-5-13" morph="none" pos="punct" start_char="1129">,</TOKEN>
<TOKEN end_char="1132" id="token-5-14" morph="none" pos="word" start_char="1131">la</TOKEN>
<TOKEN end_char="1145" id="token-5-15" morph="none" pos="word" start_char="1134">equivalencia</TOKEN>
<TOKEN end_char="1148" id="token-5-16" morph="none" pos="word" start_char="1147">se</TOKEN>
<TOKEN end_char="1155" id="token-5-17" morph="none" pos="word" start_char="1150">reduce</TOKEN>
<TOKEN end_char="1158" id="token-5-18" morph="none" pos="word" start_char="1157">al</TOKEN>
<TOKEN end_char="1163" id="token-5-19" morph="none" pos="unknown" start_char="1160">90.3</TOKEN>
<TOKEN end_char="1165" id="token-5-20" morph="none" pos="punct" start_char="1164">%.</TOKEN>
<TRANSLATED_TEXT>However, if the complete genome sequence is taken into account, the equivalence is reduced to 90.3%.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>