<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04ATFM" lang="spa" raw_text_char_length="761" raw_text_md5="85e099bc33445eb0fb2fa20032b551e8" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="148" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Originally Answered: Since the CDC announced COVID immunity only lasts 3 months, will the immunity from the vaccines in development last any longer?</ORIGINAL_TEXT>
<TOKEN end_char="10" id="token-0-0" morph="none" pos="word" start_char="1">Originally</TOKEN>
<TOKEN end_char="19" id="token-0-1" morph="none" pos="word" start_char="12">Answered</TOKEN>
<TOKEN end_char="20" id="token-0-2" morph="none" pos="punct" start_char="20">:</TOKEN>
<TOKEN end_char="26" id="token-0-3" morph="none" pos="word" start_char="22">Since</TOKEN>
<TOKEN end_char="30" id="token-0-4" morph="none" pos="word" start_char="28">the</TOKEN>
<TOKEN end_char="34" id="token-0-5" morph="none" pos="word" start_char="32">CDC</TOKEN>
<TOKEN end_char="44" id="token-0-6" morph="none" pos="word" start_char="36">announced</TOKEN>
<TOKEN end_char="50" id="token-0-7" morph="none" pos="word" start_char="46">COVID</TOKEN>
<TOKEN end_char="59" id="token-0-8" morph="none" pos="word" start_char="52">immunity</TOKEN>
<TOKEN end_char="64" id="token-0-9" morph="none" pos="word" start_char="61">only</TOKEN>
<TOKEN end_char="70" id="token-0-10" morph="none" pos="word" start_char="66">lasts</TOKEN>
<TOKEN end_char="72" id="token-0-11" morph="none" pos="word" start_char="72">3</TOKEN>
<TOKEN end_char="79" id="token-0-12" morph="none" pos="word" start_char="74">months</TOKEN>
<TOKEN end_char="80" id="token-0-13" morph="none" pos="punct" start_char="80">,</TOKEN>
<TOKEN end_char="85" id="token-0-14" morph="none" pos="word" start_char="82">will</TOKEN>
<TOKEN end_char="89" id="token-0-15" morph="none" pos="word" start_char="87">the</TOKEN>
<TOKEN end_char="98" id="token-0-16" morph="none" pos="word" start_char="91">immunity</TOKEN>
<TOKEN end_char="103" id="token-0-17" morph="none" pos="word" start_char="100">from</TOKEN>
<TOKEN end_char="107" id="token-0-18" morph="none" pos="word" start_char="105">the</TOKEN>
<TOKEN end_char="116" id="token-0-19" morph="none" pos="word" start_char="109">vaccines</TOKEN>
<TOKEN end_char="119" id="token-0-20" morph="none" pos="word" start_char="118">in</TOKEN>
<TOKEN end_char="131" id="token-0-21" morph="none" pos="word" start_char="121">development</TOKEN>
<TOKEN end_char="136" id="token-0-22" morph="none" pos="word" start_char="133">last</TOKEN>
<TOKEN end_char="140" id="token-0-23" morph="none" pos="word" start_char="138">any</TOKEN>
<TOKEN end_char="147" id="token-0-24" morph="none" pos="word" start_char="142">longer</TOKEN>
<TOKEN end_char="148" id="token-0-25" morph="none" pos="punct" start_char="148">?</TOKEN>
</SEG>
<SEG end_char="194" id="segment-1" start_char="152">
<ORIGINAL_TEXT>They didn’t say that, and it can’t be true.</ORIGINAL_TEXT>
<TOKEN end_char="155" id="token-1-0" morph="none" pos="word" start_char="152">They</TOKEN>
<TOKEN end_char="162" id="token-1-1" morph="none" pos="word" start_char="157">didn’t</TOKEN>
<TOKEN end_char="166" id="token-1-2" morph="none" pos="word" start_char="164">say</TOKEN>
<TOKEN end_char="171" id="token-1-3" morph="none" pos="word" start_char="168">that</TOKEN>
<TOKEN end_char="172" id="token-1-4" morph="none" pos="punct" start_char="172">,</TOKEN>
<TOKEN end_char="176" id="token-1-5" morph="none" pos="word" start_char="174">and</TOKEN>
<TOKEN end_char="179" id="token-1-6" morph="none" pos="word" start_char="178">it</TOKEN>
<TOKEN end_char="185" id="token-1-7" morph="none" pos="word" start_char="181">can’t</TOKEN>
<TOKEN end_char="188" id="token-1-8" morph="none" pos="word" start_char="187">be</TOKEN>
<TOKEN end_char="193" id="token-1-9" morph="none" pos="word" start_char="190">true</TOKEN>
<TOKEN end_char="194" id="token-1-10" morph="none" pos="punct" start_char="194">.</TOKEN>
</SEG>
<SEG end_char="458" id="segment-2" start_char="197">
<ORIGINAL_TEXT>If immunity lasted only three months for most people we would be seeing a wave of thousands of second-time infections in Italy, Spain, the UK, France, and New York by now from the nearly million people who had tested positive in those places by the end of April.</ORIGINAL_TEXT>
<TOKEN end_char="198" id="token-2-0" morph="none" pos="word" start_char="197">If</TOKEN>
<TOKEN end_char="207" id="token-2-1" morph="none" pos="word" start_char="200">immunity</TOKEN>
<TOKEN end_char="214" id="token-2-2" morph="none" pos="word" start_char="209">lasted</TOKEN>
<TOKEN end_char="219" id="token-2-3" morph="none" pos="word" start_char="216">only</TOKEN>
<TOKEN end_char="225" id="token-2-4" morph="none" pos="word" start_char="221">three</TOKEN>
<TOKEN end_char="232" id="token-2-5" morph="none" pos="word" start_char="227">months</TOKEN>
<TOKEN end_char="236" id="token-2-6" morph="none" pos="word" start_char="234">for</TOKEN>
<TOKEN end_char="241" id="token-2-7" morph="none" pos="word" start_char="238">most</TOKEN>
<TOKEN end_char="248" id="token-2-8" morph="none" pos="word" start_char="243">people</TOKEN>
<TOKEN end_char="251" id="token-2-9" morph="none" pos="word" start_char="250">we</TOKEN>
<TOKEN end_char="257" id="token-2-10" morph="none" pos="word" start_char="253">would</TOKEN>
<TOKEN end_char="260" id="token-2-11" morph="none" pos="word" start_char="259">be</TOKEN>
<TOKEN end_char="267" id="token-2-12" morph="none" pos="word" start_char="262">seeing</TOKEN>
<TOKEN end_char="269" id="token-2-13" morph="none" pos="word" start_char="269">a</TOKEN>
<TOKEN end_char="274" id="token-2-14" morph="none" pos="word" start_char="271">wave</TOKEN>
<TOKEN end_char="277" id="token-2-15" morph="none" pos="word" start_char="276">of</TOKEN>
<TOKEN end_char="287" id="token-2-16" morph="none" pos="word" start_char="279">thousands</TOKEN>
<TOKEN end_char="290" id="token-2-17" morph="none" pos="word" start_char="289">of</TOKEN>
<TOKEN end_char="302" id="token-2-18" morph="none" pos="unknown" start_char="292">second-time</TOKEN>
<TOKEN end_char="313" id="token-2-19" morph="none" pos="word" start_char="304">infections</TOKEN>
<TOKEN end_char="316" id="token-2-20" morph="none" pos="word" start_char="315">in</TOKEN>
<TOKEN end_char="322" id="token-2-21" morph="none" pos="word" start_char="318">Italy</TOKEN>
<TOKEN end_char="323" id="token-2-22" morph="none" pos="punct" start_char="323">,</TOKEN>
<TOKEN end_char="329" id="token-2-23" morph="none" pos="word" start_char="325">Spain</TOKEN>
<TOKEN end_char="330" id="token-2-24" morph="none" pos="punct" start_char="330">,</TOKEN>
<TOKEN end_char="334" id="token-2-25" morph="none" pos="word" start_char="332">the</TOKEN>
<TOKEN end_char="337" id="token-2-26" morph="none" pos="word" start_char="336">UK</TOKEN>
<TOKEN end_char="338" id="token-2-27" morph="none" pos="punct" start_char="338">,</TOKEN>
<TOKEN end_char="345" id="token-2-28" morph="none" pos="word" start_char="340">France</TOKEN>
<TOKEN end_char="346" id="token-2-29" morph="none" pos="punct" start_char="346">,</TOKEN>
<TOKEN end_char="350" id="token-2-30" morph="none" pos="word" start_char="348">and</TOKEN>
<TOKEN end_char="354" id="token-2-31" morph="none" pos="word" start_char="352">New</TOKEN>
<TOKEN end_char="359" id="token-2-32" morph="none" pos="word" start_char="356">York</TOKEN>
<TOKEN end_char="362" id="token-2-33" morph="none" pos="word" start_char="361">by</TOKEN>
<TOKEN end_char="366" id="token-2-34" morph="none" pos="word" start_char="364">now</TOKEN>
<TOKEN end_char="371" id="token-2-35" morph="none" pos="word" start_char="368">from</TOKEN>
<TOKEN end_char="375" id="token-2-36" morph="none" pos="word" start_char="373">the</TOKEN>
<TOKEN end_char="382" id="token-2-37" morph="none" pos="word" start_char="377">nearly</TOKEN>
<TOKEN end_char="390" id="token-2-38" morph="none" pos="word" start_char="384">million</TOKEN>
<TOKEN end_char="397" id="token-2-39" morph="none" pos="word" start_char="392">people</TOKEN>
<TOKEN end_char="401" id="token-2-40" morph="none" pos="word" start_char="399">who</TOKEN>
<TOKEN end_char="405" id="token-2-41" morph="none" pos="word" start_char="403">had</TOKEN>
<TOKEN end_char="412" id="token-2-42" morph="none" pos="word" start_char="407">tested</TOKEN>
<TOKEN end_char="421" id="token-2-43" morph="none" pos="word" start_char="414">positive</TOKEN>
<TOKEN end_char="424" id="token-2-44" morph="none" pos="word" start_char="423">in</TOKEN>
<TOKEN end_char="430" id="token-2-45" morph="none" pos="word" start_char="426">those</TOKEN>
<TOKEN end_char="437" id="token-2-46" morph="none" pos="word" start_char="432">places</TOKEN>
<TOKEN end_char="440" id="token-2-47" morph="none" pos="word" start_char="439">by</TOKEN>
<TOKEN end_char="444" id="token-2-48" morph="none" pos="word" start_char="442">the</TOKEN>
<TOKEN end_char="448" id="token-2-49" morph="none" pos="word" start_char="446">end</TOKEN>
<TOKEN end_char="451" id="token-2-50" morph="none" pos="word" start_char="450">of</TOKEN>
<TOKEN end_char="457" id="token-2-51" morph="none" pos="word" start_char="453">April</TOKEN>
<TOKEN end_char="458" id="token-2-52" morph="none" pos="punct" start_char="458">.</TOKEN>
</SEG>
<SEG end_char="470" id="segment-3" start_char="461">
<ORIGINAL_TEXT>We aren’t.</ORIGINAL_TEXT>
<TOKEN end_char="462" id="token-3-0" morph="none" pos="word" start_char="461">We</TOKEN>
<TOKEN end_char="469" id="token-3-1" morph="none" pos="word" start_char="464">aren’t</TOKEN>
<TOKEN end_char="470" id="token-3-2" morph="none" pos="punct" start_char="470">.</TOKEN>
<TRANSLATED_TEXT>We're not.</TRANSLATED_TEXT><DETECTED_LANGUAGE>nl</DETECTED_LANGUAGE></SEG>
<SEG end_char="572" id="segment-4" start_char="472">
<ORIGINAL_TEXT>We are seeing a tiny handful of probable laboratory errors in Spain, and no symptomatic reinfections.</ORIGINAL_TEXT>
<TOKEN end_char="473" id="token-4-0" morph="none" pos="word" start_char="472">We</TOKEN>
<TOKEN end_char="477" id="token-4-1" morph="none" pos="word" start_char="475">are</TOKEN>
<TOKEN end_char="484" id="token-4-2" morph="none" pos="word" start_char="479">seeing</TOKEN>
<TOKEN end_char="486" id="token-4-3" morph="none" pos="word" start_char="486">a</TOKEN>
<TOKEN end_char="491" id="token-4-4" morph="none" pos="word" start_char="488">tiny</TOKEN>
<TOKEN end_char="499" id="token-4-5" morph="none" pos="word" start_char="493">handful</TOKEN>
<TOKEN end_char="502" id="token-4-6" morph="none" pos="word" start_char="501">of</TOKEN>
<TOKEN end_char="511" id="token-4-7" morph="none" pos="word" start_char="504">probable</TOKEN>
<TOKEN end_char="522" id="token-4-8" morph="none" pos="word" start_char="513">laboratory</TOKEN>
<TOKEN end_char="529" id="token-4-9" morph="none" pos="word" start_char="524">errors</TOKEN>
<TOKEN end_char="532" id="token-4-10" morph="none" pos="word" start_char="531">in</TOKEN>
<TOKEN end_char="538" id="token-4-11" morph="none" pos="word" start_char="534">Spain</TOKEN>
<TOKEN end_char="539" id="token-4-12" morph="none" pos="punct" start_char="539">,</TOKEN>
<TOKEN end_char="543" id="token-4-13" morph="none" pos="word" start_char="541">and</TOKEN>
<TOKEN end_char="546" id="token-4-14" morph="none" pos="word" start_char="545">no</TOKEN>
<TOKEN end_char="558" id="token-4-15" morph="none" pos="word" start_char="548">symptomatic</TOKEN>
<TOKEN end_char="571" id="token-4-16" morph="none" pos="word" start_char="560">reinfections</TOKEN>
<TOKEN end_char="572" id="token-4-17" morph="none" pos="punct" start_char="572">.</TOKEN>
</SEG>
<SEG end_char="618" id="segment-5" start_char="575">
<ORIGINAL_TEXT>We know that it lasts at least three months.</ORIGINAL_TEXT>
<TOKEN end_char="576" id="token-5-0" morph="none" pos="word" start_char="575">We</TOKEN>
<TOKEN end_char="581" id="token-5-1" morph="none" pos="word" start_char="578">know</TOKEN>
<TOKEN end_char="586" id="token-5-2" morph="none" pos="word" start_char="583">that</TOKEN>
<TOKEN end_char="589" id="token-5-3" morph="none" pos="word" start_char="588">it</TOKEN>
<TOKEN end_char="595" id="token-5-4" morph="none" pos="word" start_char="591">lasts</TOKEN>
<TOKEN end_char="598" id="token-5-5" morph="none" pos="word" start_char="597">at</TOKEN>
<TOKEN end_char="604" id="token-5-6" morph="none" pos="word" start_char="600">least</TOKEN>
<TOKEN end_char="610" id="token-5-7" morph="none" pos="word" start_char="606">three</TOKEN>
<TOKEN end_char="617" id="token-5-8" morph="none" pos="word" start_char="612">months</TOKEN>
<TOKEN end_char="618" id="token-5-9" morph="none" pos="punct" start_char="618">.</TOKEN>
</SEG>
<SEG end_char="749" id="segment-6" start_char="620">
<ORIGINAL_TEXT>In fact, given essentially no reinfections after three months, it almost certainly lasts at least six months for almost everybody.</ORIGINAL_TEXT>
<TOKEN end_char="621" id="token-6-0" morph="none" pos="word" start_char="620">In</TOKEN>
<TOKEN end_char="626" id="token-6-1" morph="none" pos="word" start_char="623">fact</TOKEN>
<TOKEN end_char="627" id="token-6-2" morph="none" pos="punct" start_char="627">,</TOKEN>
<TOKEN end_char="633" id="token-6-3" morph="none" pos="word" start_char="629">given</TOKEN>
<TOKEN end_char="645" id="token-6-4" morph="none" pos="word" start_char="635">essentially</TOKEN>
<TOKEN end_char="648" id="token-6-5" morph="none" pos="word" start_char="647">no</TOKEN>
<TOKEN end_char="661" id="token-6-6" morph="none" pos="word" start_char="650">reinfections</TOKEN>
<TOKEN end_char="667" id="token-6-7" morph="none" pos="word" start_char="663">after</TOKEN>
<TOKEN end_char="673" id="token-6-8" morph="none" pos="word" start_char="669">three</TOKEN>
<TOKEN end_char="680" id="token-6-9" morph="none" pos="word" start_char="675">months</TOKEN>
<TOKEN end_char="681" id="token-6-10" morph="none" pos="punct" start_char="681">,</TOKEN>
<TOKEN end_char="684" id="token-6-11" morph="none" pos="word" start_char="683">it</TOKEN>
<TOKEN end_char="691" id="token-6-12" morph="none" pos="word" start_char="686">almost</TOKEN>
<TOKEN end_char="701" id="token-6-13" morph="none" pos="word" start_char="693">certainly</TOKEN>
<TOKEN end_char="707" id="token-6-14" morph="none" pos="word" start_char="703">lasts</TOKEN>
<TOKEN end_char="710" id="token-6-15" morph="none" pos="word" start_char="709">at</TOKEN>
<TOKEN end_char="716" id="token-6-16" morph="none" pos="word" start_char="712">least</TOKEN>
<TOKEN end_char="720" id="token-6-17" morph="none" pos="word" start_char="718">six</TOKEN>
<TOKEN end_char="727" id="token-6-18" morph="none" pos="word" start_char="722">months</TOKEN>
<TOKEN end_char="731" id="token-6-19" morph="none" pos="word" start_char="729">for</TOKEN>
<TOKEN end_char="738" id="token-6-20" morph="none" pos="word" start_char="733">almost</TOKEN>
<TOKEN end_char="748" id="token-6-21" morph="none" pos="word" start_char="740">everybody</TOKEN>
<TOKEN end_char="749" id="token-6-22" morph="none" pos="punct" start_char="749">.</TOKEN>
</SEG>
<SEG end_char="757" id="segment-7" start_char="751">
<ORIGINAL_TEXT>In thre</ORIGINAL_TEXT>
<TOKEN end_char="752" id="token-7-0" morph="none" pos="word" start_char="751">In</TOKEN>
<TOKEN end_char="757" id="token-7-1" morph="none" pos="word" start_char="754">thre</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>