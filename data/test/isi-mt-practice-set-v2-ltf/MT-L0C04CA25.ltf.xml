<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C04CA25" lang="eng" raw_text_char_length="4057" raw_text_md5="3f9246e166177c1d42c81779ef796b6a" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="71" id="segment-0" start_char="1">
<ORIGINAL_TEXT>New coronavirus spread swiftly around world from late 2019, study finds</ORIGINAL_TEXT>
<TOKEN end_char="3" id="token-0-0" morph="none" pos="word" start_char="1">New</TOKEN>
<TOKEN end_char="15" id="token-0-1" morph="none" pos="word" start_char="5">coronavirus</TOKEN>
<TOKEN end_char="22" id="token-0-2" morph="none" pos="word" start_char="17">spread</TOKEN>
<TOKEN end_char="30" id="token-0-3" morph="none" pos="word" start_char="24">swiftly</TOKEN>
<TOKEN end_char="37" id="token-0-4" morph="none" pos="word" start_char="32">around</TOKEN>
<TOKEN end_char="43" id="token-0-5" morph="none" pos="word" start_char="39">world</TOKEN>
<TOKEN end_char="48" id="token-0-6" morph="none" pos="word" start_char="45">from</TOKEN>
<TOKEN end_char="53" id="token-0-7" morph="none" pos="word" start_char="50">late</TOKEN>
<TOKEN end_char="58" id="token-0-8" morph="none" pos="word" start_char="55">2019</TOKEN>
<TOKEN end_char="59" id="token-0-9" morph="none" pos="punct" start_char="59">,</TOKEN>
<TOKEN end_char="65" id="token-0-10" morph="none" pos="word" start_char="61">study</TOKEN>
<TOKEN end_char="71" id="token-0-11" morph="none" pos="word" start_char="67">finds</TOKEN>
</SEG>
<SEG end_char="336" id="segment-1" start_char="75">
<ORIGINAL_TEXT>LONDON (Reuters) - A genetic study of samples from more than 7,500 people infected with COVID-19 suggests the new coronavirus spread quickly around the world after it emerged in China sometime between October and December last year, scientists said on Wednesday.</ORIGINAL_TEXT>
<TOKEN end_char="80" id="token-1-0" morph="none" pos="word" start_char="75">LONDON</TOKEN>
<TOKEN end_char="82" id="token-1-1" morph="none" pos="punct" start_char="82">(</TOKEN>
<TOKEN end_char="89" id="token-1-2" morph="none" pos="word" start_char="83">Reuters</TOKEN>
<TOKEN end_char="90" id="token-1-3" morph="none" pos="punct" start_char="90">)</TOKEN>
<TOKEN end_char="92" id="token-1-4" morph="none" pos="punct" start_char="92">-</TOKEN>
<TOKEN end_char="94" id="token-1-5" morph="none" pos="word" start_char="94">A</TOKEN>
<TOKEN end_char="102" id="token-1-6" morph="none" pos="word" start_char="96">genetic</TOKEN>
<TOKEN end_char="108" id="token-1-7" morph="none" pos="word" start_char="104">study</TOKEN>
<TOKEN end_char="111" id="token-1-8" morph="none" pos="word" start_char="110">of</TOKEN>
<TOKEN end_char="119" id="token-1-9" morph="none" pos="word" start_char="113">samples</TOKEN>
<TOKEN end_char="124" id="token-1-10" morph="none" pos="word" start_char="121">from</TOKEN>
<TOKEN end_char="129" id="token-1-11" morph="none" pos="word" start_char="126">more</TOKEN>
<TOKEN end_char="134" id="token-1-12" morph="none" pos="word" start_char="131">than</TOKEN>
<TOKEN end_char="140" id="token-1-13" morph="none" pos="unknown" start_char="136">7,500</TOKEN>
<TOKEN end_char="147" id="token-1-14" morph="none" pos="word" start_char="142">people</TOKEN>
<TOKEN end_char="156" id="token-1-15" morph="none" pos="word" start_char="149">infected</TOKEN>
<TOKEN end_char="161" id="token-1-16" morph="none" pos="word" start_char="158">with</TOKEN>
<TOKEN end_char="170" id="token-1-17" morph="none" pos="unknown" start_char="163">COVID-19</TOKEN>
<TOKEN end_char="179" id="token-1-18" morph="none" pos="word" start_char="172">suggests</TOKEN>
<TOKEN end_char="183" id="token-1-19" morph="none" pos="word" start_char="181">the</TOKEN>
<TOKEN end_char="187" id="token-1-20" morph="none" pos="word" start_char="185">new</TOKEN>
<TOKEN end_char="199" id="token-1-21" morph="none" pos="word" start_char="189">coronavirus</TOKEN>
<TOKEN end_char="206" id="token-1-22" morph="none" pos="word" start_char="201">spread</TOKEN>
<TOKEN end_char="214" id="token-1-23" morph="none" pos="word" start_char="208">quickly</TOKEN>
<TOKEN end_char="221" id="token-1-24" morph="none" pos="word" start_char="216">around</TOKEN>
<TOKEN end_char="225" id="token-1-25" morph="none" pos="word" start_char="223">the</TOKEN>
<TOKEN end_char="231" id="token-1-26" morph="none" pos="word" start_char="227">world</TOKEN>
<TOKEN end_char="237" id="token-1-27" morph="none" pos="word" start_char="233">after</TOKEN>
<TOKEN end_char="240" id="token-1-28" morph="none" pos="word" start_char="239">it</TOKEN>
<TOKEN end_char="248" id="token-1-29" morph="none" pos="word" start_char="242">emerged</TOKEN>
<TOKEN end_char="251" id="token-1-30" morph="none" pos="word" start_char="250">in</TOKEN>
<TOKEN end_char="257" id="token-1-31" morph="none" pos="word" start_char="253">China</TOKEN>
<TOKEN end_char="266" id="token-1-32" morph="none" pos="word" start_char="259">sometime</TOKEN>
<TOKEN end_char="274" id="token-1-33" morph="none" pos="word" start_char="268">between</TOKEN>
<TOKEN end_char="282" id="token-1-34" morph="none" pos="word" start_char="276">October</TOKEN>
<TOKEN end_char="286" id="token-1-35" morph="none" pos="word" start_char="284">and</TOKEN>
<TOKEN end_char="295" id="token-1-36" morph="none" pos="word" start_char="288">December</TOKEN>
<TOKEN end_char="300" id="token-1-37" morph="none" pos="word" start_char="297">last</TOKEN>
<TOKEN end_char="305" id="token-1-38" morph="none" pos="word" start_char="302">year</TOKEN>
<TOKEN end_char="306" id="token-1-39" morph="none" pos="punct" start_char="306">,</TOKEN>
<TOKEN end_char="317" id="token-1-40" morph="none" pos="word" start_char="308">scientists</TOKEN>
<TOKEN end_char="322" id="token-1-41" morph="none" pos="word" start_char="319">said</TOKEN>
<TOKEN end_char="325" id="token-1-42" morph="none" pos="word" start_char="324">on</TOKEN>
<TOKEN end_char="335" id="token-1-43" morph="none" pos="word" start_char="327">Wednesday</TOKEN>
<TOKEN end_char="336" id="token-1-44" morph="none" pos="punct" start_char="336">.</TOKEN>
</SEG>
<SEG end_char="673" id="segment-2" start_char="339">
<ORIGINAL_TEXT>FILE PHOTO: The ultrastructural morphology exhibited by the 2019 Novel Coronavirus (2019-nCoV), which was identified as the cause of an outbreak of respiratory illness first detected in Wuhan, China, is seen in an illustration released by the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, U.S. January 29, 2020.</ORIGINAL_TEXT>
<TOKEN end_char="342" id="token-2-0" morph="none" pos="word" start_char="339">FILE</TOKEN>
<TOKEN end_char="348" id="token-2-1" morph="none" pos="word" start_char="344">PHOTO</TOKEN>
<TOKEN end_char="349" id="token-2-2" morph="none" pos="punct" start_char="349">:</TOKEN>
<TOKEN end_char="353" id="token-2-3" morph="none" pos="word" start_char="351">The</TOKEN>
<TOKEN end_char="369" id="token-2-4" morph="none" pos="word" start_char="355">ultrastructural</TOKEN>
<TOKEN end_char="380" id="token-2-5" morph="none" pos="word" start_char="371">morphology</TOKEN>
<TOKEN end_char="390" id="token-2-6" morph="none" pos="word" start_char="382">exhibited</TOKEN>
<TOKEN end_char="393" id="token-2-7" morph="none" pos="word" start_char="392">by</TOKEN>
<TOKEN end_char="397" id="token-2-8" morph="none" pos="word" start_char="395">the</TOKEN>
<TOKEN end_char="402" id="token-2-9" morph="none" pos="word" start_char="399">2019</TOKEN>
<TOKEN end_char="408" id="token-2-10" morph="none" pos="word" start_char="404">Novel</TOKEN>
<TOKEN end_char="420" id="token-2-11" morph="none" pos="word" start_char="410">Coronavirus</TOKEN>
<TOKEN end_char="422" id="token-2-12" morph="none" pos="punct" start_char="422">(</TOKEN>
<TOKEN end_char="431" id="token-2-13" morph="none" pos="unknown" start_char="423">2019-nCoV</TOKEN>
<TOKEN end_char="433" id="token-2-14" morph="none" pos="punct" start_char="432">),</TOKEN>
<TOKEN end_char="439" id="token-2-15" morph="none" pos="word" start_char="435">which</TOKEN>
<TOKEN end_char="443" id="token-2-16" morph="none" pos="word" start_char="441">was</TOKEN>
<TOKEN end_char="454" id="token-2-17" morph="none" pos="word" start_char="445">identified</TOKEN>
<TOKEN end_char="457" id="token-2-18" morph="none" pos="word" start_char="456">as</TOKEN>
<TOKEN end_char="461" id="token-2-19" morph="none" pos="word" start_char="459">the</TOKEN>
<TOKEN end_char="467" id="token-2-20" morph="none" pos="word" start_char="463">cause</TOKEN>
<TOKEN end_char="470" id="token-2-21" morph="none" pos="word" start_char="469">of</TOKEN>
<TOKEN end_char="473" id="token-2-22" morph="none" pos="word" start_char="472">an</TOKEN>
<TOKEN end_char="482" id="token-2-23" morph="none" pos="word" start_char="475">outbreak</TOKEN>
<TOKEN end_char="485" id="token-2-24" morph="none" pos="word" start_char="484">of</TOKEN>
<TOKEN end_char="497" id="token-2-25" morph="none" pos="word" start_char="487">respiratory</TOKEN>
<TOKEN end_char="505" id="token-2-26" morph="none" pos="word" start_char="499">illness</TOKEN>
<TOKEN end_char="511" id="token-2-27" morph="none" pos="word" start_char="507">first</TOKEN>
<TOKEN end_char="520" id="token-2-28" morph="none" pos="word" start_char="513">detected</TOKEN>
<TOKEN end_char="523" id="token-2-29" morph="none" pos="word" start_char="522">in</TOKEN>
<TOKEN end_char="529" id="token-2-30" morph="none" pos="word" start_char="525">Wuhan</TOKEN>
<TOKEN end_char="530" id="token-2-31" morph="none" pos="punct" start_char="530">,</TOKEN>
<TOKEN end_char="536" id="token-2-32" morph="none" pos="word" start_char="532">China</TOKEN>
<TOKEN end_char="537" id="token-2-33" morph="none" pos="punct" start_char="537">,</TOKEN>
<TOKEN end_char="540" id="token-2-34" morph="none" pos="word" start_char="539">is</TOKEN>
<TOKEN end_char="545" id="token-2-35" morph="none" pos="word" start_char="542">seen</TOKEN>
<TOKEN end_char="548" id="token-2-36" morph="none" pos="word" start_char="547">in</TOKEN>
<TOKEN end_char="551" id="token-2-37" morph="none" pos="word" start_char="550">an</TOKEN>
<TOKEN end_char="564" id="token-2-38" morph="none" pos="word" start_char="553">illustration</TOKEN>
<TOKEN end_char="573" id="token-2-39" morph="none" pos="word" start_char="566">released</TOKEN>
<TOKEN end_char="576" id="token-2-40" morph="none" pos="word" start_char="575">by</TOKEN>
<TOKEN end_char="580" id="token-2-41" morph="none" pos="word" start_char="578">the</TOKEN>
<TOKEN end_char="588" id="token-2-42" morph="none" pos="word" start_char="582">Centers</TOKEN>
<TOKEN end_char="592" id="token-2-43" morph="none" pos="word" start_char="590">for</TOKEN>
<TOKEN end_char="600" id="token-2-44" morph="none" pos="word" start_char="594">Disease</TOKEN>
<TOKEN end_char="608" id="token-2-45" morph="none" pos="word" start_char="602">Control</TOKEN>
<TOKEN end_char="612" id="token-2-46" morph="none" pos="word" start_char="610">and</TOKEN>
<TOKEN end_char="623" id="token-2-47" morph="none" pos="word" start_char="614">Prevention</TOKEN>
<TOKEN end_char="625" id="token-2-48" morph="none" pos="punct" start_char="625">(</TOKEN>
<TOKEN end_char="628" id="token-2-49" morph="none" pos="word" start_char="626">CDC</TOKEN>
<TOKEN end_char="629" id="token-2-50" morph="none" pos="punct" start_char="629">)</TOKEN>
<TOKEN end_char="632" id="token-2-51" morph="none" pos="word" start_char="631">in</TOKEN>
<TOKEN end_char="640" id="token-2-52" morph="none" pos="word" start_char="634">Atlanta</TOKEN>
<TOKEN end_char="641" id="token-2-53" morph="none" pos="punct" start_char="641">,</TOKEN>
<TOKEN end_char="649" id="token-2-54" morph="none" pos="word" start_char="643">Georgia</TOKEN>
<TOKEN end_char="650" id="token-2-55" morph="none" pos="punct" start_char="650">,</TOKEN>
<TOKEN end_char="654" id="token-2-56" morph="none" pos="unknown" start_char="652">U.S</TOKEN>
<TOKEN end_char="655" id="token-2-57" morph="none" pos="punct" start_char="655">.</TOKEN>
<TOKEN end_char="663" id="token-2-58" morph="none" pos="word" start_char="657">January</TOKEN>
<TOKEN end_char="666" id="token-2-59" morph="none" pos="word" start_char="665">29</TOKEN>
<TOKEN end_char="667" id="token-2-60" morph="none" pos="punct" start_char="667">,</TOKEN>
<TOKEN end_char="672" id="token-2-61" morph="none" pos="word" start_char="669">2020</TOKEN>
<TOKEN end_char="673" id="token-2-62" morph="none" pos="punct" start_char="673">.</TOKEN>
</SEG>
<SEG end_char="734" id="segment-3" start_char="675">
<ORIGINAL_TEXT>Alissa Eckert, MS; Dan Higgins, MAM/CDC/Handout via REUTERS.</ORIGINAL_TEXT>
<TOKEN end_char="680" id="token-3-0" morph="none" pos="word" start_char="675">Alissa</TOKEN>
<TOKEN end_char="687" id="token-3-1" morph="none" pos="word" start_char="682">Eckert</TOKEN>
<TOKEN end_char="688" id="token-3-2" morph="none" pos="punct" start_char="688">,</TOKEN>
<TOKEN end_char="691" id="token-3-3" morph="none" pos="word" start_char="690">MS</TOKEN>
<TOKEN end_char="692" id="token-3-4" morph="none" pos="punct" start_char="692">;</TOKEN>
<TOKEN end_char="696" id="token-3-5" morph="none" pos="word" start_char="694">Dan</TOKEN>
<TOKEN end_char="704" id="token-3-6" morph="none" pos="word" start_char="698">Higgins</TOKEN>
<TOKEN end_char="705" id="token-3-7" morph="none" pos="punct" start_char="705">,</TOKEN>
<TOKEN end_char="721" id="token-3-8" morph="none" pos="unknown" start_char="707">MAM/CDC/Handout</TOKEN>
<TOKEN end_char="725" id="token-3-9" morph="none" pos="word" start_char="723">via</TOKEN>
<TOKEN end_char="733" id="token-3-10" morph="none" pos="word" start_char="727">REUTERS</TOKEN>
<TOKEN end_char="734" id="token-3-11" morph="none" pos="punct" start_char="734">.</TOKEN>
<TRANSLATED_TEXT>Alissa Eckert, MS; Dan Higgins, MAM / CDC / Handout via REUTERS.</TRANSLATED_TEXT><DETECTED_LANGUAGE>sv</DETECTED_LANGUAGE></SEG>
<SEG end_char="971" id="segment-4" start_char="738">
<ORIGINAL_TEXT>Scientists at University College London’s Genetics Institute found almost 200 recurrent genetic mutations of the new coronavirus - SARS-CoV-2 - which the UCL researchers said showed how it is adapting to its human hosts as it spreads.</ORIGINAL_TEXT>
<TOKEN end_char="747" id="token-4-0" morph="none" pos="word" start_char="738">Scientists</TOKEN>
<TOKEN end_char="750" id="token-4-1" morph="none" pos="word" start_char="749">at</TOKEN>
<TOKEN end_char="761" id="token-4-2" morph="none" pos="word" start_char="752">University</TOKEN>
<TOKEN end_char="769" id="token-4-3" morph="none" pos="word" start_char="763">College</TOKEN>
<TOKEN end_char="778" id="token-4-4" morph="none" pos="word" start_char="771">London’s</TOKEN>
<TOKEN end_char="787" id="token-4-5" morph="none" pos="word" start_char="780">Genetics</TOKEN>
<TOKEN end_char="797" id="token-4-6" morph="none" pos="word" start_char="789">Institute</TOKEN>
<TOKEN end_char="803" id="token-4-7" morph="none" pos="word" start_char="799">found</TOKEN>
<TOKEN end_char="810" id="token-4-8" morph="none" pos="word" start_char="805">almost</TOKEN>
<TOKEN end_char="814" id="token-4-9" morph="none" pos="word" start_char="812">200</TOKEN>
<TOKEN end_char="824" id="token-4-10" morph="none" pos="word" start_char="816">recurrent</TOKEN>
<TOKEN end_char="832" id="token-4-11" morph="none" pos="word" start_char="826">genetic</TOKEN>
<TOKEN end_char="842" id="token-4-12" morph="none" pos="word" start_char="834">mutations</TOKEN>
<TOKEN end_char="845" id="token-4-13" morph="none" pos="word" start_char="844">of</TOKEN>
<TOKEN end_char="849" id="token-4-14" morph="none" pos="word" start_char="847">the</TOKEN>
<TOKEN end_char="853" id="token-4-15" morph="none" pos="word" start_char="851">new</TOKEN>
<TOKEN end_char="865" id="token-4-16" morph="none" pos="word" start_char="855">coronavirus</TOKEN>
<TOKEN end_char="867" id="token-4-17" morph="none" pos="punct" start_char="867">-</TOKEN>
<TOKEN end_char="878" id="token-4-18" morph="none" pos="unknown" start_char="869">SARS-CoV-2</TOKEN>
<TOKEN end_char="880" id="token-4-19" morph="none" pos="punct" start_char="880">-</TOKEN>
<TOKEN end_char="886" id="token-4-20" morph="none" pos="word" start_char="882">which</TOKEN>
<TOKEN end_char="890" id="token-4-21" morph="none" pos="word" start_char="888">the</TOKEN>
<TOKEN end_char="894" id="token-4-22" morph="none" pos="word" start_char="892">UCL</TOKEN>
<TOKEN end_char="906" id="token-4-23" morph="none" pos="word" start_char="896">researchers</TOKEN>
<TOKEN end_char="911" id="token-4-24" morph="none" pos="word" start_char="908">said</TOKEN>
<TOKEN end_char="918" id="token-4-25" morph="none" pos="word" start_char="913">showed</TOKEN>
<TOKEN end_char="922" id="token-4-26" morph="none" pos="word" start_char="920">how</TOKEN>
<TOKEN end_char="925" id="token-4-27" morph="none" pos="word" start_char="924">it</TOKEN>
<TOKEN end_char="928" id="token-4-28" morph="none" pos="word" start_char="927">is</TOKEN>
<TOKEN end_char="937" id="token-4-29" morph="none" pos="word" start_char="930">adapting</TOKEN>
<TOKEN end_char="940" id="token-4-30" morph="none" pos="word" start_char="939">to</TOKEN>
<TOKEN end_char="944" id="token-4-31" morph="none" pos="word" start_char="942">its</TOKEN>
<TOKEN end_char="950" id="token-4-32" morph="none" pos="word" start_char="946">human</TOKEN>
<TOKEN end_char="956" id="token-4-33" morph="none" pos="word" start_char="952">hosts</TOKEN>
<TOKEN end_char="959" id="token-4-34" morph="none" pos="word" start_char="958">as</TOKEN>
<TOKEN end_char="962" id="token-4-35" morph="none" pos="word" start_char="961">it</TOKEN>
<TOKEN end_char="970" id="token-4-36" morph="none" pos="word" start_char="964">spreads</TOKEN>
<TOKEN end_char="971" id="token-4-37" morph="none" pos="punct" start_char="971">.</TOKEN>
</SEG>
<SEG end_char="1059" id="segment-5" start_char="974">
<ORIGINAL_TEXT>"Phylogenetic estimates support that the COVID-2 pandemic started sometime around Oct.</ORIGINAL_TEXT>
<TOKEN end_char="974" id="token-5-0" morph="none" pos="punct" start_char="974">"</TOKEN>
<TOKEN end_char="986" id="token-5-1" morph="none" pos="word" start_char="975">Phylogenetic</TOKEN>
<TOKEN end_char="996" id="token-5-2" morph="none" pos="word" start_char="988">estimates</TOKEN>
<TOKEN end_char="1004" id="token-5-3" morph="none" pos="word" start_char="998">support</TOKEN>
<TOKEN end_char="1009" id="token-5-4" morph="none" pos="word" start_char="1006">that</TOKEN>
<TOKEN end_char="1013" id="token-5-5" morph="none" pos="word" start_char="1011">the</TOKEN>
<TOKEN end_char="1021" id="token-5-6" morph="none" pos="unknown" start_char="1015">COVID-2</TOKEN>
<TOKEN end_char="1030" id="token-5-7" morph="none" pos="word" start_char="1023">pandemic</TOKEN>
<TOKEN end_char="1038" id="token-5-8" morph="none" pos="word" start_char="1032">started</TOKEN>
<TOKEN end_char="1047" id="token-5-9" morph="none" pos="word" start_char="1040">sometime</TOKEN>
<TOKEN end_char="1054" id="token-5-10" morph="none" pos="word" start_char="1049">around</TOKEN>
<TOKEN end_char="1058" id="token-5-11" morph="none" pos="word" start_char="1056">Oct</TOKEN>
<TOKEN end_char="1059" id="token-5-12" morph="none" pos="punct" start_char="1059">.</TOKEN>
</SEG>
<SEG end_char="1075" id="segment-6" start_char="1061">
<ORIGINAL_TEXT>6, 2019 to Dec.</ORIGINAL_TEXT>
<TOKEN end_char="1061" id="token-6-0" morph="none" pos="word" start_char="1061">6</TOKEN>
<TOKEN end_char="1062" id="token-6-1" morph="none" pos="punct" start_char="1062">,</TOKEN>
<TOKEN end_char="1067" id="token-6-2" morph="none" pos="word" start_char="1064">2019</TOKEN>
<TOKEN end_char="1070" id="token-6-3" morph="none" pos="word" start_char="1069">to</TOKEN>
<TOKEN end_char="1074" id="token-6-4" morph="none" pos="word" start_char="1072">Dec</TOKEN>
<TOKEN end_char="1075" id="token-6-5" morph="none" pos="punct" start_char="1075">.</TOKEN>
</SEG>
<SEG end_char="1270" id="segment-7" start_char="1077">
<ORIGINAL_TEXT>11, 2019, which corresponds to the time of the host jump into humans," the research team, co-led by Francois Balloux, wrote in a study published in the journal Infection, Genetics and Evolution.</ORIGINAL_TEXT>
<TOKEN end_char="1078" id="token-7-0" morph="none" pos="word" start_char="1077">11</TOKEN>
<TOKEN end_char="1079" id="token-7-1" morph="none" pos="punct" start_char="1079">,</TOKEN>
<TOKEN end_char="1084" id="token-7-2" morph="none" pos="word" start_char="1081">2019</TOKEN>
<TOKEN end_char="1085" id="token-7-3" morph="none" pos="punct" start_char="1085">,</TOKEN>
<TOKEN end_char="1091" id="token-7-4" morph="none" pos="word" start_char="1087">which</TOKEN>
<TOKEN end_char="1103" id="token-7-5" morph="none" pos="word" start_char="1093">corresponds</TOKEN>
<TOKEN end_char="1106" id="token-7-6" morph="none" pos="word" start_char="1105">to</TOKEN>
<TOKEN end_char="1110" id="token-7-7" morph="none" pos="word" start_char="1108">the</TOKEN>
<TOKEN end_char="1115" id="token-7-8" morph="none" pos="word" start_char="1112">time</TOKEN>
<TOKEN end_char="1118" id="token-7-9" morph="none" pos="word" start_char="1117">of</TOKEN>
<TOKEN end_char="1122" id="token-7-10" morph="none" pos="word" start_char="1120">the</TOKEN>
<TOKEN end_char="1127" id="token-7-11" morph="none" pos="word" start_char="1124">host</TOKEN>
<TOKEN end_char="1132" id="token-7-12" morph="none" pos="word" start_char="1129">jump</TOKEN>
<TOKEN end_char="1137" id="token-7-13" morph="none" pos="word" start_char="1134">into</TOKEN>
<TOKEN end_char="1144" id="token-7-14" morph="none" pos="word" start_char="1139">humans</TOKEN>
<TOKEN end_char="1146" id="token-7-15" morph="none" pos="punct" start_char="1145">,"</TOKEN>
<TOKEN end_char="1150" id="token-7-16" morph="none" pos="word" start_char="1148">the</TOKEN>
<TOKEN end_char="1159" id="token-7-17" morph="none" pos="word" start_char="1152">research</TOKEN>
<TOKEN end_char="1164" id="token-7-18" morph="none" pos="word" start_char="1161">team</TOKEN>
<TOKEN end_char="1165" id="token-7-19" morph="none" pos="punct" start_char="1165">,</TOKEN>
<TOKEN end_char="1172" id="token-7-20" morph="none" pos="unknown" start_char="1167">co-led</TOKEN>
<TOKEN end_char="1175" id="token-7-21" morph="none" pos="word" start_char="1174">by</TOKEN>
<TOKEN end_char="1184" id="token-7-22" morph="none" pos="word" start_char="1177">Francois</TOKEN>
<TOKEN end_char="1192" id="token-7-23" morph="none" pos="word" start_char="1186">Balloux</TOKEN>
<TOKEN end_char="1193" id="token-7-24" morph="none" pos="punct" start_char="1193">,</TOKEN>
<TOKEN end_char="1199" id="token-7-25" morph="none" pos="word" start_char="1195">wrote</TOKEN>
<TOKEN end_char="1202" id="token-7-26" morph="none" pos="word" start_char="1201">in</TOKEN>
<TOKEN end_char="1204" id="token-7-27" morph="none" pos="word" start_char="1204">a</TOKEN>
<TOKEN end_char="1210" id="token-7-28" morph="none" pos="word" start_char="1206">study</TOKEN>
<TOKEN end_char="1220" id="token-7-29" morph="none" pos="word" start_char="1212">published</TOKEN>
<TOKEN end_char="1223" id="token-7-30" morph="none" pos="word" start_char="1222">in</TOKEN>
<TOKEN end_char="1227" id="token-7-31" morph="none" pos="word" start_char="1225">the</TOKEN>
<TOKEN end_char="1235" id="token-7-32" morph="none" pos="word" start_char="1229">journal</TOKEN>
<TOKEN end_char="1245" id="token-7-33" morph="none" pos="word" start_char="1237">Infection</TOKEN>
<TOKEN end_char="1246" id="token-7-34" morph="none" pos="punct" start_char="1246">,</TOKEN>
<TOKEN end_char="1255" id="token-7-35" morph="none" pos="word" start_char="1248">Genetics</TOKEN>
<TOKEN end_char="1259" id="token-7-36" morph="none" pos="word" start_char="1257">and</TOKEN>
<TOKEN end_char="1269" id="token-7-37" morph="none" pos="word" start_char="1261">Evolution</TOKEN>
<TOKEN end_char="1270" id="token-7-38" morph="none" pos="punct" start_char="1270">.</TOKEN>
</SEG>
<SEG end_char="1515" id="segment-8" start_char="1273">
<ORIGINAL_TEXT>Balloux said the analysis also found that the virus was and is mutating, as normally happens with viruses, and that a large proportion of the global genetic diversity of the virus causing COVID-19 was found in all of the hardest-hit countries.</ORIGINAL_TEXT>
<TOKEN end_char="1279" id="token-8-0" morph="none" pos="word" start_char="1273">Balloux</TOKEN>
<TOKEN end_char="1284" id="token-8-1" morph="none" pos="word" start_char="1281">said</TOKEN>
<TOKEN end_char="1288" id="token-8-2" morph="none" pos="word" start_char="1286">the</TOKEN>
<TOKEN end_char="1297" id="token-8-3" morph="none" pos="word" start_char="1290">analysis</TOKEN>
<TOKEN end_char="1302" id="token-8-4" morph="none" pos="word" start_char="1299">also</TOKEN>
<TOKEN end_char="1308" id="token-8-5" morph="none" pos="word" start_char="1304">found</TOKEN>
<TOKEN end_char="1313" id="token-8-6" morph="none" pos="word" start_char="1310">that</TOKEN>
<TOKEN end_char="1317" id="token-8-7" morph="none" pos="word" start_char="1315">the</TOKEN>
<TOKEN end_char="1323" id="token-8-8" morph="none" pos="word" start_char="1319">virus</TOKEN>
<TOKEN end_char="1327" id="token-8-9" morph="none" pos="word" start_char="1325">was</TOKEN>
<TOKEN end_char="1331" id="token-8-10" morph="none" pos="word" start_char="1329">and</TOKEN>
<TOKEN end_char="1334" id="token-8-11" morph="none" pos="word" start_char="1333">is</TOKEN>
<TOKEN end_char="1343" id="token-8-12" morph="none" pos="word" start_char="1336">mutating</TOKEN>
<TOKEN end_char="1344" id="token-8-13" morph="none" pos="punct" start_char="1344">,</TOKEN>
<TOKEN end_char="1347" id="token-8-14" morph="none" pos="word" start_char="1346">as</TOKEN>
<TOKEN end_char="1356" id="token-8-15" morph="none" pos="word" start_char="1349">normally</TOKEN>
<TOKEN end_char="1364" id="token-8-16" morph="none" pos="word" start_char="1358">happens</TOKEN>
<TOKEN end_char="1369" id="token-8-17" morph="none" pos="word" start_char="1366">with</TOKEN>
<TOKEN end_char="1377" id="token-8-18" morph="none" pos="word" start_char="1371">viruses</TOKEN>
<TOKEN end_char="1378" id="token-8-19" morph="none" pos="punct" start_char="1378">,</TOKEN>
<TOKEN end_char="1382" id="token-8-20" morph="none" pos="word" start_char="1380">and</TOKEN>
<TOKEN end_char="1387" id="token-8-21" morph="none" pos="word" start_char="1384">that</TOKEN>
<TOKEN end_char="1389" id="token-8-22" morph="none" pos="word" start_char="1389">a</TOKEN>
<TOKEN end_char="1395" id="token-8-23" morph="none" pos="word" start_char="1391">large</TOKEN>
<TOKEN end_char="1406" id="token-8-24" morph="none" pos="word" start_char="1397">proportion</TOKEN>
<TOKEN end_char="1409" id="token-8-25" morph="none" pos="word" start_char="1408">of</TOKEN>
<TOKEN end_char="1413" id="token-8-26" morph="none" pos="word" start_char="1411">the</TOKEN>
<TOKEN end_char="1420" id="token-8-27" morph="none" pos="word" start_char="1415">global</TOKEN>
<TOKEN end_char="1428" id="token-8-28" morph="none" pos="word" start_char="1422">genetic</TOKEN>
<TOKEN end_char="1438" id="token-8-29" morph="none" pos="word" start_char="1430">diversity</TOKEN>
<TOKEN end_char="1441" id="token-8-30" morph="none" pos="word" start_char="1440">of</TOKEN>
<TOKEN end_char="1445" id="token-8-31" morph="none" pos="word" start_char="1443">the</TOKEN>
<TOKEN end_char="1451" id="token-8-32" morph="none" pos="word" start_char="1447">virus</TOKEN>
<TOKEN end_char="1459" id="token-8-33" morph="none" pos="word" start_char="1453">causing</TOKEN>
<TOKEN end_char="1468" id="token-8-34" morph="none" pos="unknown" start_char="1461">COVID-19</TOKEN>
<TOKEN end_char="1472" id="token-8-35" morph="none" pos="word" start_char="1470">was</TOKEN>
<TOKEN end_char="1478" id="token-8-36" morph="none" pos="word" start_char="1474">found</TOKEN>
<TOKEN end_char="1481" id="token-8-37" morph="none" pos="word" start_char="1480">in</TOKEN>
<TOKEN end_char="1485" id="token-8-38" morph="none" pos="word" start_char="1483">all</TOKEN>
<TOKEN end_char="1488" id="token-8-39" morph="none" pos="word" start_char="1487">of</TOKEN>
<TOKEN end_char="1492" id="token-8-40" morph="none" pos="word" start_char="1490">the</TOKEN>
<TOKEN end_char="1504" id="token-8-41" morph="none" pos="unknown" start_char="1494">hardest-hit</TOKEN>
<TOKEN end_char="1514" id="token-8-42" morph="none" pos="word" start_char="1506">countries</TOKEN>
<TOKEN end_char="1515" id="token-8-43" morph="none" pos="punct" start_char="1515">.</TOKEN>
</SEG>
<SEG end_char="1632" id="segment-9" start_char="1518">
<ORIGINAL_TEXT>That suggests SARS-CoV-2 was being transmitted extensively around the world from early on in the epidemic, he said.</ORIGINAL_TEXT>
<TOKEN end_char="1521" id="token-9-0" morph="none" pos="word" start_char="1518">That</TOKEN>
<TOKEN end_char="1530" id="token-9-1" morph="none" pos="word" start_char="1523">suggests</TOKEN>
<TOKEN end_char="1541" id="token-9-2" morph="none" pos="unknown" start_char="1532">SARS-CoV-2</TOKEN>
<TOKEN end_char="1545" id="token-9-3" morph="none" pos="word" start_char="1543">was</TOKEN>
<TOKEN end_char="1551" id="token-9-4" morph="none" pos="word" start_char="1547">being</TOKEN>
<TOKEN end_char="1563" id="token-9-5" morph="none" pos="word" start_char="1553">transmitted</TOKEN>
<TOKEN end_char="1575" id="token-9-6" morph="none" pos="word" start_char="1565">extensively</TOKEN>
<TOKEN end_char="1582" id="token-9-7" morph="none" pos="word" start_char="1577">around</TOKEN>
<TOKEN end_char="1586" id="token-9-8" morph="none" pos="word" start_char="1584">the</TOKEN>
<TOKEN end_char="1592" id="token-9-9" morph="none" pos="word" start_char="1588">world</TOKEN>
<TOKEN end_char="1597" id="token-9-10" morph="none" pos="word" start_char="1594">from</TOKEN>
<TOKEN end_char="1603" id="token-9-11" morph="none" pos="word" start_char="1599">early</TOKEN>
<TOKEN end_char="1606" id="token-9-12" morph="none" pos="word" start_char="1605">on</TOKEN>
<TOKEN end_char="1609" id="token-9-13" morph="none" pos="word" start_char="1608">in</TOKEN>
<TOKEN end_char="1613" id="token-9-14" morph="none" pos="word" start_char="1611">the</TOKEN>
<TOKEN end_char="1622" id="token-9-15" morph="none" pos="word" start_char="1615">epidemic</TOKEN>
<TOKEN end_char="1623" id="token-9-16" morph="none" pos="punct" start_char="1623">,</TOKEN>
<TOKEN end_char="1626" id="token-9-17" morph="none" pos="word" start_char="1625">he</TOKEN>
<TOKEN end_char="1631" id="token-9-18" morph="none" pos="word" start_char="1628">said</TOKEN>
<TOKEN end_char="1632" id="token-9-19" morph="none" pos="punct" start_char="1632">.</TOKEN>
</SEG>
<SEG end_char="1664" id="segment-10" start_char="1635">
<ORIGINAL_TEXT>"All viruses naturally mutate.</ORIGINAL_TEXT>
<TOKEN end_char="1635" id="token-10-0" morph="none" pos="punct" start_char="1635">"</TOKEN>
<TOKEN end_char="1638" id="token-10-1" morph="none" pos="word" start_char="1636">All</TOKEN>
<TOKEN end_char="1646" id="token-10-2" morph="none" pos="word" start_char="1640">viruses</TOKEN>
<TOKEN end_char="1656" id="token-10-3" morph="none" pos="word" start_char="1648">naturally</TOKEN>
<TOKEN end_char="1663" id="token-10-4" morph="none" pos="word" start_char="1658">mutate</TOKEN>
<TOKEN end_char="1664" id="token-10-5" morph="none" pos="punct" start_char="1664">.</TOKEN>
<TRANSLATED_TEXT>Alle viruser muterer naturligt.</TRANSLATED_TEXT><DETECTED_LANGUAGE>et</DETECTED_LANGUAGE></SEG>
<SEG end_char="1805" id="segment-11" start_char="1666">
<ORIGINAL_TEXT>Mutations in themselves are not a bad thing and there is nothing to suggest SARS-CoV-2 is mutating faster or slower than expected," he said.</ORIGINAL_TEXT>
<TOKEN end_char="1674" id="token-11-0" morph="none" pos="word" start_char="1666">Mutations</TOKEN>
<TOKEN end_char="1677" id="token-11-1" morph="none" pos="word" start_char="1676">in</TOKEN>
<TOKEN end_char="1688" id="token-11-2" morph="none" pos="word" start_char="1679">themselves</TOKEN>
<TOKEN end_char="1692" id="token-11-3" morph="none" pos="word" start_char="1690">are</TOKEN>
<TOKEN end_char="1696" id="token-11-4" morph="none" pos="word" start_char="1694">not</TOKEN>
<TOKEN end_char="1698" id="token-11-5" morph="none" pos="word" start_char="1698">a</TOKEN>
<TOKEN end_char="1702" id="token-11-6" morph="none" pos="word" start_char="1700">bad</TOKEN>
<TOKEN end_char="1708" id="token-11-7" morph="none" pos="word" start_char="1704">thing</TOKEN>
<TOKEN end_char="1712" id="token-11-8" morph="none" pos="word" start_char="1710">and</TOKEN>
<TOKEN end_char="1718" id="token-11-9" morph="none" pos="word" start_char="1714">there</TOKEN>
<TOKEN end_char="1721" id="token-11-10" morph="none" pos="word" start_char="1720">is</TOKEN>
<TOKEN end_char="1729" id="token-11-11" morph="none" pos="word" start_char="1723">nothing</TOKEN>
<TOKEN end_char="1732" id="token-11-12" morph="none" pos="word" start_char="1731">to</TOKEN>
<TOKEN end_char="1740" id="token-11-13" morph="none" pos="word" start_char="1734">suggest</TOKEN>
<TOKEN end_char="1751" id="token-11-14" morph="none" pos="unknown" start_char="1742">SARS-CoV-2</TOKEN>
<TOKEN end_char="1754" id="token-11-15" morph="none" pos="word" start_char="1753">is</TOKEN>
<TOKEN end_char="1763" id="token-11-16" morph="none" pos="word" start_char="1756">mutating</TOKEN>
<TOKEN end_char="1770" id="token-11-17" morph="none" pos="word" start_char="1765">faster</TOKEN>
<TOKEN end_char="1773" id="token-11-18" morph="none" pos="word" start_char="1772">or</TOKEN>
<TOKEN end_char="1780" id="token-11-19" morph="none" pos="word" start_char="1775">slower</TOKEN>
<TOKEN end_char="1785" id="token-11-20" morph="none" pos="word" start_char="1782">than</TOKEN>
<TOKEN end_char="1794" id="token-11-21" morph="none" pos="word" start_char="1787">expected</TOKEN>
<TOKEN end_char="1796" id="token-11-22" morph="none" pos="punct" start_char="1795">,"</TOKEN>
<TOKEN end_char="1799" id="token-11-23" morph="none" pos="word" start_char="1798">he</TOKEN>
<TOKEN end_char="1804" id="token-11-24" morph="none" pos="word" start_char="1801">said</TOKEN>
<TOKEN end_char="1805" id="token-11-25" morph="none" pos="punct" start_char="1805">.</TOKEN>
</SEG>
<SEG end_char="1896" id="segment-12" start_char="1807">
<ORIGINAL_TEXT>"So far, we cannot say whether SARS-CoV-2 is becoming more or less lethal and contagious."</ORIGINAL_TEXT>
<TOKEN end_char="1807" id="token-12-0" morph="none" pos="punct" start_char="1807">"</TOKEN>
<TOKEN end_char="1809" id="token-12-1" morph="none" pos="word" start_char="1808">So</TOKEN>
<TOKEN end_char="1813" id="token-12-2" morph="none" pos="word" start_char="1811">far</TOKEN>
<TOKEN end_char="1814" id="token-12-3" morph="none" pos="punct" start_char="1814">,</TOKEN>
<TOKEN end_char="1817" id="token-12-4" morph="none" pos="word" start_char="1816">we</TOKEN>
<TOKEN end_char="1824" id="token-12-5" morph="none" pos="word" start_char="1819">cannot</TOKEN>
<TOKEN end_char="1828" id="token-12-6" morph="none" pos="word" start_char="1826">say</TOKEN>
<TOKEN end_char="1836" id="token-12-7" morph="none" pos="word" start_char="1830">whether</TOKEN>
<TOKEN end_char="1847" id="token-12-8" morph="none" pos="unknown" start_char="1838">SARS-CoV-2</TOKEN>
<TOKEN end_char="1850" id="token-12-9" morph="none" pos="word" start_char="1849">is</TOKEN>
<TOKEN end_char="1859" id="token-12-10" morph="none" pos="word" start_char="1852">becoming</TOKEN>
<TOKEN end_char="1864" id="token-12-11" morph="none" pos="word" start_char="1861">more</TOKEN>
<TOKEN end_char="1867" id="token-12-12" morph="none" pos="word" start_char="1866">or</TOKEN>
<TOKEN end_char="1872" id="token-12-13" morph="none" pos="word" start_char="1869">less</TOKEN>
<TOKEN end_char="1879" id="token-12-14" morph="none" pos="word" start_char="1874">lethal</TOKEN>
<TOKEN end_char="1883" id="token-12-15" morph="none" pos="word" start_char="1881">and</TOKEN>
<TOKEN end_char="1894" id="token-12-16" morph="none" pos="word" start_char="1885">contagious</TOKEN>
<TOKEN end_char="1896" id="token-12-17" morph="none" pos="punct" start_char="1895">."</TOKEN>
</SEG>
<SEG end_char="2139" id="segment-13" start_char="1899">
<ORIGINAL_TEXT>In a second study also published on Wednesday, scientists at Britain’s University of Glasgow who also analysed SARS-CoV-2 virus samples said their findings showed that previous work suggesting there were two different strains was inaccurate.</ORIGINAL_TEXT>
<TOKEN end_char="1900" id="token-13-0" morph="none" pos="word" start_char="1899">In</TOKEN>
<TOKEN end_char="1902" id="token-13-1" morph="none" pos="word" start_char="1902">a</TOKEN>
<TOKEN end_char="1909" id="token-13-2" morph="none" pos="word" start_char="1904">second</TOKEN>
<TOKEN end_char="1915" id="token-13-3" morph="none" pos="word" start_char="1911">study</TOKEN>
<TOKEN end_char="1920" id="token-13-4" morph="none" pos="word" start_char="1917">also</TOKEN>
<TOKEN end_char="1930" id="token-13-5" morph="none" pos="word" start_char="1922">published</TOKEN>
<TOKEN end_char="1933" id="token-13-6" morph="none" pos="word" start_char="1932">on</TOKEN>
<TOKEN end_char="1943" id="token-13-7" morph="none" pos="word" start_char="1935">Wednesday</TOKEN>
<TOKEN end_char="1944" id="token-13-8" morph="none" pos="punct" start_char="1944">,</TOKEN>
<TOKEN end_char="1955" id="token-13-9" morph="none" pos="word" start_char="1946">scientists</TOKEN>
<TOKEN end_char="1958" id="token-13-10" morph="none" pos="word" start_char="1957">at</TOKEN>
<TOKEN end_char="1968" id="token-13-11" morph="none" pos="word" start_char="1960">Britain’s</TOKEN>
<TOKEN end_char="1979" id="token-13-12" morph="none" pos="word" start_char="1970">University</TOKEN>
<TOKEN end_char="1982" id="token-13-13" morph="none" pos="word" start_char="1981">of</TOKEN>
<TOKEN end_char="1990" id="token-13-14" morph="none" pos="word" start_char="1984">Glasgow</TOKEN>
<TOKEN end_char="1994" id="token-13-15" morph="none" pos="word" start_char="1992">who</TOKEN>
<TOKEN end_char="1999" id="token-13-16" morph="none" pos="word" start_char="1996">also</TOKEN>
<TOKEN end_char="2008" id="token-13-17" morph="none" pos="word" start_char="2001">analysed</TOKEN>
<TOKEN end_char="2019" id="token-13-18" morph="none" pos="unknown" start_char="2010">SARS-CoV-2</TOKEN>
<TOKEN end_char="2025" id="token-13-19" morph="none" pos="word" start_char="2021">virus</TOKEN>
<TOKEN end_char="2033" id="token-13-20" morph="none" pos="word" start_char="2027">samples</TOKEN>
<TOKEN end_char="2038" id="token-13-21" morph="none" pos="word" start_char="2035">said</TOKEN>
<TOKEN end_char="2044" id="token-13-22" morph="none" pos="word" start_char="2040">their</TOKEN>
<TOKEN end_char="2053" id="token-13-23" morph="none" pos="word" start_char="2046">findings</TOKEN>
<TOKEN end_char="2060" id="token-13-24" morph="none" pos="word" start_char="2055">showed</TOKEN>
<TOKEN end_char="2065" id="token-13-25" morph="none" pos="word" start_char="2062">that</TOKEN>
<TOKEN end_char="2074" id="token-13-26" morph="none" pos="word" start_char="2067">previous</TOKEN>
<TOKEN end_char="2079" id="token-13-27" morph="none" pos="word" start_char="2076">work</TOKEN>
<TOKEN end_char="2090" id="token-13-28" morph="none" pos="word" start_char="2081">suggesting</TOKEN>
<TOKEN end_char="2096" id="token-13-29" morph="none" pos="word" start_char="2092">there</TOKEN>
<TOKEN end_char="2101" id="token-13-30" morph="none" pos="word" start_char="2098">were</TOKEN>
<TOKEN end_char="2105" id="token-13-31" morph="none" pos="word" start_char="2103">two</TOKEN>
<TOKEN end_char="2115" id="token-13-32" morph="none" pos="word" start_char="2107">different</TOKEN>
<TOKEN end_char="2123" id="token-13-33" morph="none" pos="word" start_char="2117">strains</TOKEN>
<TOKEN end_char="2127" id="token-13-34" morph="none" pos="word" start_char="2125">was</TOKEN>
<TOKEN end_char="2138" id="token-13-35" morph="none" pos="word" start_char="2129">inaccurate</TOKEN>
<TOKEN end_char="2139" id="token-13-36" morph="none" pos="punct" start_char="2139">.</TOKEN>
</SEG>
<SEG end_char="2172" id="segment-14" start_char="2142">
<ORIGINAL_TEXT>JUST ONE VIRUS TYPE CIRCULATING</ORIGINAL_TEXT>
<TOKEN end_char="2145" id="token-14-0" morph="none" pos="word" start_char="2142">JUST</TOKEN>
<TOKEN end_char="2149" id="token-14-1" morph="none" pos="word" start_char="2147">ONE</TOKEN>
<TOKEN end_char="2155" id="token-14-2" morph="none" pos="word" start_char="2151">VIRUS</TOKEN>
<TOKEN end_char="2160" id="token-14-3" morph="none" pos="word" start_char="2157">TYPE</TOKEN>
<TOKEN end_char="2172" id="token-14-4" morph="none" pos="word" start_char="2162">CIRCULATING</TOKEN>
</SEG>
<SEG end_char="2372" id="segment-15" start_char="2176">
<ORIGINAL_TEXT>A preliminary study by Chinese scientists in March had suggested there may have been two strains of the new coronavirus causing infections there, with more of them more "aggressive" than the other.</ORIGINAL_TEXT>
<TOKEN end_char="2176" id="token-15-0" morph="none" pos="word" start_char="2176">A</TOKEN>
<TOKEN end_char="2188" id="token-15-1" morph="none" pos="word" start_char="2178">preliminary</TOKEN>
<TOKEN end_char="2194" id="token-15-2" morph="none" pos="word" start_char="2190">study</TOKEN>
<TOKEN end_char="2197" id="token-15-3" morph="none" pos="word" start_char="2196">by</TOKEN>
<TOKEN end_char="2205" id="token-15-4" morph="none" pos="word" start_char="2199">Chinese</TOKEN>
<TOKEN end_char="2216" id="token-15-5" morph="none" pos="word" start_char="2207">scientists</TOKEN>
<TOKEN end_char="2219" id="token-15-6" morph="none" pos="word" start_char="2218">in</TOKEN>
<TOKEN end_char="2225" id="token-15-7" morph="none" pos="word" start_char="2221">March</TOKEN>
<TOKEN end_char="2229" id="token-15-8" morph="none" pos="word" start_char="2227">had</TOKEN>
<TOKEN end_char="2239" id="token-15-9" morph="none" pos="word" start_char="2231">suggested</TOKEN>
<TOKEN end_char="2245" id="token-15-10" morph="none" pos="word" start_char="2241">there</TOKEN>
<TOKEN end_char="2249" id="token-15-11" morph="none" pos="word" start_char="2247">may</TOKEN>
<TOKEN end_char="2254" id="token-15-12" morph="none" pos="word" start_char="2251">have</TOKEN>
<TOKEN end_char="2259" id="token-15-13" morph="none" pos="word" start_char="2256">been</TOKEN>
<TOKEN end_char="2263" id="token-15-14" morph="none" pos="word" start_char="2261">two</TOKEN>
<TOKEN end_char="2271" id="token-15-15" morph="none" pos="word" start_char="2265">strains</TOKEN>
<TOKEN end_char="2274" id="token-15-16" morph="none" pos="word" start_char="2273">of</TOKEN>
<TOKEN end_char="2278" id="token-15-17" morph="none" pos="word" start_char="2276">the</TOKEN>
<TOKEN end_char="2282" id="token-15-18" morph="none" pos="word" start_char="2280">new</TOKEN>
<TOKEN end_char="2294" id="token-15-19" morph="none" pos="word" start_char="2284">coronavirus</TOKEN>
<TOKEN end_char="2302" id="token-15-20" morph="none" pos="word" start_char="2296">causing</TOKEN>
<TOKEN end_char="2313" id="token-15-21" morph="none" pos="word" start_char="2304">infections</TOKEN>
<TOKEN end_char="2319" id="token-15-22" morph="none" pos="word" start_char="2315">there</TOKEN>
<TOKEN end_char="2320" id="token-15-23" morph="none" pos="punct" start_char="2320">,</TOKEN>
<TOKEN end_char="2325" id="token-15-24" morph="none" pos="word" start_char="2322">with</TOKEN>
<TOKEN end_char="2330" id="token-15-25" morph="none" pos="word" start_char="2327">more</TOKEN>
<TOKEN end_char="2333" id="token-15-26" morph="none" pos="word" start_char="2332">of</TOKEN>
<TOKEN end_char="2338" id="token-15-27" morph="none" pos="word" start_char="2335">them</TOKEN>
<TOKEN end_char="2343" id="token-15-28" morph="none" pos="word" start_char="2340">more</TOKEN>
<TOKEN end_char="2345" id="token-15-29" morph="none" pos="punct" start_char="2345">"</TOKEN>
<TOKEN end_char="2355" id="token-15-30" morph="none" pos="word" start_char="2346">aggressive</TOKEN>
<TOKEN end_char="2356" id="token-15-31" morph="none" pos="punct" start_char="2356">"</TOKEN>
<TOKEN end_char="2361" id="token-15-32" morph="none" pos="word" start_char="2358">than</TOKEN>
<TOKEN end_char="2365" id="token-15-33" morph="none" pos="word" start_char="2363">the</TOKEN>
<TOKEN end_char="2371" id="token-15-34" morph="none" pos="word" start_char="2367">other</TOKEN>
<TOKEN end_char="2372" id="token-15-35" morph="none" pos="punct" start_char="2372">.</TOKEN>
</SEG>
<SEG end_char="2502" id="segment-16" start_char="2375">
<ORIGINAL_TEXT>But, publishing their analysis in the journal Virus Evolution, the Glasgow team said only one type of the virus was circulating.</ORIGINAL_TEXT>
<TOKEN end_char="2377" id="token-16-0" morph="none" pos="word" start_char="2375">But</TOKEN>
<TOKEN end_char="2378" id="token-16-1" morph="none" pos="punct" start_char="2378">,</TOKEN>
<TOKEN end_char="2389" id="token-16-2" morph="none" pos="word" start_char="2380">publishing</TOKEN>
<TOKEN end_char="2395" id="token-16-3" morph="none" pos="word" start_char="2391">their</TOKEN>
<TOKEN end_char="2404" id="token-16-4" morph="none" pos="word" start_char="2397">analysis</TOKEN>
<TOKEN end_char="2407" id="token-16-5" morph="none" pos="word" start_char="2406">in</TOKEN>
<TOKEN end_char="2411" id="token-16-6" morph="none" pos="word" start_char="2409">the</TOKEN>
<TOKEN end_char="2419" id="token-16-7" morph="none" pos="word" start_char="2413">journal</TOKEN>
<TOKEN end_char="2425" id="token-16-8" morph="none" pos="word" start_char="2421">Virus</TOKEN>
<TOKEN end_char="2435" id="token-16-9" morph="none" pos="word" start_char="2427">Evolution</TOKEN>
<TOKEN end_char="2436" id="token-16-10" morph="none" pos="punct" start_char="2436">,</TOKEN>
<TOKEN end_char="2440" id="token-16-11" morph="none" pos="word" start_char="2438">the</TOKEN>
<TOKEN end_char="2448" id="token-16-12" morph="none" pos="word" start_char="2442">Glasgow</TOKEN>
<TOKEN end_char="2453" id="token-16-13" morph="none" pos="word" start_char="2450">team</TOKEN>
<TOKEN end_char="2458" id="token-16-14" morph="none" pos="word" start_char="2455">said</TOKEN>
<TOKEN end_char="2463" id="token-16-15" morph="none" pos="word" start_char="2460">only</TOKEN>
<TOKEN end_char="2467" id="token-16-16" morph="none" pos="word" start_char="2465">one</TOKEN>
<TOKEN end_char="2472" id="token-16-17" morph="none" pos="word" start_char="2469">type</TOKEN>
<TOKEN end_char="2475" id="token-16-18" morph="none" pos="word" start_char="2474">of</TOKEN>
<TOKEN end_char="2479" id="token-16-19" morph="none" pos="word" start_char="2477">the</TOKEN>
<TOKEN end_char="2485" id="token-16-20" morph="none" pos="word" start_char="2481">virus</TOKEN>
<TOKEN end_char="2489" id="token-16-21" morph="none" pos="word" start_char="2487">was</TOKEN>
<TOKEN end_char="2501" id="token-16-22" morph="none" pos="word" start_char="2491">circulating</TOKEN>
<TOKEN end_char="2502" id="token-16-23" morph="none" pos="punct" start_char="2502">.</TOKEN>
</SEG>
<SEG end_char="2654" id="segment-17" start_char="2505">
<ORIGINAL_TEXT>More than 3.71 million people have been reported to be infected by the novel coronavirus globally and 258,186 have died, according to a Reuters tally.</ORIGINAL_TEXT>
<TOKEN end_char="2508" id="token-17-0" morph="none" pos="word" start_char="2505">More</TOKEN>
<TOKEN end_char="2513" id="token-17-1" morph="none" pos="word" start_char="2510">than</TOKEN>
<TOKEN end_char="2518" id="token-17-2" morph="none" pos="word" start_char="2515">3.71</TOKEN>
<TOKEN end_char="2526" id="token-17-3" morph="none" pos="word" start_char="2520">million</TOKEN>
<TOKEN end_char="2533" id="token-17-4" morph="none" pos="word" start_char="2528">people</TOKEN>
<TOKEN end_char="2538" id="token-17-5" morph="none" pos="word" start_char="2535">have</TOKEN>
<TOKEN end_char="2543" id="token-17-6" morph="none" pos="word" start_char="2540">been</TOKEN>
<TOKEN end_char="2552" id="token-17-7" morph="none" pos="word" start_char="2545">reported</TOKEN>
<TOKEN end_char="2555" id="token-17-8" morph="none" pos="word" start_char="2554">to</TOKEN>
<TOKEN end_char="2558" id="token-17-9" morph="none" pos="word" start_char="2557">be</TOKEN>
<TOKEN end_char="2567" id="token-17-10" morph="none" pos="word" start_char="2560">infected</TOKEN>
<TOKEN end_char="2570" id="token-17-11" morph="none" pos="word" start_char="2569">by</TOKEN>
<TOKEN end_char="2574" id="token-17-12" morph="none" pos="word" start_char="2572">the</TOKEN>
<TOKEN end_char="2580" id="token-17-13" morph="none" pos="word" start_char="2576">novel</TOKEN>
<TOKEN end_char="2592" id="token-17-14" morph="none" pos="word" start_char="2582">coronavirus</TOKEN>
<TOKEN end_char="2601" id="token-17-15" morph="none" pos="word" start_char="2594">globally</TOKEN>
<TOKEN end_char="2605" id="token-17-16" morph="none" pos="word" start_char="2603">and</TOKEN>
<TOKEN end_char="2613" id="token-17-17" morph="none" pos="unknown" start_char="2607">258,186</TOKEN>
<TOKEN end_char="2618" id="token-17-18" morph="none" pos="word" start_char="2615">have</TOKEN>
<TOKEN end_char="2623" id="token-17-19" morph="none" pos="word" start_char="2620">died</TOKEN>
<TOKEN end_char="2624" id="token-17-20" morph="none" pos="punct" start_char="2624">,</TOKEN>
<TOKEN end_char="2634" id="token-17-21" morph="none" pos="word" start_char="2626">according</TOKEN>
<TOKEN end_char="2637" id="token-17-22" morph="none" pos="word" start_char="2636">to</TOKEN>
<TOKEN end_char="2639" id="token-17-23" morph="none" pos="word" start_char="2639">a</TOKEN>
<TOKEN end_char="2647" id="token-17-24" morph="none" pos="word" start_char="2641">Reuters</TOKEN>
<TOKEN end_char="2653" id="token-17-25" morph="none" pos="word" start_char="2649">tally</TOKEN>
<TOKEN end_char="2654" id="token-17-26" morph="none" pos="punct" start_char="2654">.</TOKEN>
</SEG>
<SEG end_char="2783" id="segment-18" start_char="2657">
<ORIGINAL_TEXT>Cases have been reported in more than 210 countries and territories since they were first identified in China in December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2661" id="token-18-0" morph="none" pos="word" start_char="2657">Cases</TOKEN>
<TOKEN end_char="2666" id="token-18-1" morph="none" pos="word" start_char="2663">have</TOKEN>
<TOKEN end_char="2671" id="token-18-2" morph="none" pos="word" start_char="2668">been</TOKEN>
<TOKEN end_char="2680" id="token-18-3" morph="none" pos="word" start_char="2673">reported</TOKEN>
<TOKEN end_char="2683" id="token-18-4" morph="none" pos="word" start_char="2682">in</TOKEN>
<TOKEN end_char="2688" id="token-18-5" morph="none" pos="word" start_char="2685">more</TOKEN>
<TOKEN end_char="2693" id="token-18-6" morph="none" pos="word" start_char="2690">than</TOKEN>
<TOKEN end_char="2697" id="token-18-7" morph="none" pos="word" start_char="2695">210</TOKEN>
<TOKEN end_char="2707" id="token-18-8" morph="none" pos="word" start_char="2699">countries</TOKEN>
<TOKEN end_char="2711" id="token-18-9" morph="none" pos="word" start_char="2709">and</TOKEN>
<TOKEN end_char="2723" id="token-18-10" morph="none" pos="word" start_char="2713">territories</TOKEN>
<TOKEN end_char="2729" id="token-18-11" morph="none" pos="word" start_char="2725">since</TOKEN>
<TOKEN end_char="2734" id="token-18-12" morph="none" pos="word" start_char="2731">they</TOKEN>
<TOKEN end_char="2739" id="token-18-13" morph="none" pos="word" start_char="2736">were</TOKEN>
<TOKEN end_char="2745" id="token-18-14" morph="none" pos="word" start_char="2741">first</TOKEN>
<TOKEN end_char="2756" id="token-18-15" morph="none" pos="word" start_char="2747">identified</TOKEN>
<TOKEN end_char="2759" id="token-18-16" morph="none" pos="word" start_char="2758">in</TOKEN>
<TOKEN end_char="2765" id="token-18-17" morph="none" pos="word" start_char="2761">China</TOKEN>
<TOKEN end_char="2768" id="token-18-18" morph="none" pos="word" start_char="2767">in</TOKEN>
<TOKEN end_char="2777" id="token-18-19" morph="none" pos="word" start_char="2770">December</TOKEN>
<TOKEN end_char="2782" id="token-18-20" morph="none" pos="word" start_char="2779">2019</TOKEN>
<TOKEN end_char="2783" id="token-18-21" morph="none" pos="punct" start_char="2783">.</TOKEN>
</SEG>
<SEG end_char="3043" id="segment-19" start_char="2786">
<ORIGINAL_TEXT>The genetic studies offer "fascinating" insights into the evolution of the virus, and emphasise that it is "a moving target with an unknown evolutionary destination", said Jonathan Stoye, head of the division of virology at Britain’s Francis Crick Institute.</ORIGINAL_TEXT>
<TOKEN end_char="2788" id="token-19-0" morph="none" pos="word" start_char="2786">The</TOKEN>
<TOKEN end_char="2796" id="token-19-1" morph="none" pos="word" start_char="2790">genetic</TOKEN>
<TOKEN end_char="2804" id="token-19-2" morph="none" pos="word" start_char="2798">studies</TOKEN>
<TOKEN end_char="2810" id="token-19-3" morph="none" pos="word" start_char="2806">offer</TOKEN>
<TOKEN end_char="2812" id="token-19-4" morph="none" pos="punct" start_char="2812">"</TOKEN>
<TOKEN end_char="2823" id="token-19-5" morph="none" pos="word" start_char="2813">fascinating</TOKEN>
<TOKEN end_char="2824" id="token-19-6" morph="none" pos="punct" start_char="2824">"</TOKEN>
<TOKEN end_char="2833" id="token-19-7" morph="none" pos="word" start_char="2826">insights</TOKEN>
<TOKEN end_char="2838" id="token-19-8" morph="none" pos="word" start_char="2835">into</TOKEN>
<TOKEN end_char="2842" id="token-19-9" morph="none" pos="word" start_char="2840">the</TOKEN>
<TOKEN end_char="2852" id="token-19-10" morph="none" pos="word" start_char="2844">evolution</TOKEN>
<TOKEN end_char="2855" id="token-19-11" morph="none" pos="word" start_char="2854">of</TOKEN>
<TOKEN end_char="2859" id="token-19-12" morph="none" pos="word" start_char="2857">the</TOKEN>
<TOKEN end_char="2865" id="token-19-13" morph="none" pos="word" start_char="2861">virus</TOKEN>
<TOKEN end_char="2866" id="token-19-14" morph="none" pos="punct" start_char="2866">,</TOKEN>
<TOKEN end_char="2870" id="token-19-15" morph="none" pos="word" start_char="2868">and</TOKEN>
<TOKEN end_char="2880" id="token-19-16" morph="none" pos="word" start_char="2872">emphasise</TOKEN>
<TOKEN end_char="2885" id="token-19-17" morph="none" pos="word" start_char="2882">that</TOKEN>
<TOKEN end_char="2888" id="token-19-18" morph="none" pos="word" start_char="2887">it</TOKEN>
<TOKEN end_char="2891" id="token-19-19" morph="none" pos="word" start_char="2890">is</TOKEN>
<TOKEN end_char="2893" id="token-19-20" morph="none" pos="punct" start_char="2893">"</TOKEN>
<TOKEN end_char="2894" id="token-19-21" morph="none" pos="word" start_char="2894">a</TOKEN>
<TOKEN end_char="2901" id="token-19-22" morph="none" pos="word" start_char="2896">moving</TOKEN>
<TOKEN end_char="2908" id="token-19-23" morph="none" pos="word" start_char="2903">target</TOKEN>
<TOKEN end_char="2913" id="token-19-24" morph="none" pos="word" start_char="2910">with</TOKEN>
<TOKEN end_char="2916" id="token-19-25" morph="none" pos="word" start_char="2915">an</TOKEN>
<TOKEN end_char="2924" id="token-19-26" morph="none" pos="word" start_char="2918">unknown</TOKEN>
<TOKEN end_char="2937" id="token-19-27" morph="none" pos="word" start_char="2926">evolutionary</TOKEN>
<TOKEN end_char="2949" id="token-19-28" morph="none" pos="word" start_char="2939">destination</TOKEN>
<TOKEN end_char="2951" id="token-19-29" morph="none" pos="punct" start_char="2950">",</TOKEN>
<TOKEN end_char="2956" id="token-19-30" morph="none" pos="word" start_char="2953">said</TOKEN>
<TOKEN end_char="2965" id="token-19-31" morph="none" pos="word" start_char="2958">Jonathan</TOKEN>
<TOKEN end_char="2971" id="token-19-32" morph="none" pos="word" start_char="2967">Stoye</TOKEN>
<TOKEN end_char="2972" id="token-19-33" morph="none" pos="punct" start_char="2972">,</TOKEN>
<TOKEN end_char="2977" id="token-19-34" morph="none" pos="word" start_char="2974">head</TOKEN>
<TOKEN end_char="2980" id="token-19-35" morph="none" pos="word" start_char="2979">of</TOKEN>
<TOKEN end_char="2984" id="token-19-36" morph="none" pos="word" start_char="2982">the</TOKEN>
<TOKEN end_char="2993" id="token-19-37" morph="none" pos="word" start_char="2986">division</TOKEN>
<TOKEN end_char="2996" id="token-19-38" morph="none" pos="word" start_char="2995">of</TOKEN>
<TOKEN end_char="3005" id="token-19-39" morph="none" pos="word" start_char="2998">virology</TOKEN>
<TOKEN end_char="3008" id="token-19-40" morph="none" pos="word" start_char="3007">at</TOKEN>
<TOKEN end_char="3018" id="token-19-41" morph="none" pos="word" start_char="3010">Britain’s</TOKEN>
<TOKEN end_char="3026" id="token-19-42" morph="none" pos="word" start_char="3020">Francis</TOKEN>
<TOKEN end_char="3032" id="token-19-43" morph="none" pos="word" start_char="3028">Crick</TOKEN>
<TOKEN end_char="3042" id="token-19-44" morph="none" pos="word" start_char="3034">Institute</TOKEN>
<TOKEN end_char="3043" id="token-19-45" morph="none" pos="punct" start_char="3043">.</TOKEN>
</SEG>
<SEG end_char="3194" id="segment-20" start_char="3046">
<ORIGINAL_TEXT>"All the evidence is entirely consistent with an origin towards the end of last year, and there’s no reason to question that in any way," Stoye said.</ORIGINAL_TEXT>
<TOKEN end_char="3046" id="token-20-0" morph="none" pos="punct" start_char="3046">"</TOKEN>
<TOKEN end_char="3049" id="token-20-1" morph="none" pos="word" start_char="3047">All</TOKEN>
<TOKEN end_char="3053" id="token-20-2" morph="none" pos="word" start_char="3051">the</TOKEN>
<TOKEN end_char="3062" id="token-20-3" morph="none" pos="word" start_char="3055">evidence</TOKEN>
<TOKEN end_char="3065" id="token-20-4" morph="none" pos="word" start_char="3064">is</TOKEN>
<TOKEN end_char="3074" id="token-20-5" morph="none" pos="word" start_char="3067">entirely</TOKEN>
<TOKEN end_char="3085" id="token-20-6" morph="none" pos="word" start_char="3076">consistent</TOKEN>
<TOKEN end_char="3090" id="token-20-7" morph="none" pos="word" start_char="3087">with</TOKEN>
<TOKEN end_char="3093" id="token-20-8" morph="none" pos="word" start_char="3092">an</TOKEN>
<TOKEN end_char="3100" id="token-20-9" morph="none" pos="word" start_char="3095">origin</TOKEN>
<TOKEN end_char="3108" id="token-20-10" morph="none" pos="word" start_char="3102">towards</TOKEN>
<TOKEN end_char="3112" id="token-20-11" morph="none" pos="word" start_char="3110">the</TOKEN>
<TOKEN end_char="3116" id="token-20-12" morph="none" pos="word" start_char="3114">end</TOKEN>
<TOKEN end_char="3119" id="token-20-13" morph="none" pos="word" start_char="3118">of</TOKEN>
<TOKEN end_char="3124" id="token-20-14" morph="none" pos="word" start_char="3121">last</TOKEN>
<TOKEN end_char="3129" id="token-20-15" morph="none" pos="word" start_char="3126">year</TOKEN>
<TOKEN end_char="3130" id="token-20-16" morph="none" pos="punct" start_char="3130">,</TOKEN>
<TOKEN end_char="3134" id="token-20-17" morph="none" pos="word" start_char="3132">and</TOKEN>
<TOKEN end_char="3142" id="token-20-18" morph="none" pos="word" start_char="3136">there’s</TOKEN>
<TOKEN end_char="3145" id="token-20-19" morph="none" pos="word" start_char="3144">no</TOKEN>
<TOKEN end_char="3152" id="token-20-20" morph="none" pos="word" start_char="3147">reason</TOKEN>
<TOKEN end_char="3155" id="token-20-21" morph="none" pos="word" start_char="3154">to</TOKEN>
<TOKEN end_char="3164" id="token-20-22" morph="none" pos="word" start_char="3157">question</TOKEN>
<TOKEN end_char="3169" id="token-20-23" morph="none" pos="word" start_char="3166">that</TOKEN>
<TOKEN end_char="3172" id="token-20-24" morph="none" pos="word" start_char="3171">in</TOKEN>
<TOKEN end_char="3176" id="token-20-25" morph="none" pos="word" start_char="3174">any</TOKEN>
<TOKEN end_char="3180" id="token-20-26" morph="none" pos="word" start_char="3178">way</TOKEN>
<TOKEN end_char="3182" id="token-20-27" morph="none" pos="punct" start_char="3181">,"</TOKEN>
<TOKEN end_char="3188" id="token-20-28" morph="none" pos="word" start_char="3184">Stoye</TOKEN>
<TOKEN end_char="3193" id="token-20-29" morph="none" pos="word" start_char="3190">said</TOKEN>
<TOKEN end_char="3194" id="token-20-30" morph="none" pos="punct" start_char="3194">.</TOKEN>
</SEG>
<SEG end_char="3318" id="segment-21" start_char="3197">
<ORIGINAL_TEXT>A study by French scientists published earlier this week found a man in France was infected with COVID-19 as early as Dec.</ORIGINAL_TEXT>
<TOKEN end_char="3197" id="token-21-0" morph="none" pos="word" start_char="3197">A</TOKEN>
<TOKEN end_char="3203" id="token-21-1" morph="none" pos="word" start_char="3199">study</TOKEN>
<TOKEN end_char="3206" id="token-21-2" morph="none" pos="word" start_char="3205">by</TOKEN>
<TOKEN end_char="3213" id="token-21-3" morph="none" pos="word" start_char="3208">French</TOKEN>
<TOKEN end_char="3224" id="token-21-4" morph="none" pos="word" start_char="3215">scientists</TOKEN>
<TOKEN end_char="3234" id="token-21-5" morph="none" pos="word" start_char="3226">published</TOKEN>
<TOKEN end_char="3242" id="token-21-6" morph="none" pos="word" start_char="3236">earlier</TOKEN>
<TOKEN end_char="3247" id="token-21-7" morph="none" pos="word" start_char="3244">this</TOKEN>
<TOKEN end_char="3252" id="token-21-8" morph="none" pos="word" start_char="3249">week</TOKEN>
<TOKEN end_char="3258" id="token-21-9" morph="none" pos="word" start_char="3254">found</TOKEN>
<TOKEN end_char="3260" id="token-21-10" morph="none" pos="word" start_char="3260">a</TOKEN>
<TOKEN end_char="3264" id="token-21-11" morph="none" pos="word" start_char="3262">man</TOKEN>
<TOKEN end_char="3267" id="token-21-12" morph="none" pos="word" start_char="3266">in</TOKEN>
<TOKEN end_char="3274" id="token-21-13" morph="none" pos="word" start_char="3269">France</TOKEN>
<TOKEN end_char="3278" id="token-21-14" morph="none" pos="word" start_char="3276">was</TOKEN>
<TOKEN end_char="3287" id="token-21-15" morph="none" pos="word" start_char="3280">infected</TOKEN>
<TOKEN end_char="3292" id="token-21-16" morph="none" pos="word" start_char="3289">with</TOKEN>
<TOKEN end_char="3301" id="token-21-17" morph="none" pos="unknown" start_char="3294">COVID-19</TOKEN>
<TOKEN end_char="3304" id="token-21-18" morph="none" pos="word" start_char="3303">as</TOKEN>
<TOKEN end_char="3310" id="token-21-19" morph="none" pos="word" start_char="3306">early</TOKEN>
<TOKEN end_char="3313" id="token-21-20" morph="none" pos="word" start_char="3312">as</TOKEN>
<TOKEN end_char="3317" id="token-21-21" morph="none" pos="word" start_char="3315">Dec</TOKEN>
<TOKEN end_char="3318" id="token-21-22" morph="none" pos="punct" start_char="3318">.</TOKEN>
</SEG>
<SEG end_char="3389" id="segment-22" start_char="3320">
<ORIGINAL_TEXT>27, nearly a month before authorities there confirmed the first cases.</ORIGINAL_TEXT>
<TOKEN end_char="3321" id="token-22-0" morph="none" pos="word" start_char="3320">27</TOKEN>
<TOKEN end_char="3322" id="token-22-1" morph="none" pos="punct" start_char="3322">,</TOKEN>
<TOKEN end_char="3329" id="token-22-2" morph="none" pos="word" start_char="3324">nearly</TOKEN>
<TOKEN end_char="3331" id="token-22-3" morph="none" pos="word" start_char="3331">a</TOKEN>
<TOKEN end_char="3337" id="token-22-4" morph="none" pos="word" start_char="3333">month</TOKEN>
<TOKEN end_char="3344" id="token-22-5" morph="none" pos="word" start_char="3339">before</TOKEN>
<TOKEN end_char="3356" id="token-22-6" morph="none" pos="word" start_char="3346">authorities</TOKEN>
<TOKEN end_char="3362" id="token-22-7" morph="none" pos="word" start_char="3358">there</TOKEN>
<TOKEN end_char="3372" id="token-22-8" morph="none" pos="word" start_char="3364">confirmed</TOKEN>
<TOKEN end_char="3376" id="token-22-9" morph="none" pos="word" start_char="3374">the</TOKEN>
<TOKEN end_char="3382" id="token-22-10" morph="none" pos="word" start_char="3378">first</TOKEN>
<TOKEN end_char="3388" id="token-22-11" morph="none" pos="word" start_char="3384">cases</TOKEN>
<TOKEN end_char="3389" id="token-22-12" morph="none" pos="punct" start_char="3389">.</TOKEN>
</SEG>
<SEG end_char="3531" id="segment-23" start_char="3392">
<ORIGINAL_TEXT>The World Health Organization said the French case was "not surprising" and urged countries to investigate any other early suspicious cases.</ORIGINAL_TEXT>
<TOKEN end_char="3394" id="token-23-0" morph="none" pos="word" start_char="3392">The</TOKEN>
<TOKEN end_char="3400" id="token-23-1" morph="none" pos="word" start_char="3396">World</TOKEN>
<TOKEN end_char="3407" id="token-23-2" morph="none" pos="word" start_char="3402">Health</TOKEN>
<TOKEN end_char="3420" id="token-23-3" morph="none" pos="word" start_char="3409">Organization</TOKEN>
<TOKEN end_char="3425" id="token-23-4" morph="none" pos="word" start_char="3422">said</TOKEN>
<TOKEN end_char="3429" id="token-23-5" morph="none" pos="word" start_char="3427">the</TOKEN>
<TOKEN end_char="3436" id="token-23-6" morph="none" pos="word" start_char="3431">French</TOKEN>
<TOKEN end_char="3441" id="token-23-7" morph="none" pos="word" start_char="3438">case</TOKEN>
<TOKEN end_char="3445" id="token-23-8" morph="none" pos="word" start_char="3443">was</TOKEN>
<TOKEN end_char="3447" id="token-23-9" morph="none" pos="punct" start_char="3447">"</TOKEN>
<TOKEN end_char="3450" id="token-23-10" morph="none" pos="word" start_char="3448">not</TOKEN>
<TOKEN end_char="3461" id="token-23-11" morph="none" pos="word" start_char="3452">surprising</TOKEN>
<TOKEN end_char="3462" id="token-23-12" morph="none" pos="punct" start_char="3462">"</TOKEN>
<TOKEN end_char="3466" id="token-23-13" morph="none" pos="word" start_char="3464">and</TOKEN>
<TOKEN end_char="3472" id="token-23-14" morph="none" pos="word" start_char="3468">urged</TOKEN>
<TOKEN end_char="3482" id="token-23-15" morph="none" pos="word" start_char="3474">countries</TOKEN>
<TOKEN end_char="3485" id="token-23-16" morph="none" pos="word" start_char="3484">to</TOKEN>
<TOKEN end_char="3497" id="token-23-17" morph="none" pos="word" start_char="3487">investigate</TOKEN>
<TOKEN end_char="3501" id="token-23-18" morph="none" pos="word" start_char="3499">any</TOKEN>
<TOKEN end_char="3507" id="token-23-19" morph="none" pos="word" start_char="3503">other</TOKEN>
<TOKEN end_char="3513" id="token-23-20" morph="none" pos="word" start_char="3509">early</TOKEN>
<TOKEN end_char="3524" id="token-23-21" morph="none" pos="word" start_char="3515">suspicious</TOKEN>
<TOKEN end_char="3530" id="token-23-22" morph="none" pos="word" start_char="3526">cases</TOKEN>
<TOKEN end_char="3531" id="token-23-23" morph="none" pos="punct" start_char="3531">.</TOKEN>
</SEG>
<SEG end_char="3636" id="segment-24" start_char="3534">
<ORIGINAL_TEXT>Balloux’s team screened the genomes of more than 7,500 viruses from infected patients around the world.</ORIGINAL_TEXT>
<TOKEN end_char="3542" id="token-24-0" morph="none" pos="word" start_char="3534">Balloux’s</TOKEN>
<TOKEN end_char="3547" id="token-24-1" morph="none" pos="word" start_char="3544">team</TOKEN>
<TOKEN end_char="3556" id="token-24-2" morph="none" pos="word" start_char="3549">screened</TOKEN>
<TOKEN end_char="3560" id="token-24-3" morph="none" pos="word" start_char="3558">the</TOKEN>
<TOKEN end_char="3568" id="token-24-4" morph="none" pos="word" start_char="3562">genomes</TOKEN>
<TOKEN end_char="3571" id="token-24-5" morph="none" pos="word" start_char="3570">of</TOKEN>
<TOKEN end_char="3576" id="token-24-6" morph="none" pos="word" start_char="3573">more</TOKEN>
<TOKEN end_char="3581" id="token-24-7" morph="none" pos="word" start_char="3578">than</TOKEN>
<TOKEN end_char="3587" id="token-24-8" morph="none" pos="unknown" start_char="3583">7,500</TOKEN>
<TOKEN end_char="3595" id="token-24-9" morph="none" pos="word" start_char="3589">viruses</TOKEN>
<TOKEN end_char="3600" id="token-24-10" morph="none" pos="word" start_char="3597">from</TOKEN>
<TOKEN end_char="3609" id="token-24-11" morph="none" pos="word" start_char="3602">infected</TOKEN>
<TOKEN end_char="3618" id="token-24-12" morph="none" pos="word" start_char="3611">patients</TOKEN>
<TOKEN end_char="3625" id="token-24-13" morph="none" pos="word" start_char="3620">around</TOKEN>
<TOKEN end_char="3629" id="token-24-14" morph="none" pos="word" start_char="3627">the</TOKEN>
<TOKEN end_char="3635" id="token-24-15" morph="none" pos="word" start_char="3631">world</TOKEN>
<TOKEN end_char="3636" id="token-24-16" morph="none" pos="punct" start_char="3636">.</TOKEN>
</SEG>
<SEG end_char="3831" id="segment-25" start_char="3638">
<ORIGINAL_TEXT>Their results add to a growing body of evidence that SARS-CoV-2 viruses share a common ancestor from late 2019, suggesting this was when the virus jumped from a previous animal host into people.</ORIGINAL_TEXT>
<TOKEN end_char="3642" id="token-25-0" morph="none" pos="word" start_char="3638">Their</TOKEN>
<TOKEN end_char="3650" id="token-25-1" morph="none" pos="word" start_char="3644">results</TOKEN>
<TOKEN end_char="3654" id="token-25-2" morph="none" pos="word" start_char="3652">add</TOKEN>
<TOKEN end_char="3657" id="token-25-3" morph="none" pos="word" start_char="3656">to</TOKEN>
<TOKEN end_char="3659" id="token-25-4" morph="none" pos="word" start_char="3659">a</TOKEN>
<TOKEN end_char="3667" id="token-25-5" morph="none" pos="word" start_char="3661">growing</TOKEN>
<TOKEN end_char="3672" id="token-25-6" morph="none" pos="word" start_char="3669">body</TOKEN>
<TOKEN end_char="3675" id="token-25-7" morph="none" pos="word" start_char="3674">of</TOKEN>
<TOKEN end_char="3684" id="token-25-8" morph="none" pos="word" start_char="3677">evidence</TOKEN>
<TOKEN end_char="3689" id="token-25-9" morph="none" pos="word" start_char="3686">that</TOKEN>
<TOKEN end_char="3700" id="token-25-10" morph="none" pos="unknown" start_char="3691">SARS-CoV-2</TOKEN>
<TOKEN end_char="3708" id="token-25-11" morph="none" pos="word" start_char="3702">viruses</TOKEN>
<TOKEN end_char="3714" id="token-25-12" morph="none" pos="word" start_char="3710">share</TOKEN>
<TOKEN end_char="3716" id="token-25-13" morph="none" pos="word" start_char="3716">a</TOKEN>
<TOKEN end_char="3723" id="token-25-14" morph="none" pos="word" start_char="3718">common</TOKEN>
<TOKEN end_char="3732" id="token-25-15" morph="none" pos="word" start_char="3725">ancestor</TOKEN>
<TOKEN end_char="3737" id="token-25-16" morph="none" pos="word" start_char="3734">from</TOKEN>
<TOKEN end_char="3742" id="token-25-17" morph="none" pos="word" start_char="3739">late</TOKEN>
<TOKEN end_char="3747" id="token-25-18" morph="none" pos="word" start_char="3744">2019</TOKEN>
<TOKEN end_char="3748" id="token-25-19" morph="none" pos="punct" start_char="3748">,</TOKEN>
<TOKEN end_char="3759" id="token-25-20" morph="none" pos="word" start_char="3750">suggesting</TOKEN>
<TOKEN end_char="3764" id="token-25-21" morph="none" pos="word" start_char="3761">this</TOKEN>
<TOKEN end_char="3768" id="token-25-22" morph="none" pos="word" start_char="3766">was</TOKEN>
<TOKEN end_char="3773" id="token-25-23" morph="none" pos="word" start_char="3770">when</TOKEN>
<TOKEN end_char="3777" id="token-25-24" morph="none" pos="word" start_char="3775">the</TOKEN>
<TOKEN end_char="3783" id="token-25-25" morph="none" pos="word" start_char="3779">virus</TOKEN>
<TOKEN end_char="3790" id="token-25-26" morph="none" pos="word" start_char="3785">jumped</TOKEN>
<TOKEN end_char="3795" id="token-25-27" morph="none" pos="word" start_char="3792">from</TOKEN>
<TOKEN end_char="3797" id="token-25-28" morph="none" pos="word" start_char="3797">a</TOKEN>
<TOKEN end_char="3806" id="token-25-29" morph="none" pos="word" start_char="3799">previous</TOKEN>
<TOKEN end_char="3813" id="token-25-30" morph="none" pos="word" start_char="3808">animal</TOKEN>
<TOKEN end_char="3818" id="token-25-31" morph="none" pos="word" start_char="3815">host</TOKEN>
<TOKEN end_char="3823" id="token-25-32" morph="none" pos="word" start_char="3820">into</TOKEN>
<TOKEN end_char="3830" id="token-25-33" morph="none" pos="word" start_char="3825">people</TOKEN>
<TOKEN end_char="3831" id="token-25-34" morph="none" pos="punct" start_char="3831">.</TOKEN>
</SEG>
<SEG end_char="4053" id="segment-26" start_char="3834">
<ORIGINAL_TEXT>The UCL researchers also found almost 200 small genetic changes, or mutations, in the coronavirus genomes they analysed - findings Balloux said offered helpful clues for researchers seeking to develop drugs and vaccines.</ORIGINAL_TEXT>
<TOKEN end_char="3836" id="token-26-0" morph="none" pos="word" start_char="3834">The</TOKEN>
<TOKEN end_char="3840" id="token-26-1" morph="none" pos="word" start_char="3838">UCL</TOKEN>
<TOKEN end_char="3852" id="token-26-2" morph="none" pos="word" start_char="3842">researchers</TOKEN>
<TOKEN end_char="3857" id="token-26-3" morph="none" pos="word" start_char="3854">also</TOKEN>
<TOKEN end_char="3863" id="token-26-4" morph="none" pos="word" start_char="3859">found</TOKEN>
<TOKEN end_char="3870" id="token-26-5" morph="none" pos="word" start_char="3865">almost</TOKEN>
<TOKEN end_char="3874" id="token-26-6" morph="none" pos="word" start_char="3872">200</TOKEN>
<TOKEN end_char="3880" id="token-26-7" morph="none" pos="word" start_char="3876">small</TOKEN>
<TOKEN end_char="3888" id="token-26-8" morph="none" pos="word" start_char="3882">genetic</TOKEN>
<TOKEN end_char="3896" id="token-26-9" morph="none" pos="word" start_char="3890">changes</TOKEN>
<TOKEN end_char="3897" id="token-26-10" morph="none" pos="punct" start_char="3897">,</TOKEN>
<TOKEN end_char="3900" id="token-26-11" morph="none" pos="word" start_char="3899">or</TOKEN>
<TOKEN end_char="3910" id="token-26-12" morph="none" pos="word" start_char="3902">mutations</TOKEN>
<TOKEN end_char="3911" id="token-26-13" morph="none" pos="punct" start_char="3911">,</TOKEN>
<TOKEN end_char="3914" id="token-26-14" morph="none" pos="word" start_char="3913">in</TOKEN>
<TOKEN end_char="3918" id="token-26-15" morph="none" pos="word" start_char="3916">the</TOKEN>
<TOKEN end_char="3930" id="token-26-16" morph="none" pos="word" start_char="3920">coronavirus</TOKEN>
<TOKEN end_char="3938" id="token-26-17" morph="none" pos="word" start_char="3932">genomes</TOKEN>
<TOKEN end_char="3943" id="token-26-18" morph="none" pos="word" start_char="3940">they</TOKEN>
<TOKEN end_char="3952" id="token-26-19" morph="none" pos="word" start_char="3945">analysed</TOKEN>
<TOKEN end_char="3954" id="token-26-20" morph="none" pos="punct" start_char="3954">-</TOKEN>
<TOKEN end_char="3963" id="token-26-21" morph="none" pos="word" start_char="3956">findings</TOKEN>
<TOKEN end_char="3971" id="token-26-22" morph="none" pos="word" start_char="3965">Balloux</TOKEN>
<TOKEN end_char="3976" id="token-26-23" morph="none" pos="word" start_char="3973">said</TOKEN>
<TOKEN end_char="3984" id="token-26-24" morph="none" pos="word" start_char="3978">offered</TOKEN>
<TOKEN end_char="3992" id="token-26-25" morph="none" pos="word" start_char="3986">helpful</TOKEN>
<TOKEN end_char="3998" id="token-26-26" morph="none" pos="word" start_char="3994">clues</TOKEN>
<TOKEN end_char="4002" id="token-26-27" morph="none" pos="word" start_char="4000">for</TOKEN>
<TOKEN end_char="4014" id="token-26-28" morph="none" pos="word" start_char="4004">researchers</TOKEN>
<TOKEN end_char="4022" id="token-26-29" morph="none" pos="word" start_char="4016">seeking</TOKEN>
<TOKEN end_char="4025" id="token-26-30" morph="none" pos="word" start_char="4024">to</TOKEN>
<TOKEN end_char="4033" id="token-26-31" morph="none" pos="word" start_char="4027">develop</TOKEN>
<TOKEN end_char="4039" id="token-26-32" morph="none" pos="word" start_char="4035">drugs</TOKEN>
<TOKEN end_char="4043" id="token-26-33" morph="none" pos="word" start_char="4041">and</TOKEN>
<TOKEN end_char="4052" id="token-26-34" morph="none" pos="word" start_char="4045">vaccines</TOKEN>
<TOKEN end_char="4053" id="token-26-35" morph="none" pos="punct" start_char="4053">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>