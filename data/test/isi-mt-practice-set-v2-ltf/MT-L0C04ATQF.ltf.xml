<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04ATQF" lang="spa" raw_text_char_length="5206" raw_text_md5="4cdc93bea93b00c65758502fde853ecb" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="80" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Un estudio médico alerta de que la covid-19 circulaba por Europa ya en diciembre</ORIGINAL_TEXT>
<TOKEN end_char="2" id="token-0-0" morph="none" pos="word" start_char="1">Un</TOKEN>
<TOKEN end_char="10" id="token-0-1" morph="none" pos="word" start_char="4">estudio</TOKEN>
<TOKEN end_char="17" id="token-0-2" morph="none" pos="word" start_char="12">médico</TOKEN>
<TOKEN end_char="24" id="token-0-3" morph="none" pos="word" start_char="19">alerta</TOKEN>
<TOKEN end_char="27" id="token-0-4" morph="none" pos="word" start_char="26">de</TOKEN>
<TOKEN end_char="31" id="token-0-5" morph="none" pos="word" start_char="29">que</TOKEN>
<TOKEN end_char="34" id="token-0-6" morph="none" pos="word" start_char="33">la</TOKEN>
<TOKEN end_char="43" id="token-0-7" morph="none" pos="unknown" start_char="36">covid-19</TOKEN>
<TOKEN end_char="53" id="token-0-8" morph="none" pos="word" start_char="45">circulaba</TOKEN>
<TOKEN end_char="57" id="token-0-9" morph="none" pos="word" start_char="55">por</TOKEN>
<TOKEN end_char="64" id="token-0-10" morph="none" pos="word" start_char="59">Europa</TOKEN>
<TOKEN end_char="67" id="token-0-11" morph="none" pos="word" start_char="66">ya</TOKEN>
<TOKEN end_char="70" id="token-0-12" morph="none" pos="word" start_char="69">en</TOKEN>
<TOKEN end_char="80" id="token-0-13" morph="none" pos="word" start_char="72">diciembre</TOKEN>
<TRANSLATED_TEXT>A medical study warns that covid-19 was circulating throughout Europe as early as December</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="163" id="segment-1" start_char="84">
<ORIGINAL_TEXT>Un estudio médico alerta de que la covid-19 circulaba por Europa ya en diciembre</ORIGINAL_TEXT>
<TOKEN end_char="85" id="token-1-0" morph="none" pos="word" start_char="84">Un</TOKEN>
<TOKEN end_char="93" id="token-1-1" morph="none" pos="word" start_char="87">estudio</TOKEN>
<TOKEN end_char="100" id="token-1-2" morph="none" pos="word" start_char="95">médico</TOKEN>
<TOKEN end_char="107" id="token-1-3" morph="none" pos="word" start_char="102">alerta</TOKEN>
<TOKEN end_char="110" id="token-1-4" morph="none" pos="word" start_char="109">de</TOKEN>
<TOKEN end_char="114" id="token-1-5" morph="none" pos="word" start_char="112">que</TOKEN>
<TOKEN end_char="117" id="token-1-6" morph="none" pos="word" start_char="116">la</TOKEN>
<TOKEN end_char="126" id="token-1-7" morph="none" pos="unknown" start_char="119">covid-19</TOKEN>
<TOKEN end_char="136" id="token-1-8" morph="none" pos="word" start_char="128">circulaba</TOKEN>
<TOKEN end_char="140" id="token-1-9" morph="none" pos="word" start_char="138">por</TOKEN>
<TOKEN end_char="147" id="token-1-10" morph="none" pos="word" start_char="142">Europa</TOKEN>
<TOKEN end_char="150" id="token-1-11" morph="none" pos="word" start_char="149">ya</TOKEN>
<TOKEN end_char="153" id="token-1-12" morph="none" pos="word" start_char="152">en</TOKEN>
<TOKEN end_char="163" id="token-1-13" morph="none" pos="word" start_char="155">diciembre</TOKEN>
<TRANSLATED_TEXT>A medical study warns that covid-19 was circulating throughout Europe as early as December</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="320" id="segment-2" start_char="167">
<ORIGINAL_TEXT>Tiene dirección el que hizo el estudio ese o es como las empresas intermediarias fantasma que contrata el ministro Illa a dedo, cual corrupto cualquiera??</ORIGINAL_TEXT>
<TOKEN end_char="171" id="token-2-0" morph="none" pos="word" start_char="167">Tiene</TOKEN>
<TOKEN end_char="181" id="token-2-1" morph="none" pos="word" start_char="173">dirección</TOKEN>
<TOKEN end_char="184" id="token-2-2" morph="none" pos="word" start_char="183">el</TOKEN>
<TOKEN end_char="188" id="token-2-3" morph="none" pos="word" start_char="186">que</TOKEN>
<TOKEN end_char="193" id="token-2-4" morph="none" pos="word" start_char="190">hizo</TOKEN>
<TOKEN end_char="196" id="token-2-5" morph="none" pos="word" start_char="195">el</TOKEN>
<TOKEN end_char="204" id="token-2-6" morph="none" pos="word" start_char="198">estudio</TOKEN>
<TOKEN end_char="208" id="token-2-7" morph="none" pos="word" start_char="206">ese</TOKEN>
<TOKEN end_char="210" id="token-2-8" morph="none" pos="word" start_char="210">o</TOKEN>
<TOKEN end_char="213" id="token-2-9" morph="none" pos="word" start_char="212">es</TOKEN>
<TOKEN end_char="218" id="token-2-10" morph="none" pos="word" start_char="215">como</TOKEN>
<TOKEN end_char="222" id="token-2-11" morph="none" pos="word" start_char="220">las</TOKEN>
<TOKEN end_char="231" id="token-2-12" morph="none" pos="word" start_char="224">empresas</TOKEN>
<TOKEN end_char="246" id="token-2-13" morph="none" pos="word" start_char="233">intermediarias</TOKEN>
<TOKEN end_char="255" id="token-2-14" morph="none" pos="word" start_char="248">fantasma</TOKEN>
<TOKEN end_char="259" id="token-2-15" morph="none" pos="word" start_char="257">que</TOKEN>
<TOKEN end_char="268" id="token-2-16" morph="none" pos="word" start_char="261">contrata</TOKEN>
<TOKEN end_char="271" id="token-2-17" morph="none" pos="word" start_char="270">el</TOKEN>
<TOKEN end_char="280" id="token-2-18" morph="none" pos="word" start_char="273">ministro</TOKEN>
<TOKEN end_char="285" id="token-2-19" morph="none" pos="word" start_char="282">Illa</TOKEN>
<TOKEN end_char="287" id="token-2-20" morph="none" pos="word" start_char="287">a</TOKEN>
<TOKEN end_char="292" id="token-2-21" morph="none" pos="word" start_char="289">dedo</TOKEN>
<TOKEN end_char="293" id="token-2-22" morph="none" pos="punct" start_char="293">,</TOKEN>
<TOKEN end_char="298" id="token-2-23" morph="none" pos="word" start_char="295">cual</TOKEN>
<TOKEN end_char="307" id="token-2-24" morph="none" pos="word" start_char="300">corrupto</TOKEN>
<TOKEN end_char="318" id="token-2-25" morph="none" pos="word" start_char="309">cualquiera</TOKEN>
<TOKEN end_char="320" id="token-2-26" morph="none" pos="punct" start_char="319">??</TOKEN>
<TRANSLATED_TEXT>Has direction what that study did or is it like the phantom intermediary companies that hire Minister Illa by hand, which corrupt anyone??</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="731" id="segment-3" start_char="324">
<ORIGINAL_TEXT>En diciembre me dio un chungo terrible, 2 semanas jodidos, 3 dias en cama, me costaba respirar y reir ya ni te cuento.. jodido de la garganta, tos, fiebre...y un dolor de cuerpo desde las uñas de los pies hasta la punta de los pelos, se me fue el gusto, vamos que nada tenia sabor... y como ya he comentado..soy una persona que se cuida.. lo achaque a una gripe estacional... pero NUNCA me dio de esa manera.</ORIGINAL_TEXT>
<TOKEN end_char="325" id="token-3-0" morph="none" pos="word" start_char="324">En</TOKEN>
<TOKEN end_char="335" id="token-3-1" morph="none" pos="word" start_char="327">diciembre</TOKEN>
<TOKEN end_char="338" id="token-3-2" morph="none" pos="word" start_char="337">me</TOKEN>
<TOKEN end_char="342" id="token-3-3" morph="none" pos="word" start_char="340">dio</TOKEN>
<TOKEN end_char="345" id="token-3-4" morph="none" pos="word" start_char="344">un</TOKEN>
<TOKEN end_char="352" id="token-3-5" morph="none" pos="word" start_char="347">chungo</TOKEN>
<TOKEN end_char="361" id="token-3-6" morph="none" pos="word" start_char="354">terrible</TOKEN>
<TOKEN end_char="362" id="token-3-7" morph="none" pos="punct" start_char="362">,</TOKEN>
<TOKEN end_char="364" id="token-3-8" morph="none" pos="word" start_char="364">2</TOKEN>
<TOKEN end_char="372" id="token-3-9" morph="none" pos="word" start_char="366">semanas</TOKEN>
<TOKEN end_char="380" id="token-3-10" morph="none" pos="word" start_char="374">jodidos</TOKEN>
<TOKEN end_char="381" id="token-3-11" morph="none" pos="punct" start_char="381">,</TOKEN>
<TOKEN end_char="383" id="token-3-12" morph="none" pos="word" start_char="383">3</TOKEN>
<TOKEN end_char="388" id="token-3-13" morph="none" pos="word" start_char="385">dias</TOKEN>
<TOKEN end_char="391" id="token-3-14" morph="none" pos="word" start_char="390">en</TOKEN>
<TOKEN end_char="396" id="token-3-15" morph="none" pos="word" start_char="393">cama</TOKEN>
<TOKEN end_char="397" id="token-3-16" morph="none" pos="punct" start_char="397">,</TOKEN>
<TOKEN end_char="400" id="token-3-17" morph="none" pos="word" start_char="399">me</TOKEN>
<TOKEN end_char="408" id="token-3-18" morph="none" pos="word" start_char="402">costaba</TOKEN>
<TOKEN end_char="417" id="token-3-19" morph="none" pos="word" start_char="410">respirar</TOKEN>
<TOKEN end_char="419" id="token-3-20" morph="none" pos="word" start_char="419">y</TOKEN>
<TOKEN end_char="424" id="token-3-21" morph="none" pos="word" start_char="421">reir</TOKEN>
<TOKEN end_char="427" id="token-3-22" morph="none" pos="word" start_char="426">ya</TOKEN>
<TOKEN end_char="430" id="token-3-23" morph="none" pos="word" start_char="429">ni</TOKEN>
<TOKEN end_char="433" id="token-3-24" morph="none" pos="word" start_char="432">te</TOKEN>
<TOKEN end_char="440" id="token-3-25" morph="none" pos="word" start_char="435">cuento</TOKEN>
<TOKEN end_char="442" id="token-3-26" morph="none" pos="punct" start_char="441">..</TOKEN>
<TOKEN end_char="449" id="token-3-27" morph="none" pos="word" start_char="444">jodido</TOKEN>
<TOKEN end_char="452" id="token-3-28" morph="none" pos="word" start_char="451">de</TOKEN>
<TOKEN end_char="455" id="token-3-29" morph="none" pos="word" start_char="454">la</TOKEN>
<TOKEN end_char="464" id="token-3-30" morph="none" pos="word" start_char="457">garganta</TOKEN>
<TOKEN end_char="465" id="token-3-31" morph="none" pos="punct" start_char="465">,</TOKEN>
<TOKEN end_char="469" id="token-3-32" morph="none" pos="word" start_char="467">tos</TOKEN>
<TOKEN end_char="470" id="token-3-33" morph="none" pos="punct" start_char="470">,</TOKEN>
<TOKEN end_char="481" id="token-3-34" morph="none" pos="unknown" start_char="472">fiebre...y</TOKEN>
<TOKEN end_char="484" id="token-3-35" morph="none" pos="word" start_char="483">un</TOKEN>
<TOKEN end_char="490" id="token-3-36" morph="none" pos="word" start_char="486">dolor</TOKEN>
<TOKEN end_char="493" id="token-3-37" morph="none" pos="word" start_char="492">de</TOKEN>
<TOKEN end_char="500" id="token-3-38" morph="none" pos="word" start_char="495">cuerpo</TOKEN>
<TOKEN end_char="506" id="token-3-39" morph="none" pos="word" start_char="502">desde</TOKEN>
<TOKEN end_char="510" id="token-3-40" morph="none" pos="word" start_char="508">las</TOKEN>
<TOKEN end_char="515" id="token-3-41" morph="none" pos="word" start_char="512">uñas</TOKEN>
<TOKEN end_char="518" id="token-3-42" morph="none" pos="word" start_char="517">de</TOKEN>
<TOKEN end_char="522" id="token-3-43" morph="none" pos="word" start_char="520">los</TOKEN>
<TOKEN end_char="527" id="token-3-44" morph="none" pos="word" start_char="524">pies</TOKEN>
<TOKEN end_char="533" id="token-3-45" morph="none" pos="word" start_char="529">hasta</TOKEN>
<TOKEN end_char="536" id="token-3-46" morph="none" pos="word" start_char="535">la</TOKEN>
<TOKEN end_char="542" id="token-3-47" morph="none" pos="word" start_char="538">punta</TOKEN>
<TOKEN end_char="545" id="token-3-48" morph="none" pos="word" start_char="544">de</TOKEN>
<TOKEN end_char="549" id="token-3-49" morph="none" pos="word" start_char="547">los</TOKEN>
<TOKEN end_char="555" id="token-3-50" morph="none" pos="word" start_char="551">pelos</TOKEN>
<TOKEN end_char="556" id="token-3-51" morph="none" pos="punct" start_char="556">,</TOKEN>
<TOKEN end_char="559" id="token-3-52" morph="none" pos="word" start_char="558">se</TOKEN>
<TOKEN end_char="562" id="token-3-53" morph="none" pos="word" start_char="561">me</TOKEN>
<TOKEN end_char="566" id="token-3-54" morph="none" pos="word" start_char="564">fue</TOKEN>
<TOKEN end_char="569" id="token-3-55" morph="none" pos="word" start_char="568">el</TOKEN>
<TOKEN end_char="575" id="token-3-56" morph="none" pos="word" start_char="571">gusto</TOKEN>
<TOKEN end_char="576" id="token-3-57" morph="none" pos="punct" start_char="576">,</TOKEN>
<TOKEN end_char="582" id="token-3-58" morph="none" pos="word" start_char="578">vamos</TOKEN>
<TOKEN end_char="586" id="token-3-59" morph="none" pos="word" start_char="584">que</TOKEN>
<TOKEN end_char="591" id="token-3-60" morph="none" pos="word" start_char="588">nada</TOKEN>
<TOKEN end_char="597" id="token-3-61" morph="none" pos="word" start_char="593">tenia</TOKEN>
<TOKEN end_char="603" id="token-3-62" morph="none" pos="word" start_char="599">sabor</TOKEN>
<TOKEN end_char="606" id="token-3-63" morph="none" pos="punct" start_char="604">...</TOKEN>
<TOKEN end_char="608" id="token-3-64" morph="none" pos="word" start_char="608">y</TOKEN>
<TOKEN end_char="613" id="token-3-65" morph="none" pos="word" start_char="610">como</TOKEN>
<TOKEN end_char="616" id="token-3-66" morph="none" pos="word" start_char="615">ya</TOKEN>
<TOKEN end_char="619" id="token-3-67" morph="none" pos="word" start_char="618">he</TOKEN>
<TOKEN end_char="634" id="token-3-68" morph="none" pos="unknown" start_char="621">comentado..soy</TOKEN>
<TOKEN end_char="638" id="token-3-69" morph="none" pos="word" start_char="636">una</TOKEN>
<TOKEN end_char="646" id="token-3-70" morph="none" pos="word" start_char="640">persona</TOKEN>
<TOKEN end_char="650" id="token-3-71" morph="none" pos="word" start_char="648">que</TOKEN>
<TOKEN end_char="653" id="token-3-72" morph="none" pos="word" start_char="652">se</TOKEN>
<TOKEN end_char="659" id="token-3-73" morph="none" pos="word" start_char="655">cuida</TOKEN>
<TOKEN end_char="661" id="token-3-74" morph="none" pos="punct" start_char="660">..</TOKEN>
<TOKEN end_char="664" id="token-3-75" morph="none" pos="word" start_char="663">lo</TOKEN>
<TOKEN end_char="672" id="token-3-76" morph="none" pos="word" start_char="666">achaque</TOKEN>
<TOKEN end_char="674" id="token-3-77" morph="none" pos="word" start_char="674">a</TOKEN>
<TOKEN end_char="678" id="token-3-78" morph="none" pos="word" start_char="676">una</TOKEN>
<TOKEN end_char="684" id="token-3-79" morph="none" pos="word" start_char="680">gripe</TOKEN>
<TOKEN end_char="695" id="token-3-80" morph="none" pos="word" start_char="686">estacional</TOKEN>
<TOKEN end_char="698" id="token-3-81" morph="none" pos="punct" start_char="696">...</TOKEN>
<TOKEN end_char="703" id="token-3-82" morph="none" pos="word" start_char="700">pero</TOKEN>
<TOKEN end_char="709" id="token-3-83" morph="none" pos="word" start_char="705">NUNCA</TOKEN>
<TOKEN end_char="712" id="token-3-84" morph="none" pos="word" start_char="711">me</TOKEN>
<TOKEN end_char="716" id="token-3-85" morph="none" pos="word" start_char="714">dio</TOKEN>
<TOKEN end_char="719" id="token-3-86" morph="none" pos="word" start_char="718">de</TOKEN>
<TOKEN end_char="723" id="token-3-87" morph="none" pos="word" start_char="721">esa</TOKEN>
<TOKEN end_char="730" id="token-3-88" morph="none" pos="word" start_char="725">manera</TOKEN>
<TOKEN end_char="731" id="token-3-89" morph="none" pos="punct" start_char="731">.</TOKEN>
<TRANSLATED_TEXT>In December he gave me a terrible chungo, 2 weeks fucked up, 3 days in bed, I couldn't breathe and laugh anymore.. fucked up throat, cough, fever... and a body pain from the fingernails to the tip of the hair, I got the taste, let's go that nothing had taste... and as I have already commented.. I am a person who takes care of.. he looks like a seasonal flu... but NONE gave me that way.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="799" id="segment-4" start_char="735">
<ORIGINAL_TEXT>El bicho lleva dando vueltas al mundo desde septiembre de 2019...</ORIGINAL_TEXT>
<TOKEN end_char="736" id="token-4-0" morph="none" pos="word" start_char="735">El</TOKEN>
<TOKEN end_char="742" id="token-4-1" morph="none" pos="word" start_char="738">bicho</TOKEN>
<TOKEN end_char="748" id="token-4-2" morph="none" pos="word" start_char="744">lleva</TOKEN>
<TOKEN end_char="754" id="token-4-3" morph="none" pos="word" start_char="750">dando</TOKEN>
<TOKEN end_char="762" id="token-4-4" morph="none" pos="word" start_char="756">vueltas</TOKEN>
<TOKEN end_char="765" id="token-4-5" morph="none" pos="word" start_char="764">al</TOKEN>
<TOKEN end_char="771" id="token-4-6" morph="none" pos="word" start_char="767">mundo</TOKEN>
<TOKEN end_char="777" id="token-4-7" morph="none" pos="word" start_char="773">desde</TOKEN>
<TOKEN end_char="788" id="token-4-8" morph="none" pos="word" start_char="779">septiembre</TOKEN>
<TOKEN end_char="791" id="token-4-9" morph="none" pos="word" start_char="790">de</TOKEN>
<TOKEN end_char="796" id="token-4-10" morph="none" pos="word" start_char="793">2019</TOKEN>
<TOKEN end_char="799" id="token-4-11" morph="none" pos="punct" start_char="797">...</TOKEN>
<TRANSLATED_TEXT>He has been touring the world since September 2019...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="846" id="segment-5" start_char="802">
<ORIGINAL_TEXT>Yo lo pasé en la primera semana de diciembre.</ORIGINAL_TEXT>
<TOKEN end_char="803" id="token-5-0" morph="none" pos="word" start_char="802">Yo</TOKEN>
<TOKEN end_char="806" id="token-5-1" morph="none" pos="word" start_char="805">lo</TOKEN>
<TOKEN end_char="811" id="token-5-2" morph="none" pos="word" start_char="808">pasé</TOKEN>
<TOKEN end_char="814" id="token-5-3" morph="none" pos="word" start_char="813">en</TOKEN>
<TOKEN end_char="817" id="token-5-4" morph="none" pos="word" start_char="816">la</TOKEN>
<TOKEN end_char="825" id="token-5-5" morph="none" pos="word" start_char="819">primera</TOKEN>
<TOKEN end_char="832" id="token-5-6" morph="none" pos="word" start_char="827">semana</TOKEN>
<TOKEN end_char="835" id="token-5-7" morph="none" pos="word" start_char="834">de</TOKEN>
<TOKEN end_char="845" id="token-5-8" morph="none" pos="word" start_char="837">diciembre</TOKEN>
<TOKEN end_char="846" id="token-5-9" morph="none" pos="punct" start_char="846">.</TOKEN>
<TRANSLATED_TEXT>I spent the first week of December.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1003" id="segment-6" start_char="848">
<ORIGINAL_TEXT>Mis hijas y mi mujer llegaron de viaje via Barcelona, mi mujer nada mas bajar me dijo que medio avión tosia con sequedad...que le había llamado la atención.</ORIGINAL_TEXT>
<TOKEN end_char="850" id="token-6-0" morph="none" pos="word" start_char="848">Mis</TOKEN>
<TOKEN end_char="856" id="token-6-1" morph="none" pos="word" start_char="852">hijas</TOKEN>
<TOKEN end_char="858" id="token-6-2" morph="none" pos="word" start_char="858">y</TOKEN>
<TOKEN end_char="861" id="token-6-3" morph="none" pos="word" start_char="860">mi</TOKEN>
<TOKEN end_char="867" id="token-6-4" morph="none" pos="word" start_char="863">mujer</TOKEN>
<TOKEN end_char="876" id="token-6-5" morph="none" pos="word" start_char="869">llegaron</TOKEN>
<TOKEN end_char="879" id="token-6-6" morph="none" pos="word" start_char="878">de</TOKEN>
<TOKEN end_char="885" id="token-6-7" morph="none" pos="word" start_char="881">viaje</TOKEN>
<TOKEN end_char="889" id="token-6-8" morph="none" pos="word" start_char="887">via</TOKEN>
<TOKEN end_char="899" id="token-6-9" morph="none" pos="word" start_char="891">Barcelona</TOKEN>
<TOKEN end_char="900" id="token-6-10" morph="none" pos="punct" start_char="900">,</TOKEN>
<TOKEN end_char="903" id="token-6-11" morph="none" pos="word" start_char="902">mi</TOKEN>
<TOKEN end_char="909" id="token-6-12" morph="none" pos="word" start_char="905">mujer</TOKEN>
<TOKEN end_char="914" id="token-6-13" morph="none" pos="word" start_char="911">nada</TOKEN>
<TOKEN end_char="918" id="token-6-14" morph="none" pos="word" start_char="916">mas</TOKEN>
<TOKEN end_char="924" id="token-6-15" morph="none" pos="word" start_char="920">bajar</TOKEN>
<TOKEN end_char="927" id="token-6-16" morph="none" pos="word" start_char="926">me</TOKEN>
<TOKEN end_char="932" id="token-6-17" morph="none" pos="word" start_char="929">dijo</TOKEN>
<TOKEN end_char="936" id="token-6-18" morph="none" pos="word" start_char="934">que</TOKEN>
<TOKEN end_char="942" id="token-6-19" morph="none" pos="word" start_char="938">medio</TOKEN>
<TOKEN end_char="948" id="token-6-20" morph="none" pos="word" start_char="944">avión</TOKEN>
<TOKEN end_char="954" id="token-6-21" morph="none" pos="word" start_char="950">tosia</TOKEN>
<TOKEN end_char="958" id="token-6-22" morph="none" pos="word" start_char="956">con</TOKEN>
<TOKEN end_char="973" id="token-6-23" morph="none" pos="unknown" start_char="960">sequedad...que</TOKEN>
<TOKEN end_char="976" id="token-6-24" morph="none" pos="word" start_char="975">le</TOKEN>
<TOKEN end_char="982" id="token-6-25" morph="none" pos="word" start_char="978">había</TOKEN>
<TOKEN end_char="990" id="token-6-26" morph="none" pos="word" start_char="984">llamado</TOKEN>
<TOKEN end_char="993" id="token-6-27" morph="none" pos="word" start_char="992">la</TOKEN>
<TOKEN end_char="1002" id="token-6-28" morph="none" pos="word" start_char="995">atención</TOKEN>
<TOKEN end_char="1003" id="token-6-29" morph="none" pos="punct" start_char="1003">.</TOKEN>
<TRANSLATED_TEXT>My daughters and my wife came on a trip via Barcelona, my wife nothing further down told me that half a plane was rolling with drought... which had drawn her attention.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1164" id="segment-7" start_char="1006">
<ORIGINAL_TEXT>A los quince días tosían mis dos hijas y mi mujer, y servidor la primera semana de diciembre con la peor afonía y lanrigitis de mi vida...literalmente sin voz.</ORIGINAL_TEXT>
<TOKEN end_char="1006" id="token-7-0" morph="none" pos="word" start_char="1006">A</TOKEN>
<TOKEN end_char="1010" id="token-7-1" morph="none" pos="word" start_char="1008">los</TOKEN>
<TOKEN end_char="1017" id="token-7-2" morph="none" pos="word" start_char="1012">quince</TOKEN>
<TOKEN end_char="1022" id="token-7-3" morph="none" pos="word" start_char="1019">días</TOKEN>
<TOKEN end_char="1029" id="token-7-4" morph="none" pos="word" start_char="1024">tosían</TOKEN>
<TOKEN end_char="1033" id="token-7-5" morph="none" pos="word" start_char="1031">mis</TOKEN>
<TOKEN end_char="1037" id="token-7-6" morph="none" pos="word" start_char="1035">dos</TOKEN>
<TOKEN end_char="1043" id="token-7-7" morph="none" pos="word" start_char="1039">hijas</TOKEN>
<TOKEN end_char="1045" id="token-7-8" morph="none" pos="word" start_char="1045">y</TOKEN>
<TOKEN end_char="1048" id="token-7-9" morph="none" pos="word" start_char="1047">mi</TOKEN>
<TOKEN end_char="1054" id="token-7-10" morph="none" pos="word" start_char="1050">mujer</TOKEN>
<TOKEN end_char="1055" id="token-7-11" morph="none" pos="punct" start_char="1055">,</TOKEN>
<TOKEN end_char="1057" id="token-7-12" morph="none" pos="word" start_char="1057">y</TOKEN>
<TOKEN end_char="1066" id="token-7-13" morph="none" pos="word" start_char="1059">servidor</TOKEN>
<TOKEN end_char="1069" id="token-7-14" morph="none" pos="word" start_char="1068">la</TOKEN>
<TOKEN end_char="1077" id="token-7-15" morph="none" pos="word" start_char="1071">primera</TOKEN>
<TOKEN end_char="1084" id="token-7-16" morph="none" pos="word" start_char="1079">semana</TOKEN>
<TOKEN end_char="1087" id="token-7-17" morph="none" pos="word" start_char="1086">de</TOKEN>
<TOKEN end_char="1097" id="token-7-18" morph="none" pos="word" start_char="1089">diciembre</TOKEN>
<TOKEN end_char="1101" id="token-7-19" morph="none" pos="word" start_char="1099">con</TOKEN>
<TOKEN end_char="1104" id="token-7-20" morph="none" pos="word" start_char="1103">la</TOKEN>
<TOKEN end_char="1109" id="token-7-21" morph="none" pos="word" start_char="1106">peor</TOKEN>
<TOKEN end_char="1116" id="token-7-22" morph="none" pos="word" start_char="1111">afonía</TOKEN>
<TOKEN end_char="1118" id="token-7-23" morph="none" pos="word" start_char="1118">y</TOKEN>
<TOKEN end_char="1129" id="token-7-24" morph="none" pos="word" start_char="1120">lanrigitis</TOKEN>
<TOKEN end_char="1132" id="token-7-25" morph="none" pos="word" start_char="1131">de</TOKEN>
<TOKEN end_char="1135" id="token-7-26" morph="none" pos="word" start_char="1134">mi</TOKEN>
<TOKEN end_char="1155" id="token-7-27" morph="none" pos="unknown" start_char="1137">vida...literalmente</TOKEN>
<TOKEN end_char="1159" id="token-7-28" morph="none" pos="word" start_char="1157">sin</TOKEN>
<TOKEN end_char="1163" id="token-7-29" morph="none" pos="word" start_char="1161">voz</TOKEN>
<TOKEN end_char="1164" id="token-7-30" morph="none" pos="punct" start_char="1164">.</TOKEN>
<TRANSLATED_TEXT>At fifteen days my two daughters and my wife were coughing, and I was serving the first week of December with the worst aphony and lanrigitis of my life... literally without a voice.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1222" id="segment-8" start_char="1166">
<ORIGINAL_TEXT>Tuvieron que pincharme para que me regresara algo de voz.</ORIGINAL_TEXT>
<TOKEN end_char="1173" id="token-8-0" morph="none" pos="word" start_char="1166">Tuvieron</TOKEN>
<TOKEN end_char="1177" id="token-8-1" morph="none" pos="word" start_char="1175">que</TOKEN>
<TOKEN end_char="1187" id="token-8-2" morph="none" pos="word" start_char="1179">pincharme</TOKEN>
<TOKEN end_char="1192" id="token-8-3" morph="none" pos="word" start_char="1189">para</TOKEN>
<TOKEN end_char="1196" id="token-8-4" morph="none" pos="word" start_char="1194">que</TOKEN>
<TOKEN end_char="1199" id="token-8-5" morph="none" pos="word" start_char="1198">me</TOKEN>
<TOKEN end_char="1209" id="token-8-6" morph="none" pos="word" start_char="1201">regresara</TOKEN>
<TOKEN end_char="1214" id="token-8-7" morph="none" pos="word" start_char="1211">algo</TOKEN>
<TOKEN end_char="1217" id="token-8-8" morph="none" pos="word" start_char="1216">de</TOKEN>
<TOKEN end_char="1221" id="token-8-9" morph="none" pos="word" start_char="1219">voz</TOKEN>
<TOKEN end_char="1222" id="token-8-10" morph="none" pos="punct" start_char="1222">.</TOKEN>
<TRANSLATED_TEXT>They had to pinch me to get some voice back.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1300" id="segment-9" start_char="1224">
<ORIGINAL_TEXT>Mi bebe de 5 meses entonces igual que yo, tos perruna y sin voz el pobrecito.</ORIGINAL_TEXT>
<TOKEN end_char="1225" id="token-9-0" morph="none" pos="word" start_char="1224">Mi</TOKEN>
<TOKEN end_char="1230" id="token-9-1" morph="none" pos="word" start_char="1227">bebe</TOKEN>
<TOKEN end_char="1233" id="token-9-2" morph="none" pos="word" start_char="1232">de</TOKEN>
<TOKEN end_char="1235" id="token-9-3" morph="none" pos="word" start_char="1235">5</TOKEN>
<TOKEN end_char="1241" id="token-9-4" morph="none" pos="word" start_char="1237">meses</TOKEN>
<TOKEN end_char="1250" id="token-9-5" morph="none" pos="word" start_char="1243">entonces</TOKEN>
<TOKEN end_char="1256" id="token-9-6" morph="none" pos="word" start_char="1252">igual</TOKEN>
<TOKEN end_char="1260" id="token-9-7" morph="none" pos="word" start_char="1258">que</TOKEN>
<TOKEN end_char="1263" id="token-9-8" morph="none" pos="word" start_char="1262">yo</TOKEN>
<TOKEN end_char="1264" id="token-9-9" morph="none" pos="punct" start_char="1264">,</TOKEN>
<TOKEN end_char="1268" id="token-9-10" morph="none" pos="word" start_char="1266">tos</TOKEN>
<TOKEN end_char="1276" id="token-9-11" morph="none" pos="word" start_char="1270">perruna</TOKEN>
<TOKEN end_char="1278" id="token-9-12" morph="none" pos="word" start_char="1278">y</TOKEN>
<TOKEN end_char="1282" id="token-9-13" morph="none" pos="word" start_char="1280">sin</TOKEN>
<TOKEN end_char="1286" id="token-9-14" morph="none" pos="word" start_char="1284">voz</TOKEN>
<TOKEN end_char="1289" id="token-9-15" morph="none" pos="word" start_char="1288">el</TOKEN>
<TOKEN end_char="1299" id="token-9-16" morph="none" pos="word" start_char="1291">pobrecito</TOKEN>
<TOKEN end_char="1300" id="token-9-17" morph="none" pos="punct" start_char="1300">.</TOKEN>
<TRANSLATED_TEXT>My five-month-old drink then just like me, you're a bitch and the poor guy has no voice.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1516" id="segment-10" start_char="1303">
<ORIGINAL_TEXT>Se nos fue...y al mes volvió, tanto el niño como yo pico febril de 38 y pico...dos días consecutivos...extrañisimo en mí que rara vez he tenido fiebre, y que con 37, 3 ya me estoy muriendo...pues dos días seguidos.</ORIGINAL_TEXT>
<TOKEN end_char="1304" id="token-10-0" morph="none" pos="word" start_char="1303">Se</TOKEN>
<TOKEN end_char="1308" id="token-10-1" morph="none" pos="word" start_char="1306">nos</TOKEN>
<TOKEN end_char="1316" id="token-10-2" morph="none" pos="unknown" start_char="1310">fue...y</TOKEN>
<TOKEN end_char="1319" id="token-10-3" morph="none" pos="word" start_char="1318">al</TOKEN>
<TOKEN end_char="1323" id="token-10-4" morph="none" pos="word" start_char="1321">mes</TOKEN>
<TOKEN end_char="1330" id="token-10-5" morph="none" pos="word" start_char="1325">volvió</TOKEN>
<TOKEN end_char="1331" id="token-10-6" morph="none" pos="punct" start_char="1331">,</TOKEN>
<TOKEN end_char="1337" id="token-10-7" morph="none" pos="word" start_char="1333">tanto</TOKEN>
<TOKEN end_char="1340" id="token-10-8" morph="none" pos="word" start_char="1339">el</TOKEN>
<TOKEN end_char="1345" id="token-10-9" morph="none" pos="word" start_char="1342">niño</TOKEN>
<TOKEN end_char="1350" id="token-10-10" morph="none" pos="word" start_char="1347">como</TOKEN>
<TOKEN end_char="1353" id="token-10-11" morph="none" pos="word" start_char="1352">yo</TOKEN>
<TOKEN end_char="1358" id="token-10-12" morph="none" pos="word" start_char="1355">pico</TOKEN>
<TOKEN end_char="1365" id="token-10-13" morph="none" pos="word" start_char="1360">febril</TOKEN>
<TOKEN end_char="1368" id="token-10-14" morph="none" pos="word" start_char="1367">de</TOKEN>
<TOKEN end_char="1371" id="token-10-15" morph="none" pos="word" start_char="1370">38</TOKEN>
<TOKEN end_char="1373" id="token-10-16" morph="none" pos="word" start_char="1373">y</TOKEN>
<TOKEN end_char="1384" id="token-10-17" morph="none" pos="unknown" start_char="1375">pico...dos</TOKEN>
<TOKEN end_char="1389" id="token-10-18" morph="none" pos="word" start_char="1386">días</TOKEN>
<TOKEN end_char="1416" id="token-10-19" morph="none" pos="unknown" start_char="1391">consecutivos...extrañisimo</TOKEN>
<TOKEN end_char="1419" id="token-10-20" morph="none" pos="word" start_char="1418">en</TOKEN>
<TOKEN end_char="1422" id="token-10-21" morph="none" pos="word" start_char="1421">mí</TOKEN>
<TOKEN end_char="1426" id="token-10-22" morph="none" pos="word" start_char="1424">que</TOKEN>
<TOKEN end_char="1431" id="token-10-23" morph="none" pos="word" start_char="1428">rara</TOKEN>
<TOKEN end_char="1435" id="token-10-24" morph="none" pos="word" start_char="1433">vez</TOKEN>
<TOKEN end_char="1438" id="token-10-25" morph="none" pos="word" start_char="1437">he</TOKEN>
<TOKEN end_char="1445" id="token-10-26" morph="none" pos="word" start_char="1440">tenido</TOKEN>
<TOKEN end_char="1452" id="token-10-27" morph="none" pos="word" start_char="1447">fiebre</TOKEN>
<TOKEN end_char="1453" id="token-10-28" morph="none" pos="punct" start_char="1453">,</TOKEN>
<TOKEN end_char="1455" id="token-10-29" morph="none" pos="word" start_char="1455">y</TOKEN>
<TOKEN end_char="1459" id="token-10-30" morph="none" pos="word" start_char="1457">que</TOKEN>
<TOKEN end_char="1463" id="token-10-31" morph="none" pos="word" start_char="1461">con</TOKEN>
<TOKEN end_char="1466" id="token-10-32" morph="none" pos="word" start_char="1465">37</TOKEN>
<TOKEN end_char="1467" id="token-10-33" morph="none" pos="punct" start_char="1467">,</TOKEN>
<TOKEN end_char="1469" id="token-10-34" morph="none" pos="word" start_char="1469">3</TOKEN>
<TOKEN end_char="1472" id="token-10-35" morph="none" pos="word" start_char="1471">ya</TOKEN>
<TOKEN end_char="1475" id="token-10-36" morph="none" pos="word" start_char="1474">me</TOKEN>
<TOKEN end_char="1481" id="token-10-37" morph="none" pos="word" start_char="1477">estoy</TOKEN>
<TOKEN end_char="1497" id="token-10-38" morph="none" pos="unknown" start_char="1483">muriendo...pues</TOKEN>
<TOKEN end_char="1501" id="token-10-39" morph="none" pos="word" start_char="1499">dos</TOKEN>
<TOKEN end_char="1506" id="token-10-40" morph="none" pos="word" start_char="1503">días</TOKEN>
<TOKEN end_char="1515" id="token-10-41" morph="none" pos="word" start_char="1508">seguidos</TOKEN>
<TOKEN end_char="1516" id="token-10-42" morph="none" pos="punct" start_char="1516">.</TOKEN>
<TRANSLATED_TEXT>He left us... and the month came back, both the boy and I had a fever peak of 38 and a peak... two consecutive days... we were surprised in me that I rarely had a fever, and that at 37, 3 I am already dying... for two consecutive days.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1571" id="segment-11" start_char="1519">
<ORIGINAL_TEXT>Una semana con antigripales...y se fue completamente.</ORIGINAL_TEXT>
<TOKEN end_char="1521" id="token-11-0" morph="none" pos="word" start_char="1519">Una</TOKEN>
<TOKEN end_char="1528" id="token-11-1" morph="none" pos="word" start_char="1523">semana</TOKEN>
<TOKEN end_char="1532" id="token-11-2" morph="none" pos="word" start_char="1530">con</TOKEN>
<TOKEN end_char="1549" id="token-11-3" morph="none" pos="unknown" start_char="1534">antigripales...y</TOKEN>
<TOKEN end_char="1552" id="token-11-4" morph="none" pos="word" start_char="1551">se</TOKEN>
<TOKEN end_char="1556" id="token-11-5" morph="none" pos="word" start_char="1554">fue</TOKEN>
<TOKEN end_char="1570" id="token-11-6" morph="none" pos="word" start_char="1558">completamente</TOKEN>
<TOKEN end_char="1571" id="token-11-7" morph="none" pos="punct" start_char="1571">.</TOKEN>
<TRANSLATED_TEXT>A week with flu... and he's gone completely.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1580" id="segment-12" start_char="1573">
<ORIGINAL_TEXT>Por fin.</ORIGINAL_TEXT>
<TOKEN end_char="1575" id="token-12-0" morph="none" pos="word" start_char="1573">Por</TOKEN>
<TOKEN end_char="1579" id="token-12-1" morph="none" pos="word" start_char="1577">fin</TOKEN>
<TOKEN end_char="1580" id="token-12-2" morph="none" pos="punct" start_char="1580">.</TOKEN>
</SEG>
<SEG end_char="1636" id="segment-13" start_char="1582">
<ORIGINAL_TEXT>La tos entre diciembre y final de enero jamás se fue...</ORIGINAL_TEXT>
<TOKEN end_char="1583" id="token-13-0" morph="none" pos="word" start_char="1582">La</TOKEN>
<TOKEN end_char="1587" id="token-13-1" morph="none" pos="word" start_char="1585">tos</TOKEN>
<TOKEN end_char="1593" id="token-13-2" morph="none" pos="word" start_char="1589">entre</TOKEN>
<TOKEN end_char="1603" id="token-13-3" morph="none" pos="word" start_char="1595">diciembre</TOKEN>
<TOKEN end_char="1605" id="token-13-4" morph="none" pos="word" start_char="1605">y</TOKEN>
<TOKEN end_char="1611" id="token-13-5" morph="none" pos="word" start_char="1607">final</TOKEN>
<TOKEN end_char="1614" id="token-13-6" morph="none" pos="word" start_char="1613">de</TOKEN>
<TOKEN end_char="1620" id="token-13-7" morph="none" pos="word" start_char="1616">enero</TOKEN>
<TOKEN end_char="1626" id="token-13-8" morph="none" pos="word" start_char="1622">jamás</TOKEN>
<TOKEN end_char="1629" id="token-13-9" morph="none" pos="word" start_char="1628">se</TOKEN>
<TOKEN end_char="1633" id="token-13-10" morph="none" pos="word" start_char="1631">fue</TOKEN>
<TOKEN end_char="1636" id="token-13-11" morph="none" pos="punct" start_char="1634">...</TOKEN>
<TRANSLATED_TEXT>The cough between December and the end of January never left...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1720" id="segment-14" start_char="1639">
<ORIGINAL_TEXT>A mis hijas y a mi mujer también les regresó, pero mas suave que al nene y a mí...</ORIGINAL_TEXT>
<TOKEN end_char="1639" id="token-14-0" morph="none" pos="word" start_char="1639">A</TOKEN>
<TOKEN end_char="1643" id="token-14-1" morph="none" pos="word" start_char="1641">mis</TOKEN>
<TOKEN end_char="1649" id="token-14-2" morph="none" pos="word" start_char="1645">hijas</TOKEN>
<TOKEN end_char="1651" id="token-14-3" morph="none" pos="word" start_char="1651">y</TOKEN>
<TOKEN end_char="1653" id="token-14-4" morph="none" pos="word" start_char="1653">a</TOKEN>
<TOKEN end_char="1656" id="token-14-5" morph="none" pos="word" start_char="1655">mi</TOKEN>
<TOKEN end_char="1662" id="token-14-6" morph="none" pos="word" start_char="1658">mujer</TOKEN>
<TOKEN end_char="1670" id="token-14-7" morph="none" pos="word" start_char="1664">también</TOKEN>
<TOKEN end_char="1674" id="token-14-8" morph="none" pos="word" start_char="1672">les</TOKEN>
<TOKEN end_char="1682" id="token-14-9" morph="none" pos="word" start_char="1676">regresó</TOKEN>
<TOKEN end_char="1683" id="token-14-10" morph="none" pos="punct" start_char="1683">,</TOKEN>
<TOKEN end_char="1688" id="token-14-11" morph="none" pos="word" start_char="1685">pero</TOKEN>
<TOKEN end_char="1692" id="token-14-12" morph="none" pos="word" start_char="1690">mas</TOKEN>
<TOKEN end_char="1698" id="token-14-13" morph="none" pos="word" start_char="1694">suave</TOKEN>
<TOKEN end_char="1702" id="token-14-14" morph="none" pos="word" start_char="1700">que</TOKEN>
<TOKEN end_char="1705" id="token-14-15" morph="none" pos="word" start_char="1704">al</TOKEN>
<TOKEN end_char="1710" id="token-14-16" morph="none" pos="word" start_char="1707">nene</TOKEN>
<TOKEN end_char="1712" id="token-14-17" morph="none" pos="word" start_char="1712">y</TOKEN>
<TOKEN end_char="1714" id="token-14-18" morph="none" pos="word" start_char="1714">a</TOKEN>
<TOKEN end_char="1717" id="token-14-19" morph="none" pos="word" start_char="1716">mí</TOKEN>
<TOKEN end_char="1720" id="token-14-20" morph="none" pos="punct" start_char="1718">...</TOKEN>
<TRANSLATED_TEXT>My daughters and my wife also returned, but softer than my nephew and me...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1811" id="segment-15" start_char="1723">
<ORIGINAL_TEXT>El bichito lleva muchisimas vueltas al mundo...y ya llebaba mucha en noviembre-diciembre.</ORIGINAL_TEXT>
<TOKEN end_char="1724" id="token-15-0" morph="none" pos="word" start_char="1723">El</TOKEN>
<TOKEN end_char="1732" id="token-15-1" morph="none" pos="word" start_char="1726">bichito</TOKEN>
<TOKEN end_char="1738" id="token-15-2" morph="none" pos="word" start_char="1734">lleva</TOKEN>
<TOKEN end_char="1749" id="token-15-3" morph="none" pos="word" start_char="1740">muchisimas</TOKEN>
<TOKEN end_char="1757" id="token-15-4" morph="none" pos="word" start_char="1751">vueltas</TOKEN>
<TOKEN end_char="1760" id="token-15-5" morph="none" pos="word" start_char="1759">al</TOKEN>
<TOKEN end_char="1770" id="token-15-6" morph="none" pos="unknown" start_char="1762">mundo...y</TOKEN>
<TOKEN end_char="1773" id="token-15-7" morph="none" pos="word" start_char="1772">ya</TOKEN>
<TOKEN end_char="1781" id="token-15-8" morph="none" pos="word" start_char="1775">llebaba</TOKEN>
<TOKEN end_char="1787" id="token-15-9" morph="none" pos="word" start_char="1783">mucha</TOKEN>
<TOKEN end_char="1790" id="token-15-10" morph="none" pos="word" start_char="1789">en</TOKEN>
<TOKEN end_char="1810" id="token-15-11" morph="none" pos="unknown" start_char="1792">noviembre-diciembre</TOKEN>
<TOKEN end_char="1811" id="token-15-12" morph="none" pos="punct" start_char="1811">.</TOKEN>
<TRANSLATED_TEXT>The bichito takes great turns around the world... and already has a lot in November-December.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1971" id="segment-16" start_char="1814">
<ORIGINAL_TEXT>Si oyen a Frank Cuesta en casa de Fedecojo....lo dijo bien claro: en Agosto la gente dejo de consumir pangolín porque sabían que el que lo consumía enfermaba.</ORIGINAL_TEXT>
<TOKEN end_char="1815" id="token-16-0" morph="none" pos="word" start_char="1814">Si</TOKEN>
<TOKEN end_char="1820" id="token-16-1" morph="none" pos="word" start_char="1817">oyen</TOKEN>
<TOKEN end_char="1822" id="token-16-2" morph="none" pos="word" start_char="1822">a</TOKEN>
<TOKEN end_char="1828" id="token-16-3" morph="none" pos="word" start_char="1824">Frank</TOKEN>
<TOKEN end_char="1835" id="token-16-4" morph="none" pos="word" start_char="1830">Cuesta</TOKEN>
<TOKEN end_char="1838" id="token-16-5" morph="none" pos="word" start_char="1837">en</TOKEN>
<TOKEN end_char="1843" id="token-16-6" morph="none" pos="word" start_char="1840">casa</TOKEN>
<TOKEN end_char="1846" id="token-16-7" morph="none" pos="word" start_char="1845">de</TOKEN>
<TOKEN end_char="1861" id="token-16-8" morph="none" pos="unknown" start_char="1848">Fedecojo....lo</TOKEN>
<TOKEN end_char="1866" id="token-16-9" morph="none" pos="word" start_char="1863">dijo</TOKEN>
<TOKEN end_char="1871" id="token-16-10" morph="none" pos="word" start_char="1868">bien</TOKEN>
<TOKEN end_char="1877" id="token-16-11" morph="none" pos="word" start_char="1873">claro</TOKEN>
<TOKEN end_char="1878" id="token-16-12" morph="none" pos="punct" start_char="1878">:</TOKEN>
<TOKEN end_char="1881" id="token-16-13" morph="none" pos="word" start_char="1880">en</TOKEN>
<TOKEN end_char="1888" id="token-16-14" morph="none" pos="word" start_char="1883">Agosto</TOKEN>
<TOKEN end_char="1891" id="token-16-15" morph="none" pos="word" start_char="1890">la</TOKEN>
<TOKEN end_char="1897" id="token-16-16" morph="none" pos="word" start_char="1893">gente</TOKEN>
<TOKEN end_char="1902" id="token-16-17" morph="none" pos="word" start_char="1899">dejo</TOKEN>
<TOKEN end_char="1905" id="token-16-18" morph="none" pos="word" start_char="1904">de</TOKEN>
<TOKEN end_char="1914" id="token-16-19" morph="none" pos="word" start_char="1907">consumir</TOKEN>
<TOKEN end_char="1923" id="token-16-20" morph="none" pos="word" start_char="1916">pangolín</TOKEN>
<TOKEN end_char="1930" id="token-16-21" morph="none" pos="word" start_char="1925">porque</TOKEN>
<TOKEN end_char="1937" id="token-16-22" morph="none" pos="word" start_char="1932">sabían</TOKEN>
<TOKEN end_char="1941" id="token-16-23" morph="none" pos="word" start_char="1939">que</TOKEN>
<TOKEN end_char="1944" id="token-16-24" morph="none" pos="word" start_char="1943">el</TOKEN>
<TOKEN end_char="1948" id="token-16-25" morph="none" pos="word" start_char="1946">que</TOKEN>
<TOKEN end_char="1951" id="token-16-26" morph="none" pos="word" start_char="1950">lo</TOKEN>
<TOKEN end_char="1960" id="token-16-27" morph="none" pos="word" start_char="1953">consumía</TOKEN>
<TOKEN end_char="1970" id="token-16-28" morph="none" pos="word" start_char="1962">enfermaba</TOKEN>
<TOKEN end_char="1971" id="token-16-29" morph="none" pos="punct" start_char="1971">.</TOKEN>
<TRANSLATED_TEXT>If you hear Frank Cuesta at Fedecojo's house... he said it very clearly: in August people stop consuming pangolin because they knew that the person who was consuming it was sick.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2018" id="segment-17" start_char="1973">
<ORIGINAL_TEXT>Y dejamos de ver pangonlines por los mercados.</ORIGINAL_TEXT>
<TOKEN end_char="1973" id="token-17-0" morph="none" pos="word" start_char="1973">Y</TOKEN>
<TOKEN end_char="1981" id="token-17-1" morph="none" pos="word" start_char="1975">dejamos</TOKEN>
<TOKEN end_char="1984" id="token-17-2" morph="none" pos="word" start_char="1983">de</TOKEN>
<TOKEN end_char="1988" id="token-17-3" morph="none" pos="word" start_char="1986">ver</TOKEN>
<TOKEN end_char="2000" id="token-17-4" morph="none" pos="word" start_char="1990">pangonlines</TOKEN>
<TOKEN end_char="2004" id="token-17-5" morph="none" pos="word" start_char="2002">por</TOKEN>
<TOKEN end_char="2008" id="token-17-6" morph="none" pos="word" start_char="2006">los</TOKEN>
<TOKEN end_char="2017" id="token-17-7" morph="none" pos="word" start_char="2010">mercados</TOKEN>
<TOKEN end_char="2018" id="token-17-8" morph="none" pos="punct" start_char="2018">.</TOKEN>
<TRANSLATED_TEXT>And we stop seeing pangolins for the markets.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2090" id="segment-18" start_char="2021">
<ORIGINAL_TEXT>Si hacen test de anticuerpos, la mitad de la población ya los tiene...</ORIGINAL_TEXT>
<TOKEN end_char="2022" id="token-18-0" morph="none" pos="word" start_char="2021">Si</TOKEN>
<TOKEN end_char="2028" id="token-18-1" morph="none" pos="word" start_char="2024">hacen</TOKEN>
<TOKEN end_char="2033" id="token-18-2" morph="none" pos="word" start_char="2030">test</TOKEN>
<TOKEN end_char="2036" id="token-18-3" morph="none" pos="word" start_char="2035">de</TOKEN>
<TOKEN end_char="2048" id="token-18-4" morph="none" pos="word" start_char="2038">anticuerpos</TOKEN>
<TOKEN end_char="2049" id="token-18-5" morph="none" pos="punct" start_char="2049">,</TOKEN>
<TOKEN end_char="2052" id="token-18-6" morph="none" pos="word" start_char="2051">la</TOKEN>
<TOKEN end_char="2058" id="token-18-7" morph="none" pos="word" start_char="2054">mitad</TOKEN>
<TOKEN end_char="2061" id="token-18-8" morph="none" pos="word" start_char="2060">de</TOKEN>
<TOKEN end_char="2064" id="token-18-9" morph="none" pos="word" start_char="2063">la</TOKEN>
<TOKEN end_char="2074" id="token-18-10" morph="none" pos="word" start_char="2066">población</TOKEN>
<TOKEN end_char="2077" id="token-18-11" morph="none" pos="word" start_char="2076">ya</TOKEN>
<TOKEN end_char="2081" id="token-18-12" morph="none" pos="word" start_char="2079">los</TOKEN>
<TOKEN end_char="2087" id="token-18-13" morph="none" pos="word" start_char="2083">tiene</TOKEN>
<TOKEN end_char="2090" id="token-18-14" morph="none" pos="punct" start_char="2088">...</TOKEN>
<TRANSLATED_TEXT>If they test antibodies, half the population already has them...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2121" id="segment-19" start_char="2094">
<ORIGINAL_TEXT>Y en Italia, y en Francia...</ORIGINAL_TEXT>
<TOKEN end_char="2094" id="token-19-0" morph="none" pos="word" start_char="2094">Y</TOKEN>
<TOKEN end_char="2097" id="token-19-1" morph="none" pos="word" start_char="2096">en</TOKEN>
<TOKEN end_char="2104" id="token-19-2" morph="none" pos="word" start_char="2099">Italia</TOKEN>
<TOKEN end_char="2105" id="token-19-3" morph="none" pos="punct" start_char="2105">,</TOKEN>
<TOKEN end_char="2107" id="token-19-4" morph="none" pos="word" start_char="2107">y</TOKEN>
<TOKEN end_char="2110" id="token-19-5" morph="none" pos="word" start_char="2109">en</TOKEN>
<TOKEN end_char="2118" id="token-19-6" morph="none" pos="word" start_char="2112">Francia</TOKEN>
<TOKEN end_char="2121" id="token-19-7" morph="none" pos="punct" start_char="2119">...</TOKEN>
<TRANSLATED_TEXT>And in Italy, and in France...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2131" id="segment-20" start_char="2124">
<ORIGINAL_TEXT>Francia.</ORIGINAL_TEXT>
<TOKEN end_char="2130" id="token-20-0" morph="none" pos="word" start_char="2124">Francia</TOKEN>
<TOKEN end_char="2131" id="token-20-1" morph="none" pos="punct" start_char="2131">.</TOKEN>
<TRANSLATED_TEXT>France.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2249" id="segment-21" start_char="2133">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes)</ORIGINAL_TEXT>
<TOKEN end_char="2140" id="token-21-0" morph="none" pos="word" start_char="2133">Detectan</TOKEN>
<TOKEN end_char="2143" id="token-21-1" morph="none" pos="unknown" start_char="2142">"+</TOKEN>
<TOKEN end_char="2145" id="token-21-2" morph="none" pos="word" start_char="2145">a</TOKEN>
<TOKEN end_char="2153" id="token-21-3" morph="none" pos="word" start_char="2147">covid19</TOKEN>
<TOKEN end_char="2154" id="token-21-4" morph="none" pos="punct" start_char="2154">"</TOKEN>
<TOKEN end_char="2157" id="token-21-5" morph="none" pos="word" start_char="2156">en</TOKEN>
<TOKEN end_char="2165" id="token-21-6" morph="none" pos="word" start_char="2159">muestra</TOKEN>
<TOKEN end_char="2168" id="token-21-7" morph="none" pos="word" start_char="2167">de</TOKEN>
<TOKEN end_char="2176" id="token-21-8" morph="none" pos="word" start_char="2170">enfermo</TOKEN>
<TOKEN end_char="2179" id="token-21-9" morph="none" pos="word" start_char="2178">de</TOKEN>
<TOKEN end_char="2188" id="token-21-10" morph="none" pos="word" start_char="2181">neumonía</TOKEN>
<TOKEN end_char="2192" id="token-21-11" morph="none" pos="word" start_char="2190">del</TOKEN>
<TOKEN end_char="2195" id="token-21-12" morph="none" pos="word" start_char="2194">27</TOKEN>
<TOKEN end_char="2198" id="token-21-13" morph="none" pos="word" start_char="2197">de</TOKEN>
<TOKEN end_char="2208" id="token-21-14" morph="none" pos="word" start_char="2200">diciembre</TOKEN>
<TOKEN end_char="2210" id="token-21-15" morph="none" pos="punct" start_char="2210">(</TOKEN>
<TOKEN end_char="2214" id="token-21-16" morph="none" pos="word" start_char="2211">ergo</TOKEN>
<TOKEN end_char="2216" id="token-21-17" morph="none" pos="punct" start_char="2216">"</TOKEN>
<TOKEN end_char="2225" id="token-21-18" morph="none" pos="word" start_char="2217">infectado</TOKEN>
<TOKEN end_char="2226" id="token-21-19" morph="none" pos="punct" start_char="2226">"</TOKEN>
<TOKEN end_char="2232" id="token-21-20" morph="none" pos="word" start_char="2228">circa</TOKEN>
<TOKEN end_char="2234" id="token-21-21" morph="none" pos="word" start_char="2234">3</TOKEN>
<TOKEN end_char="2242" id="token-21-22" morph="none" pos="word" start_char="2236">semanas</TOKEN>
<TOKEN end_char="2248" id="token-21-23" morph="none" pos="word" start_char="2244">antes</TOKEN>
<TOKEN end_char="2249" id="token-21-24" morph="none" pos="punct" start_char="2249">)</TOKEN>
<TRANSLATED_TEXT>They detected "+ a covid19" in a December 27 pneumonia sample (ergo "infected" about 3 weeks earlier).</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2348" id="segment-22" start_char="2252">
<ORIGINAL_TEXT>Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN end_char="2252" id="token-22-0" morph="none" pos="word" start_char="2252">Y</TOKEN>
<TOKEN end_char="2263" id="token-22-1" morph="none" pos="word" start_char="2254">pronostico</TOKEN>
<TOKEN end_char="2267" id="token-22-2" morph="none" pos="word" start_char="2265">que</TOKEN>
<TOKEN end_char="2270" id="token-22-3" morph="none" pos="word" start_char="2269">si</TOKEN>
<TOKEN end_char="2274" id="token-22-4" morph="none" pos="word" start_char="2272">nos</TOKEN>
<TOKEN end_char="2282" id="token-22-5" morph="none" pos="word" start_char="2276">ponemos</TOKEN>
<TOKEN end_char="2284" id="token-22-6" morph="none" pos="word" start_char="2284">a</TOKEN>
<TOKEN end_char="2293" id="token-22-7" morph="none" pos="word" start_char="2286">estudiar</TOKEN>
<TOKEN end_char="2302" id="token-22-8" morph="none" pos="word" start_char="2295">muestras</TOKEN>
<TOKEN end_char="2305" id="token-22-9" morph="none" pos="word" start_char="2304">de</TOKEN>
<TOKEN end_char="2310" id="token-22-10" morph="none" pos="word" start_char="2307">hace</TOKEN>
<TOKEN end_char="2313" id="token-22-11" morph="none" pos="word" start_char="2312">10</TOKEN>
<TOKEN end_char="2315" id="token-22-12" morph="none" pos="word" start_char="2315">o</TOKEN>
<TOKEN end_char="2318" id="token-22-13" morph="none" pos="word" start_char="2317">20</TOKEN>
<TOKEN end_char="2323" id="token-22-14" morph="none" pos="word" start_char="2320">años</TOKEN>
<TOKEN end_char="2331" id="token-22-15" morph="none" pos="word" start_char="2325">también</TOKEN>
<TOKEN end_char="2337" id="token-22-16" morph="none" pos="word" start_char="2333">habrá</TOKEN>
<TOKEN end_char="2339" id="token-22-17" morph="none" pos="punct" start_char="2339">"</TOKEN>
<TOKEN end_char="2346" id="token-22-18" morph="none" pos="word" start_char="2340">covid19</TOKEN>
<TOKEN end_char="2348" id="token-22-19" morph="none" pos="punct" start_char="2347">".</TOKEN>
<TRANSLATED_TEXT>And I predict that if we study samples from 10 or 20 years ago, there will also be "covid19."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2404" id="segment-23" start_char="2351">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes:</ORIGINAL_TEXT>
<TOKEN end_char="2352" id="token-23-0" morph="none" pos="word" start_char="2351">El</TOKEN>
<TOKEN end_char="2357" id="token-23-1" morph="none" pos="word" start_char="2354">test</TOKEN>
<TOKEN end_char="2360" id="token-23-2" morph="none" pos="word" start_char="2359">da</TOKEN>
<TOKEN end_char="2363" id="token-23-3" morph="none" pos="word" start_char="2362">un</TOKEN>
<TOKEN end_char="2365" id="token-23-4" morph="none" pos="word" start_char="2365">7</TOKEN>
<TOKEN end_char="2366" id="token-23-5" morph="none" pos="punct" start_char="2366">%</TOKEN>
<TOKEN end_char="2369" id="token-23-6" morph="none" pos="word" start_char="2368">de</TOKEN>
<TOKEN end_char="2374" id="token-23-7" morph="none" pos="unknown" start_char="2371">"+",</TOKEN>
<TOKEN end_char="2381" id="token-23-8" morph="none" pos="word" start_char="2376">testes</TOKEN>
<TOKEN end_char="2384" id="token-23-9" morph="none" pos="word" start_char="2383">la</TOKEN>
<TOKEN end_char="2392" id="token-23-10" morph="none" pos="word" start_char="2386">muestra</TOKEN>
<TOKEN end_char="2396" id="token-23-11" morph="none" pos="word" start_char="2394">que</TOKEN>
<TOKEN end_char="2403" id="token-23-12" morph="none" pos="word" start_char="2398">testes</TOKEN>
<TOKEN end_char="2404" id="token-23-13" morph="none" pos="punct" start_char="2404">:</TOKEN>
<TRANSLATED_TEXT>The test gives a 7% "+," tests the sample that tests:</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="2551" id="segment-24" start_char="2407">
<ORIGINAL_TEXT>Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests</ORIGINAL_TEXT>
<TOKEN end_char="2413" id="token-24-0" morph="none" pos="word" start_char="2407">Lectura</TOKEN>
<TOKEN end_char="2421" id="token-24-1" morph="none" pos="word" start_char="2415">crítica</TOKEN>
<TOKEN end_char="2424" id="token-24-2" morph="none" pos="word" start_char="2423">de</TOKEN>
<TOKEN end_char="2436" id="token-24-3" morph="none" pos="word" start_char="2426">pantallazos</TOKEN>
<TOKEN end_char="2439" id="token-24-4" morph="none" pos="word" start_char="2438">de</TOKEN>
<TOKEN end_char="2450" id="token-24-5" morph="none" pos="word" start_char="2441">prospectos</TOKEN>
<TOKEN end_char="2452" id="token-24-6" morph="none" pos="word" start_char="2452">y</TOKEN>
<TOKEN end_char="2463" id="token-24-7" morph="none" pos="word" start_char="2454">literatura</TOKEN>
<TOKEN end_char="2469" id="token-24-8" morph="none" pos="word" start_char="2465">sobre</TOKEN>
<TOKEN end_char="2471" id="token-24-9" morph="none" pos="punct" start_char="2471">"</TOKEN>
<TOKEN end_char="2476" id="token-24-10" morph="none" pos="word" start_char="2472">tests</TOKEN>
<TOKEN end_char="2479" id="token-24-11" morph="none" pos="word" start_char="2478">de</TOKEN>
<TOKEN end_char="2484" id="token-24-12" morph="none" pos="word" start_char="2481">SARS</TOKEN>
<TOKEN end_char="2488" id="token-24-13" morph="none" pos="word" start_char="2486">Cov</TOKEN>
<TOKEN end_char="2490" id="token-24-14" morph="none" pos="word" start_char="2490">2</TOKEN>
<TOKEN end_char="2491" id="token-24-15" morph="none" pos="punct" start_char="2491">"</TOKEN>
<TOKEN end_char="2494" id="token-24-16" morph="none" pos="punct" start_char="2493">("</TOKEN>
<TOKEN end_char="2513" id="token-24-17" morph="none" pos="unknown" start_char="2495">Covid19")|Infórmese</TOKEN>
<TOKEN end_char="2519" id="token-24-18" morph="none" pos="word" start_char="2515">antes</TOKEN>
<TOKEN end_char="2522" id="token-24-19" morph="none" pos="word" start_char="2521">de</TOKEN>
<TOKEN end_char="2532" id="token-24-20" morph="none" pos="word" start_char="2524">consentir</TOKEN>
<TOKEN end_char="2536" id="token-24-21" morph="none" pos="word" start_char="2534">uno</TOKEN>
<TOKEN end_char="2539" id="token-24-22" morph="none" pos="word" start_char="2538">de</TOKEN>
<TOKEN end_char="2545" id="token-24-23" morph="none" pos="word" start_char="2541">estos</TOKEN>
<TOKEN end_char="2551" id="token-24-24" morph="none" pos="word" start_char="2547">tests</TOKEN>
<TRANSLATED_TEXT>Critical reading of prospectus screens and literature on "SARS tests Cov 2" ("Covid19") | Learn before agreeing to one of these tests</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2578" id="segment-25" start_char="2554">
<ORIGINAL_TEXT>Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN end_char="2556" id="token-25-0" morph="none" pos="word" start_char="2554">Los</TOKEN>
<TOKEN end_char="2562" id="token-25-1" morph="none" pos="word" start_char="2558">tests</TOKEN>
<TOKEN end_char="2566" id="token-25-2" morph="none" pos="word" start_char="2564">son</TOKEN>
<TOKEN end_char="2570" id="token-25-3" morph="none" pos="word" start_char="2568">los</TOKEN>
<TOKEN end_char="2577" id="token-25-4" morph="none" pos="word" start_char="2572">padres</TOKEN>
<TOKEN end_char="2578" id="token-25-5" morph="none" pos="punct" start_char="2578">.</TOKEN>
<TRANSLATED_TEXT>The tests are parents.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2724" id="segment-26" start_char="2582">
<ORIGINAL_TEXT>En estos momentos se están publicando muchos "estudios" sobre el covid-19 sin contrastar y lo más probable es que la mayor parte sean erróneos.</ORIGINAL_TEXT>
<TOKEN end_char="2583" id="token-26-0" morph="none" pos="word" start_char="2582">En</TOKEN>
<TOKEN end_char="2589" id="token-26-1" morph="none" pos="word" start_char="2585">estos</TOKEN>
<TOKEN end_char="2598" id="token-26-2" morph="none" pos="word" start_char="2591">momentos</TOKEN>
<TOKEN end_char="2601" id="token-26-3" morph="none" pos="word" start_char="2600">se</TOKEN>
<TOKEN end_char="2607" id="token-26-4" morph="none" pos="word" start_char="2603">están</TOKEN>
<TOKEN end_char="2618" id="token-26-5" morph="none" pos="word" start_char="2609">publicando</TOKEN>
<TOKEN end_char="2625" id="token-26-6" morph="none" pos="word" start_char="2620">muchos</TOKEN>
<TOKEN end_char="2627" id="token-26-7" morph="none" pos="punct" start_char="2627">"</TOKEN>
<TOKEN end_char="2635" id="token-26-8" morph="none" pos="word" start_char="2628">estudios</TOKEN>
<TOKEN end_char="2636" id="token-26-9" morph="none" pos="punct" start_char="2636">"</TOKEN>
<TOKEN end_char="2642" id="token-26-10" morph="none" pos="word" start_char="2638">sobre</TOKEN>
<TOKEN end_char="2645" id="token-26-11" morph="none" pos="word" start_char="2644">el</TOKEN>
<TOKEN end_char="2654" id="token-26-12" morph="none" pos="unknown" start_char="2647">covid-19</TOKEN>
<TOKEN end_char="2658" id="token-26-13" morph="none" pos="word" start_char="2656">sin</TOKEN>
<TOKEN end_char="2669" id="token-26-14" morph="none" pos="word" start_char="2660">contrastar</TOKEN>
<TOKEN end_char="2671" id="token-26-15" morph="none" pos="word" start_char="2671">y</TOKEN>
<TOKEN end_char="2674" id="token-26-16" morph="none" pos="word" start_char="2673">lo</TOKEN>
<TOKEN end_char="2678" id="token-26-17" morph="none" pos="word" start_char="2676">más</TOKEN>
<TOKEN end_char="2687" id="token-26-18" morph="none" pos="word" start_char="2680">probable</TOKEN>
<TOKEN end_char="2690" id="token-26-19" morph="none" pos="word" start_char="2689">es</TOKEN>
<TOKEN end_char="2694" id="token-26-20" morph="none" pos="word" start_char="2692">que</TOKEN>
<TOKEN end_char="2697" id="token-26-21" morph="none" pos="word" start_char="2696">la</TOKEN>
<TOKEN end_char="2703" id="token-26-22" morph="none" pos="word" start_char="2699">mayor</TOKEN>
<TOKEN end_char="2709" id="token-26-23" morph="none" pos="word" start_char="2705">parte</TOKEN>
<TOKEN end_char="2714" id="token-26-24" morph="none" pos="word" start_char="2711">sean</TOKEN>
<TOKEN end_char="2723" id="token-26-25" morph="none" pos="word" start_char="2716">erróneos</TOKEN>
<TOKEN end_char="2724" id="token-26-26" morph="none" pos="punct" start_char="2724">.</TOKEN>
<TRANSLATED_TEXT>Many "studies" on covid-19 are currently being published without contrast, and most likely are erroneous.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2742" id="segment-27" start_char="2726">
<ORIGINAL_TEXT>Cuidado con ello.</ORIGINAL_TEXT>
<TOKEN end_char="2732" id="token-27-0" morph="none" pos="word" start_char="2726">Cuidado</TOKEN>
<TOKEN end_char="2736" id="token-27-1" morph="none" pos="word" start_char="2734">con</TOKEN>
<TOKEN end_char="2741" id="token-27-2" morph="none" pos="word" start_char="2738">ello</TOKEN>
<TOKEN end_char="2742" id="token-27-3" morph="none" pos="punct" start_char="2742">.</TOKEN>
<TRANSLATED_TEXT>Watch it.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2874" id="segment-28" start_char="2744">
<ORIGINAL_TEXT>Entre investigadores buscando financiación y periodistas a la caza de noticias sensacionalistas tenemos el caos informativo actual.</ORIGINAL_TEXT>
<TOKEN end_char="2748" id="token-28-0" morph="none" pos="word" start_char="2744">Entre</TOKEN>
<TOKEN end_char="2763" id="token-28-1" morph="none" pos="word" start_char="2750">investigadores</TOKEN>
<TOKEN end_char="2772" id="token-28-2" morph="none" pos="word" start_char="2765">buscando</TOKEN>
<TOKEN end_char="2785" id="token-28-3" morph="none" pos="word" start_char="2774">financiación</TOKEN>
<TOKEN end_char="2787" id="token-28-4" morph="none" pos="word" start_char="2787">y</TOKEN>
<TOKEN end_char="2799" id="token-28-5" morph="none" pos="word" start_char="2789">periodistas</TOKEN>
<TOKEN end_char="2801" id="token-28-6" morph="none" pos="word" start_char="2801">a</TOKEN>
<TOKEN end_char="2804" id="token-28-7" morph="none" pos="word" start_char="2803">la</TOKEN>
<TOKEN end_char="2809" id="token-28-8" morph="none" pos="word" start_char="2806">caza</TOKEN>
<TOKEN end_char="2812" id="token-28-9" morph="none" pos="word" start_char="2811">de</TOKEN>
<TOKEN end_char="2821" id="token-28-10" morph="none" pos="word" start_char="2814">noticias</TOKEN>
<TOKEN end_char="2838" id="token-28-11" morph="none" pos="word" start_char="2823">sensacionalistas</TOKEN>
<TOKEN end_char="2846" id="token-28-12" morph="none" pos="word" start_char="2840">tenemos</TOKEN>
<TOKEN end_char="2849" id="token-28-13" morph="none" pos="word" start_char="2848">el</TOKEN>
<TOKEN end_char="2854" id="token-28-14" morph="none" pos="word" start_char="2851">caos</TOKEN>
<TOKEN end_char="2866" id="token-28-15" morph="none" pos="word" start_char="2856">informativo</TOKEN>
<TOKEN end_char="2873" id="token-28-16" morph="none" pos="word" start_char="2868">actual</TOKEN>
<TOKEN end_char="2874" id="token-28-17" morph="none" pos="punct" start_char="2874">.</TOKEN>
<TRANSLATED_TEXT>Between researchers seeking funding and journalists chasing sensational news, we have the current information chaos.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2943" id="segment-29" start_char="2878">
<ORIGINAL_TEXT>Yo creo que lo ha pasado ya medio país o más , digan lo que digan.</ORIGINAL_TEXT>
<TOKEN end_char="2879" id="token-29-0" morph="none" pos="word" start_char="2878">Yo</TOKEN>
<TOKEN end_char="2884" id="token-29-1" morph="none" pos="word" start_char="2881">creo</TOKEN>
<TOKEN end_char="2888" id="token-29-2" morph="none" pos="word" start_char="2886">que</TOKEN>
<TOKEN end_char="2891" id="token-29-3" morph="none" pos="word" start_char="2890">lo</TOKEN>
<TOKEN end_char="2894" id="token-29-4" morph="none" pos="word" start_char="2893">ha</TOKEN>
<TOKEN end_char="2901" id="token-29-5" morph="none" pos="word" start_char="2896">pasado</TOKEN>
<TOKEN end_char="2904" id="token-29-6" morph="none" pos="word" start_char="2903">ya</TOKEN>
<TOKEN end_char="2910" id="token-29-7" morph="none" pos="word" start_char="2906">medio</TOKEN>
<TOKEN end_char="2915" id="token-29-8" morph="none" pos="word" start_char="2912">país</TOKEN>
<TOKEN end_char="2917" id="token-29-9" morph="none" pos="word" start_char="2917">o</TOKEN>
<TOKEN end_char="2921" id="token-29-10" morph="none" pos="word" start_char="2919">más</TOKEN>
<TOKEN end_char="2923" id="token-29-11" morph="none" pos="punct" start_char="2923">,</TOKEN>
<TOKEN end_char="2929" id="token-29-12" morph="none" pos="word" start_char="2925">digan</TOKEN>
<TOKEN end_char="2932" id="token-29-13" morph="none" pos="word" start_char="2931">lo</TOKEN>
<TOKEN end_char="2936" id="token-29-14" morph="none" pos="word" start_char="2934">que</TOKEN>
<TOKEN end_char="2942" id="token-29-15" morph="none" pos="word" start_char="2938">digan</TOKEN>
<TOKEN end_char="2943" id="token-29-16" morph="none" pos="punct" start_char="2943">.</TOKEN>
<TRANSLATED_TEXT>I think it 's been half a country or more, say what you say.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="2980" id="segment-30" start_char="2947">
<ORIGINAL_TEXT>Un estudio para no culpar a China.</ORIGINAL_TEXT>
<TOKEN end_char="2948" id="token-30-0" morph="none" pos="word" start_char="2947">Un</TOKEN>
<TOKEN end_char="2956" id="token-30-1" morph="none" pos="word" start_char="2950">estudio</TOKEN>
<TOKEN end_char="2961" id="token-30-2" morph="none" pos="word" start_char="2958">para</TOKEN>
<TOKEN end_char="2964" id="token-30-3" morph="none" pos="word" start_char="2963">no</TOKEN>
<TOKEN end_char="2971" id="token-30-4" morph="none" pos="word" start_char="2966">culpar</TOKEN>
<TOKEN end_char="2973" id="token-30-5" morph="none" pos="word" start_char="2973">a</TOKEN>
<TOKEN end_char="2979" id="token-30-6" morph="none" pos="word" start_char="2975">China</TOKEN>
<TOKEN end_char="2980" id="token-30-7" morph="none" pos="punct" start_char="2980">.</TOKEN>
<TRANSLATED_TEXT>A study not to blame China.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3041" id="segment-31" start_char="2985">
<ORIGINAL_TEXT>AYN RANDiano2 dijo: Y en Italia, y en Francia... Francia.</ORIGINAL_TEXT>
<TOKEN end_char="2987" id="token-31-0" morph="none" pos="word" start_char="2985">AYN</TOKEN>
<TOKEN end_char="2997" id="token-31-1" morph="none" pos="word" start_char="2989">RANDiano2</TOKEN>
<TOKEN end_char="3002" id="token-31-2" morph="none" pos="word" start_char="2999">dijo</TOKEN>
<TOKEN end_char="3003" id="token-31-3" morph="none" pos="punct" start_char="3003">:</TOKEN>
<TOKEN end_char="3005" id="token-31-4" morph="none" pos="word" start_char="3005">Y</TOKEN>
<TOKEN end_char="3008" id="token-31-5" morph="none" pos="word" start_char="3007">en</TOKEN>
<TOKEN end_char="3015" id="token-31-6" morph="none" pos="word" start_char="3010">Italia</TOKEN>
<TOKEN end_char="3016" id="token-31-7" morph="none" pos="punct" start_char="3016">,</TOKEN>
<TOKEN end_char="3018" id="token-31-8" morph="none" pos="word" start_char="3018">y</TOKEN>
<TOKEN end_char="3021" id="token-31-9" morph="none" pos="word" start_char="3020">en</TOKEN>
<TOKEN end_char="3029" id="token-31-10" morph="none" pos="word" start_char="3023">Francia</TOKEN>
<TOKEN end_char="3032" id="token-31-11" morph="none" pos="punct" start_char="3030">...</TOKEN>
<TOKEN end_char="3040" id="token-31-12" morph="none" pos="word" start_char="3034">Francia</TOKEN>
<TOKEN end_char="3041" id="token-31-13" morph="none" pos="punct" start_char="3041">.</TOKEN>
<TRANSLATED_TEXT>AYN RANDiano2 said: And in Italy, and in France... France.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3257" id="segment-32" start_char="3043">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes) Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN end_char="3050" id="token-32-0" morph="none" pos="word" start_char="3043">Detectan</TOKEN>
<TOKEN end_char="3053" id="token-32-1" morph="none" pos="unknown" start_char="3052">"+</TOKEN>
<TOKEN end_char="3055" id="token-32-2" morph="none" pos="word" start_char="3055">a</TOKEN>
<TOKEN end_char="3063" id="token-32-3" morph="none" pos="word" start_char="3057">covid19</TOKEN>
<TOKEN end_char="3064" id="token-32-4" morph="none" pos="punct" start_char="3064">"</TOKEN>
<TOKEN end_char="3067" id="token-32-5" morph="none" pos="word" start_char="3066">en</TOKEN>
<TOKEN end_char="3075" id="token-32-6" morph="none" pos="word" start_char="3069">muestra</TOKEN>
<TOKEN end_char="3078" id="token-32-7" morph="none" pos="word" start_char="3077">de</TOKEN>
<TOKEN end_char="3086" id="token-32-8" morph="none" pos="word" start_char="3080">enfermo</TOKEN>
<TOKEN end_char="3089" id="token-32-9" morph="none" pos="word" start_char="3088">de</TOKEN>
<TOKEN end_char="3098" id="token-32-10" morph="none" pos="word" start_char="3091">neumonía</TOKEN>
<TOKEN end_char="3102" id="token-32-11" morph="none" pos="word" start_char="3100">del</TOKEN>
<TOKEN end_char="3105" id="token-32-12" morph="none" pos="word" start_char="3104">27</TOKEN>
<TOKEN end_char="3108" id="token-32-13" morph="none" pos="word" start_char="3107">de</TOKEN>
<TOKEN end_char="3118" id="token-32-14" morph="none" pos="word" start_char="3110">diciembre</TOKEN>
<TOKEN end_char="3120" id="token-32-15" morph="none" pos="punct" start_char="3120">(</TOKEN>
<TOKEN end_char="3124" id="token-32-16" morph="none" pos="word" start_char="3121">ergo</TOKEN>
<TOKEN end_char="3126" id="token-32-17" morph="none" pos="punct" start_char="3126">"</TOKEN>
<TOKEN end_char="3135" id="token-32-18" morph="none" pos="word" start_char="3127">infectado</TOKEN>
<TOKEN end_char="3136" id="token-32-19" morph="none" pos="punct" start_char="3136">"</TOKEN>
<TOKEN end_char="3142" id="token-32-20" morph="none" pos="word" start_char="3138">circa</TOKEN>
<TOKEN end_char="3144" id="token-32-21" morph="none" pos="word" start_char="3144">3</TOKEN>
<TOKEN end_char="3152" id="token-32-22" morph="none" pos="word" start_char="3146">semanas</TOKEN>
<TOKEN end_char="3158" id="token-32-23" morph="none" pos="word" start_char="3154">antes</TOKEN>
<TOKEN end_char="3159" id="token-32-24" morph="none" pos="punct" start_char="3159">)</TOKEN>
<TOKEN end_char="3161" id="token-32-25" morph="none" pos="word" start_char="3161">Y</TOKEN>
<TOKEN end_char="3172" id="token-32-26" morph="none" pos="word" start_char="3163">pronostico</TOKEN>
<TOKEN end_char="3176" id="token-32-27" morph="none" pos="word" start_char="3174">que</TOKEN>
<TOKEN end_char="3179" id="token-32-28" morph="none" pos="word" start_char="3178">si</TOKEN>
<TOKEN end_char="3183" id="token-32-29" morph="none" pos="word" start_char="3181">nos</TOKEN>
<TOKEN end_char="3191" id="token-32-30" morph="none" pos="word" start_char="3185">ponemos</TOKEN>
<TOKEN end_char="3193" id="token-32-31" morph="none" pos="word" start_char="3193">a</TOKEN>
<TOKEN end_char="3202" id="token-32-32" morph="none" pos="word" start_char="3195">estudiar</TOKEN>
<TOKEN end_char="3211" id="token-32-33" morph="none" pos="word" start_char="3204">muestras</TOKEN>
<TOKEN end_char="3214" id="token-32-34" morph="none" pos="word" start_char="3213">de</TOKEN>
<TOKEN end_char="3219" id="token-32-35" morph="none" pos="word" start_char="3216">hace</TOKEN>
<TOKEN end_char="3222" id="token-32-36" morph="none" pos="word" start_char="3221">10</TOKEN>
<TOKEN end_char="3224" id="token-32-37" morph="none" pos="word" start_char="3224">o</TOKEN>
<TOKEN end_char="3227" id="token-32-38" morph="none" pos="word" start_char="3226">20</TOKEN>
<TOKEN end_char="3232" id="token-32-39" morph="none" pos="word" start_char="3229">años</TOKEN>
<TOKEN end_char="3240" id="token-32-40" morph="none" pos="word" start_char="3234">también</TOKEN>
<TOKEN end_char="3246" id="token-32-41" morph="none" pos="word" start_char="3242">habrá</TOKEN>
<TOKEN end_char="3248" id="token-32-42" morph="none" pos="punct" start_char="3248">"</TOKEN>
<TOKEN end_char="3255" id="token-32-43" morph="none" pos="word" start_char="3249">covid19</TOKEN>
<TOKEN end_char="3257" id="token-32-44" morph="none" pos="punct" start_char="3256">".</TOKEN>
<TRANSLATED_TEXT>They detect "+ a covid19" in a pneumonia sample from December 27 (ergo "infected" about 3 weeks earlier) and I predict that if we go to study samples from 10 or 20 years ago there will also be "covid19."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3484" id="segment-33" start_char="3259">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes: Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN end_char="3260" id="token-33-0" morph="none" pos="word" start_char="3259">El</TOKEN>
<TOKEN end_char="3265" id="token-33-1" morph="none" pos="word" start_char="3262">test</TOKEN>
<TOKEN end_char="3268" id="token-33-2" morph="none" pos="word" start_char="3267">da</TOKEN>
<TOKEN end_char="3271" id="token-33-3" morph="none" pos="word" start_char="3270">un</TOKEN>
<TOKEN end_char="3273" id="token-33-4" morph="none" pos="word" start_char="3273">7</TOKEN>
<TOKEN end_char="3274" id="token-33-5" morph="none" pos="punct" start_char="3274">%</TOKEN>
<TOKEN end_char="3277" id="token-33-6" morph="none" pos="word" start_char="3276">de</TOKEN>
<TOKEN end_char="3282" id="token-33-7" morph="none" pos="unknown" start_char="3279">"+",</TOKEN>
<TOKEN end_char="3289" id="token-33-8" morph="none" pos="word" start_char="3284">testes</TOKEN>
<TOKEN end_char="3292" id="token-33-9" morph="none" pos="word" start_char="3291">la</TOKEN>
<TOKEN end_char="3300" id="token-33-10" morph="none" pos="word" start_char="3294">muestra</TOKEN>
<TOKEN end_char="3304" id="token-33-11" morph="none" pos="word" start_char="3302">que</TOKEN>
<TOKEN end_char="3311" id="token-33-12" morph="none" pos="word" start_char="3306">testes</TOKEN>
<TOKEN end_char="3312" id="token-33-13" morph="none" pos="punct" start_char="3312">:</TOKEN>
<TOKEN end_char="3320" id="token-33-14" morph="none" pos="word" start_char="3314">Lectura</TOKEN>
<TOKEN end_char="3328" id="token-33-15" morph="none" pos="word" start_char="3322">crítica</TOKEN>
<TOKEN end_char="3331" id="token-33-16" morph="none" pos="word" start_char="3330">de</TOKEN>
<TOKEN end_char="3343" id="token-33-17" morph="none" pos="word" start_char="3333">pantallazos</TOKEN>
<TOKEN end_char="3346" id="token-33-18" morph="none" pos="word" start_char="3345">de</TOKEN>
<TOKEN end_char="3357" id="token-33-19" morph="none" pos="word" start_char="3348">prospectos</TOKEN>
<TOKEN end_char="3359" id="token-33-20" morph="none" pos="word" start_char="3359">y</TOKEN>
<TOKEN end_char="3370" id="token-33-21" morph="none" pos="word" start_char="3361">literatura</TOKEN>
<TOKEN end_char="3376" id="token-33-22" morph="none" pos="word" start_char="3372">sobre</TOKEN>
<TOKEN end_char="3378" id="token-33-23" morph="none" pos="punct" start_char="3378">"</TOKEN>
<TOKEN end_char="3383" id="token-33-24" morph="none" pos="word" start_char="3379">tests</TOKEN>
<TOKEN end_char="3386" id="token-33-25" morph="none" pos="word" start_char="3385">de</TOKEN>
<TOKEN end_char="3391" id="token-33-26" morph="none" pos="word" start_char="3388">SARS</TOKEN>
<TOKEN end_char="3395" id="token-33-27" morph="none" pos="word" start_char="3393">Cov</TOKEN>
<TOKEN end_char="3397" id="token-33-28" morph="none" pos="word" start_char="3397">2</TOKEN>
<TOKEN end_char="3398" id="token-33-29" morph="none" pos="punct" start_char="3398">"</TOKEN>
<TOKEN end_char="3401" id="token-33-30" morph="none" pos="punct" start_char="3400">("</TOKEN>
<TOKEN end_char="3420" id="token-33-31" morph="none" pos="unknown" start_char="3402">Covid19")|Infórmese</TOKEN>
<TOKEN end_char="3426" id="token-33-32" morph="none" pos="word" start_char="3422">antes</TOKEN>
<TOKEN end_char="3429" id="token-33-33" morph="none" pos="word" start_char="3428">de</TOKEN>
<TOKEN end_char="3439" id="token-33-34" morph="none" pos="word" start_char="3431">consentir</TOKEN>
<TOKEN end_char="3443" id="token-33-35" morph="none" pos="word" start_char="3441">uno</TOKEN>
<TOKEN end_char="3446" id="token-33-36" morph="none" pos="word" start_char="3445">de</TOKEN>
<TOKEN end_char="3452" id="token-33-37" morph="none" pos="word" start_char="3448">estos</TOKEN>
<TOKEN end_char="3458" id="token-33-38" morph="none" pos="word" start_char="3454">tests</TOKEN>
<TOKEN end_char="3462" id="token-33-39" morph="none" pos="word" start_char="3460">Los</TOKEN>
<TOKEN end_char="3468" id="token-33-40" morph="none" pos="word" start_char="3464">tests</TOKEN>
<TOKEN end_char="3472" id="token-33-41" morph="none" pos="word" start_char="3470">son</TOKEN>
<TOKEN end_char="3476" id="token-33-42" morph="none" pos="word" start_char="3474">los</TOKEN>
<TOKEN end_char="3483" id="token-33-43" morph="none" pos="word" start_char="3478">padres</TOKEN>
<TOKEN end_char="3484" id="token-33-44" morph="none" pos="punct" start_char="3484">.</TOKEN>
<TRANSLATED_TEXT>The test gives a 7% "+," tests the sample that tests: Critical reading of prospect screens and literature on "SARS Cov 2 tests" ("Covid19") | Learn before consenting to one of these tests The tests are parents.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3512" id="segment-34" start_char="3486">
<ORIGINAL_TEXT>Hacer clic para expandir...</ORIGINAL_TEXT>
<TOKEN end_char="3490" id="token-34-0" morph="none" pos="word" start_char="3486">Hacer</TOKEN>
<TOKEN end_char="3495" id="token-34-1" morph="none" pos="word" start_char="3492">clic</TOKEN>
<TOKEN end_char="3500" id="token-34-2" morph="none" pos="word" start_char="3497">para</TOKEN>
<TOKEN end_char="3509" id="token-34-3" morph="none" pos="word" start_char="3502">expandir</TOKEN>
<TOKEN end_char="3512" id="token-34-4" morph="none" pos="punct" start_char="3510">...</TOKEN>
<TRANSLATED_TEXT>Click to expand...</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="3762" id="segment-35" start_char="3515">
<ORIGINAL_TEXT>Pues no te diría que no...visto lo visto, la familia de los coronavirus parece ser que ya era vieja conocida de nuestros virólogos occidentales...y que en torno a un 10-15% de las gripes de temporada son provocadas por un bichito de esta familia...</ORIGINAL_TEXT>
<TOKEN end_char="3518" id="token-35-0" morph="none" pos="word" start_char="3515">Pues</TOKEN>
<TOKEN end_char="3521" id="token-35-1" morph="none" pos="word" start_char="3520">no</TOKEN>
<TOKEN end_char="3524" id="token-35-2" morph="none" pos="word" start_char="3523">te</TOKEN>
<TOKEN end_char="3530" id="token-35-3" morph="none" pos="word" start_char="3526">diría</TOKEN>
<TOKEN end_char="3534" id="token-35-4" morph="none" pos="word" start_char="3532">que</TOKEN>
<TOKEN end_char="3545" id="token-35-5" morph="none" pos="unknown" start_char="3536">no...visto</TOKEN>
<TOKEN end_char="3548" id="token-35-6" morph="none" pos="word" start_char="3547">lo</TOKEN>
<TOKEN end_char="3554" id="token-35-7" morph="none" pos="word" start_char="3550">visto</TOKEN>
<TOKEN end_char="3555" id="token-35-8" morph="none" pos="punct" start_char="3555">,</TOKEN>
<TOKEN end_char="3558" id="token-35-9" morph="none" pos="word" start_char="3557">la</TOKEN>
<TOKEN end_char="3566" id="token-35-10" morph="none" pos="word" start_char="3560">familia</TOKEN>
<TOKEN end_char="3569" id="token-35-11" morph="none" pos="word" start_char="3568">de</TOKEN>
<TOKEN end_char="3573" id="token-35-12" morph="none" pos="word" start_char="3571">los</TOKEN>
<TOKEN end_char="3585" id="token-35-13" morph="none" pos="word" start_char="3575">coronavirus</TOKEN>
<TOKEN end_char="3592" id="token-35-14" morph="none" pos="word" start_char="3587">parece</TOKEN>
<TOKEN end_char="3596" id="token-35-15" morph="none" pos="word" start_char="3594">ser</TOKEN>
<TOKEN end_char="3600" id="token-35-16" morph="none" pos="word" start_char="3598">que</TOKEN>
<TOKEN end_char="3603" id="token-35-17" morph="none" pos="word" start_char="3602">ya</TOKEN>
<TOKEN end_char="3607" id="token-35-18" morph="none" pos="word" start_char="3605">era</TOKEN>
<TOKEN end_char="3613" id="token-35-19" morph="none" pos="word" start_char="3609">vieja</TOKEN>
<TOKEN end_char="3622" id="token-35-20" morph="none" pos="word" start_char="3615">conocida</TOKEN>
<TOKEN end_char="3625" id="token-35-21" morph="none" pos="word" start_char="3624">de</TOKEN>
<TOKEN end_char="3634" id="token-35-22" morph="none" pos="word" start_char="3627">nuestros</TOKEN>
<TOKEN end_char="3644" id="token-35-23" morph="none" pos="word" start_char="3636">virólogos</TOKEN>
<TOKEN end_char="3661" id="token-35-24" morph="none" pos="unknown" start_char="3646">occidentales...y</TOKEN>
<TOKEN end_char="3665" id="token-35-25" morph="none" pos="word" start_char="3663">que</TOKEN>
<TOKEN end_char="3668" id="token-35-26" morph="none" pos="word" start_char="3667">en</TOKEN>
<TOKEN end_char="3674" id="token-35-27" morph="none" pos="word" start_char="3670">torno</TOKEN>
<TOKEN end_char="3676" id="token-35-28" morph="none" pos="word" start_char="3676">a</TOKEN>
<TOKEN end_char="3679" id="token-35-29" morph="none" pos="word" start_char="3678">un</TOKEN>
<TOKEN end_char="3685" id="token-35-30" morph="none" pos="unknown" start_char="3681">10-15</TOKEN>
<TOKEN end_char="3686" id="token-35-31" morph="none" pos="punct" start_char="3686">%</TOKEN>
<TOKEN end_char="3689" id="token-35-32" morph="none" pos="word" start_char="3688">de</TOKEN>
<TOKEN end_char="3693" id="token-35-33" morph="none" pos="word" start_char="3691">las</TOKEN>
<TOKEN end_char="3700" id="token-35-34" morph="none" pos="word" start_char="3695">gripes</TOKEN>
<TOKEN end_char="3703" id="token-35-35" morph="none" pos="word" start_char="3702">de</TOKEN>
<TOKEN end_char="3713" id="token-35-36" morph="none" pos="word" start_char="3705">temporada</TOKEN>
<TOKEN end_char="3717" id="token-35-37" morph="none" pos="word" start_char="3715">son</TOKEN>
<TOKEN end_char="3728" id="token-35-38" morph="none" pos="word" start_char="3719">provocadas</TOKEN>
<TOKEN end_char="3732" id="token-35-39" morph="none" pos="word" start_char="3730">por</TOKEN>
<TOKEN end_char="3735" id="token-35-40" morph="none" pos="word" start_char="3734">un</TOKEN>
<TOKEN end_char="3743" id="token-35-41" morph="none" pos="word" start_char="3737">bichito</TOKEN>
<TOKEN end_char="3746" id="token-35-42" morph="none" pos="word" start_char="3745">de</TOKEN>
<TOKEN end_char="3751" id="token-35-43" morph="none" pos="word" start_char="3748">esta</TOKEN>
<TOKEN end_char="3759" id="token-35-44" morph="none" pos="word" start_char="3753">familia</TOKEN>
<TOKEN end_char="3762" id="token-35-45" morph="none" pos="punct" start_char="3760">...</TOKEN>
<TRANSLATED_TEXT>Well, I wouldn't tell you that not... as seen, the coronavirus family seems to be old enough to be known to our Western virologists... and that about 10-15% of seasonal flu is caused by a bug in this family...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3888" id="segment-36" start_char="3767">
<ORIGINAL_TEXT>Venga ya si eso el virus llevaba circulando ya desde la primera maqueta de Metallica, a partir de ahi ya dejaron de molar.</ORIGINAL_TEXT>
<TOKEN end_char="3771" id="token-36-0" morph="none" pos="word" start_char="3767">Venga</TOKEN>
<TOKEN end_char="3774" id="token-36-1" morph="none" pos="word" start_char="3773">ya</TOKEN>
<TOKEN end_char="3777" id="token-36-2" morph="none" pos="word" start_char="3776">si</TOKEN>
<TOKEN end_char="3781" id="token-36-3" morph="none" pos="word" start_char="3779">eso</TOKEN>
<TOKEN end_char="3784" id="token-36-4" morph="none" pos="word" start_char="3783">el</TOKEN>
<TOKEN end_char="3790" id="token-36-5" morph="none" pos="word" start_char="3786">virus</TOKEN>
<TOKEN end_char="3798" id="token-36-6" morph="none" pos="word" start_char="3792">llevaba</TOKEN>
<TOKEN end_char="3809" id="token-36-7" morph="none" pos="word" start_char="3800">circulando</TOKEN>
<TOKEN end_char="3812" id="token-36-8" morph="none" pos="word" start_char="3811">ya</TOKEN>
<TOKEN end_char="3818" id="token-36-9" morph="none" pos="word" start_char="3814">desde</TOKEN>
<TOKEN end_char="3821" id="token-36-10" morph="none" pos="word" start_char="3820">la</TOKEN>
<TOKEN end_char="3829" id="token-36-11" morph="none" pos="word" start_char="3823">primera</TOKEN>
<TOKEN end_char="3837" id="token-36-12" morph="none" pos="word" start_char="3831">maqueta</TOKEN>
<TOKEN end_char="3840" id="token-36-13" morph="none" pos="word" start_char="3839">de</TOKEN>
<TOKEN end_char="3850" id="token-36-14" morph="none" pos="word" start_char="3842">Metallica</TOKEN>
<TOKEN end_char="3851" id="token-36-15" morph="none" pos="punct" start_char="3851">,</TOKEN>
<TOKEN end_char="3853" id="token-36-16" morph="none" pos="word" start_char="3853">a</TOKEN>
<TOKEN end_char="3860" id="token-36-17" morph="none" pos="word" start_char="3855">partir</TOKEN>
<TOKEN end_char="3863" id="token-36-18" morph="none" pos="word" start_char="3862">de</TOKEN>
<TOKEN end_char="3867" id="token-36-19" morph="none" pos="word" start_char="3865">ahi</TOKEN>
<TOKEN end_char="3870" id="token-36-20" morph="none" pos="word" start_char="3869">ya</TOKEN>
<TOKEN end_char="3878" id="token-36-21" morph="none" pos="word" start_char="3872">dejaron</TOKEN>
<TOKEN end_char="3881" id="token-36-22" morph="none" pos="word" start_char="3880">de</TOKEN>
<TOKEN end_char="3887" id="token-36-23" morph="none" pos="word" start_char="3883">molar</TOKEN>
<TOKEN end_char="3888" id="token-36-24" morph="none" pos="punct" start_char="3888">.</TOKEN>
<TRANSLATED_TEXT>Come on if that virus had been circulating since Metallica's first demo, from then on they stopped molar.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="3989" id="segment-37" start_char="3892">
<ORIGINAL_TEXT>Cuanto antes mejor, significaría que es menos grave que si todo se hubiera desencadenado en marzo.</ORIGINAL_TEXT>
<TOKEN end_char="3897" id="token-37-0" morph="none" pos="word" start_char="3892">Cuanto</TOKEN>
<TOKEN end_char="3903" id="token-37-1" morph="none" pos="word" start_char="3899">antes</TOKEN>
<TOKEN end_char="3909" id="token-37-2" morph="none" pos="word" start_char="3905">mejor</TOKEN>
<TOKEN end_char="3910" id="token-37-3" morph="none" pos="punct" start_char="3910">,</TOKEN>
<TOKEN end_char="3923" id="token-37-4" morph="none" pos="word" start_char="3912">significaría</TOKEN>
<TOKEN end_char="3927" id="token-37-5" morph="none" pos="word" start_char="3925">que</TOKEN>
<TOKEN end_char="3930" id="token-37-6" morph="none" pos="word" start_char="3929">es</TOKEN>
<TOKEN end_char="3936" id="token-37-7" morph="none" pos="word" start_char="3932">menos</TOKEN>
<TOKEN end_char="3942" id="token-37-8" morph="none" pos="word" start_char="3938">grave</TOKEN>
<TOKEN end_char="3946" id="token-37-9" morph="none" pos="word" start_char="3944">que</TOKEN>
<TOKEN end_char="3949" id="token-37-10" morph="none" pos="word" start_char="3948">si</TOKEN>
<TOKEN end_char="3954" id="token-37-11" morph="none" pos="word" start_char="3951">todo</TOKEN>
<TOKEN end_char="3957" id="token-37-12" morph="none" pos="word" start_char="3956">se</TOKEN>
<TOKEN end_char="3965" id="token-37-13" morph="none" pos="word" start_char="3959">hubiera</TOKEN>
<TOKEN end_char="3979" id="token-37-14" morph="none" pos="word" start_char="3967">desencadenado</TOKEN>
<TOKEN end_char="3982" id="token-37-15" morph="none" pos="word" start_char="3981">en</TOKEN>
<TOKEN end_char="3988" id="token-37-16" morph="none" pos="word" start_char="3984">marzo</TOKEN>
<TOKEN end_char="3989" id="token-37-17" morph="none" pos="punct" start_char="3989">.</TOKEN>
<TRANSLATED_TEXT>The sooner the better, it would mean that it is less serious than if everything had happened in March.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="4414" id="segment-38" start_char="3993">
<ORIGINAL_TEXT>Pajirri dijo: En diciembre me dio un chungo terrible, 2 semanas jodidos, 3 dias en cama, me costaba respirar y reir ya ni te cuento.. jodido de la garganta, tos, fiebre...y un dolor de cuerpo desde las uñas de los pies hasta la punta de los pelos, se me fue el gusto, vamos que nada tenia sabor... y como ya he comentado..soy una persona que se cuida.. lo achaque a una gripe estacional... pero NUNCA me dio de esa manera.</ORIGINAL_TEXT>
<TOKEN end_char="3999" id="token-38-0" morph="none" pos="word" start_char="3993">Pajirri</TOKEN>
<TOKEN end_char="4004" id="token-38-1" morph="none" pos="word" start_char="4001">dijo</TOKEN>
<TOKEN end_char="4005" id="token-38-2" morph="none" pos="punct" start_char="4005">:</TOKEN>
<TOKEN end_char="4008" id="token-38-3" morph="none" pos="word" start_char="4007">En</TOKEN>
<TOKEN end_char="4018" id="token-38-4" morph="none" pos="word" start_char="4010">diciembre</TOKEN>
<TOKEN end_char="4021" id="token-38-5" morph="none" pos="word" start_char="4020">me</TOKEN>
<TOKEN end_char="4025" id="token-38-6" morph="none" pos="word" start_char="4023">dio</TOKEN>
<TOKEN end_char="4028" id="token-38-7" morph="none" pos="word" start_char="4027">un</TOKEN>
<TOKEN end_char="4035" id="token-38-8" morph="none" pos="word" start_char="4030">chungo</TOKEN>
<TOKEN end_char="4044" id="token-38-9" morph="none" pos="word" start_char="4037">terrible</TOKEN>
<TOKEN end_char="4045" id="token-38-10" morph="none" pos="punct" start_char="4045">,</TOKEN>
<TOKEN end_char="4047" id="token-38-11" morph="none" pos="word" start_char="4047">2</TOKEN>
<TOKEN end_char="4055" id="token-38-12" morph="none" pos="word" start_char="4049">semanas</TOKEN>
<TOKEN end_char="4063" id="token-38-13" morph="none" pos="word" start_char="4057">jodidos</TOKEN>
<TOKEN end_char="4064" id="token-38-14" morph="none" pos="punct" start_char="4064">,</TOKEN>
<TOKEN end_char="4066" id="token-38-15" morph="none" pos="word" start_char="4066">3</TOKEN>
<TOKEN end_char="4071" id="token-38-16" morph="none" pos="word" start_char="4068">dias</TOKEN>
<TOKEN end_char="4074" id="token-38-17" morph="none" pos="word" start_char="4073">en</TOKEN>
<TOKEN end_char="4079" id="token-38-18" morph="none" pos="word" start_char="4076">cama</TOKEN>
<TOKEN end_char="4080" id="token-38-19" morph="none" pos="punct" start_char="4080">,</TOKEN>
<TOKEN end_char="4083" id="token-38-20" morph="none" pos="word" start_char="4082">me</TOKEN>
<TOKEN end_char="4091" id="token-38-21" morph="none" pos="word" start_char="4085">costaba</TOKEN>
<TOKEN end_char="4100" id="token-38-22" morph="none" pos="word" start_char="4093">respirar</TOKEN>
<TOKEN end_char="4102" id="token-38-23" morph="none" pos="word" start_char="4102">y</TOKEN>
<TOKEN end_char="4107" id="token-38-24" morph="none" pos="word" start_char="4104">reir</TOKEN>
<TOKEN end_char="4110" id="token-38-25" morph="none" pos="word" start_char="4109">ya</TOKEN>
<TOKEN end_char="4113" id="token-38-26" morph="none" pos="word" start_char="4112">ni</TOKEN>
<TOKEN end_char="4116" id="token-38-27" morph="none" pos="word" start_char="4115">te</TOKEN>
<TOKEN end_char="4123" id="token-38-28" morph="none" pos="word" start_char="4118">cuento</TOKEN>
<TOKEN end_char="4125" id="token-38-29" morph="none" pos="punct" start_char="4124">..</TOKEN>
<TOKEN end_char="4132" id="token-38-30" morph="none" pos="word" start_char="4127">jodido</TOKEN>
<TOKEN end_char="4135" id="token-38-31" morph="none" pos="word" start_char="4134">de</TOKEN>
<TOKEN end_char="4138" id="token-38-32" morph="none" pos="word" start_char="4137">la</TOKEN>
<TOKEN end_char="4147" id="token-38-33" morph="none" pos="word" start_char="4140">garganta</TOKEN>
<TOKEN end_char="4148" id="token-38-34" morph="none" pos="punct" start_char="4148">,</TOKEN>
<TOKEN end_char="4152" id="token-38-35" morph="none" pos="word" start_char="4150">tos</TOKEN>
<TOKEN end_char="4153" id="token-38-36" morph="none" pos="punct" start_char="4153">,</TOKEN>
<TOKEN end_char="4164" id="token-38-37" morph="none" pos="unknown" start_char="4155">fiebre...y</TOKEN>
<TOKEN end_char="4167" id="token-38-38" morph="none" pos="word" start_char="4166">un</TOKEN>
<TOKEN end_char="4173" id="token-38-39" morph="none" pos="word" start_char="4169">dolor</TOKEN>
<TOKEN end_char="4176" id="token-38-40" morph="none" pos="word" start_char="4175">de</TOKEN>
<TOKEN end_char="4183" id="token-38-41" morph="none" pos="word" start_char="4178">cuerpo</TOKEN>
<TOKEN end_char="4189" id="token-38-42" morph="none" pos="word" start_char="4185">desde</TOKEN>
<TOKEN end_char="4193" id="token-38-43" morph="none" pos="word" start_char="4191">las</TOKEN>
<TOKEN end_char="4198" id="token-38-44" morph="none" pos="word" start_char="4195">uñas</TOKEN>
<TOKEN end_char="4201" id="token-38-45" morph="none" pos="word" start_char="4200">de</TOKEN>
<TOKEN end_char="4205" id="token-38-46" morph="none" pos="word" start_char="4203">los</TOKEN>
<TOKEN end_char="4210" id="token-38-47" morph="none" pos="word" start_char="4207">pies</TOKEN>
<TOKEN end_char="4216" id="token-38-48" morph="none" pos="word" start_char="4212">hasta</TOKEN>
<TOKEN end_char="4219" id="token-38-49" morph="none" pos="word" start_char="4218">la</TOKEN>
<TOKEN end_char="4225" id="token-38-50" morph="none" pos="word" start_char="4221">punta</TOKEN>
<TOKEN end_char="4228" id="token-38-51" morph="none" pos="word" start_char="4227">de</TOKEN>
<TOKEN end_char="4232" id="token-38-52" morph="none" pos="word" start_char="4230">los</TOKEN>
<TOKEN end_char="4238" id="token-38-53" morph="none" pos="word" start_char="4234">pelos</TOKEN>
<TOKEN end_char="4239" id="token-38-54" morph="none" pos="punct" start_char="4239">,</TOKEN>
<TOKEN end_char="4242" id="token-38-55" morph="none" pos="word" start_char="4241">se</TOKEN>
<TOKEN end_char="4245" id="token-38-56" morph="none" pos="word" start_char="4244">me</TOKEN>
<TOKEN end_char="4249" id="token-38-57" morph="none" pos="word" start_char="4247">fue</TOKEN>
<TOKEN end_char="4252" id="token-38-58" morph="none" pos="word" start_char="4251">el</TOKEN>
<TOKEN end_char="4258" id="token-38-59" morph="none" pos="word" start_char="4254">gusto</TOKEN>
<TOKEN end_char="4259" id="token-38-60" morph="none" pos="punct" start_char="4259">,</TOKEN>
<TOKEN end_char="4265" id="token-38-61" morph="none" pos="word" start_char="4261">vamos</TOKEN>
<TOKEN end_char="4269" id="token-38-62" morph="none" pos="word" start_char="4267">que</TOKEN>
<TOKEN end_char="4274" id="token-38-63" morph="none" pos="word" start_char="4271">nada</TOKEN>
<TOKEN end_char="4280" id="token-38-64" morph="none" pos="word" start_char="4276">tenia</TOKEN>
<TOKEN end_char="4286" id="token-38-65" morph="none" pos="word" start_char="4282">sabor</TOKEN>
<TOKEN end_char="4289" id="token-38-66" morph="none" pos="punct" start_char="4287">...</TOKEN>
<TOKEN end_char="4291" id="token-38-67" morph="none" pos="word" start_char="4291">y</TOKEN>
<TOKEN end_char="4296" id="token-38-68" morph="none" pos="word" start_char="4293">como</TOKEN>
<TOKEN end_char="4299" id="token-38-69" morph="none" pos="word" start_char="4298">ya</TOKEN>
<TOKEN end_char="4302" id="token-38-70" morph="none" pos="word" start_char="4301">he</TOKEN>
<TOKEN end_char="4317" id="token-38-71" morph="none" pos="unknown" start_char="4304">comentado..soy</TOKEN>
<TOKEN end_char="4321" id="token-38-72" morph="none" pos="word" start_char="4319">una</TOKEN>
<TOKEN end_char="4329" id="token-38-73" morph="none" pos="word" start_char="4323">persona</TOKEN>
<TOKEN end_char="4333" id="token-38-74" morph="none" pos="word" start_char="4331">que</TOKEN>
<TOKEN end_char="4336" id="token-38-75" morph="none" pos="word" start_char="4335">se</TOKEN>
<TOKEN end_char="4342" id="token-38-76" morph="none" pos="word" start_char="4338">cuida</TOKEN>
<TOKEN end_char="4344" id="token-38-77" morph="none" pos="punct" start_char="4343">..</TOKEN>
<TOKEN end_char="4347" id="token-38-78" morph="none" pos="word" start_char="4346">lo</TOKEN>
<TOKEN end_char="4355" id="token-38-79" morph="none" pos="word" start_char="4349">achaque</TOKEN>
<TOKEN end_char="4357" id="token-38-80" morph="none" pos="word" start_char="4357">a</TOKEN>
<TOKEN end_char="4361" id="token-38-81" morph="none" pos="word" start_char="4359">una</TOKEN>
<TOKEN end_char="4367" id="token-38-82" morph="none" pos="word" start_char="4363">gripe</TOKEN>
<TOKEN end_char="4378" id="token-38-83" morph="none" pos="word" start_char="4369">estacional</TOKEN>
<TOKEN end_char="4381" id="token-38-84" morph="none" pos="punct" start_char="4379">...</TOKEN>
<TOKEN end_char="4386" id="token-38-85" morph="none" pos="word" start_char="4383">pero</TOKEN>
<TOKEN end_char="4392" id="token-38-86" morph="none" pos="word" start_char="4388">NUNCA</TOKEN>
<TOKEN end_char="4395" id="token-38-87" morph="none" pos="word" start_char="4394">me</TOKEN>
<TOKEN end_char="4399" id="token-38-88" morph="none" pos="word" start_char="4397">dio</TOKEN>
<TOKEN end_char="4402" id="token-38-89" morph="none" pos="word" start_char="4401">de</TOKEN>
<TOKEN end_char="4406" id="token-38-90" morph="none" pos="word" start_char="4404">esa</TOKEN>
<TOKEN end_char="4413" id="token-38-91" morph="none" pos="word" start_char="4408">manera</TOKEN>
<TOKEN end_char="4414" id="token-38-92" morph="none" pos="punct" start_char="4414">.</TOKEN>
<TRANSLATED_TEXT>Pajirri said: In December he gave me a terrible chungo, 2 weeks fucked, 3 days in bed, it was hard for me to breathe and laugh and I don't even tell you anymore.. shit from the throat, cough, fever... and a body pain from the fingernails to the tip of the hair, it was my taste, let's not taste anything... and as I have already commented.. I am a person who takes care.. it achaque to a seasonal flu... but NONE gave me that way.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="4449" id="segment-39" start_char="4418">
<ORIGINAL_TEXT>Muchos ya hemos sufrido el covid</ORIGINAL_TEXT>
<TOKEN end_char="4423" id="token-39-0" morph="none" pos="word" start_char="4418">Muchos</TOKEN>
<TOKEN end_char="4426" id="token-39-1" morph="none" pos="word" start_char="4425">ya</TOKEN>
<TOKEN end_char="4432" id="token-39-2" morph="none" pos="word" start_char="4428">hemos</TOKEN>
<TOKEN end_char="4440" id="token-39-3" morph="none" pos="word" start_char="4434">sufrido</TOKEN>
<TOKEN end_char="4443" id="token-39-4" morph="none" pos="word" start_char="4442">el</TOKEN>
<TOKEN end_char="4449" id="token-39-5" morph="none" pos="word" start_char="4445">covid</TOKEN>
<TRANSLATED_TEXT>Many have already suffered the covid</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="4502" id="segment-40" start_char="4453">
<ORIGINAL_TEXT>Navarrete dijo: Un estudio para no culpar a China.</ORIGINAL_TEXT>
<TOKEN end_char="4461" id="token-40-0" morph="none" pos="word" start_char="4453">Navarrete</TOKEN>
<TOKEN end_char="4466" id="token-40-1" morph="none" pos="word" start_char="4463">dijo</TOKEN>
<TOKEN end_char="4467" id="token-40-2" morph="none" pos="punct" start_char="4467">:</TOKEN>
<TOKEN end_char="4470" id="token-40-3" morph="none" pos="word" start_char="4469">Un</TOKEN>
<TOKEN end_char="4478" id="token-40-4" morph="none" pos="word" start_char="4472">estudio</TOKEN>
<TOKEN end_char="4483" id="token-40-5" morph="none" pos="word" start_char="4480">para</TOKEN>
<TOKEN end_char="4486" id="token-40-6" morph="none" pos="word" start_char="4485">no</TOKEN>
<TOKEN end_char="4493" id="token-40-7" morph="none" pos="word" start_char="4488">culpar</TOKEN>
<TOKEN end_char="4495" id="token-40-8" morph="none" pos="word" start_char="4495">a</TOKEN>
<TOKEN end_char="4501" id="token-40-9" morph="none" pos="word" start_char="4497">China</TOKEN>
<TOKEN end_char="4502" id="token-40-10" morph="none" pos="punct" start_char="4502">.</TOKEN>
<TRANSLATED_TEXT>Navarrete said: A study not to blame China.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="4531" id="segment-41" start_char="4506">
<ORIGINAL_TEXT>Los lejías sois así jajaja</ORIGINAL_TEXT>
<TOKEN end_char="4508" id="token-41-0" morph="none" pos="word" start_char="4506">Los</TOKEN>
<TOKEN end_char="4515" id="token-41-1" morph="none" pos="word" start_char="4510">lejías</TOKEN>
<TOKEN end_char="4520" id="token-41-2" morph="none" pos="word" start_char="4517">sois</TOKEN>
<TOKEN end_char="4524" id="token-41-3" morph="none" pos="word" start_char="4522">así</TOKEN>
<TOKEN end_char="4531" id="token-41-4" morph="none" pos="word" start_char="4526">jajaja</TOKEN>
<TRANSLATED_TEXT>The Levites are so jajaja</TRANSLATED_TEXT><DETECTED_LANGUAGE>et</DETECTED_LANGUAGE></SEG>
<SEG end_char="4592" id="segment-42" start_char="4536">
<ORIGINAL_TEXT>AYN RANDiano2 dijo: Y en Italia, y en Francia... Francia.</ORIGINAL_TEXT>
<TOKEN end_char="4538" id="token-42-0" morph="none" pos="word" start_char="4536">AYN</TOKEN>
<TOKEN end_char="4548" id="token-42-1" morph="none" pos="word" start_char="4540">RANDiano2</TOKEN>
<TOKEN end_char="4553" id="token-42-2" morph="none" pos="word" start_char="4550">dijo</TOKEN>
<TOKEN end_char="4554" id="token-42-3" morph="none" pos="punct" start_char="4554">:</TOKEN>
<TOKEN end_char="4556" id="token-42-4" morph="none" pos="word" start_char="4556">Y</TOKEN>
<TOKEN end_char="4559" id="token-42-5" morph="none" pos="word" start_char="4558">en</TOKEN>
<TOKEN end_char="4566" id="token-42-6" morph="none" pos="word" start_char="4561">Italia</TOKEN>
<TOKEN end_char="4567" id="token-42-7" morph="none" pos="punct" start_char="4567">,</TOKEN>
<TOKEN end_char="4569" id="token-42-8" morph="none" pos="word" start_char="4569">y</TOKEN>
<TOKEN end_char="4572" id="token-42-9" morph="none" pos="word" start_char="4571">en</TOKEN>
<TOKEN end_char="4580" id="token-42-10" morph="none" pos="word" start_char="4574">Francia</TOKEN>
<TOKEN end_char="4583" id="token-42-11" morph="none" pos="punct" start_char="4581">...</TOKEN>
<TOKEN end_char="4591" id="token-42-12" morph="none" pos="word" start_char="4585">Francia</TOKEN>
<TOKEN end_char="4592" id="token-42-13" morph="none" pos="punct" start_char="4592">.</TOKEN>
<TRANSLATED_TEXT>AYN RANDiano2 said: And in Italy, and in France... France.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="4808" id="segment-43" start_char="4594">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes) Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN end_char="4601" id="token-43-0" morph="none" pos="word" start_char="4594">Detectan</TOKEN>
<TOKEN end_char="4604" id="token-43-1" morph="none" pos="unknown" start_char="4603">"+</TOKEN>
<TOKEN end_char="4606" id="token-43-2" morph="none" pos="word" start_char="4606">a</TOKEN>
<TOKEN end_char="4614" id="token-43-3" morph="none" pos="word" start_char="4608">covid19</TOKEN>
<TOKEN end_char="4615" id="token-43-4" morph="none" pos="punct" start_char="4615">"</TOKEN>
<TOKEN end_char="4618" id="token-43-5" morph="none" pos="word" start_char="4617">en</TOKEN>
<TOKEN end_char="4626" id="token-43-6" morph="none" pos="word" start_char="4620">muestra</TOKEN>
<TOKEN end_char="4629" id="token-43-7" morph="none" pos="word" start_char="4628">de</TOKEN>
<TOKEN end_char="4637" id="token-43-8" morph="none" pos="word" start_char="4631">enfermo</TOKEN>
<TOKEN end_char="4640" id="token-43-9" morph="none" pos="word" start_char="4639">de</TOKEN>
<TOKEN end_char="4649" id="token-43-10" morph="none" pos="word" start_char="4642">neumonía</TOKEN>
<TOKEN end_char="4653" id="token-43-11" morph="none" pos="word" start_char="4651">del</TOKEN>
<TOKEN end_char="4656" id="token-43-12" morph="none" pos="word" start_char="4655">27</TOKEN>
<TOKEN end_char="4659" id="token-43-13" morph="none" pos="word" start_char="4658">de</TOKEN>
<TOKEN end_char="4669" id="token-43-14" morph="none" pos="word" start_char="4661">diciembre</TOKEN>
<TOKEN end_char="4671" id="token-43-15" morph="none" pos="punct" start_char="4671">(</TOKEN>
<TOKEN end_char="4675" id="token-43-16" morph="none" pos="word" start_char="4672">ergo</TOKEN>
<TOKEN end_char="4677" id="token-43-17" morph="none" pos="punct" start_char="4677">"</TOKEN>
<TOKEN end_char="4686" id="token-43-18" morph="none" pos="word" start_char="4678">infectado</TOKEN>
<TOKEN end_char="4687" id="token-43-19" morph="none" pos="punct" start_char="4687">"</TOKEN>
<TOKEN end_char="4693" id="token-43-20" morph="none" pos="word" start_char="4689">circa</TOKEN>
<TOKEN end_char="4695" id="token-43-21" morph="none" pos="word" start_char="4695">3</TOKEN>
<TOKEN end_char="4703" id="token-43-22" morph="none" pos="word" start_char="4697">semanas</TOKEN>
<TOKEN end_char="4709" id="token-43-23" morph="none" pos="word" start_char="4705">antes</TOKEN>
<TOKEN end_char="4710" id="token-43-24" morph="none" pos="punct" start_char="4710">)</TOKEN>
<TOKEN end_char="4712" id="token-43-25" morph="none" pos="word" start_char="4712">Y</TOKEN>
<TOKEN end_char="4723" id="token-43-26" morph="none" pos="word" start_char="4714">pronostico</TOKEN>
<TOKEN end_char="4727" id="token-43-27" morph="none" pos="word" start_char="4725">que</TOKEN>
<TOKEN end_char="4730" id="token-43-28" morph="none" pos="word" start_char="4729">si</TOKEN>
<TOKEN end_char="4734" id="token-43-29" morph="none" pos="word" start_char="4732">nos</TOKEN>
<TOKEN end_char="4742" id="token-43-30" morph="none" pos="word" start_char="4736">ponemos</TOKEN>
<TOKEN end_char="4744" id="token-43-31" morph="none" pos="word" start_char="4744">a</TOKEN>
<TOKEN end_char="4753" id="token-43-32" morph="none" pos="word" start_char="4746">estudiar</TOKEN>
<TOKEN end_char="4762" id="token-43-33" morph="none" pos="word" start_char="4755">muestras</TOKEN>
<TOKEN end_char="4765" id="token-43-34" morph="none" pos="word" start_char="4764">de</TOKEN>
<TOKEN end_char="4770" id="token-43-35" morph="none" pos="word" start_char="4767">hace</TOKEN>
<TOKEN end_char="4773" id="token-43-36" morph="none" pos="word" start_char="4772">10</TOKEN>
<TOKEN end_char="4775" id="token-43-37" morph="none" pos="word" start_char="4775">o</TOKEN>
<TOKEN end_char="4778" id="token-43-38" morph="none" pos="word" start_char="4777">20</TOKEN>
<TOKEN end_char="4783" id="token-43-39" morph="none" pos="word" start_char="4780">años</TOKEN>
<TOKEN end_char="4791" id="token-43-40" morph="none" pos="word" start_char="4785">también</TOKEN>
<TOKEN end_char="4797" id="token-43-41" morph="none" pos="word" start_char="4793">habrá</TOKEN>
<TOKEN end_char="4799" id="token-43-42" morph="none" pos="punct" start_char="4799">"</TOKEN>
<TOKEN end_char="4806" id="token-43-43" morph="none" pos="word" start_char="4800">covid19</TOKEN>
<TOKEN end_char="4808" id="token-43-44" morph="none" pos="punct" start_char="4807">".</TOKEN>
<TRANSLATED_TEXT>They detect "+ a covid19" in a pneumonia sample from December 27 (ergo "infected" about 3 weeks earlier) and I predict that if we go to study samples from 10 or 20 years ago there will also be "covid19."</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="5035" id="segment-44" start_char="4810">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes: Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN end_char="4811" id="token-44-0" morph="none" pos="word" start_char="4810">El</TOKEN>
<TOKEN end_char="4816" id="token-44-1" morph="none" pos="word" start_char="4813">test</TOKEN>
<TOKEN end_char="4819" id="token-44-2" morph="none" pos="word" start_char="4818">da</TOKEN>
<TOKEN end_char="4822" id="token-44-3" morph="none" pos="word" start_char="4821">un</TOKEN>
<TOKEN end_char="4824" id="token-44-4" morph="none" pos="word" start_char="4824">7</TOKEN>
<TOKEN end_char="4825" id="token-44-5" morph="none" pos="punct" start_char="4825">%</TOKEN>
<TOKEN end_char="4828" id="token-44-6" morph="none" pos="word" start_char="4827">de</TOKEN>
<TOKEN end_char="4833" id="token-44-7" morph="none" pos="unknown" start_char="4830">"+",</TOKEN>
<TOKEN end_char="4840" id="token-44-8" morph="none" pos="word" start_char="4835">testes</TOKEN>
<TOKEN end_char="4843" id="token-44-9" morph="none" pos="word" start_char="4842">la</TOKEN>
<TOKEN end_char="4851" id="token-44-10" morph="none" pos="word" start_char="4845">muestra</TOKEN>
<TOKEN end_char="4855" id="token-44-11" morph="none" pos="word" start_char="4853">que</TOKEN>
<TOKEN end_char="4862" id="token-44-12" morph="none" pos="word" start_char="4857">testes</TOKEN>
<TOKEN end_char="4863" id="token-44-13" morph="none" pos="punct" start_char="4863">:</TOKEN>
<TOKEN end_char="4871" id="token-44-14" morph="none" pos="word" start_char="4865">Lectura</TOKEN>
<TOKEN end_char="4879" id="token-44-15" morph="none" pos="word" start_char="4873">crítica</TOKEN>
<TOKEN end_char="4882" id="token-44-16" morph="none" pos="word" start_char="4881">de</TOKEN>
<TOKEN end_char="4894" id="token-44-17" morph="none" pos="word" start_char="4884">pantallazos</TOKEN>
<TOKEN end_char="4897" id="token-44-18" morph="none" pos="word" start_char="4896">de</TOKEN>
<TOKEN end_char="4908" id="token-44-19" morph="none" pos="word" start_char="4899">prospectos</TOKEN>
<TOKEN end_char="4910" id="token-44-20" morph="none" pos="word" start_char="4910">y</TOKEN>
<TOKEN end_char="4921" id="token-44-21" morph="none" pos="word" start_char="4912">literatura</TOKEN>
<TOKEN end_char="4927" id="token-44-22" morph="none" pos="word" start_char="4923">sobre</TOKEN>
<TOKEN end_char="4929" id="token-44-23" morph="none" pos="punct" start_char="4929">"</TOKEN>
<TOKEN end_char="4934" id="token-44-24" morph="none" pos="word" start_char="4930">tests</TOKEN>
<TOKEN end_char="4937" id="token-44-25" morph="none" pos="word" start_char="4936">de</TOKEN>
<TOKEN end_char="4942" id="token-44-26" morph="none" pos="word" start_char="4939">SARS</TOKEN>
<TOKEN end_char="4946" id="token-44-27" morph="none" pos="word" start_char="4944">Cov</TOKEN>
<TOKEN end_char="4948" id="token-44-28" morph="none" pos="word" start_char="4948">2</TOKEN>
<TOKEN end_char="4949" id="token-44-29" morph="none" pos="punct" start_char="4949">"</TOKEN>
<TOKEN end_char="4952" id="token-44-30" morph="none" pos="punct" start_char="4951">("</TOKEN>
<TOKEN end_char="4971" id="token-44-31" morph="none" pos="unknown" start_char="4953">Covid19")|Infórmese</TOKEN>
<TOKEN end_char="4977" id="token-44-32" morph="none" pos="word" start_char="4973">antes</TOKEN>
<TOKEN end_char="4980" id="token-44-33" morph="none" pos="word" start_char="4979">de</TOKEN>
<TOKEN end_char="4990" id="token-44-34" morph="none" pos="word" start_char="4982">consentir</TOKEN>
<TOKEN end_char="4994" id="token-44-35" morph="none" pos="word" start_char="4992">uno</TOKEN>
<TOKEN end_char="4997" id="token-44-36" morph="none" pos="word" start_char="4996">de</TOKEN>
<TOKEN end_char="5003" id="token-44-37" morph="none" pos="word" start_char="4999">estos</TOKEN>
<TOKEN end_char="5009" id="token-44-38" morph="none" pos="word" start_char="5005">tests</TOKEN>
<TOKEN end_char="5013" id="token-44-39" morph="none" pos="word" start_char="5011">Los</TOKEN>
<TOKEN end_char="5019" id="token-44-40" morph="none" pos="word" start_char="5015">tests</TOKEN>
<TOKEN end_char="5023" id="token-44-41" morph="none" pos="word" start_char="5021">son</TOKEN>
<TOKEN end_char="5027" id="token-44-42" morph="none" pos="word" start_char="5025">los</TOKEN>
<TOKEN end_char="5034" id="token-44-43" morph="none" pos="word" start_char="5029">padres</TOKEN>
<TOKEN end_char="5035" id="token-44-44" morph="none" pos="punct" start_char="5035">.</TOKEN>
<TRANSLATED_TEXT>The test gives a 7% "+," tests the sample that tests: Critical reading of prospect screens and literature on "SARS Cov 2 tests" ("Covid19") | Learn before consenting to one of these tests The tests are parents.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="5063" id="segment-45" start_char="5037">
<ORIGINAL_TEXT>Hacer clic para expandir...</ORIGINAL_TEXT>
<TOKEN end_char="5041" id="token-45-0" morph="none" pos="word" start_char="5037">Hacer</TOKEN>
<TOKEN end_char="5046" id="token-45-1" morph="none" pos="word" start_char="5043">clic</TOKEN>
<TOKEN end_char="5051" id="token-45-2" morph="none" pos="word" start_char="5048">para</TOKEN>
<TOKEN end_char="5060" id="token-45-3" morph="none" pos="word" start_char="5053">expandir</TOKEN>
<TOKEN end_char="5063" id="token-45-4" morph="none" pos="punct" start_char="5061">...</TOKEN>
<TRANSLATED_TEXT>Click to expand...</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="5084" id="segment-46" start_char="5066">
<ORIGINAL_TEXT>Minuto 1 del vídeo.</ORIGINAL_TEXT>
<TOKEN end_char="5071" id="token-46-0" morph="none" pos="word" start_char="5066">Minuto</TOKEN>
<TOKEN end_char="5073" id="token-46-1" morph="none" pos="word" start_char="5073">1</TOKEN>
<TOKEN end_char="5077" id="token-46-2" morph="none" pos="word" start_char="5075">del</TOKEN>
<TOKEN end_char="5083" id="token-46-3" morph="none" pos="word" start_char="5079">vídeo</TOKEN>
<TOKEN end_char="5084" id="token-46-4" morph="none" pos="punct" start_char="5084">.</TOKEN>
<TRANSLATED_TEXT>1 minute video.</TRANSLATED_TEXT><DETECTED_LANGUAGE>pt</DETECTED_LANGUAGE></SEG>
<SEG end_char="5137" id="segment-47" start_char="5086">
<ORIGINAL_TEXT>Sangre congelada del 2017 da positivo al mierdatest.</ORIGINAL_TEXT>
<TOKEN end_char="5091" id="token-47-0" morph="none" pos="word" start_char="5086">Sangre</TOKEN>
<TOKEN end_char="5101" id="token-47-1" morph="none" pos="word" start_char="5093">congelada</TOKEN>
<TOKEN end_char="5105" id="token-47-2" morph="none" pos="word" start_char="5103">del</TOKEN>
<TOKEN end_char="5110" id="token-47-3" morph="none" pos="word" start_char="5107">2017</TOKEN>
<TOKEN end_char="5113" id="token-47-4" morph="none" pos="word" start_char="5112">da</TOKEN>
<TOKEN end_char="5122" id="token-47-5" morph="none" pos="word" start_char="5115">positivo</TOKEN>
<TOKEN end_char="5125" id="token-47-6" morph="none" pos="word" start_char="5124">al</TOKEN>
<TOKEN end_char="5136" id="token-47-7" morph="none" pos="word" start_char="5127">mierdatest</TOKEN>
<TOKEN end_char="5137" id="token-47-8" morph="none" pos="punct" start_char="5137">.</TOKEN>
<TRANSLATED_TEXT>Frozen blood from 2017 is positive to mideast.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="5177" id="segment-48" start_char="5142">
<ORIGINAL_TEXT>pues tal cómo funcionan los tests...</ORIGINAL_TEXT>
<TOKEN end_char="5145" id="token-48-0" morph="none" pos="word" start_char="5142">pues</TOKEN>
<TOKEN end_char="5149" id="token-48-1" morph="none" pos="word" start_char="5147">tal</TOKEN>
<TOKEN end_char="5154" id="token-48-2" morph="none" pos="word" start_char="5151">cómo</TOKEN>
<TOKEN end_char="5164" id="token-48-3" morph="none" pos="word" start_char="5156">funcionan</TOKEN>
<TOKEN end_char="5168" id="token-48-4" morph="none" pos="word" start_char="5166">los</TOKEN>
<TOKEN end_char="5174" id="token-48-5" morph="none" pos="word" start_char="5170">tests</TOKEN>
<TOKEN end_char="5177" id="token-48-6" morph="none" pos="punct" start_char="5175">...</TOKEN>
<TRANSLATED_TEXT>So how the tests work...</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="5202" id="segment-49" start_char="5180">
<ORIGINAL_TEXT>PA FIARSE DE LA VACUNA!</ORIGINAL_TEXT>
<TOKEN end_char="5181" id="token-49-0" morph="none" pos="word" start_char="5180">PA</TOKEN>
<TOKEN end_char="5188" id="token-49-1" morph="none" pos="word" start_char="5183">FIARSE</TOKEN>
<TOKEN end_char="5191" id="token-49-2" morph="none" pos="word" start_char="5190">DE</TOKEN>
<TOKEN end_char="5194" id="token-49-3" morph="none" pos="word" start_char="5193">LA</TOKEN>
<TOKEN end_char="5201" id="token-49-4" morph="none" pos="word" start_char="5196">VACUNA</TOKEN>
<TOKEN end_char="5202" id="token-49-5" morph="none" pos="punct" start_char="5202">!</TOKEN>
<TRANSLATED_TEXT>To the wind!</TRANSLATED_TEXT><DETECTED_LANGUAGE>pt</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>