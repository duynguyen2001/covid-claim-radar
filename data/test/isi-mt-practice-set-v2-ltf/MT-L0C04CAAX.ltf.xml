<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAAX" lang="spa" raw_text_char_length="5071" raw_text_md5="a0bb9c97814b4dc318218f7e2167c7dd" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="76" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Chinese expert cites unverified study to say coronavirus originated in Spain</ORIGINAL_TEXT>
<TOKEN end_char="7" id="token-0-0" morph="none" pos="word" start_char="1">Chinese</TOKEN>
<TOKEN end_char="14" id="token-0-1" morph="none" pos="word" start_char="9">expert</TOKEN>
<TOKEN end_char="20" id="token-0-2" morph="none" pos="word" start_char="16">cites</TOKEN>
<TOKEN end_char="31" id="token-0-3" morph="none" pos="word" start_char="22">unverified</TOKEN>
<TOKEN end_char="37" id="token-0-4" morph="none" pos="word" start_char="33">study</TOKEN>
<TOKEN end_char="40" id="token-0-5" morph="none" pos="word" start_char="39">to</TOKEN>
<TOKEN end_char="44" id="token-0-6" morph="none" pos="word" start_char="42">say</TOKEN>
<TOKEN end_char="56" id="token-0-7" morph="none" pos="word" start_char="46">coronavirus</TOKEN>
<TOKEN end_char="67" id="token-0-8" morph="none" pos="word" start_char="58">originated</TOKEN>
<TOKEN end_char="70" id="token-0-9" morph="none" pos="word" start_char="69">in</TOKEN>
<TOKEN end_char="76" id="token-0-10" morph="none" pos="word" start_char="72">Spain</TOKEN>
</SEG>
<SEG end_char="231" id="segment-1" start_char="80">
<ORIGINAL_TEXT>This undated electron microscope image made available by the U.S. National Institutes of Health in February 2020 shows the Novel Coronavirus SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN end_char="83" id="token-1-0" morph="none" pos="word" start_char="80">This</TOKEN>
<TOKEN end_char="91" id="token-1-1" morph="none" pos="word" start_char="85">undated</TOKEN>
<TOKEN end_char="100" id="token-1-2" morph="none" pos="word" start_char="93">electron</TOKEN>
<TOKEN end_char="111" id="token-1-3" morph="none" pos="word" start_char="102">microscope</TOKEN>
<TOKEN end_char="117" id="token-1-4" morph="none" pos="word" start_char="113">image</TOKEN>
<TOKEN end_char="122" id="token-1-5" morph="none" pos="word" start_char="119">made</TOKEN>
<TOKEN end_char="132" id="token-1-6" morph="none" pos="word" start_char="124">available</TOKEN>
<TOKEN end_char="135" id="token-1-7" morph="none" pos="word" start_char="134">by</TOKEN>
<TOKEN end_char="139" id="token-1-8" morph="none" pos="word" start_char="137">the</TOKEN>
<TOKEN end_char="143" id="token-1-9" morph="none" pos="unknown" start_char="141">U.S</TOKEN>
<TOKEN end_char="144" id="token-1-10" morph="none" pos="punct" start_char="144">.</TOKEN>
<TOKEN end_char="153" id="token-1-11" morph="none" pos="word" start_char="146">National</TOKEN>
<TOKEN end_char="164" id="token-1-12" morph="none" pos="word" start_char="155">Institutes</TOKEN>
<TOKEN end_char="167" id="token-1-13" morph="none" pos="word" start_char="166">of</TOKEN>
<TOKEN end_char="174" id="token-1-14" morph="none" pos="word" start_char="169">Health</TOKEN>
<TOKEN end_char="177" id="token-1-15" morph="none" pos="word" start_char="176">in</TOKEN>
<TOKEN end_char="186" id="token-1-16" morph="none" pos="word" start_char="179">February</TOKEN>
<TOKEN end_char="191" id="token-1-17" morph="none" pos="word" start_char="188">2020</TOKEN>
<TOKEN end_char="197" id="token-1-18" morph="none" pos="word" start_char="193">shows</TOKEN>
<TOKEN end_char="201" id="token-1-19" morph="none" pos="word" start_char="199">the</TOKEN>
<TOKEN end_char="207" id="token-1-20" morph="none" pos="word" start_char="203">Novel</TOKEN>
<TOKEN end_char="219" id="token-1-21" morph="none" pos="word" start_char="209">Coronavirus</TOKEN>
<TOKEN end_char="230" id="token-1-22" morph="none" pos="unknown" start_char="221">SARS-CoV-2</TOKEN>
<TOKEN end_char="231" id="token-1-23" morph="none" pos="punct" start_char="231">.</TOKEN>
</SEG>
<SEG end_char="283" id="segment-2" start_char="233">
<ORIGINAL_TEXT>Also known as 2019-nCoV, the virus causes COVID-19.</ORIGINAL_TEXT>
<TOKEN end_char="236" id="token-2-0" morph="none" pos="word" start_char="233">Also</TOKEN>
<TOKEN end_char="242" id="token-2-1" morph="none" pos="word" start_char="238">known</TOKEN>
<TOKEN end_char="245" id="token-2-2" morph="none" pos="word" start_char="244">as</TOKEN>
<TOKEN end_char="255" id="token-2-3" morph="none" pos="unknown" start_char="247">2019-nCoV</TOKEN>
<TOKEN end_char="256" id="token-2-4" morph="none" pos="punct" start_char="256">,</TOKEN>
<TOKEN end_char="260" id="token-2-5" morph="none" pos="word" start_char="258">the</TOKEN>
<TOKEN end_char="266" id="token-2-6" morph="none" pos="word" start_char="262">virus</TOKEN>
<TOKEN end_char="273" id="token-2-7" morph="none" pos="word" start_char="268">causes</TOKEN>
<TOKEN end_char="282" id="token-2-8" morph="none" pos="unknown" start_char="275">COVID-19</TOKEN>
<TOKEN end_char="283" id="token-2-9" morph="none" pos="punct" start_char="283">.</TOKEN>
</SEG>
<SEG end_char="352" id="segment-3" start_char="285">
<ORIGINAL_TEXT>The sample was isolated from a patient in the U.S | NIAID-RML via AP</ORIGINAL_TEXT>
<TOKEN end_char="287" id="token-3-0" morph="none" pos="word" start_char="285">The</TOKEN>
<TOKEN end_char="294" id="token-3-1" morph="none" pos="word" start_char="289">sample</TOKEN>
<TOKEN end_char="298" id="token-3-2" morph="none" pos="word" start_char="296">was</TOKEN>
<TOKEN end_char="307" id="token-3-3" morph="none" pos="word" start_char="300">isolated</TOKEN>
<TOKEN end_char="312" id="token-3-4" morph="none" pos="word" start_char="309">from</TOKEN>
<TOKEN end_char="314" id="token-3-5" morph="none" pos="word" start_char="314">a</TOKEN>
<TOKEN end_char="322" id="token-3-6" morph="none" pos="word" start_char="316">patient</TOKEN>
<TOKEN end_char="325" id="token-3-7" morph="none" pos="word" start_char="324">in</TOKEN>
<TOKEN end_char="329" id="token-3-8" morph="none" pos="word" start_char="327">the</TOKEN>
<TOKEN end_char="333" id="token-3-9" morph="none" pos="unknown" start_char="331">U.S</TOKEN>
<TOKEN end_char="335" id="token-3-10" morph="none" pos="unknown" start_char="335">|</TOKEN>
<TOKEN end_char="345" id="token-3-11" morph="none" pos="unknown" start_char="337">NIAID-RML</TOKEN>
<TOKEN end_char="349" id="token-3-12" morph="none" pos="word" start_char="347">via</TOKEN>
<TOKEN end_char="352" id="token-3-13" morph="none" pos="word" start_char="351">AP</TOKEN>
</SEG>
<SEG end_char="608" id="segment-4" start_char="356">
<ORIGINAL_TEXT>As the World Health Organisation prepares to visit China next week for a "scoping mission" into the origins of the novel coronavirus, a senior health advisor with the Chinese government has suggested the WHO look at other countries too, including Spain.</ORIGINAL_TEXT>
<TOKEN end_char="357" id="token-4-0" morph="none" pos="word" start_char="356">As</TOKEN>
<TOKEN end_char="361" id="token-4-1" morph="none" pos="word" start_char="359">the</TOKEN>
<TOKEN end_char="367" id="token-4-2" morph="none" pos="word" start_char="363">World</TOKEN>
<TOKEN end_char="374" id="token-4-3" morph="none" pos="word" start_char="369">Health</TOKEN>
<TOKEN end_char="387" id="token-4-4" morph="none" pos="word" start_char="376">Organisation</TOKEN>
<TOKEN end_char="396" id="token-4-5" morph="none" pos="word" start_char="389">prepares</TOKEN>
<TOKEN end_char="399" id="token-4-6" morph="none" pos="word" start_char="398">to</TOKEN>
<TOKEN end_char="405" id="token-4-7" morph="none" pos="word" start_char="401">visit</TOKEN>
<TOKEN end_char="411" id="token-4-8" morph="none" pos="word" start_char="407">China</TOKEN>
<TOKEN end_char="416" id="token-4-9" morph="none" pos="word" start_char="413">next</TOKEN>
<TOKEN end_char="421" id="token-4-10" morph="none" pos="word" start_char="418">week</TOKEN>
<TOKEN end_char="425" id="token-4-11" morph="none" pos="word" start_char="423">for</TOKEN>
<TOKEN end_char="427" id="token-4-12" morph="none" pos="word" start_char="427">a</TOKEN>
<TOKEN end_char="429" id="token-4-13" morph="none" pos="punct" start_char="429">"</TOKEN>
<TOKEN end_char="436" id="token-4-14" morph="none" pos="word" start_char="430">scoping</TOKEN>
<TOKEN end_char="444" id="token-4-15" morph="none" pos="word" start_char="438">mission</TOKEN>
<TOKEN end_char="445" id="token-4-16" morph="none" pos="punct" start_char="445">"</TOKEN>
<TOKEN end_char="450" id="token-4-17" morph="none" pos="word" start_char="447">into</TOKEN>
<TOKEN end_char="454" id="token-4-18" morph="none" pos="word" start_char="452">the</TOKEN>
<TOKEN end_char="462" id="token-4-19" morph="none" pos="word" start_char="456">origins</TOKEN>
<TOKEN end_char="465" id="token-4-20" morph="none" pos="word" start_char="464">of</TOKEN>
<TOKEN end_char="469" id="token-4-21" morph="none" pos="word" start_char="467">the</TOKEN>
<TOKEN end_char="475" id="token-4-22" morph="none" pos="word" start_char="471">novel</TOKEN>
<TOKEN end_char="487" id="token-4-23" morph="none" pos="word" start_char="477">coronavirus</TOKEN>
<TOKEN end_char="488" id="token-4-24" morph="none" pos="punct" start_char="488">,</TOKEN>
<TOKEN end_char="490" id="token-4-25" morph="none" pos="word" start_char="490">a</TOKEN>
<TOKEN end_char="497" id="token-4-26" morph="none" pos="word" start_char="492">senior</TOKEN>
<TOKEN end_char="504" id="token-4-27" morph="none" pos="word" start_char="499">health</TOKEN>
<TOKEN end_char="512" id="token-4-28" morph="none" pos="word" start_char="506">advisor</TOKEN>
<TOKEN end_char="517" id="token-4-29" morph="none" pos="word" start_char="514">with</TOKEN>
<TOKEN end_char="521" id="token-4-30" morph="none" pos="word" start_char="519">the</TOKEN>
<TOKEN end_char="529" id="token-4-31" morph="none" pos="word" start_char="523">Chinese</TOKEN>
<TOKEN end_char="540" id="token-4-32" morph="none" pos="word" start_char="531">government</TOKEN>
<TOKEN end_char="544" id="token-4-33" morph="none" pos="word" start_char="542">has</TOKEN>
<TOKEN end_char="554" id="token-4-34" morph="none" pos="word" start_char="546">suggested</TOKEN>
<TOKEN end_char="558" id="token-4-35" morph="none" pos="word" start_char="556">the</TOKEN>
<TOKEN end_char="562" id="token-4-36" morph="none" pos="word" start_char="560">WHO</TOKEN>
<TOKEN end_char="567" id="token-4-37" morph="none" pos="word" start_char="564">look</TOKEN>
<TOKEN end_char="570" id="token-4-38" morph="none" pos="word" start_char="569">at</TOKEN>
<TOKEN end_char="576" id="token-4-39" morph="none" pos="word" start_char="572">other</TOKEN>
<TOKEN end_char="586" id="token-4-40" morph="none" pos="word" start_char="578">countries</TOKEN>
<TOKEN end_char="590" id="token-4-41" morph="none" pos="word" start_char="588">too</TOKEN>
<TOKEN end_char="591" id="token-4-42" morph="none" pos="punct" start_char="591">,</TOKEN>
<TOKEN end_char="601" id="token-4-43" morph="none" pos="word" start_char="593">including</TOKEN>
<TOKEN end_char="607" id="token-4-44" morph="none" pos="word" start_char="603">Spain</TOKEN>
<TOKEN end_char="608" id="token-4-45" morph="none" pos="punct" start_char="608">.</TOKEN>
</SEG>
<SEG end_char="627" id="segment-5" start_char="611">
<ORIGINAL_TEXT>Wang Guangfa told</ORIGINAL_TEXT>
<TOKEN end_char="614" id="token-5-0" morph="none" pos="word" start_char="611">Wang</TOKEN>
<TOKEN end_char="622" id="token-5-1" morph="none" pos="word" start_char="616">Guangfa</TOKEN>
<TOKEN end_char="627" id="token-5-2" morph="none" pos="word" start_char="624">told</TOKEN>
<TRANSLATED_TEXT>Wang Guangfa</TRANSLATED_TEXT><DETECTED_LANGUAGE>tl</DETECTED_LANGUAGE></SEG>
<SEG end_char="641" id="segment-6" start_char="630">
<ORIGINAL_TEXT>Global Times</ORIGINAL_TEXT>
<TOKEN end_char="635" id="token-6-0" morph="none" pos="word" start_char="630">Global</TOKEN>
<TOKEN end_char="641" id="token-6-1" morph="none" pos="word" start_char="637">Times</TOKEN>
</SEG>
<SEG end_char="904" id="segment-7" start_char="644">
<ORIGINAL_TEXT>that China is only a link in the virus transmission chain, and said that the WHO should go to "more countries such as Spain, which reported coronavirus in its wastewater sample collected in March 2019, for more comprehensive investigations on the virus origin".</ORIGINAL_TEXT>
<TOKEN end_char="647" id="token-7-0" morph="none" pos="word" start_char="644">that</TOKEN>
<TOKEN end_char="653" id="token-7-1" morph="none" pos="word" start_char="649">China</TOKEN>
<TOKEN end_char="656" id="token-7-2" morph="none" pos="word" start_char="655">is</TOKEN>
<TOKEN end_char="661" id="token-7-3" morph="none" pos="word" start_char="658">only</TOKEN>
<TOKEN end_char="663" id="token-7-4" morph="none" pos="word" start_char="663">a</TOKEN>
<TOKEN end_char="668" id="token-7-5" morph="none" pos="word" start_char="665">link</TOKEN>
<TOKEN end_char="671" id="token-7-6" morph="none" pos="word" start_char="670">in</TOKEN>
<TOKEN end_char="675" id="token-7-7" morph="none" pos="word" start_char="673">the</TOKEN>
<TOKEN end_char="681" id="token-7-8" morph="none" pos="word" start_char="677">virus</TOKEN>
<TOKEN end_char="694" id="token-7-9" morph="none" pos="word" start_char="683">transmission</TOKEN>
<TOKEN end_char="700" id="token-7-10" morph="none" pos="word" start_char="696">chain</TOKEN>
<TOKEN end_char="701" id="token-7-11" morph="none" pos="punct" start_char="701">,</TOKEN>
<TOKEN end_char="705" id="token-7-12" morph="none" pos="word" start_char="703">and</TOKEN>
<TOKEN end_char="710" id="token-7-13" morph="none" pos="word" start_char="707">said</TOKEN>
<TOKEN end_char="715" id="token-7-14" morph="none" pos="word" start_char="712">that</TOKEN>
<TOKEN end_char="719" id="token-7-15" morph="none" pos="word" start_char="717">the</TOKEN>
<TOKEN end_char="723" id="token-7-16" morph="none" pos="word" start_char="721">WHO</TOKEN>
<TOKEN end_char="730" id="token-7-17" morph="none" pos="word" start_char="725">should</TOKEN>
<TOKEN end_char="733" id="token-7-18" morph="none" pos="word" start_char="732">go</TOKEN>
<TOKEN end_char="736" id="token-7-19" morph="none" pos="word" start_char="735">to</TOKEN>
<TOKEN end_char="738" id="token-7-20" morph="none" pos="punct" start_char="738">"</TOKEN>
<TOKEN end_char="742" id="token-7-21" morph="none" pos="word" start_char="739">more</TOKEN>
<TOKEN end_char="752" id="token-7-22" morph="none" pos="word" start_char="744">countries</TOKEN>
<TOKEN end_char="757" id="token-7-23" morph="none" pos="word" start_char="754">such</TOKEN>
<TOKEN end_char="760" id="token-7-24" morph="none" pos="word" start_char="759">as</TOKEN>
<TOKEN end_char="766" id="token-7-25" morph="none" pos="word" start_char="762">Spain</TOKEN>
<TOKEN end_char="767" id="token-7-26" morph="none" pos="punct" start_char="767">,</TOKEN>
<TOKEN end_char="773" id="token-7-27" morph="none" pos="word" start_char="769">which</TOKEN>
<TOKEN end_char="782" id="token-7-28" morph="none" pos="word" start_char="775">reported</TOKEN>
<TOKEN end_char="794" id="token-7-29" morph="none" pos="word" start_char="784">coronavirus</TOKEN>
<TOKEN end_char="797" id="token-7-30" morph="none" pos="word" start_char="796">in</TOKEN>
<TOKEN end_char="801" id="token-7-31" morph="none" pos="word" start_char="799">its</TOKEN>
<TOKEN end_char="812" id="token-7-32" morph="none" pos="word" start_char="803">wastewater</TOKEN>
<TOKEN end_char="819" id="token-7-33" morph="none" pos="word" start_char="814">sample</TOKEN>
<TOKEN end_char="829" id="token-7-34" morph="none" pos="word" start_char="821">collected</TOKEN>
<TOKEN end_char="832" id="token-7-35" morph="none" pos="word" start_char="831">in</TOKEN>
<TOKEN end_char="838" id="token-7-36" morph="none" pos="word" start_char="834">March</TOKEN>
<TOKEN end_char="843" id="token-7-37" morph="none" pos="word" start_char="840">2019</TOKEN>
<TOKEN end_char="844" id="token-7-38" morph="none" pos="punct" start_char="844">,</TOKEN>
<TOKEN end_char="848" id="token-7-39" morph="none" pos="word" start_char="846">for</TOKEN>
<TOKEN end_char="853" id="token-7-40" morph="none" pos="word" start_char="850">more</TOKEN>
<TOKEN end_char="867" id="token-7-41" morph="none" pos="word" start_char="855">comprehensive</TOKEN>
<TOKEN end_char="882" id="token-7-42" morph="none" pos="word" start_char="869">investigations</TOKEN>
<TOKEN end_char="885" id="token-7-43" morph="none" pos="word" start_char="884">on</TOKEN>
<TOKEN end_char="889" id="token-7-44" morph="none" pos="word" start_char="887">the</TOKEN>
<TOKEN end_char="895" id="token-7-45" morph="none" pos="word" start_char="891">virus</TOKEN>
<TOKEN end_char="902" id="token-7-46" morph="none" pos="word" start_char="897">origin</TOKEN>
<TOKEN end_char="904" id="token-7-47" morph="none" pos="punct" start_char="903">".</TOKEN>
</SEG>
<SEG end_char="1175" id="segment-8" start_char="907">
<ORIGINAL_TEXT>Wang’s claim has since been reported by several news outlets to suggest that China referred to a pre-print study by researchers from the University of Barcelona, which claimed to find evidence of COVID-19 being present in Barcelona’s wastewater as early as March, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="912" id="token-8-0" morph="none" pos="word" start_char="907">Wang’s</TOKEN>
<TOKEN end_char="918" id="token-8-1" morph="none" pos="word" start_char="914">claim</TOKEN>
<TOKEN end_char="922" id="token-8-2" morph="none" pos="word" start_char="920">has</TOKEN>
<TOKEN end_char="928" id="token-8-3" morph="none" pos="word" start_char="924">since</TOKEN>
<TOKEN end_char="933" id="token-8-4" morph="none" pos="word" start_char="930">been</TOKEN>
<TOKEN end_char="942" id="token-8-5" morph="none" pos="word" start_char="935">reported</TOKEN>
<TOKEN end_char="945" id="token-8-6" morph="none" pos="word" start_char="944">by</TOKEN>
<TOKEN end_char="953" id="token-8-7" morph="none" pos="word" start_char="947">several</TOKEN>
<TOKEN end_char="958" id="token-8-8" morph="none" pos="word" start_char="955">news</TOKEN>
<TOKEN end_char="966" id="token-8-9" morph="none" pos="word" start_char="960">outlets</TOKEN>
<TOKEN end_char="969" id="token-8-10" morph="none" pos="word" start_char="968">to</TOKEN>
<TOKEN end_char="977" id="token-8-11" morph="none" pos="word" start_char="971">suggest</TOKEN>
<TOKEN end_char="982" id="token-8-12" morph="none" pos="word" start_char="979">that</TOKEN>
<TOKEN end_char="988" id="token-8-13" morph="none" pos="word" start_char="984">China</TOKEN>
<TOKEN end_char="997" id="token-8-14" morph="none" pos="word" start_char="990">referred</TOKEN>
<TOKEN end_char="1000" id="token-8-15" morph="none" pos="word" start_char="999">to</TOKEN>
<TOKEN end_char="1002" id="token-8-16" morph="none" pos="word" start_char="1002">a</TOKEN>
<TOKEN end_char="1012" id="token-8-17" morph="none" pos="unknown" start_char="1004">pre-print</TOKEN>
<TOKEN end_char="1018" id="token-8-18" morph="none" pos="word" start_char="1014">study</TOKEN>
<TOKEN end_char="1021" id="token-8-19" morph="none" pos="word" start_char="1020">by</TOKEN>
<TOKEN end_char="1033" id="token-8-20" morph="none" pos="word" start_char="1023">researchers</TOKEN>
<TOKEN end_char="1038" id="token-8-21" morph="none" pos="word" start_char="1035">from</TOKEN>
<TOKEN end_char="1042" id="token-8-22" morph="none" pos="word" start_char="1040">the</TOKEN>
<TOKEN end_char="1053" id="token-8-23" morph="none" pos="word" start_char="1044">University</TOKEN>
<TOKEN end_char="1056" id="token-8-24" morph="none" pos="word" start_char="1055">of</TOKEN>
<TOKEN end_char="1066" id="token-8-25" morph="none" pos="word" start_char="1058">Barcelona</TOKEN>
<TOKEN end_char="1067" id="token-8-26" morph="none" pos="punct" start_char="1067">,</TOKEN>
<TOKEN end_char="1073" id="token-8-27" morph="none" pos="word" start_char="1069">which</TOKEN>
<TOKEN end_char="1081" id="token-8-28" morph="none" pos="word" start_char="1075">claimed</TOKEN>
<TOKEN end_char="1084" id="token-8-29" morph="none" pos="word" start_char="1083">to</TOKEN>
<TOKEN end_char="1089" id="token-8-30" morph="none" pos="word" start_char="1086">find</TOKEN>
<TOKEN end_char="1098" id="token-8-31" morph="none" pos="word" start_char="1091">evidence</TOKEN>
<TOKEN end_char="1101" id="token-8-32" morph="none" pos="word" start_char="1100">of</TOKEN>
<TOKEN end_char="1110" id="token-8-33" morph="none" pos="unknown" start_char="1103">COVID-19</TOKEN>
<TOKEN end_char="1116" id="token-8-34" morph="none" pos="word" start_char="1112">being</TOKEN>
<TOKEN end_char="1124" id="token-8-35" morph="none" pos="word" start_char="1118">present</TOKEN>
<TOKEN end_char="1127" id="token-8-36" morph="none" pos="word" start_char="1126">in</TOKEN>
<TOKEN end_char="1139" id="token-8-37" morph="none" pos="word" start_char="1129">Barcelona’s</TOKEN>
<TOKEN end_char="1150" id="token-8-38" morph="none" pos="word" start_char="1141">wastewater</TOKEN>
<TOKEN end_char="1153" id="token-8-39" morph="none" pos="word" start_char="1152">as</TOKEN>
<TOKEN end_char="1159" id="token-8-40" morph="none" pos="word" start_char="1155">early</TOKEN>
<TOKEN end_char="1162" id="token-8-41" morph="none" pos="word" start_char="1161">as</TOKEN>
<TOKEN end_char="1168" id="token-8-42" morph="none" pos="word" start_char="1164">March</TOKEN>
<TOKEN end_char="1169" id="token-8-43" morph="none" pos="punct" start_char="1169">,</TOKEN>
<TOKEN end_char="1174" id="token-8-44" morph="none" pos="word" start_char="1171">2019</TOKEN>
<TOKEN end_char="1175" id="token-8-45" morph="none" pos="punct" start_char="1175">.</TOKEN>
</SEG>
<SEG end_char="1393" id="segment-9" start_char="1178">
<ORIGINAL_TEXT>The study, which was uploaded as a pre-print—before facing peer-review—on MedRxiv, tested samples of Barcelona wastewater for presence of the virus, using WHO-recommended real-time RT-PCR assays to detect SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN end_char="1180" id="token-9-0" morph="none" pos="word" start_char="1178">The</TOKEN>
<TOKEN end_char="1186" id="token-9-1" morph="none" pos="word" start_char="1182">study</TOKEN>
<TOKEN end_char="1187" id="token-9-2" morph="none" pos="punct" start_char="1187">,</TOKEN>
<TOKEN end_char="1193" id="token-9-3" morph="none" pos="word" start_char="1189">which</TOKEN>
<TOKEN end_char="1197" id="token-9-4" morph="none" pos="word" start_char="1195">was</TOKEN>
<TOKEN end_char="1206" id="token-9-5" morph="none" pos="word" start_char="1199">uploaded</TOKEN>
<TOKEN end_char="1209" id="token-9-6" morph="none" pos="word" start_char="1208">as</TOKEN>
<TOKEN end_char="1211" id="token-9-7" morph="none" pos="word" start_char="1211">a</TOKEN>
<TOKEN end_char="1228" id="token-9-8" morph="none" pos="unknown" start_char="1213">pre-print—before</TOKEN>
<TOKEN end_char="1235" id="token-9-9" morph="none" pos="word" start_char="1230">facing</TOKEN>
<TOKEN end_char="1250" id="token-9-10" morph="none" pos="unknown" start_char="1237">peer-review—on</TOKEN>
<TOKEN end_char="1258" id="token-9-11" morph="none" pos="word" start_char="1252">MedRxiv</TOKEN>
<TOKEN end_char="1259" id="token-9-12" morph="none" pos="punct" start_char="1259">,</TOKEN>
<TOKEN end_char="1266" id="token-9-13" morph="none" pos="word" start_char="1261">tested</TOKEN>
<TOKEN end_char="1274" id="token-9-14" morph="none" pos="word" start_char="1268">samples</TOKEN>
<TOKEN end_char="1277" id="token-9-15" morph="none" pos="word" start_char="1276">of</TOKEN>
<TOKEN end_char="1287" id="token-9-16" morph="none" pos="word" start_char="1279">Barcelona</TOKEN>
<TOKEN end_char="1298" id="token-9-17" morph="none" pos="word" start_char="1289">wastewater</TOKEN>
<TOKEN end_char="1302" id="token-9-18" morph="none" pos="word" start_char="1300">for</TOKEN>
<TOKEN end_char="1311" id="token-9-19" morph="none" pos="word" start_char="1304">presence</TOKEN>
<TOKEN end_char="1314" id="token-9-20" morph="none" pos="word" start_char="1313">of</TOKEN>
<TOKEN end_char="1318" id="token-9-21" morph="none" pos="word" start_char="1316">the</TOKEN>
<TOKEN end_char="1324" id="token-9-22" morph="none" pos="word" start_char="1320">virus</TOKEN>
<TOKEN end_char="1325" id="token-9-23" morph="none" pos="punct" start_char="1325">,</TOKEN>
<TOKEN end_char="1331" id="token-9-24" morph="none" pos="word" start_char="1327">using</TOKEN>
<TOKEN end_char="1347" id="token-9-25" morph="none" pos="unknown" start_char="1333">WHO-recommended</TOKEN>
<TOKEN end_char="1357" id="token-9-26" morph="none" pos="unknown" start_char="1349">real-time</TOKEN>
<TOKEN end_char="1364" id="token-9-27" morph="none" pos="unknown" start_char="1359">RT-PCR</TOKEN>
<TOKEN end_char="1371" id="token-9-28" morph="none" pos="word" start_char="1366">assays</TOKEN>
<TOKEN end_char="1374" id="token-9-29" morph="none" pos="word" start_char="1373">to</TOKEN>
<TOKEN end_char="1381" id="token-9-30" morph="none" pos="word" start_char="1376">detect</TOKEN>
<TOKEN end_char="1392" id="token-9-31" morph="none" pos="unknown" start_char="1383">SARS-CoV-2</TOKEN>
<TOKEN end_char="1393" id="token-9-32" morph="none" pos="punct" start_char="1393">.</TOKEN>
</SEG>
<SEG end_char="1714" id="segment-10" start_char="1396">
<ORIGINAL_TEXT>While the first known cases of the virus in Spain were see in February, with the coronavirus widely believed to have originated in Wuhan around December in 2019, the study found, from one frozen sample of wastewater from Barcelona dating to March 2019, presence of the virus nine months before it was detected in Spain.</ORIGINAL_TEXT>
<TOKEN end_char="1400" id="token-10-0" morph="none" pos="word" start_char="1396">While</TOKEN>
<TOKEN end_char="1404" id="token-10-1" morph="none" pos="word" start_char="1402">the</TOKEN>
<TOKEN end_char="1410" id="token-10-2" morph="none" pos="word" start_char="1406">first</TOKEN>
<TOKEN end_char="1416" id="token-10-3" morph="none" pos="word" start_char="1412">known</TOKEN>
<TOKEN end_char="1422" id="token-10-4" morph="none" pos="word" start_char="1418">cases</TOKEN>
<TOKEN end_char="1425" id="token-10-5" morph="none" pos="word" start_char="1424">of</TOKEN>
<TOKEN end_char="1429" id="token-10-6" morph="none" pos="word" start_char="1427">the</TOKEN>
<TOKEN end_char="1435" id="token-10-7" morph="none" pos="word" start_char="1431">virus</TOKEN>
<TOKEN end_char="1438" id="token-10-8" morph="none" pos="word" start_char="1437">in</TOKEN>
<TOKEN end_char="1444" id="token-10-9" morph="none" pos="word" start_char="1440">Spain</TOKEN>
<TOKEN end_char="1449" id="token-10-10" morph="none" pos="word" start_char="1446">were</TOKEN>
<TOKEN end_char="1453" id="token-10-11" morph="none" pos="word" start_char="1451">see</TOKEN>
<TOKEN end_char="1456" id="token-10-12" morph="none" pos="word" start_char="1455">in</TOKEN>
<TOKEN end_char="1465" id="token-10-13" morph="none" pos="word" start_char="1458">February</TOKEN>
<TOKEN end_char="1466" id="token-10-14" morph="none" pos="punct" start_char="1466">,</TOKEN>
<TOKEN end_char="1471" id="token-10-15" morph="none" pos="word" start_char="1468">with</TOKEN>
<TOKEN end_char="1475" id="token-10-16" morph="none" pos="word" start_char="1473">the</TOKEN>
<TOKEN end_char="1487" id="token-10-17" morph="none" pos="word" start_char="1477">coronavirus</TOKEN>
<TOKEN end_char="1494" id="token-10-18" morph="none" pos="word" start_char="1489">widely</TOKEN>
<TOKEN end_char="1503" id="token-10-19" morph="none" pos="word" start_char="1496">believed</TOKEN>
<TOKEN end_char="1506" id="token-10-20" morph="none" pos="word" start_char="1505">to</TOKEN>
<TOKEN end_char="1511" id="token-10-21" morph="none" pos="word" start_char="1508">have</TOKEN>
<TOKEN end_char="1522" id="token-10-22" morph="none" pos="word" start_char="1513">originated</TOKEN>
<TOKEN end_char="1525" id="token-10-23" morph="none" pos="word" start_char="1524">in</TOKEN>
<TOKEN end_char="1531" id="token-10-24" morph="none" pos="word" start_char="1527">Wuhan</TOKEN>
<TOKEN end_char="1538" id="token-10-25" morph="none" pos="word" start_char="1533">around</TOKEN>
<TOKEN end_char="1547" id="token-10-26" morph="none" pos="word" start_char="1540">December</TOKEN>
<TOKEN end_char="1550" id="token-10-27" morph="none" pos="word" start_char="1549">in</TOKEN>
<TOKEN end_char="1555" id="token-10-28" morph="none" pos="word" start_char="1552">2019</TOKEN>
<TOKEN end_char="1556" id="token-10-29" morph="none" pos="punct" start_char="1556">,</TOKEN>
<TOKEN end_char="1560" id="token-10-30" morph="none" pos="word" start_char="1558">the</TOKEN>
<TOKEN end_char="1566" id="token-10-31" morph="none" pos="word" start_char="1562">study</TOKEN>
<TOKEN end_char="1572" id="token-10-32" morph="none" pos="word" start_char="1568">found</TOKEN>
<TOKEN end_char="1573" id="token-10-33" morph="none" pos="punct" start_char="1573">,</TOKEN>
<TOKEN end_char="1578" id="token-10-34" morph="none" pos="word" start_char="1575">from</TOKEN>
<TOKEN end_char="1582" id="token-10-35" morph="none" pos="word" start_char="1580">one</TOKEN>
<TOKEN end_char="1589" id="token-10-36" morph="none" pos="word" start_char="1584">frozen</TOKEN>
<TOKEN end_char="1596" id="token-10-37" morph="none" pos="word" start_char="1591">sample</TOKEN>
<TOKEN end_char="1599" id="token-10-38" morph="none" pos="word" start_char="1598">of</TOKEN>
<TOKEN end_char="1610" id="token-10-39" morph="none" pos="word" start_char="1601">wastewater</TOKEN>
<TOKEN end_char="1615" id="token-10-40" morph="none" pos="word" start_char="1612">from</TOKEN>
<TOKEN end_char="1625" id="token-10-41" morph="none" pos="word" start_char="1617">Barcelona</TOKEN>
<TOKEN end_char="1632" id="token-10-42" morph="none" pos="word" start_char="1627">dating</TOKEN>
<TOKEN end_char="1635" id="token-10-43" morph="none" pos="word" start_char="1634">to</TOKEN>
<TOKEN end_char="1641" id="token-10-44" morph="none" pos="word" start_char="1637">March</TOKEN>
<TOKEN end_char="1646" id="token-10-45" morph="none" pos="word" start_char="1643">2019</TOKEN>
<TOKEN end_char="1647" id="token-10-46" morph="none" pos="punct" start_char="1647">,</TOKEN>
<TOKEN end_char="1656" id="token-10-47" morph="none" pos="word" start_char="1649">presence</TOKEN>
<TOKEN end_char="1659" id="token-10-48" morph="none" pos="word" start_char="1658">of</TOKEN>
<TOKEN end_char="1663" id="token-10-49" morph="none" pos="word" start_char="1661">the</TOKEN>
<TOKEN end_char="1669" id="token-10-50" morph="none" pos="word" start_char="1665">virus</TOKEN>
<TOKEN end_char="1674" id="token-10-51" morph="none" pos="word" start_char="1671">nine</TOKEN>
<TOKEN end_char="1681" id="token-10-52" morph="none" pos="word" start_char="1676">months</TOKEN>
<TOKEN end_char="1688" id="token-10-53" morph="none" pos="word" start_char="1683">before</TOKEN>
<TOKEN end_char="1691" id="token-10-54" morph="none" pos="word" start_char="1690">it</TOKEN>
<TOKEN end_char="1695" id="token-10-55" morph="none" pos="word" start_char="1693">was</TOKEN>
<TOKEN end_char="1704" id="token-10-56" morph="none" pos="word" start_char="1697">detected</TOKEN>
<TOKEN end_char="1707" id="token-10-57" morph="none" pos="word" start_char="1706">in</TOKEN>
<TOKEN end_char="1713" id="token-10-58" morph="none" pos="word" start_char="1709">Spain</TOKEN>
<TOKEN end_char="1714" id="token-10-59" morph="none" pos="punct" start_char="1714">.</TOKEN>
</SEG>
<SEG end_char="1893" id="segment-11" start_char="1717">
<ORIGINAL_TEXT>Since the virus can be detected in human feces, wastewater analyses have proven a useful tool for measuring when and in what quantity the virus was present in different regions.</ORIGINAL_TEXT>
<TOKEN end_char="1721" id="token-11-0" morph="none" pos="word" start_char="1717">Since</TOKEN>
<TOKEN end_char="1725" id="token-11-1" morph="none" pos="word" start_char="1723">the</TOKEN>
<TOKEN end_char="1731" id="token-11-2" morph="none" pos="word" start_char="1727">virus</TOKEN>
<TOKEN end_char="1735" id="token-11-3" morph="none" pos="word" start_char="1733">can</TOKEN>
<TOKEN end_char="1738" id="token-11-4" morph="none" pos="word" start_char="1737">be</TOKEN>
<TOKEN end_char="1747" id="token-11-5" morph="none" pos="word" start_char="1740">detected</TOKEN>
<TOKEN end_char="1750" id="token-11-6" morph="none" pos="word" start_char="1749">in</TOKEN>
<TOKEN end_char="1756" id="token-11-7" morph="none" pos="word" start_char="1752">human</TOKEN>
<TOKEN end_char="1762" id="token-11-8" morph="none" pos="word" start_char="1758">feces</TOKEN>
<TOKEN end_char="1763" id="token-11-9" morph="none" pos="punct" start_char="1763">,</TOKEN>
<TOKEN end_char="1774" id="token-11-10" morph="none" pos="word" start_char="1765">wastewater</TOKEN>
<TOKEN end_char="1783" id="token-11-11" morph="none" pos="word" start_char="1776">analyses</TOKEN>
<TOKEN end_char="1788" id="token-11-12" morph="none" pos="word" start_char="1785">have</TOKEN>
<TOKEN end_char="1795" id="token-11-13" morph="none" pos="word" start_char="1790">proven</TOKEN>
<TOKEN end_char="1797" id="token-11-14" morph="none" pos="word" start_char="1797">a</TOKEN>
<TOKEN end_char="1804" id="token-11-15" morph="none" pos="word" start_char="1799">useful</TOKEN>
<TOKEN end_char="1809" id="token-11-16" morph="none" pos="word" start_char="1806">tool</TOKEN>
<TOKEN end_char="1813" id="token-11-17" morph="none" pos="word" start_char="1811">for</TOKEN>
<TOKEN end_char="1823" id="token-11-18" morph="none" pos="word" start_char="1815">measuring</TOKEN>
<TOKEN end_char="1828" id="token-11-19" morph="none" pos="word" start_char="1825">when</TOKEN>
<TOKEN end_char="1832" id="token-11-20" morph="none" pos="word" start_char="1830">and</TOKEN>
<TOKEN end_char="1835" id="token-11-21" morph="none" pos="word" start_char="1834">in</TOKEN>
<TOKEN end_char="1840" id="token-11-22" morph="none" pos="word" start_char="1837">what</TOKEN>
<TOKEN end_char="1849" id="token-11-23" morph="none" pos="word" start_char="1842">quantity</TOKEN>
<TOKEN end_char="1853" id="token-11-24" morph="none" pos="word" start_char="1851">the</TOKEN>
<TOKEN end_char="1859" id="token-11-25" morph="none" pos="word" start_char="1855">virus</TOKEN>
<TOKEN end_char="1863" id="token-11-26" morph="none" pos="word" start_char="1861">was</TOKEN>
<TOKEN end_char="1871" id="token-11-27" morph="none" pos="word" start_char="1865">present</TOKEN>
<TOKEN end_char="1874" id="token-11-28" morph="none" pos="word" start_char="1873">in</TOKEN>
<TOKEN end_char="1884" id="token-11-29" morph="none" pos="word" start_char="1876">different</TOKEN>
<TOKEN end_char="1892" id="token-11-30" morph="none" pos="word" start_char="1886">regions</TOKEN>
<TOKEN end_char="1893" id="token-11-31" morph="none" pos="punct" start_char="1893">.</TOKEN>
</SEG>
<SEG end_char="2007" id="segment-12" start_char="1895">
<ORIGINAL_TEXT>A wastewater study in Northern Italy found the presence of the virus in wastewater from before Christmas of 2019.</ORIGINAL_TEXT>
<TOKEN end_char="1895" id="token-12-0" morph="none" pos="word" start_char="1895">A</TOKEN>
<TOKEN end_char="1906" id="token-12-1" morph="none" pos="word" start_char="1897">wastewater</TOKEN>
<TOKEN end_char="1912" id="token-12-2" morph="none" pos="word" start_char="1908">study</TOKEN>
<TOKEN end_char="1915" id="token-12-3" morph="none" pos="word" start_char="1914">in</TOKEN>
<TOKEN end_char="1924" id="token-12-4" morph="none" pos="word" start_char="1917">Northern</TOKEN>
<TOKEN end_char="1930" id="token-12-5" morph="none" pos="word" start_char="1926">Italy</TOKEN>
<TOKEN end_char="1936" id="token-12-6" morph="none" pos="word" start_char="1932">found</TOKEN>
<TOKEN end_char="1940" id="token-12-7" morph="none" pos="word" start_char="1938">the</TOKEN>
<TOKEN end_char="1949" id="token-12-8" morph="none" pos="word" start_char="1942">presence</TOKEN>
<TOKEN end_char="1952" id="token-12-9" morph="none" pos="word" start_char="1951">of</TOKEN>
<TOKEN end_char="1956" id="token-12-10" morph="none" pos="word" start_char="1954">the</TOKEN>
<TOKEN end_char="1962" id="token-12-11" morph="none" pos="word" start_char="1958">virus</TOKEN>
<TOKEN end_char="1965" id="token-12-12" morph="none" pos="word" start_char="1964">in</TOKEN>
<TOKEN end_char="1976" id="token-12-13" morph="none" pos="word" start_char="1967">wastewater</TOKEN>
<TOKEN end_char="1981" id="token-12-14" morph="none" pos="word" start_char="1978">from</TOKEN>
<TOKEN end_char="1988" id="token-12-15" morph="none" pos="word" start_char="1983">before</TOKEN>
<TOKEN end_char="1998" id="token-12-16" morph="none" pos="word" start_char="1990">Christmas</TOKEN>
<TOKEN end_char="2001" id="token-12-17" morph="none" pos="word" start_char="2000">of</TOKEN>
<TOKEN end_char="2006" id="token-12-18" morph="none" pos="word" start_char="2003">2019</TOKEN>
<TOKEN end_char="2007" id="token-12-19" morph="none" pos="punct" start_char="2007">.</TOKEN>
</SEG>
<SEG end_char="2123" id="segment-13" start_char="2009">
<ORIGINAL_TEXT>However, the University of Barcelona (UoB) study suggests a much earlier date of incidence than any other analysis.</ORIGINAL_TEXT>
<TOKEN end_char="2015" id="token-13-0" morph="none" pos="word" start_char="2009">However</TOKEN>
<TOKEN end_char="2016" id="token-13-1" morph="none" pos="punct" start_char="2016">,</TOKEN>
<TOKEN end_char="2020" id="token-13-2" morph="none" pos="word" start_char="2018">the</TOKEN>
<TOKEN end_char="2031" id="token-13-3" morph="none" pos="word" start_char="2022">University</TOKEN>
<TOKEN end_char="2034" id="token-13-4" morph="none" pos="word" start_char="2033">of</TOKEN>
<TOKEN end_char="2044" id="token-13-5" morph="none" pos="word" start_char="2036">Barcelona</TOKEN>
<TOKEN end_char="2046" id="token-13-6" morph="none" pos="punct" start_char="2046">(</TOKEN>
<TOKEN end_char="2049" id="token-13-7" morph="none" pos="word" start_char="2047">UoB</TOKEN>
<TOKEN end_char="2050" id="token-13-8" morph="none" pos="punct" start_char="2050">)</TOKEN>
<TOKEN end_char="2056" id="token-13-9" morph="none" pos="word" start_char="2052">study</TOKEN>
<TOKEN end_char="2065" id="token-13-10" morph="none" pos="word" start_char="2058">suggests</TOKEN>
<TOKEN end_char="2067" id="token-13-11" morph="none" pos="word" start_char="2067">a</TOKEN>
<TOKEN end_char="2072" id="token-13-12" morph="none" pos="word" start_char="2069">much</TOKEN>
<TOKEN end_char="2080" id="token-13-13" morph="none" pos="word" start_char="2074">earlier</TOKEN>
<TOKEN end_char="2085" id="token-13-14" morph="none" pos="word" start_char="2082">date</TOKEN>
<TOKEN end_char="2088" id="token-13-15" morph="none" pos="word" start_char="2087">of</TOKEN>
<TOKEN end_char="2098" id="token-13-16" morph="none" pos="word" start_char="2090">incidence</TOKEN>
<TOKEN end_char="2103" id="token-13-17" morph="none" pos="word" start_char="2100">than</TOKEN>
<TOKEN end_char="2107" id="token-13-18" morph="none" pos="word" start_char="2105">any</TOKEN>
<TOKEN end_char="2113" id="token-13-19" morph="none" pos="word" start_char="2109">other</TOKEN>
<TOKEN end_char="2122" id="token-13-20" morph="none" pos="word" start_char="2115">analysis</TOKEN>
<TOKEN end_char="2123" id="token-13-21" morph="none" pos="punct" start_char="2123">.</TOKEN>
</SEG>
<SEG end_char="2303" id="segment-14" start_char="2126">
<ORIGINAL_TEXT>"Most COVID-19 cases show mild influenza-like symptoms and it has been suggested that some uncharacterised influenza cases may have masked COVID-19 cases in the 2019-2020 season.</ORIGINAL_TEXT>
<TOKEN end_char="2126" id="token-14-0" morph="none" pos="punct" start_char="2126">"</TOKEN>
<TOKEN end_char="2130" id="token-14-1" morph="none" pos="word" start_char="2127">Most</TOKEN>
<TOKEN end_char="2139" id="token-14-2" morph="none" pos="unknown" start_char="2132">COVID-19</TOKEN>
<TOKEN end_char="2145" id="token-14-3" morph="none" pos="word" start_char="2141">cases</TOKEN>
<TOKEN end_char="2150" id="token-14-4" morph="none" pos="word" start_char="2147">show</TOKEN>
<TOKEN end_char="2155" id="token-14-5" morph="none" pos="word" start_char="2152">mild</TOKEN>
<TOKEN end_char="2170" id="token-14-6" morph="none" pos="unknown" start_char="2157">influenza-like</TOKEN>
<TOKEN end_char="2179" id="token-14-7" morph="none" pos="word" start_char="2172">symptoms</TOKEN>
<TOKEN end_char="2183" id="token-14-8" morph="none" pos="word" start_char="2181">and</TOKEN>
<TOKEN end_char="2186" id="token-14-9" morph="none" pos="word" start_char="2185">it</TOKEN>
<TOKEN end_char="2190" id="token-14-10" morph="none" pos="word" start_char="2188">has</TOKEN>
<TOKEN end_char="2195" id="token-14-11" morph="none" pos="word" start_char="2192">been</TOKEN>
<TOKEN end_char="2205" id="token-14-12" morph="none" pos="word" start_char="2197">suggested</TOKEN>
<TOKEN end_char="2210" id="token-14-13" morph="none" pos="word" start_char="2207">that</TOKEN>
<TOKEN end_char="2215" id="token-14-14" morph="none" pos="word" start_char="2212">some</TOKEN>
<TOKEN end_char="2231" id="token-14-15" morph="none" pos="word" start_char="2217">uncharacterised</TOKEN>
<TOKEN end_char="2241" id="token-14-16" morph="none" pos="word" start_char="2233">influenza</TOKEN>
<TOKEN end_char="2247" id="token-14-17" morph="none" pos="word" start_char="2243">cases</TOKEN>
<TOKEN end_char="2251" id="token-14-18" morph="none" pos="word" start_char="2249">may</TOKEN>
<TOKEN end_char="2256" id="token-14-19" morph="none" pos="word" start_char="2253">have</TOKEN>
<TOKEN end_char="2263" id="token-14-20" morph="none" pos="word" start_char="2258">masked</TOKEN>
<TOKEN end_char="2272" id="token-14-21" morph="none" pos="unknown" start_char="2265">COVID-19</TOKEN>
<TOKEN end_char="2278" id="token-14-22" morph="none" pos="word" start_char="2274">cases</TOKEN>
<TOKEN end_char="2281" id="token-14-23" morph="none" pos="word" start_char="2280">in</TOKEN>
<TOKEN end_char="2285" id="token-14-24" morph="none" pos="word" start_char="2283">the</TOKEN>
<TOKEN end_char="2295" id="token-14-25" morph="none" pos="unknown" start_char="2287">2019-2020</TOKEN>
<TOKEN end_char="2302" id="token-14-26" morph="none" pos="word" start_char="2297">season</TOKEN>
<TOKEN end_char="2303" id="token-14-27" morph="none" pos="punct" start_char="2303">.</TOKEN>
</SEG>
<SEG end_char="2406" id="segment-15" start_char="2305">
<ORIGINAL_TEXT>This possibility prompted us to analyse some archival WWTP samples from January 2018 to December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2308" id="token-15-0" morph="none" pos="word" start_char="2305">This</TOKEN>
<TOKEN end_char="2320" id="token-15-1" morph="none" pos="word" start_char="2310">possibility</TOKEN>
<TOKEN end_char="2329" id="token-15-2" morph="none" pos="word" start_char="2322">prompted</TOKEN>
<TOKEN end_char="2332" id="token-15-3" morph="none" pos="word" start_char="2331">us</TOKEN>
<TOKEN end_char="2335" id="token-15-4" morph="none" pos="word" start_char="2334">to</TOKEN>
<TOKEN end_char="2343" id="token-15-5" morph="none" pos="word" start_char="2337">analyse</TOKEN>
<TOKEN end_char="2348" id="token-15-6" morph="none" pos="word" start_char="2345">some</TOKEN>
<TOKEN end_char="2357" id="token-15-7" morph="none" pos="word" start_char="2350">archival</TOKEN>
<TOKEN end_char="2362" id="token-15-8" morph="none" pos="word" start_char="2359">WWTP</TOKEN>
<TOKEN end_char="2370" id="token-15-9" morph="none" pos="word" start_char="2364">samples</TOKEN>
<TOKEN end_char="2375" id="token-15-10" morph="none" pos="word" start_char="2372">from</TOKEN>
<TOKEN end_char="2383" id="token-15-11" morph="none" pos="word" start_char="2377">January</TOKEN>
<TOKEN end_char="2388" id="token-15-12" morph="none" pos="word" start_char="2385">2018</TOKEN>
<TOKEN end_char="2391" id="token-15-13" morph="none" pos="word" start_char="2390">to</TOKEN>
<TOKEN end_char="2400" id="token-15-14" morph="none" pos="word" start_char="2393">December</TOKEN>
<TOKEN end_char="2405" id="token-15-15" morph="none" pos="word" start_char="2402">2019</TOKEN>
<TOKEN end_char="2406" id="token-15-16" morph="none" pos="punct" start_char="2406">.</TOKEN>
</SEG>
<SEG end_char="2574" id="segment-16" start_char="2408">
<ORIGINAL_TEXT>All samples came out to be negative for the presence of SARS-CoV-2 genomes with the exception of March 12, 2019, in which both IP2 and IP4 target assays were positive.</ORIGINAL_TEXT>
<TOKEN end_char="2410" id="token-16-0" morph="none" pos="word" start_char="2408">All</TOKEN>
<TOKEN end_char="2418" id="token-16-1" morph="none" pos="word" start_char="2412">samples</TOKEN>
<TOKEN end_char="2423" id="token-16-2" morph="none" pos="word" start_char="2420">came</TOKEN>
<TOKEN end_char="2427" id="token-16-3" morph="none" pos="word" start_char="2425">out</TOKEN>
<TOKEN end_char="2430" id="token-16-4" morph="none" pos="word" start_char="2429">to</TOKEN>
<TOKEN end_char="2433" id="token-16-5" morph="none" pos="word" start_char="2432">be</TOKEN>
<TOKEN end_char="2442" id="token-16-6" morph="none" pos="word" start_char="2435">negative</TOKEN>
<TOKEN end_char="2446" id="token-16-7" morph="none" pos="word" start_char="2444">for</TOKEN>
<TOKEN end_char="2450" id="token-16-8" morph="none" pos="word" start_char="2448">the</TOKEN>
<TOKEN end_char="2459" id="token-16-9" morph="none" pos="word" start_char="2452">presence</TOKEN>
<TOKEN end_char="2462" id="token-16-10" morph="none" pos="word" start_char="2461">of</TOKEN>
<TOKEN end_char="2473" id="token-16-11" morph="none" pos="unknown" start_char="2464">SARS-CoV-2</TOKEN>
<TOKEN end_char="2481" id="token-16-12" morph="none" pos="word" start_char="2475">genomes</TOKEN>
<TOKEN end_char="2486" id="token-16-13" morph="none" pos="word" start_char="2483">with</TOKEN>
<TOKEN end_char="2490" id="token-16-14" morph="none" pos="word" start_char="2488">the</TOKEN>
<TOKEN end_char="2500" id="token-16-15" morph="none" pos="word" start_char="2492">exception</TOKEN>
<TOKEN end_char="2503" id="token-16-16" morph="none" pos="word" start_char="2502">of</TOKEN>
<TOKEN end_char="2509" id="token-16-17" morph="none" pos="word" start_char="2505">March</TOKEN>
<TOKEN end_char="2512" id="token-16-18" morph="none" pos="word" start_char="2511">12</TOKEN>
<TOKEN end_char="2513" id="token-16-19" morph="none" pos="punct" start_char="2513">,</TOKEN>
<TOKEN end_char="2518" id="token-16-20" morph="none" pos="word" start_char="2515">2019</TOKEN>
<TOKEN end_char="2519" id="token-16-21" morph="none" pos="punct" start_char="2519">,</TOKEN>
<TOKEN end_char="2522" id="token-16-22" morph="none" pos="word" start_char="2521">in</TOKEN>
<TOKEN end_char="2528" id="token-16-23" morph="none" pos="word" start_char="2524">which</TOKEN>
<TOKEN end_char="2533" id="token-16-24" morph="none" pos="word" start_char="2530">both</TOKEN>
<TOKEN end_char="2537" id="token-16-25" morph="none" pos="word" start_char="2535">IP2</TOKEN>
<TOKEN end_char="2541" id="token-16-26" morph="none" pos="word" start_char="2539">and</TOKEN>
<TOKEN end_char="2545" id="token-16-27" morph="none" pos="word" start_char="2543">IP4</TOKEN>
<TOKEN end_char="2552" id="token-16-28" morph="none" pos="word" start_char="2547">target</TOKEN>
<TOKEN end_char="2559" id="token-16-29" morph="none" pos="word" start_char="2554">assays</TOKEN>
<TOKEN end_char="2564" id="token-16-30" morph="none" pos="word" start_char="2561">were</TOKEN>
<TOKEN end_char="2573" id="token-16-31" morph="none" pos="word" start_char="2566">positive</TOKEN>
<TOKEN end_char="2574" id="token-16-32" morph="none" pos="punct" start_char="2574">.</TOKEN>
</SEG>
<SEG end_char="2716" id="segment-17" start_char="2576">
<ORIGINAL_TEXT>This striking finding indicates circulation of the virus in Barcelona long before the report of any COVID-19 case worldwide," the study says.</ORIGINAL_TEXT>
<TOKEN end_char="2579" id="token-17-0" morph="none" pos="word" start_char="2576">This</TOKEN>
<TOKEN end_char="2588" id="token-17-1" morph="none" pos="word" start_char="2581">striking</TOKEN>
<TOKEN end_char="2596" id="token-17-2" morph="none" pos="word" start_char="2590">finding</TOKEN>
<TOKEN end_char="2606" id="token-17-3" morph="none" pos="word" start_char="2598">indicates</TOKEN>
<TOKEN end_char="2618" id="token-17-4" morph="none" pos="word" start_char="2608">circulation</TOKEN>
<TOKEN end_char="2621" id="token-17-5" morph="none" pos="word" start_char="2620">of</TOKEN>
<TOKEN end_char="2625" id="token-17-6" morph="none" pos="word" start_char="2623">the</TOKEN>
<TOKEN end_char="2631" id="token-17-7" morph="none" pos="word" start_char="2627">virus</TOKEN>
<TOKEN end_char="2634" id="token-17-8" morph="none" pos="word" start_char="2633">in</TOKEN>
<TOKEN end_char="2644" id="token-17-9" morph="none" pos="word" start_char="2636">Barcelona</TOKEN>
<TOKEN end_char="2649" id="token-17-10" morph="none" pos="word" start_char="2646">long</TOKEN>
<TOKEN end_char="2656" id="token-17-11" morph="none" pos="word" start_char="2651">before</TOKEN>
<TOKEN end_char="2660" id="token-17-12" morph="none" pos="word" start_char="2658">the</TOKEN>
<TOKEN end_char="2667" id="token-17-13" morph="none" pos="word" start_char="2662">report</TOKEN>
<TOKEN end_char="2670" id="token-17-14" morph="none" pos="word" start_char="2669">of</TOKEN>
<TOKEN end_char="2674" id="token-17-15" morph="none" pos="word" start_char="2672">any</TOKEN>
<TOKEN end_char="2683" id="token-17-16" morph="none" pos="unknown" start_char="2676">COVID-19</TOKEN>
<TOKEN end_char="2688" id="token-17-17" morph="none" pos="word" start_char="2685">case</TOKEN>
<TOKEN end_char="2698" id="token-17-18" morph="none" pos="word" start_char="2690">worldwide</TOKEN>
<TOKEN end_char="2700" id="token-17-19" morph="none" pos="punct" start_char="2699">,"</TOKEN>
<TOKEN end_char="2704" id="token-17-20" morph="none" pos="word" start_char="2702">the</TOKEN>
<TOKEN end_char="2710" id="token-17-21" morph="none" pos="word" start_char="2706">study</TOKEN>
<TOKEN end_char="2715" id="token-17-22" morph="none" pos="word" start_char="2712">says</TOKEN>
<TOKEN end_char="2716" id="token-17-23" morph="none" pos="punct" start_char="2716">.</TOKEN>
</SEG>
<SEG end_char="2798" id="segment-18" start_char="2719">
<ORIGINAL_TEXT>The lead author, Albert Bosch, has studied wastewater viruses for over 40 years.</ORIGINAL_TEXT>
<TOKEN end_char="2721" id="token-18-0" morph="none" pos="word" start_char="2719">The</TOKEN>
<TOKEN end_char="2726" id="token-18-1" morph="none" pos="word" start_char="2723">lead</TOKEN>
<TOKEN end_char="2733" id="token-18-2" morph="none" pos="word" start_char="2728">author</TOKEN>
<TOKEN end_char="2734" id="token-18-3" morph="none" pos="punct" start_char="2734">,</TOKEN>
<TOKEN end_char="2741" id="token-18-4" morph="none" pos="word" start_char="2736">Albert</TOKEN>
<TOKEN end_char="2747" id="token-18-5" morph="none" pos="word" start_char="2743">Bosch</TOKEN>
<TOKEN end_char="2748" id="token-18-6" morph="none" pos="punct" start_char="2748">,</TOKEN>
<TOKEN end_char="2752" id="token-18-7" morph="none" pos="word" start_char="2750">has</TOKEN>
<TOKEN end_char="2760" id="token-18-8" morph="none" pos="word" start_char="2754">studied</TOKEN>
<TOKEN end_char="2771" id="token-18-9" morph="none" pos="word" start_char="2762">wastewater</TOKEN>
<TOKEN end_char="2779" id="token-18-10" morph="none" pos="word" start_char="2773">viruses</TOKEN>
<TOKEN end_char="2783" id="token-18-11" morph="none" pos="word" start_char="2781">for</TOKEN>
<TOKEN end_char="2788" id="token-18-12" morph="none" pos="word" start_char="2785">over</TOKEN>
<TOKEN end_char="2791" id="token-18-13" morph="none" pos="word" start_char="2790">40</TOKEN>
<TOKEN end_char="2797" id="token-18-14" morph="none" pos="word" start_char="2793">years</TOKEN>
<TOKEN end_char="2798" id="token-18-15" morph="none" pos="punct" start_char="2798">.</TOKEN>
</SEG>
<SEG end_char="2849" id="segment-19" start_char="2800">
<ORIGINAL_TEXT>However, the study’s results are being challenged.</ORIGINAL_TEXT>
<TOKEN end_char="2806" id="token-19-0" morph="none" pos="word" start_char="2800">However</TOKEN>
<TOKEN end_char="2807" id="token-19-1" morph="none" pos="punct" start_char="2807">,</TOKEN>
<TOKEN end_char="2811" id="token-19-2" morph="none" pos="word" start_char="2809">the</TOKEN>
<TOKEN end_char="2819" id="token-19-3" morph="none" pos="word" start_char="2813">study’s</TOKEN>
<TOKEN end_char="2827" id="token-19-4" morph="none" pos="word" start_char="2821">results</TOKEN>
<TOKEN end_char="2831" id="token-19-5" morph="none" pos="word" start_char="2829">are</TOKEN>
<TOKEN end_char="2837" id="token-19-6" morph="none" pos="word" start_char="2833">being</TOKEN>
<TOKEN end_char="2848" id="token-19-7" morph="none" pos="word" start_char="2839">challenged</TOKEN>
<TOKEN end_char="2849" id="token-19-8" morph="none" pos="punct" start_char="2849">.</TOKEN>
</SEG>
<SEG end_char="3117" id="segment-20" start_char="2851">
<ORIGINAL_TEXT>Dutch microbiologist Elisabeth Bik points out in a post on the Science Integrity Digest blog that while the study appears to show evidence of the virus being present in Spain a few weeks before it was officially discovered, the test from March 2019 is not conclusive.</ORIGINAL_TEXT>
<TOKEN end_char="2855" id="token-20-0" morph="none" pos="word" start_char="2851">Dutch</TOKEN>
<TOKEN end_char="2870" id="token-20-1" morph="none" pos="word" start_char="2857">microbiologist</TOKEN>
<TOKEN end_char="2880" id="token-20-2" morph="none" pos="word" start_char="2872">Elisabeth</TOKEN>
<TOKEN end_char="2884" id="token-20-3" morph="none" pos="word" start_char="2882">Bik</TOKEN>
<TOKEN end_char="2891" id="token-20-4" morph="none" pos="word" start_char="2886">points</TOKEN>
<TOKEN end_char="2895" id="token-20-5" morph="none" pos="word" start_char="2893">out</TOKEN>
<TOKEN end_char="2898" id="token-20-6" morph="none" pos="word" start_char="2897">in</TOKEN>
<TOKEN end_char="2900" id="token-20-7" morph="none" pos="word" start_char="2900">a</TOKEN>
<TOKEN end_char="2905" id="token-20-8" morph="none" pos="word" start_char="2902">post</TOKEN>
<TOKEN end_char="2908" id="token-20-9" morph="none" pos="word" start_char="2907">on</TOKEN>
<TOKEN end_char="2912" id="token-20-10" morph="none" pos="word" start_char="2910">the</TOKEN>
<TOKEN end_char="2920" id="token-20-11" morph="none" pos="word" start_char="2914">Science</TOKEN>
<TOKEN end_char="2930" id="token-20-12" morph="none" pos="word" start_char="2922">Integrity</TOKEN>
<TOKEN end_char="2937" id="token-20-13" morph="none" pos="word" start_char="2932">Digest</TOKEN>
<TOKEN end_char="2942" id="token-20-14" morph="none" pos="word" start_char="2939">blog</TOKEN>
<TOKEN end_char="2947" id="token-20-15" morph="none" pos="word" start_char="2944">that</TOKEN>
<TOKEN end_char="2953" id="token-20-16" morph="none" pos="word" start_char="2949">while</TOKEN>
<TOKEN end_char="2957" id="token-20-17" morph="none" pos="word" start_char="2955">the</TOKEN>
<TOKEN end_char="2963" id="token-20-18" morph="none" pos="word" start_char="2959">study</TOKEN>
<TOKEN end_char="2971" id="token-20-19" morph="none" pos="word" start_char="2965">appears</TOKEN>
<TOKEN end_char="2974" id="token-20-20" morph="none" pos="word" start_char="2973">to</TOKEN>
<TOKEN end_char="2979" id="token-20-21" morph="none" pos="word" start_char="2976">show</TOKEN>
<TOKEN end_char="2988" id="token-20-22" morph="none" pos="word" start_char="2981">evidence</TOKEN>
<TOKEN end_char="2991" id="token-20-23" morph="none" pos="word" start_char="2990">of</TOKEN>
<TOKEN end_char="2995" id="token-20-24" morph="none" pos="word" start_char="2993">the</TOKEN>
<TOKEN end_char="3001" id="token-20-25" morph="none" pos="word" start_char="2997">virus</TOKEN>
<TOKEN end_char="3007" id="token-20-26" morph="none" pos="word" start_char="3003">being</TOKEN>
<TOKEN end_char="3015" id="token-20-27" morph="none" pos="word" start_char="3009">present</TOKEN>
<TOKEN end_char="3018" id="token-20-28" morph="none" pos="word" start_char="3017">in</TOKEN>
<TOKEN end_char="3024" id="token-20-29" morph="none" pos="word" start_char="3020">Spain</TOKEN>
<TOKEN end_char="3026" id="token-20-30" morph="none" pos="word" start_char="3026">a</TOKEN>
<TOKEN end_char="3030" id="token-20-31" morph="none" pos="word" start_char="3028">few</TOKEN>
<TOKEN end_char="3036" id="token-20-32" morph="none" pos="word" start_char="3032">weeks</TOKEN>
<TOKEN end_char="3043" id="token-20-33" morph="none" pos="word" start_char="3038">before</TOKEN>
<TOKEN end_char="3046" id="token-20-34" morph="none" pos="word" start_char="3045">it</TOKEN>
<TOKEN end_char="3050" id="token-20-35" morph="none" pos="word" start_char="3048">was</TOKEN>
<TOKEN end_char="3061" id="token-20-36" morph="none" pos="word" start_char="3052">officially</TOKEN>
<TOKEN end_char="3072" id="token-20-37" morph="none" pos="word" start_char="3063">discovered</TOKEN>
<TOKEN end_char="3073" id="token-20-38" morph="none" pos="punct" start_char="3073">,</TOKEN>
<TOKEN end_char="3077" id="token-20-39" morph="none" pos="word" start_char="3075">the</TOKEN>
<TOKEN end_char="3082" id="token-20-40" morph="none" pos="word" start_char="3079">test</TOKEN>
<TOKEN end_char="3087" id="token-20-41" morph="none" pos="word" start_char="3084">from</TOKEN>
<TOKEN end_char="3093" id="token-20-42" morph="none" pos="word" start_char="3089">March</TOKEN>
<TOKEN end_char="3098" id="token-20-43" morph="none" pos="word" start_char="3095">2019</TOKEN>
<TOKEN end_char="3101" id="token-20-44" morph="none" pos="word" start_char="3100">is</TOKEN>
<TOKEN end_char="3105" id="token-20-45" morph="none" pos="word" start_char="3103">not</TOKEN>
<TOKEN end_char="3116" id="token-20-46" morph="none" pos="word" start_char="3107">conclusive</TOKEN>
<TOKEN end_char="3117" id="token-20-47" morph="none" pos="punct" start_char="3117">.</TOKEN>
</SEG>
<SEG end_char="3250" id="segment-21" start_char="3119">
<ORIGINAL_TEXT>Only one sample was tested, and of the WHO-recommended tests for SARS-CoV-2, only two of five were positive, on IP2 and IP4 targets.</ORIGINAL_TEXT>
<TOKEN end_char="3122" id="token-21-0" morph="none" pos="word" start_char="3119">Only</TOKEN>
<TOKEN end_char="3126" id="token-21-1" morph="none" pos="word" start_char="3124">one</TOKEN>
<TOKEN end_char="3133" id="token-21-2" morph="none" pos="word" start_char="3128">sample</TOKEN>
<TOKEN end_char="3137" id="token-21-3" morph="none" pos="word" start_char="3135">was</TOKEN>
<TOKEN end_char="3144" id="token-21-4" morph="none" pos="word" start_char="3139">tested</TOKEN>
<TOKEN end_char="3145" id="token-21-5" morph="none" pos="punct" start_char="3145">,</TOKEN>
<TOKEN end_char="3149" id="token-21-6" morph="none" pos="word" start_char="3147">and</TOKEN>
<TOKEN end_char="3152" id="token-21-7" morph="none" pos="word" start_char="3151">of</TOKEN>
<TOKEN end_char="3156" id="token-21-8" morph="none" pos="word" start_char="3154">the</TOKEN>
<TOKEN end_char="3172" id="token-21-9" morph="none" pos="unknown" start_char="3158">WHO-recommended</TOKEN>
<TOKEN end_char="3178" id="token-21-10" morph="none" pos="word" start_char="3174">tests</TOKEN>
<TOKEN end_char="3182" id="token-21-11" morph="none" pos="word" start_char="3180">for</TOKEN>
<TOKEN end_char="3193" id="token-21-12" morph="none" pos="unknown" start_char="3184">SARS-CoV-2</TOKEN>
<TOKEN end_char="3194" id="token-21-13" morph="none" pos="punct" start_char="3194">,</TOKEN>
<TOKEN end_char="3199" id="token-21-14" morph="none" pos="word" start_char="3196">only</TOKEN>
<TOKEN end_char="3203" id="token-21-15" morph="none" pos="word" start_char="3201">two</TOKEN>
<TOKEN end_char="3206" id="token-21-16" morph="none" pos="word" start_char="3205">of</TOKEN>
<TOKEN end_char="3211" id="token-21-17" morph="none" pos="word" start_char="3208">five</TOKEN>
<TOKEN end_char="3216" id="token-21-18" morph="none" pos="word" start_char="3213">were</TOKEN>
<TOKEN end_char="3225" id="token-21-19" morph="none" pos="word" start_char="3218">positive</TOKEN>
<TOKEN end_char="3226" id="token-21-20" morph="none" pos="punct" start_char="3226">,</TOKEN>
<TOKEN end_char="3229" id="token-21-21" morph="none" pos="word" start_char="3228">on</TOKEN>
<TOKEN end_char="3233" id="token-21-22" morph="none" pos="word" start_char="3231">IP2</TOKEN>
<TOKEN end_char="3237" id="token-21-23" morph="none" pos="word" start_char="3235">and</TOKEN>
<TOKEN end_char="3241" id="token-21-24" morph="none" pos="word" start_char="3239">IP4</TOKEN>
<TOKEN end_char="3249" id="token-21-25" morph="none" pos="word" start_char="3243">targets</TOKEN>
<TOKEN end_char="3250" id="token-21-26" morph="none" pos="punct" start_char="3250">.</TOKEN>
</SEG>
<SEG end_char="3383" id="segment-22" start_char="3253">
<ORIGINAL_TEXT>Bik points out that the WHO norms suggest an ‘E gene’ assay (investigative procedure to assess the presence of an analyte) be done.</ORIGINAL_TEXT>
<TOKEN end_char="3255" id="token-22-0" morph="none" pos="word" start_char="3253">Bik</TOKEN>
<TOKEN end_char="3262" id="token-22-1" morph="none" pos="word" start_char="3257">points</TOKEN>
<TOKEN end_char="3266" id="token-22-2" morph="none" pos="word" start_char="3264">out</TOKEN>
<TOKEN end_char="3271" id="token-22-3" morph="none" pos="word" start_char="3268">that</TOKEN>
<TOKEN end_char="3275" id="token-22-4" morph="none" pos="word" start_char="3273">the</TOKEN>
<TOKEN end_char="3279" id="token-22-5" morph="none" pos="word" start_char="3277">WHO</TOKEN>
<TOKEN end_char="3285" id="token-22-6" morph="none" pos="word" start_char="3281">norms</TOKEN>
<TOKEN end_char="3293" id="token-22-7" morph="none" pos="word" start_char="3287">suggest</TOKEN>
<TOKEN end_char="3296" id="token-22-8" morph="none" pos="word" start_char="3295">an</TOKEN>
<TOKEN end_char="3298" id="token-22-9" morph="none" pos="punct" start_char="3298">‘</TOKEN>
<TOKEN end_char="3299" id="token-22-10" morph="none" pos="word" start_char="3299">E</TOKEN>
<TOKEN end_char="3304" id="token-22-11" morph="none" pos="word" start_char="3301">gene</TOKEN>
<TOKEN end_char="3305" id="token-22-12" morph="none" pos="punct" start_char="3305">’</TOKEN>
<TOKEN end_char="3311" id="token-22-13" morph="none" pos="word" start_char="3307">assay</TOKEN>
<TOKEN end_char="3313" id="token-22-14" morph="none" pos="punct" start_char="3313">(</TOKEN>
<TOKEN end_char="3326" id="token-22-15" morph="none" pos="word" start_char="3314">investigative</TOKEN>
<TOKEN end_char="3336" id="token-22-16" morph="none" pos="word" start_char="3328">procedure</TOKEN>
<TOKEN end_char="3339" id="token-22-17" morph="none" pos="word" start_char="3338">to</TOKEN>
<TOKEN end_char="3346" id="token-22-18" morph="none" pos="word" start_char="3341">assess</TOKEN>
<TOKEN end_char="3350" id="token-22-19" morph="none" pos="word" start_char="3348">the</TOKEN>
<TOKEN end_char="3359" id="token-22-20" morph="none" pos="word" start_char="3352">presence</TOKEN>
<TOKEN end_char="3362" id="token-22-21" morph="none" pos="word" start_char="3361">of</TOKEN>
<TOKEN end_char="3365" id="token-22-22" morph="none" pos="word" start_char="3364">an</TOKEN>
<TOKEN end_char="3373" id="token-22-23" morph="none" pos="word" start_char="3367">analyte</TOKEN>
<TOKEN end_char="3374" id="token-22-24" morph="none" pos="punct" start_char="3374">)</TOKEN>
<TOKEN end_char="3377" id="token-22-25" morph="none" pos="word" start_char="3376">be</TOKEN>
<TOKEN end_char="3382" id="token-22-26" morph="none" pos="word" start_char="3379">done</TOKEN>
<TOKEN end_char="3383" id="token-22-27" morph="none" pos="punct" start_char="3383">.</TOKEN>
</SEG>
<SEG end_char="3438" id="segment-23" start_char="3385">
<ORIGINAL_TEXT>However, in the UoB study, this test came up negative.</ORIGINAL_TEXT>
<TOKEN end_char="3391" id="token-23-0" morph="none" pos="word" start_char="3385">However</TOKEN>
<TOKEN end_char="3392" id="token-23-1" morph="none" pos="punct" start_char="3392">,</TOKEN>
<TOKEN end_char="3395" id="token-23-2" morph="none" pos="word" start_char="3394">in</TOKEN>
<TOKEN end_char="3399" id="token-23-3" morph="none" pos="word" start_char="3397">the</TOKEN>
<TOKEN end_char="3403" id="token-23-4" morph="none" pos="word" start_char="3401">UoB</TOKEN>
<TOKEN end_char="3409" id="token-23-5" morph="none" pos="word" start_char="3405">study</TOKEN>
<TOKEN end_char="3410" id="token-23-6" morph="none" pos="punct" start_char="3410">,</TOKEN>
<TOKEN end_char="3415" id="token-23-7" morph="none" pos="word" start_char="3412">this</TOKEN>
<TOKEN end_char="3420" id="token-23-8" morph="none" pos="word" start_char="3417">test</TOKEN>
<TOKEN end_char="3425" id="token-23-9" morph="none" pos="word" start_char="3422">came</TOKEN>
<TOKEN end_char="3428" id="token-23-10" morph="none" pos="word" start_char="3427">up</TOKEN>
<TOKEN end_char="3437" id="token-23-11" morph="none" pos="word" start_char="3430">negative</TOKEN>
<TOKEN end_char="3438" id="token-23-12" morph="none" pos="punct" start_char="3438">.</TOKEN>
</SEG>
<SEG end_char="3615" id="segment-24" start_char="3440">
<ORIGINAL_TEXT>This, combined with the nine-month gap between the March sample and the next positive, suggests that more research and peer-review is required to validate the study’s findings.</ORIGINAL_TEXT>
<TOKEN end_char="3443" id="token-24-0" morph="none" pos="word" start_char="3440">This</TOKEN>
<TOKEN end_char="3444" id="token-24-1" morph="none" pos="punct" start_char="3444">,</TOKEN>
<TOKEN end_char="3453" id="token-24-2" morph="none" pos="word" start_char="3446">combined</TOKEN>
<TOKEN end_char="3458" id="token-24-3" morph="none" pos="word" start_char="3455">with</TOKEN>
<TOKEN end_char="3462" id="token-24-4" morph="none" pos="word" start_char="3460">the</TOKEN>
<TOKEN end_char="3473" id="token-24-5" morph="none" pos="unknown" start_char="3464">nine-month</TOKEN>
<TOKEN end_char="3477" id="token-24-6" morph="none" pos="word" start_char="3475">gap</TOKEN>
<TOKEN end_char="3485" id="token-24-7" morph="none" pos="word" start_char="3479">between</TOKEN>
<TOKEN end_char="3489" id="token-24-8" morph="none" pos="word" start_char="3487">the</TOKEN>
<TOKEN end_char="3495" id="token-24-9" morph="none" pos="word" start_char="3491">March</TOKEN>
<TOKEN end_char="3502" id="token-24-10" morph="none" pos="word" start_char="3497">sample</TOKEN>
<TOKEN end_char="3506" id="token-24-11" morph="none" pos="word" start_char="3504">and</TOKEN>
<TOKEN end_char="3510" id="token-24-12" morph="none" pos="word" start_char="3508">the</TOKEN>
<TOKEN end_char="3515" id="token-24-13" morph="none" pos="word" start_char="3512">next</TOKEN>
<TOKEN end_char="3524" id="token-24-14" morph="none" pos="word" start_char="3517">positive</TOKEN>
<TOKEN end_char="3525" id="token-24-15" morph="none" pos="punct" start_char="3525">,</TOKEN>
<TOKEN end_char="3534" id="token-24-16" morph="none" pos="word" start_char="3527">suggests</TOKEN>
<TOKEN end_char="3539" id="token-24-17" morph="none" pos="word" start_char="3536">that</TOKEN>
<TOKEN end_char="3544" id="token-24-18" morph="none" pos="word" start_char="3541">more</TOKEN>
<TOKEN end_char="3553" id="token-24-19" morph="none" pos="word" start_char="3546">research</TOKEN>
<TOKEN end_char="3557" id="token-24-20" morph="none" pos="word" start_char="3555">and</TOKEN>
<TOKEN end_char="3569" id="token-24-21" morph="none" pos="unknown" start_char="3559">peer-review</TOKEN>
<TOKEN end_char="3572" id="token-24-22" morph="none" pos="word" start_char="3571">is</TOKEN>
<TOKEN end_char="3581" id="token-24-23" morph="none" pos="word" start_char="3574">required</TOKEN>
<TOKEN end_char="3584" id="token-24-24" morph="none" pos="word" start_char="3583">to</TOKEN>
<TOKEN end_char="3593" id="token-24-25" morph="none" pos="word" start_char="3586">validate</TOKEN>
<TOKEN end_char="3597" id="token-24-26" morph="none" pos="word" start_char="3595">the</TOKEN>
<TOKEN end_char="3605" id="token-24-27" morph="none" pos="word" start_char="3599">study’s</TOKEN>
<TOKEN end_char="3614" id="token-24-28" morph="none" pos="word" start_char="3607">findings</TOKEN>
<TOKEN end_char="3615" id="token-24-29" morph="none" pos="punct" start_char="3615">.</TOKEN>
</SEG>
<SEG end_char="3723" id="segment-25" start_char="3618">
<ORIGINAL_TEXT>This is not the first time China has tried to suggest that the coronavirus emerged outside of its borders.</ORIGINAL_TEXT>
<TOKEN end_char="3621" id="token-25-0" morph="none" pos="word" start_char="3618">This</TOKEN>
<TOKEN end_char="3624" id="token-25-1" morph="none" pos="word" start_char="3623">is</TOKEN>
<TOKEN end_char="3628" id="token-25-2" morph="none" pos="word" start_char="3626">not</TOKEN>
<TOKEN end_char="3632" id="token-25-3" morph="none" pos="word" start_char="3630">the</TOKEN>
<TOKEN end_char="3638" id="token-25-4" morph="none" pos="word" start_char="3634">first</TOKEN>
<TOKEN end_char="3643" id="token-25-5" morph="none" pos="word" start_char="3640">time</TOKEN>
<TOKEN end_char="3649" id="token-25-6" morph="none" pos="word" start_char="3645">China</TOKEN>
<TOKEN end_char="3653" id="token-25-7" morph="none" pos="word" start_char="3651">has</TOKEN>
<TOKEN end_char="3659" id="token-25-8" morph="none" pos="word" start_char="3655">tried</TOKEN>
<TOKEN end_char="3662" id="token-25-9" morph="none" pos="word" start_char="3661">to</TOKEN>
<TOKEN end_char="3670" id="token-25-10" morph="none" pos="word" start_char="3664">suggest</TOKEN>
<TOKEN end_char="3675" id="token-25-11" morph="none" pos="word" start_char="3672">that</TOKEN>
<TOKEN end_char="3679" id="token-25-12" morph="none" pos="word" start_char="3677">the</TOKEN>
<TOKEN end_char="3691" id="token-25-13" morph="none" pos="word" start_char="3681">coronavirus</TOKEN>
<TOKEN end_char="3699" id="token-25-14" morph="none" pos="word" start_char="3693">emerged</TOKEN>
<TOKEN end_char="3707" id="token-25-15" morph="none" pos="word" start_char="3701">outside</TOKEN>
<TOKEN end_char="3710" id="token-25-16" morph="none" pos="word" start_char="3709">of</TOKEN>
<TOKEN end_char="3714" id="token-25-17" morph="none" pos="word" start_char="3712">its</TOKEN>
<TOKEN end_char="3722" id="token-25-18" morph="none" pos="word" start_char="3716">borders</TOKEN>
<TOKEN end_char="3723" id="token-25-19" morph="none" pos="punct" start_char="3723">.</TOKEN>
</SEG>
<SEG end_char="3925" id="segment-26" start_char="3725">
<ORIGINAL_TEXT>In March, China’s Foreign Ministry spokesperson Zhao Lijian suggested that the US Army could have brought the virus to Wuhan during the Military World games that took place in the city in October 2019.</ORIGINAL_TEXT>
<TOKEN end_char="3726" id="token-26-0" morph="none" pos="word" start_char="3725">In</TOKEN>
<TOKEN end_char="3732" id="token-26-1" morph="none" pos="word" start_char="3728">March</TOKEN>
<TOKEN end_char="3733" id="token-26-2" morph="none" pos="punct" start_char="3733">,</TOKEN>
<TOKEN end_char="3741" id="token-26-3" morph="none" pos="word" start_char="3735">China’s</TOKEN>
<TOKEN end_char="3749" id="token-26-4" morph="none" pos="word" start_char="3743">Foreign</TOKEN>
<TOKEN end_char="3758" id="token-26-5" morph="none" pos="word" start_char="3751">Ministry</TOKEN>
<TOKEN end_char="3771" id="token-26-6" morph="none" pos="word" start_char="3760">spokesperson</TOKEN>
<TOKEN end_char="3776" id="token-26-7" morph="none" pos="word" start_char="3773">Zhao</TOKEN>
<TOKEN end_char="3783" id="token-26-8" morph="none" pos="word" start_char="3778">Lijian</TOKEN>
<TOKEN end_char="3793" id="token-26-9" morph="none" pos="word" start_char="3785">suggested</TOKEN>
<TOKEN end_char="3798" id="token-26-10" morph="none" pos="word" start_char="3795">that</TOKEN>
<TOKEN end_char="3802" id="token-26-11" morph="none" pos="word" start_char="3800">the</TOKEN>
<TOKEN end_char="3805" id="token-26-12" morph="none" pos="word" start_char="3804">US</TOKEN>
<TOKEN end_char="3810" id="token-26-13" morph="none" pos="word" start_char="3807">Army</TOKEN>
<TOKEN end_char="3816" id="token-26-14" morph="none" pos="word" start_char="3812">could</TOKEN>
<TOKEN end_char="3821" id="token-26-15" morph="none" pos="word" start_char="3818">have</TOKEN>
<TOKEN end_char="3829" id="token-26-16" morph="none" pos="word" start_char="3823">brought</TOKEN>
<TOKEN end_char="3833" id="token-26-17" morph="none" pos="word" start_char="3831">the</TOKEN>
<TOKEN end_char="3839" id="token-26-18" morph="none" pos="word" start_char="3835">virus</TOKEN>
<TOKEN end_char="3842" id="token-26-19" morph="none" pos="word" start_char="3841">to</TOKEN>
<TOKEN end_char="3848" id="token-26-20" morph="none" pos="word" start_char="3844">Wuhan</TOKEN>
<TOKEN end_char="3855" id="token-26-21" morph="none" pos="word" start_char="3850">during</TOKEN>
<TOKEN end_char="3859" id="token-26-22" morph="none" pos="word" start_char="3857">the</TOKEN>
<TOKEN end_char="3868" id="token-26-23" morph="none" pos="word" start_char="3861">Military</TOKEN>
<TOKEN end_char="3874" id="token-26-24" morph="none" pos="word" start_char="3870">World</TOKEN>
<TOKEN end_char="3880" id="token-26-25" morph="none" pos="word" start_char="3876">games</TOKEN>
<TOKEN end_char="3885" id="token-26-26" morph="none" pos="word" start_char="3882">that</TOKEN>
<TOKEN end_char="3890" id="token-26-27" morph="none" pos="word" start_char="3887">took</TOKEN>
<TOKEN end_char="3896" id="token-26-28" morph="none" pos="word" start_char="3892">place</TOKEN>
<TOKEN end_char="3899" id="token-26-29" morph="none" pos="word" start_char="3898">in</TOKEN>
<TOKEN end_char="3903" id="token-26-30" morph="none" pos="word" start_char="3901">the</TOKEN>
<TOKEN end_char="3908" id="token-26-31" morph="none" pos="word" start_char="3905">city</TOKEN>
<TOKEN end_char="3911" id="token-26-32" morph="none" pos="word" start_char="3910">in</TOKEN>
<TOKEN end_char="3919" id="token-26-33" morph="none" pos="word" start_char="3913">October</TOKEN>
<TOKEN end_char="3924" id="token-26-34" morph="none" pos="word" start_char="3921">2019</TOKEN>
<TOKEN end_char="3925" id="token-26-35" morph="none" pos="punct" start_char="3925">.</TOKEN>
</SEG>
<SEG end_char="4062" id="segment-27" start_char="3927">
<ORIGINAL_TEXT>Several athletes who attended the games claimed to have suffered symptoms similar to that of the coronavirus during their stay in China.</ORIGINAL_TEXT>
<TOKEN end_char="3933" id="token-27-0" morph="none" pos="word" start_char="3927">Several</TOKEN>
<TOKEN end_char="3942" id="token-27-1" morph="none" pos="word" start_char="3935">athletes</TOKEN>
<TOKEN end_char="3946" id="token-27-2" morph="none" pos="word" start_char="3944">who</TOKEN>
<TOKEN end_char="3955" id="token-27-3" morph="none" pos="word" start_char="3948">attended</TOKEN>
<TOKEN end_char="3959" id="token-27-4" morph="none" pos="word" start_char="3957">the</TOKEN>
<TOKEN end_char="3965" id="token-27-5" morph="none" pos="word" start_char="3961">games</TOKEN>
<TOKEN end_char="3973" id="token-27-6" morph="none" pos="word" start_char="3967">claimed</TOKEN>
<TOKEN end_char="3976" id="token-27-7" morph="none" pos="word" start_char="3975">to</TOKEN>
<TOKEN end_char="3981" id="token-27-8" morph="none" pos="word" start_char="3978">have</TOKEN>
<TOKEN end_char="3990" id="token-27-9" morph="none" pos="word" start_char="3983">suffered</TOKEN>
<TOKEN end_char="3999" id="token-27-10" morph="none" pos="word" start_char="3992">symptoms</TOKEN>
<TOKEN end_char="4007" id="token-27-11" morph="none" pos="word" start_char="4001">similar</TOKEN>
<TOKEN end_char="4010" id="token-27-12" morph="none" pos="word" start_char="4009">to</TOKEN>
<TOKEN end_char="4015" id="token-27-13" morph="none" pos="word" start_char="4012">that</TOKEN>
<TOKEN end_char="4018" id="token-27-14" morph="none" pos="word" start_char="4017">of</TOKEN>
<TOKEN end_char="4022" id="token-27-15" morph="none" pos="word" start_char="4020">the</TOKEN>
<TOKEN end_char="4034" id="token-27-16" morph="none" pos="word" start_char="4024">coronavirus</TOKEN>
<TOKEN end_char="4041" id="token-27-17" morph="none" pos="word" start_char="4036">during</TOKEN>
<TOKEN end_char="4047" id="token-27-18" morph="none" pos="word" start_char="4043">their</TOKEN>
<TOKEN end_char="4052" id="token-27-19" morph="none" pos="word" start_char="4049">stay</TOKEN>
<TOKEN end_char="4055" id="token-27-20" morph="none" pos="word" start_char="4054">in</TOKEN>
<TOKEN end_char="4061" id="token-27-21" morph="none" pos="word" start_char="4057">China</TOKEN>
<TOKEN end_char="4062" id="token-27-22" morph="none" pos="punct" start_char="4062">.</TOKEN>
</SEG>
<SEG end_char="4151" id="segment-28" start_char="4065">
<ORIGINAL_TEXT>There are other reports that suggest an earlier origin to the coronavirus than thought.</ORIGINAL_TEXT>
<TOKEN end_char="4069" id="token-28-0" morph="none" pos="word" start_char="4065">There</TOKEN>
<TOKEN end_char="4073" id="token-28-1" morph="none" pos="word" start_char="4071">are</TOKEN>
<TOKEN end_char="4079" id="token-28-2" morph="none" pos="word" start_char="4075">other</TOKEN>
<TOKEN end_char="4087" id="token-28-3" morph="none" pos="word" start_char="4081">reports</TOKEN>
<TOKEN end_char="4092" id="token-28-4" morph="none" pos="word" start_char="4089">that</TOKEN>
<TOKEN end_char="4100" id="token-28-5" morph="none" pos="word" start_char="4094">suggest</TOKEN>
<TOKEN end_char="4103" id="token-28-6" morph="none" pos="word" start_char="4102">an</TOKEN>
<TOKEN end_char="4111" id="token-28-7" morph="none" pos="word" start_char="4105">earlier</TOKEN>
<TOKEN end_char="4118" id="token-28-8" morph="none" pos="word" start_char="4113">origin</TOKEN>
<TOKEN end_char="4121" id="token-28-9" morph="none" pos="word" start_char="4120">to</TOKEN>
<TOKEN end_char="4125" id="token-28-10" morph="none" pos="word" start_char="4123">the</TOKEN>
<TOKEN end_char="4137" id="token-28-11" morph="none" pos="word" start_char="4127">coronavirus</TOKEN>
<TOKEN end_char="4142" id="token-28-12" morph="none" pos="word" start_char="4139">than</TOKEN>
<TOKEN end_char="4150" id="token-28-13" morph="none" pos="word" start_char="4144">thought</TOKEN>
<TOKEN end_char="4151" id="token-28-14" morph="none" pos="punct" start_char="4151">.</TOKEN>
</SEG>
<SEG end_char="4163" id="segment-29" start_char="4153">
<ORIGINAL_TEXT>A report by</ORIGINAL_TEXT>
<TOKEN end_char="4153" id="token-29-0" morph="none" pos="word" start_char="4153">A</TOKEN>
<TOKEN end_char="4160" id="token-29-1" morph="none" pos="word" start_char="4155">report</TOKEN>
<TOKEN end_char="4163" id="token-29-2" morph="none" pos="word" start_char="4162">by</TOKEN>
</SEG>
<SEG end_char="4186" id="segment-30" start_char="4166">
<ORIGINAL_TEXT>The American Prospect</ORIGINAL_TEXT>
<TOKEN end_char="4168" id="token-30-0" morph="none" pos="word" start_char="4166">The</TOKEN>
<TOKEN end_char="4177" id="token-30-1" morph="none" pos="word" start_char="4170">American</TOKEN>
<TOKEN end_char="4186" id="token-30-2" morph="none" pos="word" start_char="4179">Prospect</TOKEN>
</SEG>
<SEG end_char="4335" id="segment-31" start_char="4189">
<ORIGINAL_TEXT>suggested "a strong correlation" in COVID-19 cases reported at US military bases that are home bases of members of the US teams that went to Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="4197" id="token-31-0" morph="none" pos="word" start_char="4189">suggested</TOKEN>
<TOKEN end_char="4199" id="token-31-1" morph="none" pos="punct" start_char="4199">"</TOKEN>
<TOKEN end_char="4200" id="token-31-2" morph="none" pos="word" start_char="4200">a</TOKEN>
<TOKEN end_char="4207" id="token-31-3" morph="none" pos="word" start_char="4202">strong</TOKEN>
<TOKEN end_char="4219" id="token-31-4" morph="none" pos="word" start_char="4209">correlation</TOKEN>
<TOKEN end_char="4220" id="token-31-5" morph="none" pos="punct" start_char="4220">"</TOKEN>
<TOKEN end_char="4223" id="token-31-6" morph="none" pos="word" start_char="4222">in</TOKEN>
<TOKEN end_char="4232" id="token-31-7" morph="none" pos="unknown" start_char="4225">COVID-19</TOKEN>
<TOKEN end_char="4238" id="token-31-8" morph="none" pos="word" start_char="4234">cases</TOKEN>
<TOKEN end_char="4247" id="token-31-9" morph="none" pos="word" start_char="4240">reported</TOKEN>
<TOKEN end_char="4250" id="token-31-10" morph="none" pos="word" start_char="4249">at</TOKEN>
<TOKEN end_char="4253" id="token-31-11" morph="none" pos="word" start_char="4252">US</TOKEN>
<TOKEN end_char="4262" id="token-31-12" morph="none" pos="word" start_char="4255">military</TOKEN>
<TOKEN end_char="4268" id="token-31-13" morph="none" pos="word" start_char="4264">bases</TOKEN>
<TOKEN end_char="4273" id="token-31-14" morph="none" pos="word" start_char="4270">that</TOKEN>
<TOKEN end_char="4277" id="token-31-15" morph="none" pos="word" start_char="4275">are</TOKEN>
<TOKEN end_char="4282" id="token-31-16" morph="none" pos="word" start_char="4279">home</TOKEN>
<TOKEN end_char="4288" id="token-31-17" morph="none" pos="word" start_char="4284">bases</TOKEN>
<TOKEN end_char="4291" id="token-31-18" morph="none" pos="word" start_char="4290">of</TOKEN>
<TOKEN end_char="4299" id="token-31-19" morph="none" pos="word" start_char="4293">members</TOKEN>
<TOKEN end_char="4302" id="token-31-20" morph="none" pos="word" start_char="4301">of</TOKEN>
<TOKEN end_char="4306" id="token-31-21" morph="none" pos="word" start_char="4304">the</TOKEN>
<TOKEN end_char="4309" id="token-31-22" morph="none" pos="word" start_char="4308">US</TOKEN>
<TOKEN end_char="4315" id="token-31-23" morph="none" pos="word" start_char="4311">teams</TOKEN>
<TOKEN end_char="4320" id="token-31-24" morph="none" pos="word" start_char="4317">that</TOKEN>
<TOKEN end_char="4325" id="token-31-25" morph="none" pos="word" start_char="4322">went</TOKEN>
<TOKEN end_char="4328" id="token-31-26" morph="none" pos="word" start_char="4327">to</TOKEN>
<TOKEN end_char="4334" id="token-31-27" morph="none" pos="word" start_char="4330">Wuhan</TOKEN>
<TOKEN end_char="4335" id="token-31-28" morph="none" pos="punct" start_char="4335">.</TOKEN>
</SEG>
<SEG end_char="4611" id="segment-32" start_char="4337">
<ORIGINAL_TEXT>A study by researchers from Harvard Medical School and the Boston University of Public Health used satellite data of hospital parking lots in tandem with search engine queries for coronavirus symptoms to suggest that it could have emerged in China as early as August of 2019.</ORIGINAL_TEXT>
<TOKEN end_char="4337" id="token-32-0" morph="none" pos="word" start_char="4337">A</TOKEN>
<TOKEN end_char="4343" id="token-32-1" morph="none" pos="word" start_char="4339">study</TOKEN>
<TOKEN end_char="4346" id="token-32-2" morph="none" pos="word" start_char="4345">by</TOKEN>
<TOKEN end_char="4358" id="token-32-3" morph="none" pos="word" start_char="4348">researchers</TOKEN>
<TOKEN end_char="4363" id="token-32-4" morph="none" pos="word" start_char="4360">from</TOKEN>
<TOKEN end_char="4371" id="token-32-5" morph="none" pos="word" start_char="4365">Harvard</TOKEN>
<TOKEN end_char="4379" id="token-32-6" morph="none" pos="word" start_char="4373">Medical</TOKEN>
<TOKEN end_char="4386" id="token-32-7" morph="none" pos="word" start_char="4381">School</TOKEN>
<TOKEN end_char="4390" id="token-32-8" morph="none" pos="word" start_char="4388">and</TOKEN>
<TOKEN end_char="4394" id="token-32-9" morph="none" pos="word" start_char="4392">the</TOKEN>
<TOKEN end_char="4401" id="token-32-10" morph="none" pos="word" start_char="4396">Boston</TOKEN>
<TOKEN end_char="4412" id="token-32-11" morph="none" pos="word" start_char="4403">University</TOKEN>
<TOKEN end_char="4415" id="token-32-12" morph="none" pos="word" start_char="4414">of</TOKEN>
<TOKEN end_char="4422" id="token-32-13" morph="none" pos="word" start_char="4417">Public</TOKEN>
<TOKEN end_char="4429" id="token-32-14" morph="none" pos="word" start_char="4424">Health</TOKEN>
<TOKEN end_char="4434" id="token-32-15" morph="none" pos="word" start_char="4431">used</TOKEN>
<TOKEN end_char="4444" id="token-32-16" morph="none" pos="word" start_char="4436">satellite</TOKEN>
<TOKEN end_char="4449" id="token-32-17" morph="none" pos="word" start_char="4446">data</TOKEN>
<TOKEN end_char="4452" id="token-32-18" morph="none" pos="word" start_char="4451">of</TOKEN>
<TOKEN end_char="4461" id="token-32-19" morph="none" pos="word" start_char="4454">hospital</TOKEN>
<TOKEN end_char="4469" id="token-32-20" morph="none" pos="word" start_char="4463">parking</TOKEN>
<TOKEN end_char="4474" id="token-32-21" morph="none" pos="word" start_char="4471">lots</TOKEN>
<TOKEN end_char="4477" id="token-32-22" morph="none" pos="word" start_char="4476">in</TOKEN>
<TOKEN end_char="4484" id="token-32-23" morph="none" pos="word" start_char="4479">tandem</TOKEN>
<TOKEN end_char="4489" id="token-32-24" morph="none" pos="word" start_char="4486">with</TOKEN>
<TOKEN end_char="4496" id="token-32-25" morph="none" pos="word" start_char="4491">search</TOKEN>
<TOKEN end_char="4503" id="token-32-26" morph="none" pos="word" start_char="4498">engine</TOKEN>
<TOKEN end_char="4511" id="token-32-27" morph="none" pos="word" start_char="4505">queries</TOKEN>
<TOKEN end_char="4515" id="token-32-28" morph="none" pos="word" start_char="4513">for</TOKEN>
<TOKEN end_char="4527" id="token-32-29" morph="none" pos="word" start_char="4517">coronavirus</TOKEN>
<TOKEN end_char="4536" id="token-32-30" morph="none" pos="word" start_char="4529">symptoms</TOKEN>
<TOKEN end_char="4539" id="token-32-31" morph="none" pos="word" start_char="4538">to</TOKEN>
<TOKEN end_char="4547" id="token-32-32" morph="none" pos="word" start_char="4541">suggest</TOKEN>
<TOKEN end_char="4552" id="token-32-33" morph="none" pos="word" start_char="4549">that</TOKEN>
<TOKEN end_char="4555" id="token-32-34" morph="none" pos="word" start_char="4554">it</TOKEN>
<TOKEN end_char="4561" id="token-32-35" morph="none" pos="word" start_char="4557">could</TOKEN>
<TOKEN end_char="4566" id="token-32-36" morph="none" pos="word" start_char="4563">have</TOKEN>
<TOKEN end_char="4574" id="token-32-37" morph="none" pos="word" start_char="4568">emerged</TOKEN>
<TOKEN end_char="4577" id="token-32-38" morph="none" pos="word" start_char="4576">in</TOKEN>
<TOKEN end_char="4583" id="token-32-39" morph="none" pos="word" start_char="4579">China</TOKEN>
<TOKEN end_char="4586" id="token-32-40" morph="none" pos="word" start_char="4585">as</TOKEN>
<TOKEN end_char="4592" id="token-32-41" morph="none" pos="word" start_char="4588">early</TOKEN>
<TOKEN end_char="4595" id="token-32-42" morph="none" pos="word" start_char="4594">as</TOKEN>
<TOKEN end_char="4602" id="token-32-43" morph="none" pos="word" start_char="4597">August</TOKEN>
<TOKEN end_char="4605" id="token-32-44" morph="none" pos="word" start_char="4604">of</TOKEN>
<TOKEN end_char="4610" id="token-32-45" morph="none" pos="word" start_char="4607">2019</TOKEN>
<TOKEN end_char="4611" id="token-32-46" morph="none" pos="punct" start_char="4611">.</TOKEN>
</SEG>
<SEG end_char="4818" id="segment-33" start_char="4614">
<ORIGINAL_TEXT>WHO Chief Scientist Dr Soumya Swaminathan told ANI that a WHO team would be visiting China next week to investigate the origins of the coronavirus, saying that a "thorough investigation" needed to be done.</ORIGINAL_TEXT>
<TOKEN end_char="4616" id="token-33-0" morph="none" pos="word" start_char="4614">WHO</TOKEN>
<TOKEN end_char="4622" id="token-33-1" morph="none" pos="word" start_char="4618">Chief</TOKEN>
<TOKEN end_char="4632" id="token-33-2" morph="none" pos="word" start_char="4624">Scientist</TOKEN>
<TOKEN end_char="4635" id="token-33-3" morph="none" pos="word" start_char="4634">Dr</TOKEN>
<TOKEN end_char="4642" id="token-33-4" morph="none" pos="word" start_char="4637">Soumya</TOKEN>
<TOKEN end_char="4654" id="token-33-5" morph="none" pos="word" start_char="4644">Swaminathan</TOKEN>
<TOKEN end_char="4659" id="token-33-6" morph="none" pos="word" start_char="4656">told</TOKEN>
<TOKEN end_char="4663" id="token-33-7" morph="none" pos="word" start_char="4661">ANI</TOKEN>
<TOKEN end_char="4668" id="token-33-8" morph="none" pos="word" start_char="4665">that</TOKEN>
<TOKEN end_char="4670" id="token-33-9" morph="none" pos="word" start_char="4670">a</TOKEN>
<TOKEN end_char="4674" id="token-33-10" morph="none" pos="word" start_char="4672">WHO</TOKEN>
<TOKEN end_char="4679" id="token-33-11" morph="none" pos="word" start_char="4676">team</TOKEN>
<TOKEN end_char="4685" id="token-33-12" morph="none" pos="word" start_char="4681">would</TOKEN>
<TOKEN end_char="4688" id="token-33-13" morph="none" pos="word" start_char="4687">be</TOKEN>
<TOKEN end_char="4697" id="token-33-14" morph="none" pos="word" start_char="4690">visiting</TOKEN>
<TOKEN end_char="4703" id="token-33-15" morph="none" pos="word" start_char="4699">China</TOKEN>
<TOKEN end_char="4708" id="token-33-16" morph="none" pos="word" start_char="4705">next</TOKEN>
<TOKEN end_char="4713" id="token-33-17" morph="none" pos="word" start_char="4710">week</TOKEN>
<TOKEN end_char="4716" id="token-33-18" morph="none" pos="word" start_char="4715">to</TOKEN>
<TOKEN end_char="4728" id="token-33-19" morph="none" pos="word" start_char="4718">investigate</TOKEN>
<TOKEN end_char="4732" id="token-33-20" morph="none" pos="word" start_char="4730">the</TOKEN>
<TOKEN end_char="4740" id="token-33-21" morph="none" pos="word" start_char="4734">origins</TOKEN>
<TOKEN end_char="4743" id="token-33-22" morph="none" pos="word" start_char="4742">of</TOKEN>
<TOKEN end_char="4747" id="token-33-23" morph="none" pos="word" start_char="4745">the</TOKEN>
<TOKEN end_char="4759" id="token-33-24" morph="none" pos="word" start_char="4749">coronavirus</TOKEN>
<TOKEN end_char="4760" id="token-33-25" morph="none" pos="punct" start_char="4760">,</TOKEN>
<TOKEN end_char="4767" id="token-33-26" morph="none" pos="word" start_char="4762">saying</TOKEN>
<TOKEN end_char="4772" id="token-33-27" morph="none" pos="word" start_char="4769">that</TOKEN>
<TOKEN end_char="4774" id="token-33-28" morph="none" pos="word" start_char="4774">a</TOKEN>
<TOKEN end_char="4776" id="token-33-29" morph="none" pos="punct" start_char="4776">"</TOKEN>
<TOKEN end_char="4784" id="token-33-30" morph="none" pos="word" start_char="4777">thorough</TOKEN>
<TOKEN end_char="4798" id="token-33-31" morph="none" pos="word" start_char="4786">investigation</TOKEN>
<TOKEN end_char="4799" id="token-33-32" morph="none" pos="punct" start_char="4799">"</TOKEN>
<TOKEN end_char="4806" id="token-33-33" morph="none" pos="word" start_char="4801">needed</TOKEN>
<TOKEN end_char="4809" id="token-33-34" morph="none" pos="word" start_char="4808">to</TOKEN>
<TOKEN end_char="4812" id="token-33-35" morph="none" pos="word" start_char="4811">be</TOKEN>
<TOKEN end_char="4817" id="token-33-36" morph="none" pos="word" start_char="4814">done</TOKEN>
<TOKEN end_char="4818" id="token-33-37" morph="none" pos="punct" start_char="4818">.</TOKEN>
</SEG>
<SEG end_char="4948" id="segment-34" start_char="4821">
<ORIGINAL_TEXT>"What is needed now is a good investigation going back before December to find out where and how it jumped from animal to human.</ORIGINAL_TEXT>
<TOKEN end_char="4821" id="token-34-0" morph="none" pos="punct" start_char="4821">"</TOKEN>
<TOKEN end_char="4825" id="token-34-1" morph="none" pos="word" start_char="4822">What</TOKEN>
<TOKEN end_char="4828" id="token-34-2" morph="none" pos="word" start_char="4827">is</TOKEN>
<TOKEN end_char="4835" id="token-34-3" morph="none" pos="word" start_char="4830">needed</TOKEN>
<TOKEN end_char="4839" id="token-34-4" morph="none" pos="word" start_char="4837">now</TOKEN>
<TOKEN end_char="4842" id="token-34-5" morph="none" pos="word" start_char="4841">is</TOKEN>
<TOKEN end_char="4844" id="token-34-6" morph="none" pos="word" start_char="4844">a</TOKEN>
<TOKEN end_char="4849" id="token-34-7" morph="none" pos="word" start_char="4846">good</TOKEN>
<TOKEN end_char="4863" id="token-34-8" morph="none" pos="word" start_char="4851">investigation</TOKEN>
<TOKEN end_char="4869" id="token-34-9" morph="none" pos="word" start_char="4865">going</TOKEN>
<TOKEN end_char="4874" id="token-34-10" morph="none" pos="word" start_char="4871">back</TOKEN>
<TOKEN end_char="4881" id="token-34-11" morph="none" pos="word" start_char="4876">before</TOKEN>
<TOKEN end_char="4890" id="token-34-12" morph="none" pos="word" start_char="4883">December</TOKEN>
<TOKEN end_char="4893" id="token-34-13" morph="none" pos="word" start_char="4892">to</TOKEN>
<TOKEN end_char="4898" id="token-34-14" morph="none" pos="word" start_char="4895">find</TOKEN>
<TOKEN end_char="4902" id="token-34-15" morph="none" pos="word" start_char="4900">out</TOKEN>
<TOKEN end_char="4908" id="token-34-16" morph="none" pos="word" start_char="4904">where</TOKEN>
<TOKEN end_char="4912" id="token-34-17" morph="none" pos="word" start_char="4910">and</TOKEN>
<TOKEN end_char="4916" id="token-34-18" morph="none" pos="word" start_char="4914">how</TOKEN>
<TOKEN end_char="4919" id="token-34-19" morph="none" pos="word" start_char="4918">it</TOKEN>
<TOKEN end_char="4926" id="token-34-20" morph="none" pos="word" start_char="4921">jumped</TOKEN>
<TOKEN end_char="4931" id="token-34-21" morph="none" pos="word" start_char="4928">from</TOKEN>
<TOKEN end_char="4938" id="token-34-22" morph="none" pos="word" start_char="4933">animal</TOKEN>
<TOKEN end_char="4941" id="token-34-23" morph="none" pos="word" start_char="4940">to</TOKEN>
<TOKEN end_char="4947" id="token-34-24" morph="none" pos="word" start_char="4943">human</TOKEN>
<TOKEN end_char="4948" id="token-34-25" morph="none" pos="punct" start_char="4948">.</TOKEN>
</SEG>
<SEG end_char="5056" id="segment-35" start_char="4950">
<ORIGINAL_TEXT>Was there any intermediate animal or not or it directly jumped from bat to humans which are also possible?"</ORIGINAL_TEXT>
<TOKEN end_char="4952" id="token-35-0" morph="none" pos="word" start_char="4950">Was</TOKEN>
<TOKEN end_char="4958" id="token-35-1" morph="none" pos="word" start_char="4954">there</TOKEN>
<TOKEN end_char="4962" id="token-35-2" morph="none" pos="word" start_char="4960">any</TOKEN>
<TOKEN end_char="4975" id="token-35-3" morph="none" pos="word" start_char="4964">intermediate</TOKEN>
<TOKEN end_char="4982" id="token-35-4" morph="none" pos="word" start_char="4977">animal</TOKEN>
<TOKEN end_char="4985" id="token-35-5" morph="none" pos="word" start_char="4984">or</TOKEN>
<TOKEN end_char="4989" id="token-35-6" morph="none" pos="word" start_char="4987">not</TOKEN>
<TOKEN end_char="4992" id="token-35-7" morph="none" pos="word" start_char="4991">or</TOKEN>
<TOKEN end_char="4995" id="token-35-8" morph="none" pos="word" start_char="4994">it</TOKEN>
<TOKEN end_char="5004" id="token-35-9" morph="none" pos="word" start_char="4997">directly</TOKEN>
<TOKEN end_char="5011" id="token-35-10" morph="none" pos="word" start_char="5006">jumped</TOKEN>
<TOKEN end_char="5016" id="token-35-11" morph="none" pos="word" start_char="5013">from</TOKEN>
<TOKEN end_char="5020" id="token-35-12" morph="none" pos="word" start_char="5018">bat</TOKEN>
<TOKEN end_char="5023" id="token-35-13" morph="none" pos="word" start_char="5022">to</TOKEN>
<TOKEN end_char="5030" id="token-35-14" morph="none" pos="word" start_char="5025">humans</TOKEN>
<TOKEN end_char="5036" id="token-35-15" morph="none" pos="word" start_char="5032">which</TOKEN>
<TOKEN end_char="5040" id="token-35-16" morph="none" pos="word" start_char="5038">are</TOKEN>
<TOKEN end_char="5045" id="token-35-17" morph="none" pos="word" start_char="5042">also</TOKEN>
<TOKEN end_char="5054" id="token-35-18" morph="none" pos="word" start_char="5047">possible</TOKEN>
<TOKEN end_char="5056" id="token-35-19" morph="none" pos="punct" start_char="5055">?"</TOKEN>
</SEG>
<SEG end_char="5067" id="segment-36" start_char="5058">
<ORIGINAL_TEXT>she asked.</ORIGINAL_TEXT>
<TOKEN end_char="5060" id="token-36-0" morph="none" pos="word" start_char="5058">she</TOKEN>
<TOKEN end_char="5066" id="token-36-1" morph="none" pos="word" start_char="5062">asked</TOKEN>
<TOKEN end_char="5067" id="token-36-2" morph="none" pos="punct" start_char="5067">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>