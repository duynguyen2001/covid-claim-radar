<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C049DX4" lang="eng" raw_text_char_length="3164" raw_text_md5="147db34f7cd07a873d03357b2e0d9423" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="68" id="segment-0" start_char="1">
<ORIGINAL_TEXT>French scientist who discovered HIV insists COVID-19 is lab creation</ORIGINAL_TEXT>
<TOKEN end_char="6" id="token-0-0" morph="none" pos="word" start_char="1">French</TOKEN>
<TOKEN end_char="16" id="token-0-1" morph="none" pos="word" start_char="8">scientist</TOKEN>
<TOKEN end_char="20" id="token-0-2" morph="none" pos="word" start_char="18">who</TOKEN>
<TOKEN end_char="31" id="token-0-3" morph="none" pos="word" start_char="22">discovered</TOKEN>
<TOKEN end_char="35" id="token-0-4" morph="none" pos="word" start_char="33">HIV</TOKEN>
<TOKEN end_char="43" id="token-0-5" morph="none" pos="word" start_char="37">insists</TOKEN>
<TOKEN end_char="52" id="token-0-6" morph="none" pos="unknown" start_char="45">COVID-19</TOKEN>
<TOKEN end_char="55" id="token-0-7" morph="none" pos="word" start_char="54">is</TOKEN>
<TOKEN end_char="59" id="token-0-8" morph="none" pos="word" start_char="57">lab</TOKEN>
<TOKEN end_char="68" id="token-0-9" morph="none" pos="word" start_char="61">creation</TOKEN>
</SEG>
<SEG end_char="290" id="segment-1" start_char="72">
<ORIGINAL_TEXT>A 2008 Nobel Prize recipient for discovering the human immunodeficiency virus (HIV) has weighed in on the controversy about the origin of the coronavirus disease 2019 (COVID-19), and insisted that it is laboratory-made.</ORIGINAL_TEXT>
<TOKEN end_char="72" id="token-1-0" morph="none" pos="word" start_char="72">A</TOKEN>
<TOKEN end_char="77" id="token-1-1" morph="none" pos="word" start_char="74">2008</TOKEN>
<TOKEN end_char="83" id="token-1-2" morph="none" pos="word" start_char="79">Nobel</TOKEN>
<TOKEN end_char="89" id="token-1-3" morph="none" pos="word" start_char="85">Prize</TOKEN>
<TOKEN end_char="99" id="token-1-4" morph="none" pos="word" start_char="91">recipient</TOKEN>
<TOKEN end_char="103" id="token-1-5" morph="none" pos="word" start_char="101">for</TOKEN>
<TOKEN end_char="115" id="token-1-6" morph="none" pos="word" start_char="105">discovering</TOKEN>
<TOKEN end_char="119" id="token-1-7" morph="none" pos="word" start_char="117">the</TOKEN>
<TOKEN end_char="125" id="token-1-8" morph="none" pos="word" start_char="121">human</TOKEN>
<TOKEN end_char="142" id="token-1-9" morph="none" pos="word" start_char="127">immunodeficiency</TOKEN>
<TOKEN end_char="148" id="token-1-10" morph="none" pos="word" start_char="144">virus</TOKEN>
<TOKEN end_char="150" id="token-1-11" morph="none" pos="punct" start_char="150">(</TOKEN>
<TOKEN end_char="153" id="token-1-12" morph="none" pos="word" start_char="151">HIV</TOKEN>
<TOKEN end_char="154" id="token-1-13" morph="none" pos="punct" start_char="154">)</TOKEN>
<TOKEN end_char="158" id="token-1-14" morph="none" pos="word" start_char="156">has</TOKEN>
<TOKEN end_char="166" id="token-1-15" morph="none" pos="word" start_char="160">weighed</TOKEN>
<TOKEN end_char="169" id="token-1-16" morph="none" pos="word" start_char="168">in</TOKEN>
<TOKEN end_char="172" id="token-1-17" morph="none" pos="word" start_char="171">on</TOKEN>
<TOKEN end_char="176" id="token-1-18" morph="none" pos="word" start_char="174">the</TOKEN>
<TOKEN end_char="188" id="token-1-19" morph="none" pos="word" start_char="178">controversy</TOKEN>
<TOKEN end_char="194" id="token-1-20" morph="none" pos="word" start_char="190">about</TOKEN>
<TOKEN end_char="198" id="token-1-21" morph="none" pos="word" start_char="196">the</TOKEN>
<TOKEN end_char="205" id="token-1-22" morph="none" pos="word" start_char="200">origin</TOKEN>
<TOKEN end_char="208" id="token-1-23" morph="none" pos="word" start_char="207">of</TOKEN>
<TOKEN end_char="212" id="token-1-24" morph="none" pos="word" start_char="210">the</TOKEN>
<TOKEN end_char="224" id="token-1-25" morph="none" pos="word" start_char="214">coronavirus</TOKEN>
<TOKEN end_char="232" id="token-1-26" morph="none" pos="word" start_char="226">disease</TOKEN>
<TOKEN end_char="237" id="token-1-27" morph="none" pos="word" start_char="234">2019</TOKEN>
<TOKEN end_char="239" id="token-1-28" morph="none" pos="punct" start_char="239">(</TOKEN>
<TOKEN end_char="247" id="token-1-29" morph="none" pos="unknown" start_char="240">COVID-19</TOKEN>
<TOKEN end_char="249" id="token-1-30" morph="none" pos="punct" start_char="248">),</TOKEN>
<TOKEN end_char="253" id="token-1-31" morph="none" pos="word" start_char="251">and</TOKEN>
<TOKEN end_char="262" id="token-1-32" morph="none" pos="word" start_char="255">insisted</TOKEN>
<TOKEN end_char="267" id="token-1-33" morph="none" pos="word" start_char="264">that</TOKEN>
<TOKEN end_char="270" id="token-1-34" morph="none" pos="word" start_char="269">it</TOKEN>
<TOKEN end_char="273" id="token-1-35" morph="none" pos="word" start_char="272">is</TOKEN>
<TOKEN end_char="289" id="token-1-36" morph="none" pos="unknown" start_char="275">laboratory-made</TOKEN>
<TOKEN end_char="290" id="token-1-37" morph="none" pos="punct" start_char="290">.</TOKEN>
</SEG>
<SEG end_char="569" id="segment-2" start_char="293">
<ORIGINAL_TEXT>Four days prior to US President Donald Trump's threat of new tariffs against Beijing, claiming there is evidence linking the coronavirus to a lab in China's ground-zero city of Wuhan, French scientist Luc Montagnier who discovered HIV said COVID-19 was a creation of an expert.</ORIGINAL_TEXT>
<TOKEN end_char="296" id="token-2-0" morph="none" pos="word" start_char="293">Four</TOKEN>
<TOKEN end_char="301" id="token-2-1" morph="none" pos="word" start_char="298">days</TOKEN>
<TOKEN end_char="307" id="token-2-2" morph="none" pos="word" start_char="303">prior</TOKEN>
<TOKEN end_char="310" id="token-2-3" morph="none" pos="word" start_char="309">to</TOKEN>
<TOKEN end_char="313" id="token-2-4" morph="none" pos="word" start_char="312">US</TOKEN>
<TOKEN end_char="323" id="token-2-5" morph="none" pos="word" start_char="315">President</TOKEN>
<TOKEN end_char="330" id="token-2-6" morph="none" pos="word" start_char="325">Donald</TOKEN>
<TOKEN end_char="338" id="token-2-7" morph="none" pos="word" start_char="332">Trump's</TOKEN>
<TOKEN end_char="345" id="token-2-8" morph="none" pos="word" start_char="340">threat</TOKEN>
<TOKEN end_char="348" id="token-2-9" morph="none" pos="word" start_char="347">of</TOKEN>
<TOKEN end_char="352" id="token-2-10" morph="none" pos="word" start_char="350">new</TOKEN>
<TOKEN end_char="360" id="token-2-11" morph="none" pos="word" start_char="354">tariffs</TOKEN>
<TOKEN end_char="368" id="token-2-12" morph="none" pos="word" start_char="362">against</TOKEN>
<TOKEN end_char="376" id="token-2-13" morph="none" pos="word" start_char="370">Beijing</TOKEN>
<TOKEN end_char="377" id="token-2-14" morph="none" pos="punct" start_char="377">,</TOKEN>
<TOKEN end_char="386" id="token-2-15" morph="none" pos="word" start_char="379">claiming</TOKEN>
<TOKEN end_char="392" id="token-2-16" morph="none" pos="word" start_char="388">there</TOKEN>
<TOKEN end_char="395" id="token-2-17" morph="none" pos="word" start_char="394">is</TOKEN>
<TOKEN end_char="404" id="token-2-18" morph="none" pos="word" start_char="397">evidence</TOKEN>
<TOKEN end_char="412" id="token-2-19" morph="none" pos="word" start_char="406">linking</TOKEN>
<TOKEN end_char="416" id="token-2-20" morph="none" pos="word" start_char="414">the</TOKEN>
<TOKEN end_char="428" id="token-2-21" morph="none" pos="word" start_char="418">coronavirus</TOKEN>
<TOKEN end_char="431" id="token-2-22" morph="none" pos="word" start_char="430">to</TOKEN>
<TOKEN end_char="433" id="token-2-23" morph="none" pos="word" start_char="433">a</TOKEN>
<TOKEN end_char="437" id="token-2-24" morph="none" pos="word" start_char="435">lab</TOKEN>
<TOKEN end_char="440" id="token-2-25" morph="none" pos="word" start_char="439">in</TOKEN>
<TOKEN end_char="448" id="token-2-26" morph="none" pos="word" start_char="442">China's</TOKEN>
<TOKEN end_char="460" id="token-2-27" morph="none" pos="unknown" start_char="450">ground-zero</TOKEN>
<TOKEN end_char="465" id="token-2-28" morph="none" pos="word" start_char="462">city</TOKEN>
<TOKEN end_char="468" id="token-2-29" morph="none" pos="word" start_char="467">of</TOKEN>
<TOKEN end_char="474" id="token-2-30" morph="none" pos="word" start_char="470">Wuhan</TOKEN>
<TOKEN end_char="475" id="token-2-31" morph="none" pos="punct" start_char="475">,</TOKEN>
<TOKEN end_char="482" id="token-2-32" morph="none" pos="word" start_char="477">French</TOKEN>
<TOKEN end_char="492" id="token-2-33" morph="none" pos="word" start_char="484">scientist</TOKEN>
<TOKEN end_char="496" id="token-2-34" morph="none" pos="word" start_char="494">Luc</TOKEN>
<TOKEN end_char="507" id="token-2-35" morph="none" pos="word" start_char="498">Montagnier</TOKEN>
<TOKEN end_char="511" id="token-2-36" morph="none" pos="word" start_char="509">who</TOKEN>
<TOKEN end_char="522" id="token-2-37" morph="none" pos="word" start_char="513">discovered</TOKEN>
<TOKEN end_char="526" id="token-2-38" morph="none" pos="word" start_char="524">HIV</TOKEN>
<TOKEN end_char="531" id="token-2-39" morph="none" pos="word" start_char="528">said</TOKEN>
<TOKEN end_char="540" id="token-2-40" morph="none" pos="unknown" start_char="533">COVID-19</TOKEN>
<TOKEN end_char="544" id="token-2-41" morph="none" pos="word" start_char="542">was</TOKEN>
<TOKEN end_char="546" id="token-2-42" morph="none" pos="word" start_char="546">a</TOKEN>
<TOKEN end_char="555" id="token-2-43" morph="none" pos="word" start_char="548">creation</TOKEN>
<TOKEN end_char="558" id="token-2-44" morph="none" pos="word" start_char="557">of</TOKEN>
<TOKEN end_char="561" id="token-2-45" morph="none" pos="word" start_char="560">an</TOKEN>
<TOKEN end_char="568" id="token-2-46" morph="none" pos="word" start_char="563">expert</TOKEN>
<TOKEN end_char="569" id="token-2-47" morph="none" pos="punct" start_char="569">.</TOKEN>
</SEG>
<SEG end_char="804" id="segment-3" start_char="572">
<ORIGINAL_TEXT>Interviewed on the CNews channel in France, Montagnier asserted that the virus had been designed by molecular biologists, stating that it contains genetic elements of HIV, and that its characteristics could not have arisen naturally.</ORIGINAL_TEXT>
<TOKEN end_char="582" id="token-3-0" morph="none" pos="word" start_char="572">Interviewed</TOKEN>
<TOKEN end_char="585" id="token-3-1" morph="none" pos="word" start_char="584">on</TOKEN>
<TOKEN end_char="589" id="token-3-2" morph="none" pos="word" start_char="587">the</TOKEN>
<TOKEN end_char="595" id="token-3-3" morph="none" pos="word" start_char="591">CNews</TOKEN>
<TOKEN end_char="603" id="token-3-4" morph="none" pos="word" start_char="597">channel</TOKEN>
<TOKEN end_char="606" id="token-3-5" morph="none" pos="word" start_char="605">in</TOKEN>
<TOKEN end_char="613" id="token-3-6" morph="none" pos="word" start_char="608">France</TOKEN>
<TOKEN end_char="614" id="token-3-7" morph="none" pos="punct" start_char="614">,</TOKEN>
<TOKEN end_char="625" id="token-3-8" morph="none" pos="word" start_char="616">Montagnier</TOKEN>
<TOKEN end_char="634" id="token-3-9" morph="none" pos="word" start_char="627">asserted</TOKEN>
<TOKEN end_char="639" id="token-3-10" morph="none" pos="word" start_char="636">that</TOKEN>
<TOKEN end_char="643" id="token-3-11" morph="none" pos="word" start_char="641">the</TOKEN>
<TOKEN end_char="649" id="token-3-12" morph="none" pos="word" start_char="645">virus</TOKEN>
<TOKEN end_char="653" id="token-3-13" morph="none" pos="word" start_char="651">had</TOKEN>
<TOKEN end_char="658" id="token-3-14" morph="none" pos="word" start_char="655">been</TOKEN>
<TOKEN end_char="667" id="token-3-15" morph="none" pos="word" start_char="660">designed</TOKEN>
<TOKEN end_char="670" id="token-3-16" morph="none" pos="word" start_char="669">by</TOKEN>
<TOKEN end_char="680" id="token-3-17" morph="none" pos="word" start_char="672">molecular</TOKEN>
<TOKEN end_char="691" id="token-3-18" morph="none" pos="word" start_char="682">biologists</TOKEN>
<TOKEN end_char="692" id="token-3-19" morph="none" pos="punct" start_char="692">,</TOKEN>
<TOKEN end_char="700" id="token-3-20" morph="none" pos="word" start_char="694">stating</TOKEN>
<TOKEN end_char="705" id="token-3-21" morph="none" pos="word" start_char="702">that</TOKEN>
<TOKEN end_char="708" id="token-3-22" morph="none" pos="word" start_char="707">it</TOKEN>
<TOKEN end_char="717" id="token-3-23" morph="none" pos="word" start_char="710">contains</TOKEN>
<TOKEN end_char="725" id="token-3-24" morph="none" pos="word" start_char="719">genetic</TOKEN>
<TOKEN end_char="734" id="token-3-25" morph="none" pos="word" start_char="727">elements</TOKEN>
<TOKEN end_char="737" id="token-3-26" morph="none" pos="word" start_char="736">of</TOKEN>
<TOKEN end_char="741" id="token-3-27" morph="none" pos="word" start_char="739">HIV</TOKEN>
<TOKEN end_char="742" id="token-3-28" morph="none" pos="punct" start_char="742">,</TOKEN>
<TOKEN end_char="746" id="token-3-29" morph="none" pos="word" start_char="744">and</TOKEN>
<TOKEN end_char="751" id="token-3-30" morph="none" pos="word" start_char="748">that</TOKEN>
<TOKEN end_char="755" id="token-3-31" morph="none" pos="word" start_char="753">its</TOKEN>
<TOKEN end_char="771" id="token-3-32" morph="none" pos="word" start_char="757">characteristics</TOKEN>
<TOKEN end_char="777" id="token-3-33" morph="none" pos="word" start_char="773">could</TOKEN>
<TOKEN end_char="781" id="token-3-34" morph="none" pos="word" start_char="779">not</TOKEN>
<TOKEN end_char="786" id="token-3-35" morph="none" pos="word" start_char="783">have</TOKEN>
<TOKEN end_char="793" id="token-3-36" morph="none" pos="word" start_char="788">arisen</TOKEN>
<TOKEN end_char="803" id="token-3-37" morph="none" pos="word" start_char="795">naturally</TOKEN>
<TOKEN end_char="804" id="token-3-38" morph="none" pos="punct" start_char="804">.</TOKEN>
</SEG>
<SEG end_char="923" id="segment-4" start_char="807">
<ORIGINAL_TEXT>Asked by the CNews interviewer what the goal of these molecular biologists could be, Montagnier said it wasn’t clear.</ORIGINAL_TEXT>
<TOKEN end_char="811" id="token-4-0" morph="none" pos="word" start_char="807">Asked</TOKEN>
<TOKEN end_char="814" id="token-4-1" morph="none" pos="word" start_char="813">by</TOKEN>
<TOKEN end_char="818" id="token-4-2" morph="none" pos="word" start_char="816">the</TOKEN>
<TOKEN end_char="824" id="token-4-3" morph="none" pos="word" start_char="820">CNews</TOKEN>
<TOKEN end_char="836" id="token-4-4" morph="none" pos="word" start_char="826">interviewer</TOKEN>
<TOKEN end_char="841" id="token-4-5" morph="none" pos="word" start_char="838">what</TOKEN>
<TOKEN end_char="845" id="token-4-6" morph="none" pos="word" start_char="843">the</TOKEN>
<TOKEN end_char="850" id="token-4-7" morph="none" pos="word" start_char="847">goal</TOKEN>
<TOKEN end_char="853" id="token-4-8" morph="none" pos="word" start_char="852">of</TOKEN>
<TOKEN end_char="859" id="token-4-9" morph="none" pos="word" start_char="855">these</TOKEN>
<TOKEN end_char="869" id="token-4-10" morph="none" pos="word" start_char="861">molecular</TOKEN>
<TOKEN end_char="880" id="token-4-11" morph="none" pos="word" start_char="871">biologists</TOKEN>
<TOKEN end_char="886" id="token-4-12" morph="none" pos="word" start_char="882">could</TOKEN>
<TOKEN end_char="889" id="token-4-13" morph="none" pos="word" start_char="888">be</TOKEN>
<TOKEN end_char="890" id="token-4-14" morph="none" pos="punct" start_char="890">,</TOKEN>
<TOKEN end_char="901" id="token-4-15" morph="none" pos="word" start_char="892">Montagnier</TOKEN>
<TOKEN end_char="906" id="token-4-16" morph="none" pos="word" start_char="903">said</TOKEN>
<TOKEN end_char="909" id="token-4-17" morph="none" pos="word" start_char="908">it</TOKEN>
<TOKEN end_char="916" id="token-4-18" morph="none" pos="word" start_char="911">wasn’t</TOKEN>
<TOKEN end_char="922" id="token-4-19" morph="none" pos="word" start_char="918">clear</TOKEN>
<TOKEN end_char="923" id="token-4-20" morph="none" pos="punct" start_char="923">.</TOKEN>
</SEG>
<SEG end_char="968" id="segment-5" start_char="925">
<ORIGINAL_TEXT>"My job," he said, "is to expose the facts."</ORIGINAL_TEXT>
<TOKEN end_char="925" id="token-5-0" morph="none" pos="punct" start_char="925">"</TOKEN>
<TOKEN end_char="927" id="token-5-1" morph="none" pos="word" start_char="926">My</TOKEN>
<TOKEN end_char="931" id="token-5-2" morph="none" pos="word" start_char="929">job</TOKEN>
<TOKEN end_char="933" id="token-5-3" morph="none" pos="punct" start_char="932">,"</TOKEN>
<TOKEN end_char="936" id="token-5-4" morph="none" pos="word" start_char="935">he</TOKEN>
<TOKEN end_char="941" id="token-5-5" morph="none" pos="word" start_char="938">said</TOKEN>
<TOKEN end_char="942" id="token-5-6" morph="none" pos="punct" start_char="942">,</TOKEN>
<TOKEN end_char="944" id="token-5-7" morph="none" pos="punct" start_char="944">"</TOKEN>
<TOKEN end_char="946" id="token-5-8" morph="none" pos="word" start_char="945">is</TOKEN>
<TOKEN end_char="949" id="token-5-9" morph="none" pos="word" start_char="948">to</TOKEN>
<TOKEN end_char="956" id="token-5-10" morph="none" pos="word" start_char="951">expose</TOKEN>
<TOKEN end_char="960" id="token-5-11" morph="none" pos="word" start_char="958">the</TOKEN>
<TOKEN end_char="966" id="token-5-12" morph="none" pos="word" start_char="962">facts</TOKEN>
<TOKEN end_char="968" id="token-5-13" morph="none" pos="punct" start_char="967">."</TOKEN>
</SEG>
<SEG end_char="1104" id="segment-6" start_char="971">
<ORIGINAL_TEXT>Montagnier said that "he didn’t know who had done it, or why," but suggested that possibly the goal had been to make an AIDS vaccine."</ORIGINAL_TEXT>
<TOKEN end_char="980" id="token-6-0" morph="none" pos="word" start_char="971">Montagnier</TOKEN>
<TOKEN end_char="985" id="token-6-1" morph="none" pos="word" start_char="982">said</TOKEN>
<TOKEN end_char="990" id="token-6-2" morph="none" pos="word" start_char="987">that</TOKEN>
<TOKEN end_char="992" id="token-6-3" morph="none" pos="punct" start_char="992">"</TOKEN>
<TOKEN end_char="994" id="token-6-4" morph="none" pos="word" start_char="993">he</TOKEN>
<TOKEN end_char="1001" id="token-6-5" morph="none" pos="word" start_char="996">didn’t</TOKEN>
<TOKEN end_char="1006" id="token-6-6" morph="none" pos="word" start_char="1003">know</TOKEN>
<TOKEN end_char="1010" id="token-6-7" morph="none" pos="word" start_char="1008">who</TOKEN>
<TOKEN end_char="1014" id="token-6-8" morph="none" pos="word" start_char="1012">had</TOKEN>
<TOKEN end_char="1019" id="token-6-9" morph="none" pos="word" start_char="1016">done</TOKEN>
<TOKEN end_char="1022" id="token-6-10" morph="none" pos="word" start_char="1021">it</TOKEN>
<TOKEN end_char="1023" id="token-6-11" morph="none" pos="punct" start_char="1023">,</TOKEN>
<TOKEN end_char="1026" id="token-6-12" morph="none" pos="word" start_char="1025">or</TOKEN>
<TOKEN end_char="1030" id="token-6-13" morph="none" pos="word" start_char="1028">why</TOKEN>
<TOKEN end_char="1032" id="token-6-14" morph="none" pos="punct" start_char="1031">,"</TOKEN>
<TOKEN end_char="1036" id="token-6-15" morph="none" pos="word" start_char="1034">but</TOKEN>
<TOKEN end_char="1046" id="token-6-16" morph="none" pos="word" start_char="1038">suggested</TOKEN>
<TOKEN end_char="1051" id="token-6-17" morph="none" pos="word" start_char="1048">that</TOKEN>
<TOKEN end_char="1060" id="token-6-18" morph="none" pos="word" start_char="1053">possibly</TOKEN>
<TOKEN end_char="1064" id="token-6-19" morph="none" pos="word" start_char="1062">the</TOKEN>
<TOKEN end_char="1069" id="token-6-20" morph="none" pos="word" start_char="1066">goal</TOKEN>
<TOKEN end_char="1073" id="token-6-21" morph="none" pos="word" start_char="1071">had</TOKEN>
<TOKEN end_char="1078" id="token-6-22" morph="none" pos="word" start_char="1075">been</TOKEN>
<TOKEN end_char="1081" id="token-6-23" morph="none" pos="word" start_char="1080">to</TOKEN>
<TOKEN end_char="1086" id="token-6-24" morph="none" pos="word" start_char="1083">make</TOKEN>
<TOKEN end_char="1089" id="token-6-25" morph="none" pos="word" start_char="1088">an</TOKEN>
<TOKEN end_char="1094" id="token-6-26" morph="none" pos="word" start_char="1091">AIDS</TOKEN>
<TOKEN end_char="1102" id="token-6-27" morph="none" pos="word" start_char="1096">vaccine</TOKEN>
<TOKEN end_char="1104" id="token-6-28" morph="none" pos="punct" start_char="1103">."</TOKEN>
</SEG>
<SEG end_char="1243" id="segment-7" start_char="1107">
<ORIGINAL_TEXT>But he said that the lab virus is "a professional job… a very meticulous job," describing its genome as being a "clockwork of sequences."</ORIGINAL_TEXT>
<TOKEN end_char="1109" id="token-7-0" morph="none" pos="word" start_char="1107">But</TOKEN>
<TOKEN end_char="1112" id="token-7-1" morph="none" pos="word" start_char="1111">he</TOKEN>
<TOKEN end_char="1117" id="token-7-2" morph="none" pos="word" start_char="1114">said</TOKEN>
<TOKEN end_char="1122" id="token-7-3" morph="none" pos="word" start_char="1119">that</TOKEN>
<TOKEN end_char="1126" id="token-7-4" morph="none" pos="word" start_char="1124">the</TOKEN>
<TOKEN end_char="1130" id="token-7-5" morph="none" pos="word" start_char="1128">lab</TOKEN>
<TOKEN end_char="1136" id="token-7-6" morph="none" pos="word" start_char="1132">virus</TOKEN>
<TOKEN end_char="1139" id="token-7-7" morph="none" pos="word" start_char="1138">is</TOKEN>
<TOKEN end_char="1141" id="token-7-8" morph="none" pos="punct" start_char="1141">"</TOKEN>
<TOKEN end_char="1142" id="token-7-9" morph="none" pos="word" start_char="1142">a</TOKEN>
<TOKEN end_char="1155" id="token-7-10" morph="none" pos="word" start_char="1144">professional</TOKEN>
<TOKEN end_char="1159" id="token-7-11" morph="none" pos="word" start_char="1157">job</TOKEN>
<TOKEN end_char="1160" id="token-7-12" morph="none" pos="punct" start_char="1160">…</TOKEN>
<TOKEN end_char="1162" id="token-7-13" morph="none" pos="word" start_char="1162">a</TOKEN>
<TOKEN end_char="1167" id="token-7-14" morph="none" pos="word" start_char="1164">very</TOKEN>
<TOKEN end_char="1178" id="token-7-15" morph="none" pos="word" start_char="1169">meticulous</TOKEN>
<TOKEN end_char="1182" id="token-7-16" morph="none" pos="word" start_char="1180">job</TOKEN>
<TOKEN end_char="1184" id="token-7-17" morph="none" pos="punct" start_char="1183">,"</TOKEN>
<TOKEN end_char="1195" id="token-7-18" morph="none" pos="word" start_char="1186">describing</TOKEN>
<TOKEN end_char="1199" id="token-7-19" morph="none" pos="word" start_char="1197">its</TOKEN>
<TOKEN end_char="1206" id="token-7-20" morph="none" pos="word" start_char="1201">genome</TOKEN>
<TOKEN end_char="1209" id="token-7-21" morph="none" pos="word" start_char="1208">as</TOKEN>
<TOKEN end_char="1215" id="token-7-22" morph="none" pos="word" start_char="1211">being</TOKEN>
<TOKEN end_char="1217" id="token-7-23" morph="none" pos="word" start_char="1217">a</TOKEN>
<TOKEN end_char="1219" id="token-7-24" morph="none" pos="punct" start_char="1219">"</TOKEN>
<TOKEN end_char="1228" id="token-7-25" morph="none" pos="word" start_char="1220">clockwork</TOKEN>
<TOKEN end_char="1231" id="token-7-26" morph="none" pos="word" start_char="1230">of</TOKEN>
<TOKEN end_char="1241" id="token-7-27" morph="none" pos="word" start_char="1233">sequences</TOKEN>
<TOKEN end_char="1243" id="token-7-28" morph="none" pos="punct" start_char="1242">."</TOKEN>
</SEG>
<SEG end_char="1429" id="segment-8" start_char="1246">
<ORIGINAL_TEXT>"There’s a part which is obviously the classic virus, and there’s another mainly coming from the bat, but that part has added sequences, particularly from HIV–the AIDS virus," he said.</ORIGINAL_TEXT>
<TOKEN end_char="1246" id="token-8-0" morph="none" pos="punct" start_char="1246">"</TOKEN>
<TOKEN end_char="1253" id="token-8-1" morph="none" pos="word" start_char="1247">There’s</TOKEN>
<TOKEN end_char="1255" id="token-8-2" morph="none" pos="word" start_char="1255">a</TOKEN>
<TOKEN end_char="1260" id="token-8-3" morph="none" pos="word" start_char="1257">part</TOKEN>
<TOKEN end_char="1266" id="token-8-4" morph="none" pos="word" start_char="1262">which</TOKEN>
<TOKEN end_char="1269" id="token-8-5" morph="none" pos="word" start_char="1268">is</TOKEN>
<TOKEN end_char="1279" id="token-8-6" morph="none" pos="word" start_char="1271">obviously</TOKEN>
<TOKEN end_char="1283" id="token-8-7" morph="none" pos="word" start_char="1281">the</TOKEN>
<TOKEN end_char="1291" id="token-8-8" morph="none" pos="word" start_char="1285">classic</TOKEN>
<TOKEN end_char="1297" id="token-8-9" morph="none" pos="word" start_char="1293">virus</TOKEN>
<TOKEN end_char="1298" id="token-8-10" morph="none" pos="punct" start_char="1298">,</TOKEN>
<TOKEN end_char="1302" id="token-8-11" morph="none" pos="word" start_char="1300">and</TOKEN>
<TOKEN end_char="1310" id="token-8-12" morph="none" pos="word" start_char="1304">there’s</TOKEN>
<TOKEN end_char="1318" id="token-8-13" morph="none" pos="word" start_char="1312">another</TOKEN>
<TOKEN end_char="1325" id="token-8-14" morph="none" pos="word" start_char="1320">mainly</TOKEN>
<TOKEN end_char="1332" id="token-8-15" morph="none" pos="word" start_char="1327">coming</TOKEN>
<TOKEN end_char="1337" id="token-8-16" morph="none" pos="word" start_char="1334">from</TOKEN>
<TOKEN end_char="1341" id="token-8-17" morph="none" pos="word" start_char="1339">the</TOKEN>
<TOKEN end_char="1345" id="token-8-18" morph="none" pos="word" start_char="1343">bat</TOKEN>
<TOKEN end_char="1346" id="token-8-19" morph="none" pos="punct" start_char="1346">,</TOKEN>
<TOKEN end_char="1350" id="token-8-20" morph="none" pos="word" start_char="1348">but</TOKEN>
<TOKEN end_char="1355" id="token-8-21" morph="none" pos="word" start_char="1352">that</TOKEN>
<TOKEN end_char="1360" id="token-8-22" morph="none" pos="word" start_char="1357">part</TOKEN>
<TOKEN end_char="1364" id="token-8-23" morph="none" pos="word" start_char="1362">has</TOKEN>
<TOKEN end_char="1370" id="token-8-24" morph="none" pos="word" start_char="1366">added</TOKEN>
<TOKEN end_char="1380" id="token-8-25" morph="none" pos="word" start_char="1372">sequences</TOKEN>
<TOKEN end_char="1381" id="token-8-26" morph="none" pos="punct" start_char="1381">,</TOKEN>
<TOKEN end_char="1394" id="token-8-27" morph="none" pos="word" start_char="1383">particularly</TOKEN>
<TOKEN end_char="1399" id="token-8-28" morph="none" pos="word" start_char="1396">from</TOKEN>
<TOKEN end_char="1407" id="token-8-29" morph="none" pos="unknown" start_char="1401">HIV–the</TOKEN>
<TOKEN end_char="1412" id="token-8-30" morph="none" pos="word" start_char="1409">AIDS</TOKEN>
<TOKEN end_char="1418" id="token-8-31" morph="none" pos="word" start_char="1414">virus</TOKEN>
<TOKEN end_char="1420" id="token-8-32" morph="none" pos="punct" start_char="1419">,"</TOKEN>
<TOKEN end_char="1423" id="token-8-33" morph="none" pos="word" start_char="1422">he</TOKEN>
<TOKEN end_char="1428" id="token-8-34" morph="none" pos="word" start_char="1425">said</TOKEN>
<TOKEN end_char="1429" id="token-8-35" morph="none" pos="punct" start_char="1429">.</TOKEN>
</SEG>
<SEG end_char="1439" id="segment-9" start_char="1432">
<ORIGINAL_TEXT>'Absurd'</ORIGINAL_TEXT>
<TOKEN end_char="1432" id="token-9-0" morph="none" pos="punct" start_char="1432">'</TOKEN>
<TOKEN end_char="1438" id="token-9-1" morph="none" pos="word" start_char="1433">Absurd</TOKEN>
<TOKEN end_char="1439" id="token-9-2" morph="none" pos="punct" start_char="1439">'</TOKEN>
<TRANSLATED_TEXT>Absurd</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="1540" id="segment-10" start_char="1442">
<ORIGINAL_TEXT>Other experts, however, are not buying Montagnier's explanation, with one even calling it "absurd."</ORIGINAL_TEXT>
<TOKEN end_char="1446" id="token-10-0" morph="none" pos="word" start_char="1442">Other</TOKEN>
<TOKEN end_char="1454" id="token-10-1" morph="none" pos="word" start_char="1448">experts</TOKEN>
<TOKEN end_char="1455" id="token-10-2" morph="none" pos="punct" start_char="1455">,</TOKEN>
<TOKEN end_char="1463" id="token-10-3" morph="none" pos="word" start_char="1457">however</TOKEN>
<TOKEN end_char="1464" id="token-10-4" morph="none" pos="punct" start_char="1464">,</TOKEN>
<TOKEN end_char="1468" id="token-10-5" morph="none" pos="word" start_char="1466">are</TOKEN>
<TOKEN end_char="1472" id="token-10-6" morph="none" pos="word" start_char="1470">not</TOKEN>
<TOKEN end_char="1479" id="token-10-7" morph="none" pos="word" start_char="1474">buying</TOKEN>
<TOKEN end_char="1492" id="token-10-8" morph="none" pos="word" start_char="1481">Montagnier's</TOKEN>
<TOKEN end_char="1504" id="token-10-9" morph="none" pos="word" start_char="1494">explanation</TOKEN>
<TOKEN end_char="1505" id="token-10-10" morph="none" pos="punct" start_char="1505">,</TOKEN>
<TOKEN end_char="1510" id="token-10-11" morph="none" pos="word" start_char="1507">with</TOKEN>
<TOKEN end_char="1514" id="token-10-12" morph="none" pos="word" start_char="1512">one</TOKEN>
<TOKEN end_char="1519" id="token-10-13" morph="none" pos="word" start_char="1516">even</TOKEN>
<TOKEN end_char="1527" id="token-10-14" morph="none" pos="word" start_char="1521">calling</TOKEN>
<TOKEN end_char="1530" id="token-10-15" morph="none" pos="word" start_char="1529">it</TOKEN>
<TOKEN end_char="1532" id="token-10-16" morph="none" pos="punct" start_char="1532">"</TOKEN>
<TOKEN end_char="1538" id="token-10-17" morph="none" pos="word" start_char="1533">absurd</TOKEN>
<TOKEN end_char="1540" id="token-10-18" morph="none" pos="punct" start_char="1539">."</TOKEN>
</SEG>
<SEG end_char="1792" id="segment-11" start_char="1543">
<ORIGINAL_TEXT>Virologist Etienne Simon-Lorière from the Pasteur Institute in Paris said suggesting that the virus was man-made because of the "small... pieces of genomes" in it that are anyway found in other viruses of the same family simply "does not make sense."</ORIGINAL_TEXT>
<TOKEN end_char="1552" id="token-11-0" morph="none" pos="word" start_char="1543">Virologist</TOKEN>
<TOKEN end_char="1560" id="token-11-1" morph="none" pos="word" start_char="1554">Etienne</TOKEN>
<TOKEN end_char="1574" id="token-11-2" morph="none" pos="unknown" start_char="1562">Simon-Lorière</TOKEN>
<TOKEN end_char="1579" id="token-11-3" morph="none" pos="word" start_char="1576">from</TOKEN>
<TOKEN end_char="1583" id="token-11-4" morph="none" pos="word" start_char="1581">the</TOKEN>
<TOKEN end_char="1591" id="token-11-5" morph="none" pos="word" start_char="1585">Pasteur</TOKEN>
<TOKEN end_char="1601" id="token-11-6" morph="none" pos="word" start_char="1593">Institute</TOKEN>
<TOKEN end_char="1604" id="token-11-7" morph="none" pos="word" start_char="1603">in</TOKEN>
<TOKEN end_char="1610" id="token-11-8" morph="none" pos="word" start_char="1606">Paris</TOKEN>
<TOKEN end_char="1615" id="token-11-9" morph="none" pos="word" start_char="1612">said</TOKEN>
<TOKEN end_char="1626" id="token-11-10" morph="none" pos="word" start_char="1617">suggesting</TOKEN>
<TOKEN end_char="1631" id="token-11-11" morph="none" pos="word" start_char="1628">that</TOKEN>
<TOKEN end_char="1635" id="token-11-12" morph="none" pos="word" start_char="1633">the</TOKEN>
<TOKEN end_char="1641" id="token-11-13" morph="none" pos="word" start_char="1637">virus</TOKEN>
<TOKEN end_char="1645" id="token-11-14" morph="none" pos="word" start_char="1643">was</TOKEN>
<TOKEN end_char="1654" id="token-11-15" morph="none" pos="unknown" start_char="1647">man-made</TOKEN>
<TOKEN end_char="1662" id="token-11-16" morph="none" pos="word" start_char="1656">because</TOKEN>
<TOKEN end_char="1665" id="token-11-17" morph="none" pos="word" start_char="1664">of</TOKEN>
<TOKEN end_char="1669" id="token-11-18" morph="none" pos="word" start_char="1667">the</TOKEN>
<TOKEN end_char="1671" id="token-11-19" morph="none" pos="punct" start_char="1671">"</TOKEN>
<TOKEN end_char="1676" id="token-11-20" morph="none" pos="word" start_char="1672">small</TOKEN>
<TOKEN end_char="1679" id="token-11-21" morph="none" pos="punct" start_char="1677">...</TOKEN>
<TOKEN end_char="1686" id="token-11-22" morph="none" pos="word" start_char="1681">pieces</TOKEN>
<TOKEN end_char="1689" id="token-11-23" morph="none" pos="word" start_char="1688">of</TOKEN>
<TOKEN end_char="1697" id="token-11-24" morph="none" pos="word" start_char="1691">genomes</TOKEN>
<TOKEN end_char="1698" id="token-11-25" morph="none" pos="punct" start_char="1698">"</TOKEN>
<TOKEN end_char="1701" id="token-11-26" morph="none" pos="word" start_char="1700">in</TOKEN>
<TOKEN end_char="1704" id="token-11-27" morph="none" pos="word" start_char="1703">it</TOKEN>
<TOKEN end_char="1709" id="token-11-28" morph="none" pos="word" start_char="1706">that</TOKEN>
<TOKEN end_char="1713" id="token-11-29" morph="none" pos="word" start_char="1711">are</TOKEN>
<TOKEN end_char="1720" id="token-11-30" morph="none" pos="word" start_char="1715">anyway</TOKEN>
<TOKEN end_char="1726" id="token-11-31" morph="none" pos="word" start_char="1722">found</TOKEN>
<TOKEN end_char="1729" id="token-11-32" morph="none" pos="word" start_char="1728">in</TOKEN>
<TOKEN end_char="1735" id="token-11-33" morph="none" pos="word" start_char="1731">other</TOKEN>
<TOKEN end_char="1743" id="token-11-34" morph="none" pos="word" start_char="1737">viruses</TOKEN>
<TOKEN end_char="1746" id="token-11-35" morph="none" pos="word" start_char="1745">of</TOKEN>
<TOKEN end_char="1750" id="token-11-36" morph="none" pos="word" start_char="1748">the</TOKEN>
<TOKEN end_char="1755" id="token-11-37" morph="none" pos="word" start_char="1752">same</TOKEN>
<TOKEN end_char="1762" id="token-11-38" morph="none" pos="word" start_char="1757">family</TOKEN>
<TOKEN end_char="1769" id="token-11-39" morph="none" pos="word" start_char="1764">simply</TOKEN>
<TOKEN end_char="1771" id="token-11-40" morph="none" pos="punct" start_char="1771">"</TOKEN>
<TOKEN end_char="1775" id="token-11-41" morph="none" pos="word" start_char="1772">does</TOKEN>
<TOKEN end_char="1779" id="token-11-42" morph="none" pos="word" start_char="1777">not</TOKEN>
<TOKEN end_char="1784" id="token-11-43" morph="none" pos="word" start_char="1781">make</TOKEN>
<TOKEN end_char="1790" id="token-11-44" morph="none" pos="word" start_char="1786">sense</TOKEN>
<TOKEN end_char="1792" id="token-11-45" morph="none" pos="punct" start_char="1791">."</TOKEN>
</SEG>
<SEG end_char="1940" id="segment-12" start_char="1795">
<ORIGINAL_TEXT>He said the genome pieces found in the SARS-CoV-2 "actually look like lots of sequences in the genetic material of bacteria, viruses and plants...</ORIGINAL_TEXT>
<TOKEN end_char="1796" id="token-12-0" morph="none" pos="word" start_char="1795">He</TOKEN>
<TOKEN end_char="1801" id="token-12-1" morph="none" pos="word" start_char="1798">said</TOKEN>
<TOKEN end_char="1805" id="token-12-2" morph="none" pos="word" start_char="1803">the</TOKEN>
<TOKEN end_char="1812" id="token-12-3" morph="none" pos="word" start_char="1807">genome</TOKEN>
<TOKEN end_char="1819" id="token-12-4" morph="none" pos="word" start_char="1814">pieces</TOKEN>
<TOKEN end_char="1825" id="token-12-5" morph="none" pos="word" start_char="1821">found</TOKEN>
<TOKEN end_char="1828" id="token-12-6" morph="none" pos="word" start_char="1827">in</TOKEN>
<TOKEN end_char="1832" id="token-12-7" morph="none" pos="word" start_char="1830">the</TOKEN>
<TOKEN end_char="1843" id="token-12-8" morph="none" pos="unknown" start_char="1834">SARS-CoV-2</TOKEN>
<TOKEN end_char="1845" id="token-12-9" morph="none" pos="punct" start_char="1845">"</TOKEN>
<TOKEN end_char="1853" id="token-12-10" morph="none" pos="word" start_char="1846">actually</TOKEN>
<TOKEN end_char="1858" id="token-12-11" morph="none" pos="word" start_char="1855">look</TOKEN>
<TOKEN end_char="1863" id="token-12-12" morph="none" pos="word" start_char="1860">like</TOKEN>
<TOKEN end_char="1868" id="token-12-13" morph="none" pos="word" start_char="1865">lots</TOKEN>
<TOKEN end_char="1871" id="token-12-14" morph="none" pos="word" start_char="1870">of</TOKEN>
<TOKEN end_char="1881" id="token-12-15" morph="none" pos="word" start_char="1873">sequences</TOKEN>
<TOKEN end_char="1884" id="token-12-16" morph="none" pos="word" start_char="1883">in</TOKEN>
<TOKEN end_char="1888" id="token-12-17" morph="none" pos="word" start_char="1886">the</TOKEN>
<TOKEN end_char="1896" id="token-12-18" morph="none" pos="word" start_char="1890">genetic</TOKEN>
<TOKEN end_char="1905" id="token-12-19" morph="none" pos="word" start_char="1898">material</TOKEN>
<TOKEN end_char="1908" id="token-12-20" morph="none" pos="word" start_char="1907">of</TOKEN>
<TOKEN end_char="1917" id="token-12-21" morph="none" pos="word" start_char="1910">bacteria</TOKEN>
<TOKEN end_char="1918" id="token-12-22" morph="none" pos="punct" start_char="1918">,</TOKEN>
<TOKEN end_char="1926" id="token-12-23" morph="none" pos="word" start_char="1920">viruses</TOKEN>
<TOKEN end_char="1930" id="token-12-24" morph="none" pos="word" start_char="1928">and</TOKEN>
<TOKEN end_char="1937" id="token-12-25" morph="none" pos="word" start_char="1932">plants</TOKEN>
<TOKEN end_char="1940" id="token-12-26" morph="none" pos="punct" start_char="1938">...</TOKEN>
</SEG>
<SEG end_char="2064" id="segment-13" start_char="1942">
<ORIGINAL_TEXT>If we take a word from a book and that word resembles that of another book, can we say that one has copied from the other?"</ORIGINAL_TEXT>
<TOKEN end_char="1943" id="token-13-0" morph="none" pos="word" start_char="1942">If</TOKEN>
<TOKEN end_char="1946" id="token-13-1" morph="none" pos="word" start_char="1945">we</TOKEN>
<TOKEN end_char="1951" id="token-13-2" morph="none" pos="word" start_char="1948">take</TOKEN>
<TOKEN end_char="1953" id="token-13-3" morph="none" pos="word" start_char="1953">a</TOKEN>
<TOKEN end_char="1958" id="token-13-4" morph="none" pos="word" start_char="1955">word</TOKEN>
<TOKEN end_char="1963" id="token-13-5" morph="none" pos="word" start_char="1960">from</TOKEN>
<TOKEN end_char="1965" id="token-13-6" morph="none" pos="word" start_char="1965">a</TOKEN>
<TOKEN end_char="1970" id="token-13-7" morph="none" pos="word" start_char="1967">book</TOKEN>
<TOKEN end_char="1974" id="token-13-8" morph="none" pos="word" start_char="1972">and</TOKEN>
<TOKEN end_char="1979" id="token-13-9" morph="none" pos="word" start_char="1976">that</TOKEN>
<TOKEN end_char="1984" id="token-13-10" morph="none" pos="word" start_char="1981">word</TOKEN>
<TOKEN end_char="1994" id="token-13-11" morph="none" pos="word" start_char="1986">resembles</TOKEN>
<TOKEN end_char="1999" id="token-13-12" morph="none" pos="word" start_char="1996">that</TOKEN>
<TOKEN end_char="2002" id="token-13-13" morph="none" pos="word" start_char="2001">of</TOKEN>
<TOKEN end_char="2010" id="token-13-14" morph="none" pos="word" start_char="2004">another</TOKEN>
<TOKEN end_char="2015" id="token-13-15" morph="none" pos="word" start_char="2012">book</TOKEN>
<TOKEN end_char="2016" id="token-13-16" morph="none" pos="punct" start_char="2016">,</TOKEN>
<TOKEN end_char="2020" id="token-13-17" morph="none" pos="word" start_char="2018">can</TOKEN>
<TOKEN end_char="2023" id="token-13-18" morph="none" pos="word" start_char="2022">we</TOKEN>
<TOKEN end_char="2027" id="token-13-19" morph="none" pos="word" start_char="2025">say</TOKEN>
<TOKEN end_char="2032" id="token-13-20" morph="none" pos="word" start_char="2029">that</TOKEN>
<TOKEN end_char="2036" id="token-13-21" morph="none" pos="word" start_char="2034">one</TOKEN>
<TOKEN end_char="2040" id="token-13-22" morph="none" pos="word" start_char="2038">has</TOKEN>
<TOKEN end_char="2047" id="token-13-23" morph="none" pos="word" start_char="2042">copied</TOKEN>
<TOKEN end_char="2052" id="token-13-24" morph="none" pos="word" start_char="2049">from</TOKEN>
<TOKEN end_char="2056" id="token-13-25" morph="none" pos="word" start_char="2054">the</TOKEN>
<TOKEN end_char="2062" id="token-13-26" morph="none" pos="word" start_char="2058">other</TOKEN>
<TOKEN end_char="2064" id="token-13-27" morph="none" pos="punct" start_char="2063">?"</TOKEN>
</SEG>
<SEG end_char="2242" id="segment-14" start_char="2067">
<ORIGINAL_TEXT>Others noted that the paper cited by Montagnier to back up his theory and published by researchers in India was not peer-reviewed and had already been withdrawn by its authors.</ORIGINAL_TEXT>
<TOKEN end_char="2072" id="token-14-0" morph="none" pos="word" start_char="2067">Others</TOKEN>
<TOKEN end_char="2078" id="token-14-1" morph="none" pos="word" start_char="2074">noted</TOKEN>
<TOKEN end_char="2083" id="token-14-2" morph="none" pos="word" start_char="2080">that</TOKEN>
<TOKEN end_char="2087" id="token-14-3" morph="none" pos="word" start_char="2085">the</TOKEN>
<TOKEN end_char="2093" id="token-14-4" morph="none" pos="word" start_char="2089">paper</TOKEN>
<TOKEN end_char="2099" id="token-14-5" morph="none" pos="word" start_char="2095">cited</TOKEN>
<TOKEN end_char="2102" id="token-14-6" morph="none" pos="word" start_char="2101">by</TOKEN>
<TOKEN end_char="2113" id="token-14-7" morph="none" pos="word" start_char="2104">Montagnier</TOKEN>
<TOKEN end_char="2116" id="token-14-8" morph="none" pos="word" start_char="2115">to</TOKEN>
<TOKEN end_char="2121" id="token-14-9" morph="none" pos="word" start_char="2118">back</TOKEN>
<TOKEN end_char="2124" id="token-14-10" morph="none" pos="word" start_char="2123">up</TOKEN>
<TOKEN end_char="2128" id="token-14-11" morph="none" pos="word" start_char="2126">his</TOKEN>
<TOKEN end_char="2135" id="token-14-12" morph="none" pos="word" start_char="2130">theory</TOKEN>
<TOKEN end_char="2139" id="token-14-13" morph="none" pos="word" start_char="2137">and</TOKEN>
<TOKEN end_char="2149" id="token-14-14" morph="none" pos="word" start_char="2141">published</TOKEN>
<TOKEN end_char="2152" id="token-14-15" morph="none" pos="word" start_char="2151">by</TOKEN>
<TOKEN end_char="2164" id="token-14-16" morph="none" pos="word" start_char="2154">researchers</TOKEN>
<TOKEN end_char="2167" id="token-14-17" morph="none" pos="word" start_char="2166">in</TOKEN>
<TOKEN end_char="2173" id="token-14-18" morph="none" pos="word" start_char="2169">India</TOKEN>
<TOKEN end_char="2177" id="token-14-19" morph="none" pos="word" start_char="2175">was</TOKEN>
<TOKEN end_char="2181" id="token-14-20" morph="none" pos="word" start_char="2179">not</TOKEN>
<TOKEN end_char="2195" id="token-14-21" morph="none" pos="unknown" start_char="2183">peer-reviewed</TOKEN>
<TOKEN end_char="2199" id="token-14-22" morph="none" pos="word" start_char="2197">and</TOKEN>
<TOKEN end_char="2203" id="token-14-23" morph="none" pos="word" start_char="2201">had</TOKEN>
<TOKEN end_char="2211" id="token-14-24" morph="none" pos="word" start_char="2205">already</TOKEN>
<TOKEN end_char="2216" id="token-14-25" morph="none" pos="word" start_char="2213">been</TOKEN>
<TOKEN end_char="2226" id="token-14-26" morph="none" pos="word" start_char="2218">withdrawn</TOKEN>
<TOKEN end_char="2229" id="token-14-27" morph="none" pos="word" start_char="2228">by</TOKEN>
<TOKEN end_char="2233" id="token-14-28" morph="none" pos="word" start_char="2231">its</TOKEN>
<TOKEN end_char="2241" id="token-14-29" morph="none" pos="word" start_char="2235">authors</TOKEN>
<TOKEN end_char="2242" id="token-14-30" morph="none" pos="punct" start_char="2242">.</TOKEN>
</SEG>
<SEG end_char="2268" id="segment-15" start_char="2245">
<ORIGINAL_TEXT>COVID-19 origin question</ORIGINAL_TEXT>
<TOKEN end_char="2252" id="token-15-0" morph="none" pos="unknown" start_char="2245">COVID-19</TOKEN>
<TOKEN end_char="2259" id="token-15-1" morph="none" pos="word" start_char="2254">origin</TOKEN>
<TOKEN end_char="2268" id="token-15-2" morph="none" pos="word" start_char="2261">question</TOKEN>
<TRANSLATED_TEXT>COVID-19 ursprungsbericht</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="2473" id="segment-16" start_char="2271">
<ORIGINAL_TEXT>Montagnier asserted that questions on the true origin of the virus would continue to be raised for a long time as the whole world has already spent trillions of dollars in the fight against the pandemic.</ORIGINAL_TEXT>
<TOKEN end_char="2280" id="token-16-0" morph="none" pos="word" start_char="2271">Montagnier</TOKEN>
<TOKEN end_char="2289" id="token-16-1" morph="none" pos="word" start_char="2282">asserted</TOKEN>
<TOKEN end_char="2294" id="token-16-2" morph="none" pos="word" start_char="2291">that</TOKEN>
<TOKEN end_char="2304" id="token-16-3" morph="none" pos="word" start_char="2296">questions</TOKEN>
<TOKEN end_char="2307" id="token-16-4" morph="none" pos="word" start_char="2306">on</TOKEN>
<TOKEN end_char="2311" id="token-16-5" morph="none" pos="word" start_char="2309">the</TOKEN>
<TOKEN end_char="2316" id="token-16-6" morph="none" pos="word" start_char="2313">true</TOKEN>
<TOKEN end_char="2323" id="token-16-7" morph="none" pos="word" start_char="2318">origin</TOKEN>
<TOKEN end_char="2326" id="token-16-8" morph="none" pos="word" start_char="2325">of</TOKEN>
<TOKEN end_char="2330" id="token-16-9" morph="none" pos="word" start_char="2328">the</TOKEN>
<TOKEN end_char="2336" id="token-16-10" morph="none" pos="word" start_char="2332">virus</TOKEN>
<TOKEN end_char="2342" id="token-16-11" morph="none" pos="word" start_char="2338">would</TOKEN>
<TOKEN end_char="2351" id="token-16-12" morph="none" pos="word" start_char="2344">continue</TOKEN>
<TOKEN end_char="2354" id="token-16-13" morph="none" pos="word" start_char="2353">to</TOKEN>
<TOKEN end_char="2357" id="token-16-14" morph="none" pos="word" start_char="2356">be</TOKEN>
<TOKEN end_char="2364" id="token-16-15" morph="none" pos="word" start_char="2359">raised</TOKEN>
<TOKEN end_char="2368" id="token-16-16" morph="none" pos="word" start_char="2366">for</TOKEN>
<TOKEN end_char="2370" id="token-16-17" morph="none" pos="word" start_char="2370">a</TOKEN>
<TOKEN end_char="2375" id="token-16-18" morph="none" pos="word" start_char="2372">long</TOKEN>
<TOKEN end_char="2380" id="token-16-19" morph="none" pos="word" start_char="2377">time</TOKEN>
<TOKEN end_char="2383" id="token-16-20" morph="none" pos="word" start_char="2382">as</TOKEN>
<TOKEN end_char="2387" id="token-16-21" morph="none" pos="word" start_char="2385">the</TOKEN>
<TOKEN end_char="2393" id="token-16-22" morph="none" pos="word" start_char="2389">whole</TOKEN>
<TOKEN end_char="2399" id="token-16-23" morph="none" pos="word" start_char="2395">world</TOKEN>
<TOKEN end_char="2403" id="token-16-24" morph="none" pos="word" start_char="2401">has</TOKEN>
<TOKEN end_char="2411" id="token-16-25" morph="none" pos="word" start_char="2405">already</TOKEN>
<TOKEN end_char="2417" id="token-16-26" morph="none" pos="word" start_char="2413">spent</TOKEN>
<TOKEN end_char="2427" id="token-16-27" morph="none" pos="word" start_char="2419">trillions</TOKEN>
<TOKEN end_char="2430" id="token-16-28" morph="none" pos="word" start_char="2429">of</TOKEN>
<TOKEN end_char="2438" id="token-16-29" morph="none" pos="word" start_char="2432">dollars</TOKEN>
<TOKEN end_char="2441" id="token-16-30" morph="none" pos="word" start_char="2440">in</TOKEN>
<TOKEN end_char="2445" id="token-16-31" morph="none" pos="word" start_char="2443">the</TOKEN>
<TOKEN end_char="2451" id="token-16-32" morph="none" pos="word" start_char="2447">fight</TOKEN>
<TOKEN end_char="2459" id="token-16-33" morph="none" pos="word" start_char="2453">against</TOKEN>
<TOKEN end_char="2463" id="token-16-34" morph="none" pos="word" start_char="2461">the</TOKEN>
<TOKEN end_char="2472" id="token-16-35" morph="none" pos="word" start_char="2465">pandemic</TOKEN>
<TOKEN end_char="2473" id="token-16-36" morph="none" pos="punct" start_char="2473">.</TOKEN>
</SEG>
<SEG end_char="2646" id="segment-17" start_char="2476">
<ORIGINAL_TEXT>He inferred that the allegedly man-made virus, whose genome consists of a "clockwork of sequences" and includes elements of HIV, could not have been assembled by amateurs.</ORIGINAL_TEXT>
<TOKEN end_char="2477" id="token-17-0" morph="none" pos="word" start_char="2476">He</TOKEN>
<TOKEN end_char="2486" id="token-17-1" morph="none" pos="word" start_char="2479">inferred</TOKEN>
<TOKEN end_char="2491" id="token-17-2" morph="none" pos="word" start_char="2488">that</TOKEN>
<TOKEN end_char="2495" id="token-17-3" morph="none" pos="word" start_char="2493">the</TOKEN>
<TOKEN end_char="2505" id="token-17-4" morph="none" pos="word" start_char="2497">allegedly</TOKEN>
<TOKEN end_char="2514" id="token-17-5" morph="none" pos="unknown" start_char="2507">man-made</TOKEN>
<TOKEN end_char="2520" id="token-17-6" morph="none" pos="word" start_char="2516">virus</TOKEN>
<TOKEN end_char="2521" id="token-17-7" morph="none" pos="punct" start_char="2521">,</TOKEN>
<TOKEN end_char="2527" id="token-17-8" morph="none" pos="word" start_char="2523">whose</TOKEN>
<TOKEN end_char="2534" id="token-17-9" morph="none" pos="word" start_char="2529">genome</TOKEN>
<TOKEN end_char="2543" id="token-17-10" morph="none" pos="word" start_char="2536">consists</TOKEN>
<TOKEN end_char="2546" id="token-17-11" morph="none" pos="word" start_char="2545">of</TOKEN>
<TOKEN end_char="2548" id="token-17-12" morph="none" pos="word" start_char="2548">a</TOKEN>
<TOKEN end_char="2550" id="token-17-13" morph="none" pos="punct" start_char="2550">"</TOKEN>
<TOKEN end_char="2559" id="token-17-14" morph="none" pos="word" start_char="2551">clockwork</TOKEN>
<TOKEN end_char="2562" id="token-17-15" morph="none" pos="word" start_char="2561">of</TOKEN>
<TOKEN end_char="2572" id="token-17-16" morph="none" pos="word" start_char="2564">sequences</TOKEN>
<TOKEN end_char="2573" id="token-17-17" morph="none" pos="punct" start_char="2573">"</TOKEN>
<TOKEN end_char="2577" id="token-17-18" morph="none" pos="word" start_char="2575">and</TOKEN>
<TOKEN end_char="2586" id="token-17-19" morph="none" pos="word" start_char="2579">includes</TOKEN>
<TOKEN end_char="2595" id="token-17-20" morph="none" pos="word" start_char="2588">elements</TOKEN>
<TOKEN end_char="2598" id="token-17-21" morph="none" pos="word" start_char="2597">of</TOKEN>
<TOKEN end_char="2602" id="token-17-22" morph="none" pos="word" start_char="2600">HIV</TOKEN>
<TOKEN end_char="2603" id="token-17-23" morph="none" pos="punct" start_char="2603">,</TOKEN>
<TOKEN end_char="2609" id="token-17-24" morph="none" pos="word" start_char="2605">could</TOKEN>
<TOKEN end_char="2613" id="token-17-25" morph="none" pos="word" start_char="2611">not</TOKEN>
<TOKEN end_char="2618" id="token-17-26" morph="none" pos="word" start_char="2615">have</TOKEN>
<TOKEN end_char="2623" id="token-17-27" morph="none" pos="word" start_char="2620">been</TOKEN>
<TOKEN end_char="2633" id="token-17-28" morph="none" pos="word" start_char="2625">assembled</TOKEN>
<TOKEN end_char="2636" id="token-17-29" morph="none" pos="word" start_char="2635">by</TOKEN>
<TOKEN end_char="2645" id="token-17-30" morph="none" pos="word" start_char="2638">amateurs</TOKEN>
<TOKEN end_char="2646" id="token-17-31" morph="none" pos="punct" start_char="2646">.</TOKEN>
</SEG>
<SEG end_char="2753" id="segment-18" start_char="2649">
<ORIGINAL_TEXT>And questions about its origin and purpose would not escape scrutiny, and are unlikely to disappear soon.</ORIGINAL_TEXT>
<TOKEN end_char="2651" id="token-18-0" morph="none" pos="word" start_char="2649">And</TOKEN>
<TOKEN end_char="2661" id="token-18-1" morph="none" pos="word" start_char="2653">questions</TOKEN>
<TOKEN end_char="2667" id="token-18-2" morph="none" pos="word" start_char="2663">about</TOKEN>
<TOKEN end_char="2671" id="token-18-3" morph="none" pos="word" start_char="2669">its</TOKEN>
<TOKEN end_char="2678" id="token-18-4" morph="none" pos="word" start_char="2673">origin</TOKEN>
<TOKEN end_char="2682" id="token-18-5" morph="none" pos="word" start_char="2680">and</TOKEN>
<TOKEN end_char="2690" id="token-18-6" morph="none" pos="word" start_char="2684">purpose</TOKEN>
<TOKEN end_char="2696" id="token-18-7" morph="none" pos="word" start_char="2692">would</TOKEN>
<TOKEN end_char="2700" id="token-18-8" morph="none" pos="word" start_char="2698">not</TOKEN>
<TOKEN end_char="2707" id="token-18-9" morph="none" pos="word" start_char="2702">escape</TOKEN>
<TOKEN end_char="2716" id="token-18-10" morph="none" pos="word" start_char="2709">scrutiny</TOKEN>
<TOKEN end_char="2717" id="token-18-11" morph="none" pos="punct" start_char="2717">,</TOKEN>
<TOKEN end_char="2721" id="token-18-12" morph="none" pos="word" start_char="2719">and</TOKEN>
<TOKEN end_char="2725" id="token-18-13" morph="none" pos="word" start_char="2723">are</TOKEN>
<TOKEN end_char="2734" id="token-18-14" morph="none" pos="word" start_char="2727">unlikely</TOKEN>
<TOKEN end_char="2737" id="token-18-15" morph="none" pos="word" start_char="2736">to</TOKEN>
<TOKEN end_char="2747" id="token-18-16" morph="none" pos="word" start_char="2739">disappear</TOKEN>
<TOKEN end_char="2752" id="token-18-17" morph="none" pos="word" start_char="2749">soon</TOKEN>
<TOKEN end_char="2753" id="token-18-18" morph="none" pos="punct" start_char="2753">.</TOKEN>
</SEG>
<SEG end_char="2855" id="segment-19" start_char="2756">
<ORIGINAL_TEXT>On April 3, Blomberg reported that the global cost of COVID-19 was estimated to be at $4.1 trillion.</ORIGINAL_TEXT>
<TOKEN end_char="2757" id="token-19-0" morph="none" pos="word" start_char="2756">On</TOKEN>
<TOKEN end_char="2763" id="token-19-1" morph="none" pos="word" start_char="2759">April</TOKEN>
<TOKEN end_char="2765" id="token-19-2" morph="none" pos="word" start_char="2765">3</TOKEN>
<TOKEN end_char="2766" id="token-19-3" morph="none" pos="punct" start_char="2766">,</TOKEN>
<TOKEN end_char="2775" id="token-19-4" morph="none" pos="word" start_char="2768">Blomberg</TOKEN>
<TOKEN end_char="2784" id="token-19-5" morph="none" pos="word" start_char="2777">reported</TOKEN>
<TOKEN end_char="2789" id="token-19-6" morph="none" pos="word" start_char="2786">that</TOKEN>
<TOKEN end_char="2793" id="token-19-7" morph="none" pos="word" start_char="2791">the</TOKEN>
<TOKEN end_char="2800" id="token-19-8" morph="none" pos="word" start_char="2795">global</TOKEN>
<TOKEN end_char="2805" id="token-19-9" morph="none" pos="word" start_char="2802">cost</TOKEN>
<TOKEN end_char="2808" id="token-19-10" morph="none" pos="word" start_char="2807">of</TOKEN>
<TOKEN end_char="2817" id="token-19-11" morph="none" pos="unknown" start_char="2810">COVID-19</TOKEN>
<TOKEN end_char="2821" id="token-19-12" morph="none" pos="word" start_char="2819">was</TOKEN>
<TOKEN end_char="2831" id="token-19-13" morph="none" pos="word" start_char="2823">estimated</TOKEN>
<TOKEN end_char="2834" id="token-19-14" morph="none" pos="word" start_char="2833">to</TOKEN>
<TOKEN end_char="2837" id="token-19-15" morph="none" pos="word" start_char="2836">be</TOKEN>
<TOKEN end_char="2840" id="token-19-16" morph="none" pos="word" start_char="2839">at</TOKEN>
<TOKEN end_char="2845" id="token-19-17" morph="none" pos="unknown" start_char="2842">$4.1</TOKEN>
<TOKEN end_char="2854" id="token-19-18" morph="none" pos="word" start_char="2847">trillion</TOKEN>
<TOKEN end_char="2855" id="token-19-19" morph="none" pos="punct" start_char="2855">.</TOKEN>
</SEG>
<SEG end_char="3028" id="segment-20" start_char="2858">
<ORIGINAL_TEXT>A CNBC report on April 9 predicted that the resulting coronavirus depression would be bigger than the one in 2008, and would bleed the world of up to $20 trillion or more.</ORIGINAL_TEXT>
<TOKEN end_char="2858" id="token-20-0" morph="none" pos="word" start_char="2858">A</TOKEN>
<TOKEN end_char="2863" id="token-20-1" morph="none" pos="word" start_char="2860">CNBC</TOKEN>
<TOKEN end_char="2870" id="token-20-2" morph="none" pos="word" start_char="2865">report</TOKEN>
<TOKEN end_char="2873" id="token-20-3" morph="none" pos="word" start_char="2872">on</TOKEN>
<TOKEN end_char="2879" id="token-20-4" morph="none" pos="word" start_char="2875">April</TOKEN>
<TOKEN end_char="2881" id="token-20-5" morph="none" pos="word" start_char="2881">9</TOKEN>
<TOKEN end_char="2891" id="token-20-6" morph="none" pos="word" start_char="2883">predicted</TOKEN>
<TOKEN end_char="2896" id="token-20-7" morph="none" pos="word" start_char="2893">that</TOKEN>
<TOKEN end_char="2900" id="token-20-8" morph="none" pos="word" start_char="2898">the</TOKEN>
<TOKEN end_char="2910" id="token-20-9" morph="none" pos="word" start_char="2902">resulting</TOKEN>
<TOKEN end_char="2922" id="token-20-10" morph="none" pos="word" start_char="2912">coronavirus</TOKEN>
<TOKEN end_char="2933" id="token-20-11" morph="none" pos="word" start_char="2924">depression</TOKEN>
<TOKEN end_char="2939" id="token-20-12" morph="none" pos="word" start_char="2935">would</TOKEN>
<TOKEN end_char="2942" id="token-20-13" morph="none" pos="word" start_char="2941">be</TOKEN>
<TOKEN end_char="2949" id="token-20-14" morph="none" pos="word" start_char="2944">bigger</TOKEN>
<TOKEN end_char="2954" id="token-20-15" morph="none" pos="word" start_char="2951">than</TOKEN>
<TOKEN end_char="2958" id="token-20-16" morph="none" pos="word" start_char="2956">the</TOKEN>
<TOKEN end_char="2962" id="token-20-17" morph="none" pos="word" start_char="2960">one</TOKEN>
<TOKEN end_char="2965" id="token-20-18" morph="none" pos="word" start_char="2964">in</TOKEN>
<TOKEN end_char="2970" id="token-20-19" morph="none" pos="word" start_char="2967">2008</TOKEN>
<TOKEN end_char="2971" id="token-20-20" morph="none" pos="punct" start_char="2971">,</TOKEN>
<TOKEN end_char="2975" id="token-20-21" morph="none" pos="word" start_char="2973">and</TOKEN>
<TOKEN end_char="2981" id="token-20-22" morph="none" pos="word" start_char="2977">would</TOKEN>
<TOKEN end_char="2987" id="token-20-23" morph="none" pos="word" start_char="2983">bleed</TOKEN>
<TOKEN end_char="2991" id="token-20-24" morph="none" pos="word" start_char="2989">the</TOKEN>
<TOKEN end_char="2997" id="token-20-25" morph="none" pos="word" start_char="2993">world</TOKEN>
<TOKEN end_char="3000" id="token-20-26" morph="none" pos="word" start_char="2999">of</TOKEN>
<TOKEN end_char="3003" id="token-20-27" morph="none" pos="word" start_char="3002">up</TOKEN>
<TOKEN end_char="3006" id="token-20-28" morph="none" pos="word" start_char="3005">to</TOKEN>
<TOKEN end_char="3010" id="token-20-29" morph="none" pos="unknown" start_char="3008">$20</TOKEN>
<TOKEN end_char="3019" id="token-20-30" morph="none" pos="word" start_char="3012">trillion</TOKEN>
<TOKEN end_char="3022" id="token-20-31" morph="none" pos="word" start_char="3021">or</TOKEN>
<TOKEN end_char="3027" id="token-20-32" morph="none" pos="word" start_char="3024">more</TOKEN>
<TOKEN end_char="3028" id="token-20-33" morph="none" pos="punct" start_char="3028">.</TOKEN>
</SEG>
<SEG end_char="3145" id="segment-21" start_char="3031">
<ORIGINAL_TEXT>As of May 1, 2020 the global total of COVID-19 cases was at 3,308,233 with 234,105 deaths and 1,042,819 recoveries.</ORIGINAL_TEXT>
<TOKEN end_char="3032" id="token-21-0" morph="none" pos="word" start_char="3031">As</TOKEN>
<TOKEN end_char="3035" id="token-21-1" morph="none" pos="word" start_char="3034">of</TOKEN>
<TOKEN end_char="3039" id="token-21-2" morph="none" pos="word" start_char="3037">May</TOKEN>
<TOKEN end_char="3041" id="token-21-3" morph="none" pos="word" start_char="3041">1</TOKEN>
<TOKEN end_char="3042" id="token-21-4" morph="none" pos="punct" start_char="3042">,</TOKEN>
<TOKEN end_char="3047" id="token-21-5" morph="none" pos="word" start_char="3044">2020</TOKEN>
<TOKEN end_char="3051" id="token-21-6" morph="none" pos="word" start_char="3049">the</TOKEN>
<TOKEN end_char="3058" id="token-21-7" morph="none" pos="word" start_char="3053">global</TOKEN>
<TOKEN end_char="3064" id="token-21-8" morph="none" pos="word" start_char="3060">total</TOKEN>
<TOKEN end_char="3067" id="token-21-9" morph="none" pos="word" start_char="3066">of</TOKEN>
<TOKEN end_char="3076" id="token-21-10" morph="none" pos="unknown" start_char="3069">COVID-19</TOKEN>
<TOKEN end_char="3082" id="token-21-11" morph="none" pos="word" start_char="3078">cases</TOKEN>
<TOKEN end_char="3086" id="token-21-12" morph="none" pos="word" start_char="3084">was</TOKEN>
<TOKEN end_char="3089" id="token-21-13" morph="none" pos="word" start_char="3088">at</TOKEN>
<TOKEN end_char="3099" id="token-21-14" morph="none" pos="unknown" start_char="3091">3,308,233</TOKEN>
<TOKEN end_char="3104" id="token-21-15" morph="none" pos="word" start_char="3101">with</TOKEN>
<TOKEN end_char="3112" id="token-21-16" morph="none" pos="unknown" start_char="3106">234,105</TOKEN>
<TOKEN end_char="3119" id="token-21-17" morph="none" pos="word" start_char="3114">deaths</TOKEN>
<TOKEN end_char="3123" id="token-21-18" morph="none" pos="word" start_char="3121">and</TOKEN>
<TOKEN end_char="3133" id="token-21-19" morph="none" pos="unknown" start_char="3125">1,042,819</TOKEN>
<TOKEN end_char="3144" id="token-21-20" morph="none" pos="word" start_char="3135">recoveries</TOKEN>
<TOKEN end_char="3145" id="token-21-21" morph="none" pos="punct" start_char="3145">.</TOKEN>
</SEG>
<SEG end_char="3160" id="segment-22" start_char="3147">
<ORIGINAL_TEXT>—MDM, GMA News</ORIGINAL_TEXT>
<TOKEN end_char="3147" id="token-22-0" morph="none" pos="punct" start_char="3147">—</TOKEN>
<TOKEN end_char="3150" id="token-22-1" morph="none" pos="word" start_char="3148">MDM</TOKEN>
<TOKEN end_char="3151" id="token-22-2" morph="none" pos="punct" start_char="3151">,</TOKEN>
<TOKEN end_char="3155" id="token-22-3" morph="none" pos="word" start_char="3153">GMA</TOKEN>
<TOKEN end_char="3160" id="token-22-4" morph="none" pos="word" start_char="3157">News</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>