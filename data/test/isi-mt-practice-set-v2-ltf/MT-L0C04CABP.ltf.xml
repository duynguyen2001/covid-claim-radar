<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C04CABP" lang="eng" raw_text_char_length="6102" raw_text_md5="6de4182e05ed20cbd8820c0f19bba0cd" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="70" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Coronavirus: Fact-checking claims it might have started in August 2019</ORIGINAL_TEXT>
<TOKEN end_char="11" id="token-0-0" morph="none" pos="word" start_char="1">Coronavirus</TOKEN>
<TOKEN end_char="12" id="token-0-1" morph="none" pos="punct" start_char="12">:</TOKEN>
<TOKEN end_char="26" id="token-0-2" morph="none" pos="unknown" start_char="14">Fact-checking</TOKEN>
<TOKEN end_char="33" id="token-0-3" morph="none" pos="word" start_char="28">claims</TOKEN>
<TOKEN end_char="36" id="token-0-4" morph="none" pos="word" start_char="35">it</TOKEN>
<TOKEN end_char="42" id="token-0-5" morph="none" pos="word" start_char="38">might</TOKEN>
<TOKEN end_char="47" id="token-0-6" morph="none" pos="word" start_char="44">have</TOKEN>
<TOKEN end_char="55" id="token-0-7" morph="none" pos="word" start_char="49">started</TOKEN>
<TOKEN end_char="58" id="token-0-8" morph="none" pos="word" start_char="57">in</TOKEN>
<TOKEN end_char="65" id="token-0-9" morph="none" pos="word" start_char="60">August</TOKEN>
<TOKEN end_char="70" id="token-0-10" morph="none" pos="word" start_char="67">2019</TOKEN>
</SEG>
<SEG end_char="85" id="segment-1" start_char="74">
<ORIGINAL_TEXT>Getty Images</ORIGINAL_TEXT>
<TOKEN end_char="78" id="token-1-0" morph="none" pos="word" start_char="74">Getty</TOKEN>
<TOKEN end_char="85" id="token-1-1" morph="none" pos="word" start_char="80">Images</TOKEN>
</SEG>
<SEG end_char="182" id="segment-2" start_char="88">
<ORIGINAL_TEXT>The coronavirus outbreak was first seen in Wuhan - but was it circulating earlier than thought?</ORIGINAL_TEXT>
<TOKEN end_char="90" id="token-2-0" morph="none" pos="word" start_char="88">The</TOKEN>
<TOKEN end_char="102" id="token-2-1" morph="none" pos="word" start_char="92">coronavirus</TOKEN>
<TOKEN end_char="111" id="token-2-2" morph="none" pos="word" start_char="104">outbreak</TOKEN>
<TOKEN end_char="115" id="token-2-3" morph="none" pos="word" start_char="113">was</TOKEN>
<TOKEN end_char="121" id="token-2-4" morph="none" pos="word" start_char="117">first</TOKEN>
<TOKEN end_char="126" id="token-2-5" morph="none" pos="word" start_char="123">seen</TOKEN>
<TOKEN end_char="129" id="token-2-6" morph="none" pos="word" start_char="128">in</TOKEN>
<TOKEN end_char="135" id="token-2-7" morph="none" pos="word" start_char="131">Wuhan</TOKEN>
<TOKEN end_char="137" id="token-2-8" morph="none" pos="punct" start_char="137">-</TOKEN>
<TOKEN end_char="141" id="token-2-9" morph="none" pos="word" start_char="139">but</TOKEN>
<TOKEN end_char="145" id="token-2-10" morph="none" pos="word" start_char="143">was</TOKEN>
<TOKEN end_char="148" id="token-2-11" morph="none" pos="word" start_char="147">it</TOKEN>
<TOKEN end_char="160" id="token-2-12" morph="none" pos="word" start_char="150">circulating</TOKEN>
<TOKEN end_char="168" id="token-2-13" morph="none" pos="word" start_char="162">earlier</TOKEN>
<TOKEN end_char="173" id="token-2-14" morph="none" pos="word" start_char="170">than</TOKEN>
<TOKEN end_char="181" id="token-2-15" morph="none" pos="word" start_char="175">thought</TOKEN>
<TOKEN end_char="182" id="token-2-16" morph="none" pos="punct" start_char="182">?</TOKEN>
</SEG>
<SEG end_char="345" id="segment-3" start_char="186">
<ORIGINAL_TEXT>There's been criticism of a study from the US suggesting that the coronavirus could have been present in the Chinese city of Wuhan as early as August last year.</ORIGINAL_TEXT>
<TOKEN end_char="192" id="token-3-0" morph="none" pos="word" start_char="186">There's</TOKEN>
<TOKEN end_char="197" id="token-3-1" morph="none" pos="word" start_char="194">been</TOKEN>
<TOKEN end_char="207" id="token-3-2" morph="none" pos="word" start_char="199">criticism</TOKEN>
<TOKEN end_char="210" id="token-3-3" morph="none" pos="word" start_char="209">of</TOKEN>
<TOKEN end_char="212" id="token-3-4" morph="none" pos="word" start_char="212">a</TOKEN>
<TOKEN end_char="218" id="token-3-5" morph="none" pos="word" start_char="214">study</TOKEN>
<TOKEN end_char="223" id="token-3-6" morph="none" pos="word" start_char="220">from</TOKEN>
<TOKEN end_char="227" id="token-3-7" morph="none" pos="word" start_char="225">the</TOKEN>
<TOKEN end_char="230" id="token-3-8" morph="none" pos="word" start_char="229">US</TOKEN>
<TOKEN end_char="241" id="token-3-9" morph="none" pos="word" start_char="232">suggesting</TOKEN>
<TOKEN end_char="246" id="token-3-10" morph="none" pos="word" start_char="243">that</TOKEN>
<TOKEN end_char="250" id="token-3-11" morph="none" pos="word" start_char="248">the</TOKEN>
<TOKEN end_char="262" id="token-3-12" morph="none" pos="word" start_char="252">coronavirus</TOKEN>
<TOKEN end_char="268" id="token-3-13" morph="none" pos="word" start_char="264">could</TOKEN>
<TOKEN end_char="273" id="token-3-14" morph="none" pos="word" start_char="270">have</TOKEN>
<TOKEN end_char="278" id="token-3-15" morph="none" pos="word" start_char="275">been</TOKEN>
<TOKEN end_char="286" id="token-3-16" morph="none" pos="word" start_char="280">present</TOKEN>
<TOKEN end_char="289" id="token-3-17" morph="none" pos="word" start_char="288">in</TOKEN>
<TOKEN end_char="293" id="token-3-18" morph="none" pos="word" start_char="291">the</TOKEN>
<TOKEN end_char="301" id="token-3-19" morph="none" pos="word" start_char="295">Chinese</TOKEN>
<TOKEN end_char="306" id="token-3-20" morph="none" pos="word" start_char="303">city</TOKEN>
<TOKEN end_char="309" id="token-3-21" morph="none" pos="word" start_char="308">of</TOKEN>
<TOKEN end_char="315" id="token-3-22" morph="none" pos="word" start_char="311">Wuhan</TOKEN>
<TOKEN end_char="318" id="token-3-23" morph="none" pos="word" start_char="317">as</TOKEN>
<TOKEN end_char="324" id="token-3-24" morph="none" pos="word" start_char="320">early</TOKEN>
<TOKEN end_char="327" id="token-3-25" morph="none" pos="word" start_char="326">as</TOKEN>
<TOKEN end_char="334" id="token-3-26" morph="none" pos="word" start_char="329">August</TOKEN>
<TOKEN end_char="339" id="token-3-27" morph="none" pos="word" start_char="336">last</TOKEN>
<TOKEN end_char="344" id="token-3-28" morph="none" pos="word" start_char="341">year</TOKEN>
<TOKEN end_char="345" id="token-3-29" morph="none" pos="punct" start_char="345">.</TOKEN>
</SEG>
<SEG end_char="521" id="segment-4" start_char="348">
<ORIGINAL_TEXT>The study by Harvard University, widely publicised when released earlier this month, has been dismissed by China and had its methodology challenged by independent scientists.</ORIGINAL_TEXT>
<TOKEN end_char="350" id="token-4-0" morph="none" pos="word" start_char="348">The</TOKEN>
<TOKEN end_char="356" id="token-4-1" morph="none" pos="word" start_char="352">study</TOKEN>
<TOKEN end_char="359" id="token-4-2" morph="none" pos="word" start_char="358">by</TOKEN>
<TOKEN end_char="367" id="token-4-3" morph="none" pos="word" start_char="361">Harvard</TOKEN>
<TOKEN end_char="378" id="token-4-4" morph="none" pos="word" start_char="369">University</TOKEN>
<TOKEN end_char="379" id="token-4-5" morph="none" pos="punct" start_char="379">,</TOKEN>
<TOKEN end_char="386" id="token-4-6" morph="none" pos="word" start_char="381">widely</TOKEN>
<TOKEN end_char="397" id="token-4-7" morph="none" pos="word" start_char="388">publicised</TOKEN>
<TOKEN end_char="402" id="token-4-8" morph="none" pos="word" start_char="399">when</TOKEN>
<TOKEN end_char="411" id="token-4-9" morph="none" pos="word" start_char="404">released</TOKEN>
<TOKEN end_char="419" id="token-4-10" morph="none" pos="word" start_char="413">earlier</TOKEN>
<TOKEN end_char="424" id="token-4-11" morph="none" pos="word" start_char="421">this</TOKEN>
<TOKEN end_char="430" id="token-4-12" morph="none" pos="word" start_char="426">month</TOKEN>
<TOKEN end_char="431" id="token-4-13" morph="none" pos="punct" start_char="431">,</TOKEN>
<TOKEN end_char="435" id="token-4-14" morph="none" pos="word" start_char="433">has</TOKEN>
<TOKEN end_char="440" id="token-4-15" morph="none" pos="word" start_char="437">been</TOKEN>
<TOKEN end_char="450" id="token-4-16" morph="none" pos="word" start_char="442">dismissed</TOKEN>
<TOKEN end_char="453" id="token-4-17" morph="none" pos="word" start_char="452">by</TOKEN>
<TOKEN end_char="459" id="token-4-18" morph="none" pos="word" start_char="455">China</TOKEN>
<TOKEN end_char="463" id="token-4-19" morph="none" pos="word" start_char="461">and</TOKEN>
<TOKEN end_char="467" id="token-4-20" morph="none" pos="word" start_char="465">had</TOKEN>
<TOKEN end_char="471" id="token-4-21" morph="none" pos="word" start_char="469">its</TOKEN>
<TOKEN end_char="483" id="token-4-22" morph="none" pos="word" start_char="473">methodology</TOKEN>
<TOKEN end_char="494" id="token-4-23" morph="none" pos="word" start_char="485">challenged</TOKEN>
<TOKEN end_char="497" id="token-4-24" morph="none" pos="word" start_char="496">by</TOKEN>
<TOKEN end_char="509" id="token-4-25" morph="none" pos="word" start_char="499">independent</TOKEN>
<TOKEN end_char="520" id="token-4-26" morph="none" pos="word" start_char="511">scientists</TOKEN>
<TOKEN end_char="521" id="token-4-27" morph="none" pos="punct" start_char="521">.</TOKEN>
</SEG>
<SEG end_char="711" id="segment-5" start_char="524">
<ORIGINAL_TEXT>We've looked at the techniques used in the report and found shortcomings both in the use of satellite imagery and online search data, raising significant doubts about its overall findings.</ORIGINAL_TEXT>
<TOKEN end_char="528" id="token-5-0" morph="none" pos="word" start_char="524">We've</TOKEN>
<TOKEN end_char="535" id="token-5-1" morph="none" pos="word" start_char="530">looked</TOKEN>
<TOKEN end_char="538" id="token-5-2" morph="none" pos="word" start_char="537">at</TOKEN>
<TOKEN end_char="542" id="token-5-3" morph="none" pos="word" start_char="540">the</TOKEN>
<TOKEN end_char="553" id="token-5-4" morph="none" pos="word" start_char="544">techniques</TOKEN>
<TOKEN end_char="558" id="token-5-5" morph="none" pos="word" start_char="555">used</TOKEN>
<TOKEN end_char="561" id="token-5-6" morph="none" pos="word" start_char="560">in</TOKEN>
<TOKEN end_char="565" id="token-5-7" morph="none" pos="word" start_char="563">the</TOKEN>
<TOKEN end_char="572" id="token-5-8" morph="none" pos="word" start_char="567">report</TOKEN>
<TOKEN end_char="576" id="token-5-9" morph="none" pos="word" start_char="574">and</TOKEN>
<TOKEN end_char="582" id="token-5-10" morph="none" pos="word" start_char="578">found</TOKEN>
<TOKEN end_char="595" id="token-5-11" morph="none" pos="word" start_char="584">shortcomings</TOKEN>
<TOKEN end_char="600" id="token-5-12" morph="none" pos="word" start_char="597">both</TOKEN>
<TOKEN end_char="603" id="token-5-13" morph="none" pos="word" start_char="602">in</TOKEN>
<TOKEN end_char="607" id="token-5-14" morph="none" pos="word" start_char="605">the</TOKEN>
<TOKEN end_char="611" id="token-5-15" morph="none" pos="word" start_char="609">use</TOKEN>
<TOKEN end_char="614" id="token-5-16" morph="none" pos="word" start_char="613">of</TOKEN>
<TOKEN end_char="624" id="token-5-17" morph="none" pos="word" start_char="616">satellite</TOKEN>
<TOKEN end_char="632" id="token-5-18" morph="none" pos="word" start_char="626">imagery</TOKEN>
<TOKEN end_char="636" id="token-5-19" morph="none" pos="word" start_char="634">and</TOKEN>
<TOKEN end_char="643" id="token-5-20" morph="none" pos="word" start_char="638">online</TOKEN>
<TOKEN end_char="650" id="token-5-21" morph="none" pos="word" start_char="645">search</TOKEN>
<TOKEN end_char="655" id="token-5-22" morph="none" pos="word" start_char="652">data</TOKEN>
<TOKEN end_char="656" id="token-5-23" morph="none" pos="punct" start_char="656">,</TOKEN>
<TOKEN end_char="664" id="token-5-24" morph="none" pos="word" start_char="658">raising</TOKEN>
<TOKEN end_char="676" id="token-5-25" morph="none" pos="word" start_char="666">significant</TOKEN>
<TOKEN end_char="683" id="token-5-26" morph="none" pos="word" start_char="678">doubts</TOKEN>
<TOKEN end_char="689" id="token-5-27" morph="none" pos="word" start_char="685">about</TOKEN>
<TOKEN end_char="693" id="token-5-28" morph="none" pos="word" start_char="691">its</TOKEN>
<TOKEN end_char="701" id="token-5-29" morph="none" pos="word" start_char="695">overall</TOKEN>
<TOKEN end_char="710" id="token-5-30" morph="none" pos="word" start_char="703">findings</TOKEN>
<TOKEN end_char="711" id="token-5-31" morph="none" pos="punct" start_char="711">.</TOKEN>
</SEG>
<SEG end_char="739" id="segment-6" start_char="714">
<ORIGINAL_TEXT>What did the research say?</ORIGINAL_TEXT>
<TOKEN end_char="717" id="token-6-0" morph="none" pos="word" start_char="714">What</TOKEN>
<TOKEN end_char="721" id="token-6-1" morph="none" pos="word" start_char="719">did</TOKEN>
<TOKEN end_char="725" id="token-6-2" morph="none" pos="word" start_char="723">the</TOKEN>
<TOKEN end_char="734" id="token-6-3" morph="none" pos="word" start_char="727">research</TOKEN>
<TOKEN end_char="738" id="token-6-4" morph="none" pos="word" start_char="736">say</TOKEN>
<TOKEN end_char="739" id="token-6-5" morph="none" pos="punct" start_char="739">?</TOKEN>
</SEG>
<SEG end_char="898" id="segment-7" start_char="743">
<ORIGINAL_TEXT>The research is based on satellite imagery of traffic movements around hospitals in Wuhan and the tracking of online searches for specific medical symptoms.</ORIGINAL_TEXT>
<TOKEN end_char="745" id="token-7-0" morph="none" pos="word" start_char="743">The</TOKEN>
<TOKEN end_char="754" id="token-7-1" morph="none" pos="word" start_char="747">research</TOKEN>
<TOKEN end_char="757" id="token-7-2" morph="none" pos="word" start_char="756">is</TOKEN>
<TOKEN end_char="763" id="token-7-3" morph="none" pos="word" start_char="759">based</TOKEN>
<TOKEN end_char="766" id="token-7-4" morph="none" pos="word" start_char="765">on</TOKEN>
<TOKEN end_char="776" id="token-7-5" morph="none" pos="word" start_char="768">satellite</TOKEN>
<TOKEN end_char="784" id="token-7-6" morph="none" pos="word" start_char="778">imagery</TOKEN>
<TOKEN end_char="787" id="token-7-7" morph="none" pos="word" start_char="786">of</TOKEN>
<TOKEN end_char="795" id="token-7-8" morph="none" pos="word" start_char="789">traffic</TOKEN>
<TOKEN end_char="805" id="token-7-9" morph="none" pos="word" start_char="797">movements</TOKEN>
<TOKEN end_char="812" id="token-7-10" morph="none" pos="word" start_char="807">around</TOKEN>
<TOKEN end_char="822" id="token-7-11" morph="none" pos="word" start_char="814">hospitals</TOKEN>
<TOKEN end_char="825" id="token-7-12" morph="none" pos="word" start_char="824">in</TOKEN>
<TOKEN end_char="831" id="token-7-13" morph="none" pos="word" start_char="827">Wuhan</TOKEN>
<TOKEN end_char="835" id="token-7-14" morph="none" pos="word" start_char="833">and</TOKEN>
<TOKEN end_char="839" id="token-7-15" morph="none" pos="word" start_char="837">the</TOKEN>
<TOKEN end_char="848" id="token-7-16" morph="none" pos="word" start_char="841">tracking</TOKEN>
<TOKEN end_char="851" id="token-7-17" morph="none" pos="word" start_char="850">of</TOKEN>
<TOKEN end_char="858" id="token-7-18" morph="none" pos="word" start_char="853">online</TOKEN>
<TOKEN end_char="867" id="token-7-19" morph="none" pos="word" start_char="860">searches</TOKEN>
<TOKEN end_char="871" id="token-7-20" morph="none" pos="word" start_char="869">for</TOKEN>
<TOKEN end_char="880" id="token-7-21" morph="none" pos="word" start_char="873">specific</TOKEN>
<TOKEN end_char="888" id="token-7-22" morph="none" pos="word" start_char="882">medical</TOKEN>
<TOKEN end_char="897" id="token-7-23" morph="none" pos="word" start_char="890">symptoms</TOKEN>
<TOKEN end_char="898" id="token-7-24" morph="none" pos="punct" start_char="898">.</TOKEN>
</SEG>
<SEG end_char="1026" id="segment-8" start_char="901">
<ORIGINAL_TEXT>It says there was a noticeable rise in vehicles parking outside six hospitals in the city from late August to 1 December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="902" id="token-8-0" morph="none" pos="word" start_char="901">It</TOKEN>
<TOKEN end_char="907" id="token-8-1" morph="none" pos="word" start_char="904">says</TOKEN>
<TOKEN end_char="913" id="token-8-2" morph="none" pos="word" start_char="909">there</TOKEN>
<TOKEN end_char="917" id="token-8-3" morph="none" pos="word" start_char="915">was</TOKEN>
<TOKEN end_char="919" id="token-8-4" morph="none" pos="word" start_char="919">a</TOKEN>
<TOKEN end_char="930" id="token-8-5" morph="none" pos="word" start_char="921">noticeable</TOKEN>
<TOKEN end_char="935" id="token-8-6" morph="none" pos="word" start_char="932">rise</TOKEN>
<TOKEN end_char="938" id="token-8-7" morph="none" pos="word" start_char="937">in</TOKEN>
<TOKEN end_char="947" id="token-8-8" morph="none" pos="word" start_char="940">vehicles</TOKEN>
<TOKEN end_char="955" id="token-8-9" morph="none" pos="word" start_char="949">parking</TOKEN>
<TOKEN end_char="963" id="token-8-10" morph="none" pos="word" start_char="957">outside</TOKEN>
<TOKEN end_char="967" id="token-8-11" morph="none" pos="word" start_char="965">six</TOKEN>
<TOKEN end_char="977" id="token-8-12" morph="none" pos="word" start_char="969">hospitals</TOKEN>
<TOKEN end_char="980" id="token-8-13" morph="none" pos="word" start_char="979">in</TOKEN>
<TOKEN end_char="984" id="token-8-14" morph="none" pos="word" start_char="982">the</TOKEN>
<TOKEN end_char="989" id="token-8-15" morph="none" pos="word" start_char="986">city</TOKEN>
<TOKEN end_char="994" id="token-8-16" morph="none" pos="word" start_char="991">from</TOKEN>
<TOKEN end_char="999" id="token-8-17" morph="none" pos="word" start_char="996">late</TOKEN>
<TOKEN end_char="1006" id="token-8-18" morph="none" pos="word" start_char="1001">August</TOKEN>
<TOKEN end_char="1009" id="token-8-19" morph="none" pos="word" start_char="1008">to</TOKEN>
<TOKEN end_char="1011" id="token-8-20" morph="none" pos="word" start_char="1011">1</TOKEN>
<TOKEN end_char="1020" id="token-8-21" morph="none" pos="word" start_char="1013">December</TOKEN>
<TOKEN end_char="1025" id="token-8-22" morph="none" pos="word" start_char="1022">2019</TOKEN>
<TOKEN end_char="1026" id="token-8-23" morph="none" pos="punct" start_char="1026">.</TOKEN>
</SEG>
<SEG end_char="1164" id="segment-9" start_char="1029">
<ORIGINAL_TEXT>This coincided, says the Harvard report, with an increase in searches for possible coronavirus symptoms such as "cough" and "diarrhoea".</ORIGINAL_TEXT>
<TOKEN end_char="1032" id="token-9-0" morph="none" pos="word" start_char="1029">This</TOKEN>
<TOKEN end_char="1042" id="token-9-1" morph="none" pos="word" start_char="1034">coincided</TOKEN>
<TOKEN end_char="1043" id="token-9-2" morph="none" pos="punct" start_char="1043">,</TOKEN>
<TOKEN end_char="1048" id="token-9-3" morph="none" pos="word" start_char="1045">says</TOKEN>
<TOKEN end_char="1052" id="token-9-4" morph="none" pos="word" start_char="1050">the</TOKEN>
<TOKEN end_char="1060" id="token-9-5" morph="none" pos="word" start_char="1054">Harvard</TOKEN>
<TOKEN end_char="1067" id="token-9-6" morph="none" pos="word" start_char="1062">report</TOKEN>
<TOKEN end_char="1068" id="token-9-7" morph="none" pos="punct" start_char="1068">,</TOKEN>
<TOKEN end_char="1073" id="token-9-8" morph="none" pos="word" start_char="1070">with</TOKEN>
<TOKEN end_char="1076" id="token-9-9" morph="none" pos="word" start_char="1075">an</TOKEN>
<TOKEN end_char="1085" id="token-9-10" morph="none" pos="word" start_char="1078">increase</TOKEN>
<TOKEN end_char="1088" id="token-9-11" morph="none" pos="word" start_char="1087">in</TOKEN>
<TOKEN end_char="1097" id="token-9-12" morph="none" pos="word" start_char="1090">searches</TOKEN>
<TOKEN end_char="1101" id="token-9-13" morph="none" pos="word" start_char="1099">for</TOKEN>
<TOKEN end_char="1110" id="token-9-14" morph="none" pos="word" start_char="1103">possible</TOKEN>
<TOKEN end_char="1122" id="token-9-15" morph="none" pos="word" start_char="1112">coronavirus</TOKEN>
<TOKEN end_char="1131" id="token-9-16" morph="none" pos="word" start_char="1124">symptoms</TOKEN>
<TOKEN end_char="1136" id="token-9-17" morph="none" pos="word" start_char="1133">such</TOKEN>
<TOKEN end_char="1139" id="token-9-18" morph="none" pos="word" start_char="1138">as</TOKEN>
<TOKEN end_char="1141" id="token-9-19" morph="none" pos="punct" start_char="1141">"</TOKEN>
<TOKEN end_char="1146" id="token-9-20" morph="none" pos="word" start_char="1142">cough</TOKEN>
<TOKEN end_char="1147" id="token-9-21" morph="none" pos="punct" start_char="1147">"</TOKEN>
<TOKEN end_char="1151" id="token-9-22" morph="none" pos="word" start_char="1149">and</TOKEN>
<TOKEN end_char="1153" id="token-9-23" morph="none" pos="punct" start_char="1153">"</TOKEN>
<TOKEN end_char="1162" id="token-9-24" morph="none" pos="word" start_char="1154">diarrhoea</TOKEN>
<TOKEN end_char="1164" id="token-9-25" morph="none" pos="punct" start_char="1163">".</TOKEN>
</SEG>
<SEG end_char="1178" id="segment-10" start_char="1167">
<ORIGINAL_TEXT>Getty Images</ORIGINAL_TEXT>
<TOKEN end_char="1171" id="token-10-0" morph="none" pos="word" start_char="1167">Getty</TOKEN>
<TOKEN end_char="1178" id="token-10-1" morph="none" pos="word" start_char="1173">Images</TOKEN>
</SEG>
<SEG end_char="1241" id="segment-11" start_char="1181">
<ORIGINAL_TEXT>image captionThe researchers monitored Wuhan traffic patterns</ORIGINAL_TEXT>
<TOKEN end_char="1185" id="token-11-0" morph="none" pos="word" start_char="1181">image</TOKEN>
<TOKEN end_char="1196" id="token-11-1" morph="none" pos="word" start_char="1187">captionThe</TOKEN>
<TOKEN end_char="1208" id="token-11-2" morph="none" pos="word" start_char="1198">researchers</TOKEN>
<TOKEN end_char="1218" id="token-11-3" morph="none" pos="word" start_char="1210">monitored</TOKEN>
<TOKEN end_char="1224" id="token-11-4" morph="none" pos="word" start_char="1220">Wuhan</TOKEN>
<TOKEN end_char="1232" id="token-11-5" morph="none" pos="word" start_char="1226">traffic</TOKEN>
<TOKEN end_char="1241" id="token-11-6" morph="none" pos="word" start_char="1234">patterns</TOKEN>
</SEG>
<SEG end_char="1362" id="segment-12" start_char="1245">
<ORIGINAL_TEXT>This would be an important finding because the earliest reported case in Wuhan wasn't until the beginning of December.</ORIGINAL_TEXT>
<TOKEN end_char="1248" id="token-12-0" morph="none" pos="word" start_char="1245">This</TOKEN>
<TOKEN end_char="1254" id="token-12-1" morph="none" pos="word" start_char="1250">would</TOKEN>
<TOKEN end_char="1257" id="token-12-2" morph="none" pos="word" start_char="1256">be</TOKEN>
<TOKEN end_char="1260" id="token-12-3" morph="none" pos="word" start_char="1259">an</TOKEN>
<TOKEN end_char="1270" id="token-12-4" morph="none" pos="word" start_char="1262">important</TOKEN>
<TOKEN end_char="1278" id="token-12-5" morph="none" pos="word" start_char="1272">finding</TOKEN>
<TOKEN end_char="1286" id="token-12-6" morph="none" pos="word" start_char="1280">because</TOKEN>
<TOKEN end_char="1290" id="token-12-7" morph="none" pos="word" start_char="1288">the</TOKEN>
<TOKEN end_char="1299" id="token-12-8" morph="none" pos="word" start_char="1292">earliest</TOKEN>
<TOKEN end_char="1308" id="token-12-9" morph="none" pos="word" start_char="1301">reported</TOKEN>
<TOKEN end_char="1313" id="token-12-10" morph="none" pos="word" start_char="1310">case</TOKEN>
<TOKEN end_char="1316" id="token-12-11" morph="none" pos="word" start_char="1315">in</TOKEN>
<TOKEN end_char="1322" id="token-12-12" morph="none" pos="word" start_char="1318">Wuhan</TOKEN>
<TOKEN end_char="1329" id="token-12-13" morph="none" pos="word" start_char="1324">wasn't</TOKEN>
<TOKEN end_char="1335" id="token-12-14" morph="none" pos="word" start_char="1331">until</TOKEN>
<TOKEN end_char="1339" id="token-12-15" morph="none" pos="word" start_char="1337">the</TOKEN>
<TOKEN end_char="1349" id="token-12-16" morph="none" pos="word" start_char="1341">beginning</TOKEN>
<TOKEN end_char="1352" id="token-12-17" morph="none" pos="word" start_char="1351">of</TOKEN>
<TOKEN end_char="1361" id="token-12-18" morph="none" pos="word" start_char="1354">December</TOKEN>
<TOKEN end_char="1362" id="token-12-19" morph="none" pos="punct" start_char="1362">.</TOKEN>
</SEG>
<SEG end_char="1597" id="segment-13" start_char="1365">
<ORIGINAL_TEXT>The academics write: "While we cannot confirm if the increased volume was directly related to the new virus, our evidence supports other recent work showing that emergence happened before identification at the Huanan Seafood market."</ORIGINAL_TEXT>
<TOKEN end_char="1367" id="token-13-0" morph="none" pos="word" start_char="1365">The</TOKEN>
<TOKEN end_char="1377" id="token-13-1" morph="none" pos="word" start_char="1369">academics</TOKEN>
<TOKEN end_char="1383" id="token-13-2" morph="none" pos="word" start_char="1379">write</TOKEN>
<TOKEN end_char="1384" id="token-13-3" morph="none" pos="punct" start_char="1384">:</TOKEN>
<TOKEN end_char="1386" id="token-13-4" morph="none" pos="punct" start_char="1386">"</TOKEN>
<TOKEN end_char="1391" id="token-13-5" morph="none" pos="word" start_char="1387">While</TOKEN>
<TOKEN end_char="1394" id="token-13-6" morph="none" pos="word" start_char="1393">we</TOKEN>
<TOKEN end_char="1401" id="token-13-7" morph="none" pos="word" start_char="1396">cannot</TOKEN>
<TOKEN end_char="1409" id="token-13-8" morph="none" pos="word" start_char="1403">confirm</TOKEN>
<TOKEN end_char="1412" id="token-13-9" morph="none" pos="word" start_char="1411">if</TOKEN>
<TOKEN end_char="1416" id="token-13-10" morph="none" pos="word" start_char="1414">the</TOKEN>
<TOKEN end_char="1426" id="token-13-11" morph="none" pos="word" start_char="1418">increased</TOKEN>
<TOKEN end_char="1433" id="token-13-12" morph="none" pos="word" start_char="1428">volume</TOKEN>
<TOKEN end_char="1437" id="token-13-13" morph="none" pos="word" start_char="1435">was</TOKEN>
<TOKEN end_char="1446" id="token-13-14" morph="none" pos="word" start_char="1439">directly</TOKEN>
<TOKEN end_char="1454" id="token-13-15" morph="none" pos="word" start_char="1448">related</TOKEN>
<TOKEN end_char="1457" id="token-13-16" morph="none" pos="word" start_char="1456">to</TOKEN>
<TOKEN end_char="1461" id="token-13-17" morph="none" pos="word" start_char="1459">the</TOKEN>
<TOKEN end_char="1465" id="token-13-18" morph="none" pos="word" start_char="1463">new</TOKEN>
<TOKEN end_char="1471" id="token-13-19" morph="none" pos="word" start_char="1467">virus</TOKEN>
<TOKEN end_char="1472" id="token-13-20" morph="none" pos="punct" start_char="1472">,</TOKEN>
<TOKEN end_char="1476" id="token-13-21" morph="none" pos="word" start_char="1474">our</TOKEN>
<TOKEN end_char="1485" id="token-13-22" morph="none" pos="word" start_char="1478">evidence</TOKEN>
<TOKEN end_char="1494" id="token-13-23" morph="none" pos="word" start_char="1487">supports</TOKEN>
<TOKEN end_char="1500" id="token-13-24" morph="none" pos="word" start_char="1496">other</TOKEN>
<TOKEN end_char="1507" id="token-13-25" morph="none" pos="word" start_char="1502">recent</TOKEN>
<TOKEN end_char="1512" id="token-13-26" morph="none" pos="word" start_char="1509">work</TOKEN>
<TOKEN end_char="1520" id="token-13-27" morph="none" pos="word" start_char="1514">showing</TOKEN>
<TOKEN end_char="1525" id="token-13-28" morph="none" pos="word" start_char="1522">that</TOKEN>
<TOKEN end_char="1535" id="token-13-29" morph="none" pos="word" start_char="1527">emergence</TOKEN>
<TOKEN end_char="1544" id="token-13-30" morph="none" pos="word" start_char="1537">happened</TOKEN>
<TOKEN end_char="1551" id="token-13-31" morph="none" pos="word" start_char="1546">before</TOKEN>
<TOKEN end_char="1566" id="token-13-32" morph="none" pos="word" start_char="1553">identification</TOKEN>
<TOKEN end_char="1569" id="token-13-33" morph="none" pos="word" start_char="1568">at</TOKEN>
<TOKEN end_char="1573" id="token-13-34" morph="none" pos="word" start_char="1571">the</TOKEN>
<TOKEN end_char="1580" id="token-13-35" morph="none" pos="word" start_char="1575">Huanan</TOKEN>
<TOKEN end_char="1588" id="token-13-36" morph="none" pos="word" start_char="1582">Seafood</TOKEN>
<TOKEN end_char="1595" id="token-13-37" morph="none" pos="word" start_char="1590">market</TOKEN>
<TOKEN end_char="1597" id="token-13-38" morph="none" pos="punct" start_char="1596">."</TOKEN>
</SEG>
<SEG end_char="1805" id="segment-14" start_char="1600">
<ORIGINAL_TEXT>The Harvard study has gained a lot of traction in the media, with President Trump, who has been highly critical of China's pandemic response, tweeting a Fox News item highlighting the researchers' findings.</ORIGINAL_TEXT>
<TOKEN end_char="1602" id="token-14-0" morph="none" pos="word" start_char="1600">The</TOKEN>
<TOKEN end_char="1610" id="token-14-1" morph="none" pos="word" start_char="1604">Harvard</TOKEN>
<TOKEN end_char="1616" id="token-14-2" morph="none" pos="word" start_char="1612">study</TOKEN>
<TOKEN end_char="1620" id="token-14-3" morph="none" pos="word" start_char="1618">has</TOKEN>
<TOKEN end_char="1627" id="token-14-4" morph="none" pos="word" start_char="1622">gained</TOKEN>
<TOKEN end_char="1629" id="token-14-5" morph="none" pos="word" start_char="1629">a</TOKEN>
<TOKEN end_char="1633" id="token-14-6" morph="none" pos="word" start_char="1631">lot</TOKEN>
<TOKEN end_char="1636" id="token-14-7" morph="none" pos="word" start_char="1635">of</TOKEN>
<TOKEN end_char="1645" id="token-14-8" morph="none" pos="word" start_char="1638">traction</TOKEN>
<TOKEN end_char="1648" id="token-14-9" morph="none" pos="word" start_char="1647">in</TOKEN>
<TOKEN end_char="1652" id="token-14-10" morph="none" pos="word" start_char="1650">the</TOKEN>
<TOKEN end_char="1658" id="token-14-11" morph="none" pos="word" start_char="1654">media</TOKEN>
<TOKEN end_char="1659" id="token-14-12" morph="none" pos="punct" start_char="1659">,</TOKEN>
<TOKEN end_char="1664" id="token-14-13" morph="none" pos="word" start_char="1661">with</TOKEN>
<TOKEN end_char="1674" id="token-14-14" morph="none" pos="word" start_char="1666">President</TOKEN>
<TOKEN end_char="1680" id="token-14-15" morph="none" pos="word" start_char="1676">Trump</TOKEN>
<TOKEN end_char="1681" id="token-14-16" morph="none" pos="punct" start_char="1681">,</TOKEN>
<TOKEN end_char="1685" id="token-14-17" morph="none" pos="word" start_char="1683">who</TOKEN>
<TOKEN end_char="1689" id="token-14-18" morph="none" pos="word" start_char="1687">has</TOKEN>
<TOKEN end_char="1694" id="token-14-19" morph="none" pos="word" start_char="1691">been</TOKEN>
<TOKEN end_char="1701" id="token-14-20" morph="none" pos="word" start_char="1696">highly</TOKEN>
<TOKEN end_char="1710" id="token-14-21" morph="none" pos="word" start_char="1703">critical</TOKEN>
<TOKEN end_char="1713" id="token-14-22" morph="none" pos="word" start_char="1712">of</TOKEN>
<TOKEN end_char="1721" id="token-14-23" morph="none" pos="word" start_char="1715">China's</TOKEN>
<TOKEN end_char="1730" id="token-14-24" morph="none" pos="word" start_char="1723">pandemic</TOKEN>
<TOKEN end_char="1739" id="token-14-25" morph="none" pos="word" start_char="1732">response</TOKEN>
<TOKEN end_char="1740" id="token-14-26" morph="none" pos="punct" start_char="1740">,</TOKEN>
<TOKEN end_char="1749" id="token-14-27" morph="none" pos="word" start_char="1742">tweeting</TOKEN>
<TOKEN end_char="1751" id="token-14-28" morph="none" pos="word" start_char="1751">a</TOKEN>
<TOKEN end_char="1755" id="token-14-29" morph="none" pos="word" start_char="1753">Fox</TOKEN>
<TOKEN end_char="1760" id="token-14-30" morph="none" pos="word" start_char="1757">News</TOKEN>
<TOKEN end_char="1765" id="token-14-31" morph="none" pos="word" start_char="1762">item</TOKEN>
<TOKEN end_char="1778" id="token-14-32" morph="none" pos="word" start_char="1767">highlighting</TOKEN>
<TOKEN end_char="1782" id="token-14-33" morph="none" pos="word" start_char="1780">the</TOKEN>
<TOKEN end_char="1794" id="token-14-34" morph="none" pos="word" start_char="1784">researchers</TOKEN>
<TOKEN end_char="1795" id="token-14-35" morph="none" pos="punct" start_char="1795">'</TOKEN>
<TOKEN end_char="1804" id="token-14-36" morph="none" pos="word" start_char="1797">findings</TOKEN>
<TOKEN end_char="1805" id="token-14-37" morph="none" pos="punct" start_char="1805">.</TOKEN>
</SEG>
<SEG end_char="1862" id="segment-15" start_char="1807">
<ORIGINAL_TEXT>The tweet has been viewed more than three million times.</ORIGINAL_TEXT>
<TOKEN end_char="1809" id="token-15-0" morph="none" pos="word" start_char="1807">The</TOKEN>
<TOKEN end_char="1815" id="token-15-1" morph="none" pos="word" start_char="1811">tweet</TOKEN>
<TOKEN end_char="1819" id="token-15-2" morph="none" pos="word" start_char="1817">has</TOKEN>
<TOKEN end_char="1824" id="token-15-3" morph="none" pos="word" start_char="1821">been</TOKEN>
<TOKEN end_char="1831" id="token-15-4" morph="none" pos="word" start_char="1826">viewed</TOKEN>
<TOKEN end_char="1836" id="token-15-5" morph="none" pos="word" start_char="1833">more</TOKEN>
<TOKEN end_char="1841" id="token-15-6" morph="none" pos="word" start_char="1838">than</TOKEN>
<TOKEN end_char="1847" id="token-15-7" morph="none" pos="word" start_char="1843">three</TOKEN>
<TOKEN end_char="1855" id="token-15-8" morph="none" pos="word" start_char="1849">million</TOKEN>
<TOKEN end_char="1861" id="token-15-9" morph="none" pos="word" start_char="1857">times</TOKEN>
<TOKEN end_char="1862" id="token-15-10" morph="none" pos="punct" start_char="1862">.</TOKEN>
</SEG>
<SEG end_char="1897" id="segment-16" start_char="1865">
<ORIGINAL_TEXT>So, does their evidence stand up?</ORIGINAL_TEXT>
<TOKEN end_char="1866" id="token-16-0" morph="none" pos="word" start_char="1865">So</TOKEN>
<TOKEN end_char="1867" id="token-16-1" morph="none" pos="punct" start_char="1867">,</TOKEN>
<TOKEN end_char="1872" id="token-16-2" morph="none" pos="word" start_char="1869">does</TOKEN>
<TOKEN end_char="1878" id="token-16-3" morph="none" pos="word" start_char="1874">their</TOKEN>
<TOKEN end_char="1887" id="token-16-4" morph="none" pos="word" start_char="1880">evidence</TOKEN>
<TOKEN end_char="1893" id="token-16-5" morph="none" pos="word" start_char="1889">stand</TOKEN>
<TOKEN end_char="1896" id="token-16-6" morph="none" pos="word" start_char="1895">up</TOKEN>
<TOKEN end_char="1897" id="token-16-7" morph="none" pos="punct" start_char="1897">?</TOKEN>
</SEG>
<SEG end_char="2154" id="segment-17" start_char="1901">
<ORIGINAL_TEXT>The study, which hasn't been peer-reviewed - the process by which academic papers get checked by other scientists - claims there was an increase in online queries for coronavirus symptoms, particularly "diarrhoea", on popular Chinese search engine Baidu.</ORIGINAL_TEXT>
<TOKEN end_char="1903" id="token-17-0" morph="none" pos="word" start_char="1901">The</TOKEN>
<TOKEN end_char="1909" id="token-17-1" morph="none" pos="word" start_char="1905">study</TOKEN>
<TOKEN end_char="1910" id="token-17-2" morph="none" pos="punct" start_char="1910">,</TOKEN>
<TOKEN end_char="1916" id="token-17-3" morph="none" pos="word" start_char="1912">which</TOKEN>
<TOKEN end_char="1923" id="token-17-4" morph="none" pos="word" start_char="1918">hasn't</TOKEN>
<TOKEN end_char="1928" id="token-17-5" morph="none" pos="word" start_char="1925">been</TOKEN>
<TOKEN end_char="1942" id="token-17-6" morph="none" pos="unknown" start_char="1930">peer-reviewed</TOKEN>
<TOKEN end_char="1944" id="token-17-7" morph="none" pos="punct" start_char="1944">-</TOKEN>
<TOKEN end_char="1948" id="token-17-8" morph="none" pos="word" start_char="1946">the</TOKEN>
<TOKEN end_char="1956" id="token-17-9" morph="none" pos="word" start_char="1950">process</TOKEN>
<TOKEN end_char="1959" id="token-17-10" morph="none" pos="word" start_char="1958">by</TOKEN>
<TOKEN end_char="1965" id="token-17-11" morph="none" pos="word" start_char="1961">which</TOKEN>
<TOKEN end_char="1974" id="token-17-12" morph="none" pos="word" start_char="1967">academic</TOKEN>
<TOKEN end_char="1981" id="token-17-13" morph="none" pos="word" start_char="1976">papers</TOKEN>
<TOKEN end_char="1985" id="token-17-14" morph="none" pos="word" start_char="1983">get</TOKEN>
<TOKEN end_char="1993" id="token-17-15" morph="none" pos="word" start_char="1987">checked</TOKEN>
<TOKEN end_char="1996" id="token-17-16" morph="none" pos="word" start_char="1995">by</TOKEN>
<TOKEN end_char="2002" id="token-17-17" morph="none" pos="word" start_char="1998">other</TOKEN>
<TOKEN end_char="2013" id="token-17-18" morph="none" pos="word" start_char="2004">scientists</TOKEN>
<TOKEN end_char="2015" id="token-17-19" morph="none" pos="punct" start_char="2015">-</TOKEN>
<TOKEN end_char="2022" id="token-17-20" morph="none" pos="word" start_char="2017">claims</TOKEN>
<TOKEN end_char="2028" id="token-17-21" morph="none" pos="word" start_char="2024">there</TOKEN>
<TOKEN end_char="2032" id="token-17-22" morph="none" pos="word" start_char="2030">was</TOKEN>
<TOKEN end_char="2035" id="token-17-23" morph="none" pos="word" start_char="2034">an</TOKEN>
<TOKEN end_char="2044" id="token-17-24" morph="none" pos="word" start_char="2037">increase</TOKEN>
<TOKEN end_char="2047" id="token-17-25" morph="none" pos="word" start_char="2046">in</TOKEN>
<TOKEN end_char="2054" id="token-17-26" morph="none" pos="word" start_char="2049">online</TOKEN>
<TOKEN end_char="2062" id="token-17-27" morph="none" pos="word" start_char="2056">queries</TOKEN>
<TOKEN end_char="2066" id="token-17-28" morph="none" pos="word" start_char="2064">for</TOKEN>
<TOKEN end_char="2078" id="token-17-29" morph="none" pos="word" start_char="2068">coronavirus</TOKEN>
<TOKEN end_char="2087" id="token-17-30" morph="none" pos="word" start_char="2080">symptoms</TOKEN>
<TOKEN end_char="2088" id="token-17-31" morph="none" pos="punct" start_char="2088">,</TOKEN>
<TOKEN end_char="2101" id="token-17-32" morph="none" pos="word" start_char="2090">particularly</TOKEN>
<TOKEN end_char="2103" id="token-17-33" morph="none" pos="punct" start_char="2103">"</TOKEN>
<TOKEN end_char="2112" id="token-17-34" morph="none" pos="word" start_char="2104">diarrhoea</TOKEN>
<TOKEN end_char="2114" id="token-17-35" morph="none" pos="punct" start_char="2113">",</TOKEN>
<TOKEN end_char="2117" id="token-17-36" morph="none" pos="word" start_char="2116">on</TOKEN>
<TOKEN end_char="2125" id="token-17-37" morph="none" pos="word" start_char="2119">popular</TOKEN>
<TOKEN end_char="2133" id="token-17-38" morph="none" pos="word" start_char="2127">Chinese</TOKEN>
<TOKEN end_char="2140" id="token-17-39" morph="none" pos="word" start_char="2135">search</TOKEN>
<TOKEN end_char="2147" id="token-17-40" morph="none" pos="word" start_char="2142">engine</TOKEN>
<TOKEN end_char="2153" id="token-17-41" morph="none" pos="word" start_char="2149">Baidu</TOKEN>
<TOKEN end_char="2154" id="token-17-42" morph="none" pos="punct" start_char="2154">.</TOKEN>
</SEG>
<SEG end_char="2300" id="segment-18" start_char="2157">
<ORIGINAL_TEXT>However, Baidu company officials have disputed their findings, saying there was in fact a decrease in searches for "diarrhoea" over this period.</ORIGINAL_TEXT>
<TOKEN end_char="2163" id="token-18-0" morph="none" pos="word" start_char="2157">However</TOKEN>
<TOKEN end_char="2164" id="token-18-1" morph="none" pos="punct" start_char="2164">,</TOKEN>
<TOKEN end_char="2170" id="token-18-2" morph="none" pos="word" start_char="2166">Baidu</TOKEN>
<TOKEN end_char="2178" id="token-18-3" morph="none" pos="word" start_char="2172">company</TOKEN>
<TOKEN end_char="2188" id="token-18-4" morph="none" pos="word" start_char="2180">officials</TOKEN>
<TOKEN end_char="2193" id="token-18-5" morph="none" pos="word" start_char="2190">have</TOKEN>
<TOKEN end_char="2202" id="token-18-6" morph="none" pos="word" start_char="2195">disputed</TOKEN>
<TOKEN end_char="2208" id="token-18-7" morph="none" pos="word" start_char="2204">their</TOKEN>
<TOKEN end_char="2217" id="token-18-8" morph="none" pos="word" start_char="2210">findings</TOKEN>
<TOKEN end_char="2218" id="token-18-9" morph="none" pos="punct" start_char="2218">,</TOKEN>
<TOKEN end_char="2225" id="token-18-10" morph="none" pos="word" start_char="2220">saying</TOKEN>
<TOKEN end_char="2231" id="token-18-11" morph="none" pos="word" start_char="2227">there</TOKEN>
<TOKEN end_char="2235" id="token-18-12" morph="none" pos="word" start_char="2233">was</TOKEN>
<TOKEN end_char="2238" id="token-18-13" morph="none" pos="word" start_char="2237">in</TOKEN>
<TOKEN end_char="2243" id="token-18-14" morph="none" pos="word" start_char="2240">fact</TOKEN>
<TOKEN end_char="2245" id="token-18-15" morph="none" pos="word" start_char="2245">a</TOKEN>
<TOKEN end_char="2254" id="token-18-16" morph="none" pos="word" start_char="2247">decrease</TOKEN>
<TOKEN end_char="2257" id="token-18-17" morph="none" pos="word" start_char="2256">in</TOKEN>
<TOKEN end_char="2266" id="token-18-18" morph="none" pos="word" start_char="2259">searches</TOKEN>
<TOKEN end_char="2270" id="token-18-19" morph="none" pos="word" start_char="2268">for</TOKEN>
<TOKEN end_char="2272" id="token-18-20" morph="none" pos="punct" start_char="2272">"</TOKEN>
<TOKEN end_char="2281" id="token-18-21" morph="none" pos="word" start_char="2273">diarrhoea</TOKEN>
<TOKEN end_char="2282" id="token-18-22" morph="none" pos="punct" start_char="2282">"</TOKEN>
<TOKEN end_char="2287" id="token-18-23" morph="none" pos="word" start_char="2284">over</TOKEN>
<TOKEN end_char="2292" id="token-18-24" morph="none" pos="word" start_char="2289">this</TOKEN>
<TOKEN end_char="2299" id="token-18-25" morph="none" pos="word" start_char="2294">period</TOKEN>
<TOKEN end_char="2300" id="token-18-26" morph="none" pos="punct" start_char="2300">.</TOKEN>
</SEG>
<SEG end_char="2322" id="segment-19" start_char="2303">
<ORIGINAL_TEXT>So, what's going on?</ORIGINAL_TEXT>
<TOKEN end_char="2304" id="token-19-0" morph="none" pos="word" start_char="2303">So</TOKEN>
<TOKEN end_char="2305" id="token-19-1" morph="none" pos="punct" start_char="2305">,</TOKEN>
<TOKEN end_char="2312" id="token-19-2" morph="none" pos="word" start_char="2307">what's</TOKEN>
<TOKEN end_char="2318" id="token-19-3" morph="none" pos="word" start_char="2314">going</TOKEN>
<TOKEN end_char="2321" id="token-19-4" morph="none" pos="word" start_char="2320">on</TOKEN>
<TOKEN end_char="2322" id="token-19-5" morph="none" pos="punct" start_char="2322">?</TOKEN>
</SEG>
<SEG end_char="2430" id="segment-20" start_char="2326">
<ORIGINAL_TEXT>The term used in the Harvard University paper actually translates from Chinese as "symptom of diarrhoea".</ORIGINAL_TEXT>
<TOKEN end_char="2328" id="token-20-0" morph="none" pos="word" start_char="2326">The</TOKEN>
<TOKEN end_char="2333" id="token-20-1" morph="none" pos="word" start_char="2330">term</TOKEN>
<TOKEN end_char="2338" id="token-20-2" morph="none" pos="word" start_char="2335">used</TOKEN>
<TOKEN end_char="2341" id="token-20-3" morph="none" pos="word" start_char="2340">in</TOKEN>
<TOKEN end_char="2345" id="token-20-4" morph="none" pos="word" start_char="2343">the</TOKEN>
<TOKEN end_char="2353" id="token-20-5" morph="none" pos="word" start_char="2347">Harvard</TOKEN>
<TOKEN end_char="2364" id="token-20-6" morph="none" pos="word" start_char="2355">University</TOKEN>
<TOKEN end_char="2370" id="token-20-7" morph="none" pos="word" start_char="2366">paper</TOKEN>
<TOKEN end_char="2379" id="token-20-8" morph="none" pos="word" start_char="2372">actually</TOKEN>
<TOKEN end_char="2390" id="token-20-9" morph="none" pos="word" start_char="2381">translates</TOKEN>
<TOKEN end_char="2395" id="token-20-10" morph="none" pos="word" start_char="2392">from</TOKEN>
<TOKEN end_char="2403" id="token-20-11" morph="none" pos="word" start_char="2397">Chinese</TOKEN>
<TOKEN end_char="2406" id="token-20-12" morph="none" pos="word" start_char="2405">as</TOKEN>
<TOKEN end_char="2408" id="token-20-13" morph="none" pos="punct" start_char="2408">"</TOKEN>
<TOKEN end_char="2415" id="token-20-14" morph="none" pos="word" start_char="2409">symptom</TOKEN>
<TOKEN end_char="2418" id="token-20-15" morph="none" pos="word" start_char="2417">of</TOKEN>
<TOKEN end_char="2428" id="token-20-16" morph="none" pos="word" start_char="2420">diarrhoea</TOKEN>
<TOKEN end_char="2430" id="token-20-17" morph="none" pos="punct" start_char="2429">".</TOKEN>
</SEG>
<SEG end_char="2546" id="segment-21" start_char="2433">
<ORIGINAL_TEXT>We checked this on Baidu's tool that allows users to analyse the popularity of search queries, like Google Trends.</ORIGINAL_TEXT>
<TOKEN end_char="2434" id="token-21-0" morph="none" pos="word" start_char="2433">We</TOKEN>
<TOKEN end_char="2442" id="token-21-1" morph="none" pos="word" start_char="2436">checked</TOKEN>
<TOKEN end_char="2447" id="token-21-2" morph="none" pos="word" start_char="2444">this</TOKEN>
<TOKEN end_char="2450" id="token-21-3" morph="none" pos="word" start_char="2449">on</TOKEN>
<TOKEN end_char="2458" id="token-21-4" morph="none" pos="word" start_char="2452">Baidu's</TOKEN>
<TOKEN end_char="2463" id="token-21-5" morph="none" pos="word" start_char="2460">tool</TOKEN>
<TOKEN end_char="2468" id="token-21-6" morph="none" pos="word" start_char="2465">that</TOKEN>
<TOKEN end_char="2475" id="token-21-7" morph="none" pos="word" start_char="2470">allows</TOKEN>
<TOKEN end_char="2481" id="token-21-8" morph="none" pos="word" start_char="2477">users</TOKEN>
<TOKEN end_char="2484" id="token-21-9" morph="none" pos="word" start_char="2483">to</TOKEN>
<TOKEN end_char="2492" id="token-21-10" morph="none" pos="word" start_char="2486">analyse</TOKEN>
<TOKEN end_char="2496" id="token-21-11" morph="none" pos="word" start_char="2494">the</TOKEN>
<TOKEN end_char="2507" id="token-21-12" morph="none" pos="word" start_char="2498">popularity</TOKEN>
<TOKEN end_char="2510" id="token-21-13" morph="none" pos="word" start_char="2509">of</TOKEN>
<TOKEN end_char="2517" id="token-21-14" morph="none" pos="word" start_char="2512">search</TOKEN>
<TOKEN end_char="2525" id="token-21-15" morph="none" pos="word" start_char="2519">queries</TOKEN>
<TOKEN end_char="2526" id="token-21-16" morph="none" pos="punct" start_char="2526">,</TOKEN>
<TOKEN end_char="2531" id="token-21-17" morph="none" pos="word" start_char="2528">like</TOKEN>
<TOKEN end_char="2538" id="token-21-18" morph="none" pos="word" start_char="2533">Google</TOKEN>
<TOKEN end_char="2545" id="token-21-19" morph="none" pos="word" start_char="2540">Trends</TOKEN>
<TOKEN end_char="2546" id="token-21-20" morph="none" pos="punct" start_char="2546">.</TOKEN>
</SEG>
<SEG end_char="2644" id="segment-22" start_char="2549">
<ORIGINAL_TEXT>The search-term "symptom of diarrhoea" does indeed show an increase in queries from August 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2551" id="token-22-0" morph="none" pos="word" start_char="2549">The</TOKEN>
<TOKEN end_char="2563" id="token-22-1" morph="none" pos="unknown" start_char="2553">search-term</TOKEN>
<TOKEN end_char="2565" id="token-22-2" morph="none" pos="punct" start_char="2565">"</TOKEN>
<TOKEN end_char="2572" id="token-22-3" morph="none" pos="word" start_char="2566">symptom</TOKEN>
<TOKEN end_char="2575" id="token-22-4" morph="none" pos="word" start_char="2574">of</TOKEN>
<TOKEN end_char="2585" id="token-22-5" morph="none" pos="word" start_char="2577">diarrhoea</TOKEN>
<TOKEN end_char="2586" id="token-22-6" morph="none" pos="punct" start_char="2586">"</TOKEN>
<TOKEN end_char="2591" id="token-22-7" morph="none" pos="word" start_char="2588">does</TOKEN>
<TOKEN end_char="2598" id="token-22-8" morph="none" pos="word" start_char="2593">indeed</TOKEN>
<TOKEN end_char="2603" id="token-22-9" morph="none" pos="word" start_char="2600">show</TOKEN>
<TOKEN end_char="2606" id="token-22-10" morph="none" pos="word" start_char="2605">an</TOKEN>
<TOKEN end_char="2615" id="token-22-11" morph="none" pos="word" start_char="2608">increase</TOKEN>
<TOKEN end_char="2618" id="token-22-12" morph="none" pos="word" start_char="2617">in</TOKEN>
<TOKEN end_char="2626" id="token-22-13" morph="none" pos="word" start_char="2620">queries</TOKEN>
<TOKEN end_char="2631" id="token-22-14" morph="none" pos="word" start_char="2628">from</TOKEN>
<TOKEN end_char="2638" id="token-22-15" morph="none" pos="word" start_char="2633">August</TOKEN>
<TOKEN end_char="2643" id="token-22-16" morph="none" pos="word" start_char="2640">2019</TOKEN>
<TOKEN end_char="2644" id="token-22-17" morph="none" pos="punct" start_char="2644">.</TOKEN>
</SEG>
<SEG end_char="2801" id="segment-23" start_char="2647">
<ORIGINAL_TEXT>However, we also ran the term "diarrhoea", a more common search-term in Wuhan, and it actually showed a decrease from August 2019 until the outbreak began.</ORIGINAL_TEXT>
<TOKEN end_char="2653" id="token-23-0" morph="none" pos="word" start_char="2647">However</TOKEN>
<TOKEN end_char="2654" id="token-23-1" morph="none" pos="punct" start_char="2654">,</TOKEN>
<TOKEN end_char="2657" id="token-23-2" morph="none" pos="word" start_char="2656">we</TOKEN>
<TOKEN end_char="2662" id="token-23-3" morph="none" pos="word" start_char="2659">also</TOKEN>
<TOKEN end_char="2666" id="token-23-4" morph="none" pos="word" start_char="2664">ran</TOKEN>
<TOKEN end_char="2670" id="token-23-5" morph="none" pos="word" start_char="2668">the</TOKEN>
<TOKEN end_char="2675" id="token-23-6" morph="none" pos="word" start_char="2672">term</TOKEN>
<TOKEN end_char="2677" id="token-23-7" morph="none" pos="punct" start_char="2677">"</TOKEN>
<TOKEN end_char="2686" id="token-23-8" morph="none" pos="word" start_char="2678">diarrhoea</TOKEN>
<TOKEN end_char="2688" id="token-23-9" morph="none" pos="punct" start_char="2687">",</TOKEN>
<TOKEN end_char="2690" id="token-23-10" morph="none" pos="word" start_char="2690">a</TOKEN>
<TOKEN end_char="2695" id="token-23-11" morph="none" pos="word" start_char="2692">more</TOKEN>
<TOKEN end_char="2702" id="token-23-12" morph="none" pos="word" start_char="2697">common</TOKEN>
<TOKEN end_char="2714" id="token-23-13" morph="none" pos="unknown" start_char="2704">search-term</TOKEN>
<TOKEN end_char="2717" id="token-23-14" morph="none" pos="word" start_char="2716">in</TOKEN>
<TOKEN end_char="2723" id="token-23-15" morph="none" pos="word" start_char="2719">Wuhan</TOKEN>
<TOKEN end_char="2724" id="token-23-16" morph="none" pos="punct" start_char="2724">,</TOKEN>
<TOKEN end_char="2728" id="token-23-17" morph="none" pos="word" start_char="2726">and</TOKEN>
<TOKEN end_char="2731" id="token-23-18" morph="none" pos="word" start_char="2730">it</TOKEN>
<TOKEN end_char="2740" id="token-23-19" morph="none" pos="word" start_char="2733">actually</TOKEN>
<TOKEN end_char="2747" id="token-23-20" morph="none" pos="word" start_char="2742">showed</TOKEN>
<TOKEN end_char="2749" id="token-23-21" morph="none" pos="word" start_char="2749">a</TOKEN>
<TOKEN end_char="2758" id="token-23-22" morph="none" pos="word" start_char="2751">decrease</TOKEN>
<TOKEN end_char="2763" id="token-23-23" morph="none" pos="word" start_char="2760">from</TOKEN>
<TOKEN end_char="2770" id="token-23-24" morph="none" pos="word" start_char="2765">August</TOKEN>
<TOKEN end_char="2775" id="token-23-25" morph="none" pos="word" start_char="2772">2019</TOKEN>
<TOKEN end_char="2781" id="token-23-26" morph="none" pos="word" start_char="2777">until</TOKEN>
<TOKEN end_char="2785" id="token-23-27" morph="none" pos="word" start_char="2783">the</TOKEN>
<TOKEN end_char="2794" id="token-23-28" morph="none" pos="word" start_char="2787">outbreak</TOKEN>
<TOKEN end_char="2800" id="token-23-29" morph="none" pos="word" start_char="2796">began</TOKEN>
<TOKEN end_char="2801" id="token-23-30" morph="none" pos="punct" start_char="2801">.</TOKEN>
</SEG>
<SEG end_char="3047" id="segment-24" start_char="2804">
<ORIGINAL_TEXT>A lead author of the Harvard paper, Benjamin Rader, told the BBC that "the search term we chose for 'diarrhoea' was chosen because it was the best match for confirmed cases of Covid-19 and was suggested as a related search term to coronavirus".</ORIGINAL_TEXT>
<TOKEN end_char="2804" id="token-24-0" morph="none" pos="word" start_char="2804">A</TOKEN>
<TOKEN end_char="2809" id="token-24-1" morph="none" pos="word" start_char="2806">lead</TOKEN>
<TOKEN end_char="2816" id="token-24-2" morph="none" pos="word" start_char="2811">author</TOKEN>
<TOKEN end_char="2819" id="token-24-3" morph="none" pos="word" start_char="2818">of</TOKEN>
<TOKEN end_char="2823" id="token-24-4" morph="none" pos="word" start_char="2821">the</TOKEN>
<TOKEN end_char="2831" id="token-24-5" morph="none" pos="word" start_char="2825">Harvard</TOKEN>
<TOKEN end_char="2837" id="token-24-6" morph="none" pos="word" start_char="2833">paper</TOKEN>
<TOKEN end_char="2838" id="token-24-7" morph="none" pos="punct" start_char="2838">,</TOKEN>
<TOKEN end_char="2847" id="token-24-8" morph="none" pos="word" start_char="2840">Benjamin</TOKEN>
<TOKEN end_char="2853" id="token-24-9" morph="none" pos="word" start_char="2849">Rader</TOKEN>
<TOKEN end_char="2854" id="token-24-10" morph="none" pos="punct" start_char="2854">,</TOKEN>
<TOKEN end_char="2859" id="token-24-11" morph="none" pos="word" start_char="2856">told</TOKEN>
<TOKEN end_char="2863" id="token-24-12" morph="none" pos="word" start_char="2861">the</TOKEN>
<TOKEN end_char="2867" id="token-24-13" morph="none" pos="word" start_char="2865">BBC</TOKEN>
<TOKEN end_char="2872" id="token-24-14" morph="none" pos="word" start_char="2869">that</TOKEN>
<TOKEN end_char="2874" id="token-24-15" morph="none" pos="punct" start_char="2874">"</TOKEN>
<TOKEN end_char="2877" id="token-24-16" morph="none" pos="word" start_char="2875">the</TOKEN>
<TOKEN end_char="2884" id="token-24-17" morph="none" pos="word" start_char="2879">search</TOKEN>
<TOKEN end_char="2889" id="token-24-18" morph="none" pos="word" start_char="2886">term</TOKEN>
<TOKEN end_char="2892" id="token-24-19" morph="none" pos="word" start_char="2891">we</TOKEN>
<TOKEN end_char="2898" id="token-24-20" morph="none" pos="word" start_char="2894">chose</TOKEN>
<TOKEN end_char="2902" id="token-24-21" morph="none" pos="word" start_char="2900">for</TOKEN>
<TOKEN end_char="2904" id="token-24-22" morph="none" pos="punct" start_char="2904">'</TOKEN>
<TOKEN end_char="2913" id="token-24-23" morph="none" pos="word" start_char="2905">diarrhoea</TOKEN>
<TOKEN end_char="2914" id="token-24-24" morph="none" pos="punct" start_char="2914">'</TOKEN>
<TOKEN end_char="2918" id="token-24-25" morph="none" pos="word" start_char="2916">was</TOKEN>
<TOKEN end_char="2925" id="token-24-26" morph="none" pos="word" start_char="2920">chosen</TOKEN>
<TOKEN end_char="2933" id="token-24-27" morph="none" pos="word" start_char="2927">because</TOKEN>
<TOKEN end_char="2936" id="token-24-28" morph="none" pos="word" start_char="2935">it</TOKEN>
<TOKEN end_char="2940" id="token-24-29" morph="none" pos="word" start_char="2938">was</TOKEN>
<TOKEN end_char="2944" id="token-24-30" morph="none" pos="word" start_char="2942">the</TOKEN>
<TOKEN end_char="2949" id="token-24-31" morph="none" pos="word" start_char="2946">best</TOKEN>
<TOKEN end_char="2955" id="token-24-32" morph="none" pos="word" start_char="2951">match</TOKEN>
<TOKEN end_char="2959" id="token-24-33" morph="none" pos="word" start_char="2957">for</TOKEN>
<TOKEN end_char="2969" id="token-24-34" morph="none" pos="word" start_char="2961">confirmed</TOKEN>
<TOKEN end_char="2975" id="token-24-35" morph="none" pos="word" start_char="2971">cases</TOKEN>
<TOKEN end_char="2978" id="token-24-36" morph="none" pos="word" start_char="2977">of</TOKEN>
<TOKEN end_char="2987" id="token-24-37" morph="none" pos="unknown" start_char="2980">Covid-19</TOKEN>
<TOKEN end_char="2991" id="token-24-38" morph="none" pos="word" start_char="2989">and</TOKEN>
<TOKEN end_char="2995" id="token-24-39" morph="none" pos="word" start_char="2993">was</TOKEN>
<TOKEN end_char="3005" id="token-24-40" morph="none" pos="word" start_char="2997">suggested</TOKEN>
<TOKEN end_char="3008" id="token-24-41" morph="none" pos="word" start_char="3007">as</TOKEN>
<TOKEN end_char="3010" id="token-24-42" morph="none" pos="word" start_char="3010">a</TOKEN>
<TOKEN end_char="3018" id="token-24-43" morph="none" pos="word" start_char="3012">related</TOKEN>
<TOKEN end_char="3025" id="token-24-44" morph="none" pos="word" start_char="3020">search</TOKEN>
<TOKEN end_char="3030" id="token-24-45" morph="none" pos="word" start_char="3027">term</TOKEN>
<TOKEN end_char="3033" id="token-24-46" morph="none" pos="word" start_char="3032">to</TOKEN>
<TOKEN end_char="3045" id="token-24-47" morph="none" pos="word" start_char="3035">coronavirus</TOKEN>
<TOKEN end_char="3047" id="token-24-48" morph="none" pos="punct" start_char="3046">".</TOKEN>
</SEG>
<SEG end_char="3178" id="segment-25" start_char="3050">
<ORIGINAL_TEXT>We also looked at the popularity of searches for "fever" and "difficulty in breathing", two other common symptoms of coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="3051" id="token-25-0" morph="none" pos="word" start_char="3050">We</TOKEN>
<TOKEN end_char="3056" id="token-25-1" morph="none" pos="word" start_char="3053">also</TOKEN>
<TOKEN end_char="3063" id="token-25-2" morph="none" pos="word" start_char="3058">looked</TOKEN>
<TOKEN end_char="3066" id="token-25-3" morph="none" pos="word" start_char="3065">at</TOKEN>
<TOKEN end_char="3070" id="token-25-4" morph="none" pos="word" start_char="3068">the</TOKEN>
<TOKEN end_char="3081" id="token-25-5" morph="none" pos="word" start_char="3072">popularity</TOKEN>
<TOKEN end_char="3084" id="token-25-6" morph="none" pos="word" start_char="3083">of</TOKEN>
<TOKEN end_char="3093" id="token-25-7" morph="none" pos="word" start_char="3086">searches</TOKEN>
<TOKEN end_char="3097" id="token-25-8" morph="none" pos="word" start_char="3095">for</TOKEN>
<TOKEN end_char="3099" id="token-25-9" morph="none" pos="punct" start_char="3099">"</TOKEN>
<TOKEN end_char="3104" id="token-25-10" morph="none" pos="word" start_char="3100">fever</TOKEN>
<TOKEN end_char="3105" id="token-25-11" morph="none" pos="punct" start_char="3105">"</TOKEN>
<TOKEN end_char="3109" id="token-25-12" morph="none" pos="word" start_char="3107">and</TOKEN>
<TOKEN end_char="3111" id="token-25-13" morph="none" pos="punct" start_char="3111">"</TOKEN>
<TOKEN end_char="3121" id="token-25-14" morph="none" pos="word" start_char="3112">difficulty</TOKEN>
<TOKEN end_char="3124" id="token-25-15" morph="none" pos="word" start_char="3123">in</TOKEN>
<TOKEN end_char="3134" id="token-25-16" morph="none" pos="word" start_char="3126">breathing</TOKEN>
<TOKEN end_char="3136" id="token-25-17" morph="none" pos="punct" start_char="3135">",</TOKEN>
<TOKEN end_char="3140" id="token-25-18" morph="none" pos="word" start_char="3138">two</TOKEN>
<TOKEN end_char="3146" id="token-25-19" morph="none" pos="word" start_char="3142">other</TOKEN>
<TOKEN end_char="3153" id="token-25-20" morph="none" pos="word" start_char="3148">common</TOKEN>
<TOKEN end_char="3162" id="token-25-21" morph="none" pos="word" start_char="3155">symptoms</TOKEN>
<TOKEN end_char="3165" id="token-25-22" morph="none" pos="word" start_char="3164">of</TOKEN>
<TOKEN end_char="3177" id="token-25-23" morph="none" pos="word" start_char="3167">coronavirus</TOKEN>
<TOKEN end_char="3178" id="token-25-24" morph="none" pos="punct" start_char="3178">.</TOKEN>
</SEG>
<SEG end_char="3340" id="segment-26" start_char="3181">
<ORIGINAL_TEXT>Searches for "fever" increased a small amount after August at a similar rate to "cough", and queries for "difficulty in breathing" decreased in the same period.</ORIGINAL_TEXT>
<TOKEN end_char="3188" id="token-26-0" morph="none" pos="word" start_char="3181">Searches</TOKEN>
<TOKEN end_char="3192" id="token-26-1" morph="none" pos="word" start_char="3190">for</TOKEN>
<TOKEN end_char="3194" id="token-26-2" morph="none" pos="punct" start_char="3194">"</TOKEN>
<TOKEN end_char="3199" id="token-26-3" morph="none" pos="word" start_char="3195">fever</TOKEN>
<TOKEN end_char="3200" id="token-26-4" morph="none" pos="punct" start_char="3200">"</TOKEN>
<TOKEN end_char="3210" id="token-26-5" morph="none" pos="word" start_char="3202">increased</TOKEN>
<TOKEN end_char="3212" id="token-26-6" morph="none" pos="word" start_char="3212">a</TOKEN>
<TOKEN end_char="3218" id="token-26-7" morph="none" pos="word" start_char="3214">small</TOKEN>
<TOKEN end_char="3225" id="token-26-8" morph="none" pos="word" start_char="3220">amount</TOKEN>
<TOKEN end_char="3231" id="token-26-9" morph="none" pos="word" start_char="3227">after</TOKEN>
<TOKEN end_char="3238" id="token-26-10" morph="none" pos="word" start_char="3233">August</TOKEN>
<TOKEN end_char="3241" id="token-26-11" morph="none" pos="word" start_char="3240">at</TOKEN>
<TOKEN end_char="3243" id="token-26-12" morph="none" pos="word" start_char="3243">a</TOKEN>
<TOKEN end_char="3251" id="token-26-13" morph="none" pos="word" start_char="3245">similar</TOKEN>
<TOKEN end_char="3256" id="token-26-14" morph="none" pos="word" start_char="3253">rate</TOKEN>
<TOKEN end_char="3259" id="token-26-15" morph="none" pos="word" start_char="3258">to</TOKEN>
<TOKEN end_char="3261" id="token-26-16" morph="none" pos="punct" start_char="3261">"</TOKEN>
<TOKEN end_char="3266" id="token-26-17" morph="none" pos="word" start_char="3262">cough</TOKEN>
<TOKEN end_char="3268" id="token-26-18" morph="none" pos="punct" start_char="3267">",</TOKEN>
<TOKEN end_char="3272" id="token-26-19" morph="none" pos="word" start_char="3270">and</TOKEN>
<TOKEN end_char="3280" id="token-26-20" morph="none" pos="word" start_char="3274">queries</TOKEN>
<TOKEN end_char="3284" id="token-26-21" morph="none" pos="word" start_char="3282">for</TOKEN>
<TOKEN end_char="3286" id="token-26-22" morph="none" pos="punct" start_char="3286">"</TOKEN>
<TOKEN end_char="3296" id="token-26-23" morph="none" pos="word" start_char="3287">difficulty</TOKEN>
<TOKEN end_char="3299" id="token-26-24" morph="none" pos="word" start_char="3298">in</TOKEN>
<TOKEN end_char="3309" id="token-26-25" morph="none" pos="word" start_char="3301">breathing</TOKEN>
<TOKEN end_char="3310" id="token-26-26" morph="none" pos="punct" start_char="3310">"</TOKEN>
<TOKEN end_char="3320" id="token-26-27" morph="none" pos="word" start_char="3312">decreased</TOKEN>
<TOKEN end_char="3323" id="token-26-28" morph="none" pos="word" start_char="3322">in</TOKEN>
<TOKEN end_char="3327" id="token-26-29" morph="none" pos="word" start_char="3325">the</TOKEN>
<TOKEN end_char="3332" id="token-26-30" morph="none" pos="word" start_char="3329">same</TOKEN>
<TOKEN end_char="3339" id="token-26-31" morph="none" pos="word" start_char="3334">period</TOKEN>
<TOKEN end_char="3340" id="token-26-32" morph="none" pos="punct" start_char="3340">.</TOKEN>
</SEG>
<SEG end_char="3443" id="segment-27" start_char="3343">
<ORIGINAL_TEXT>There have also been questions raised about the study using diarrhoea as an indicator of the disease.</ORIGINAL_TEXT>
<TOKEN end_char="3347" id="token-27-0" morph="none" pos="word" start_char="3343">There</TOKEN>
<TOKEN end_char="3352" id="token-27-1" morph="none" pos="word" start_char="3349">have</TOKEN>
<TOKEN end_char="3357" id="token-27-2" morph="none" pos="word" start_char="3354">also</TOKEN>
<TOKEN end_char="3362" id="token-27-3" morph="none" pos="word" start_char="3359">been</TOKEN>
<TOKEN end_char="3372" id="token-27-4" morph="none" pos="word" start_char="3364">questions</TOKEN>
<TOKEN end_char="3379" id="token-27-5" morph="none" pos="word" start_char="3374">raised</TOKEN>
<TOKEN end_char="3385" id="token-27-6" morph="none" pos="word" start_char="3381">about</TOKEN>
<TOKEN end_char="3389" id="token-27-7" morph="none" pos="word" start_char="3387">the</TOKEN>
<TOKEN end_char="3395" id="token-27-8" morph="none" pos="word" start_char="3391">study</TOKEN>
<TOKEN end_char="3401" id="token-27-9" morph="none" pos="word" start_char="3397">using</TOKEN>
<TOKEN end_char="3411" id="token-27-10" morph="none" pos="word" start_char="3403">diarrhoea</TOKEN>
<TOKEN end_char="3414" id="token-27-11" morph="none" pos="word" start_char="3413">as</TOKEN>
<TOKEN end_char="3417" id="token-27-12" morph="none" pos="word" start_char="3416">an</TOKEN>
<TOKEN end_char="3427" id="token-27-13" morph="none" pos="word" start_char="3419">indicator</TOKEN>
<TOKEN end_char="3430" id="token-27-14" morph="none" pos="word" start_char="3429">of</TOKEN>
<TOKEN end_char="3434" id="token-27-15" morph="none" pos="word" start_char="3432">the</TOKEN>
<TOKEN end_char="3442" id="token-27-16" morph="none" pos="word" start_char="3436">disease</TOKEN>
<TOKEN end_char="3443" id="token-27-17" morph="none" pos="punct" start_char="3443">.</TOKEN>
</SEG>
<SEG end_char="3627" id="segment-28" start_char="3446">
<ORIGINAL_TEXT>A large-scale UK study of nearly 17,000 coronavirus patients found that diarrhoea was the seventh most common symptom, well below the top three: cough, fever and shortness of breath.</ORIGINAL_TEXT>
<TOKEN end_char="3446" id="token-28-0" morph="none" pos="word" start_char="3446">A</TOKEN>
<TOKEN end_char="3458" id="token-28-1" morph="none" pos="unknown" start_char="3448">large-scale</TOKEN>
<TOKEN end_char="3461" id="token-28-2" morph="none" pos="word" start_char="3460">UK</TOKEN>
<TOKEN end_char="3467" id="token-28-3" morph="none" pos="word" start_char="3463">study</TOKEN>
<TOKEN end_char="3470" id="token-28-4" morph="none" pos="word" start_char="3469">of</TOKEN>
<TOKEN end_char="3477" id="token-28-5" morph="none" pos="word" start_char="3472">nearly</TOKEN>
<TOKEN end_char="3484" id="token-28-6" morph="none" pos="unknown" start_char="3479">17,000</TOKEN>
<TOKEN end_char="3496" id="token-28-7" morph="none" pos="word" start_char="3486">coronavirus</TOKEN>
<TOKEN end_char="3505" id="token-28-8" morph="none" pos="word" start_char="3498">patients</TOKEN>
<TOKEN end_char="3511" id="token-28-9" morph="none" pos="word" start_char="3507">found</TOKEN>
<TOKEN end_char="3516" id="token-28-10" morph="none" pos="word" start_char="3513">that</TOKEN>
<TOKEN end_char="3526" id="token-28-11" morph="none" pos="word" start_char="3518">diarrhoea</TOKEN>
<TOKEN end_char="3530" id="token-28-12" morph="none" pos="word" start_char="3528">was</TOKEN>
<TOKEN end_char="3534" id="token-28-13" morph="none" pos="word" start_char="3532">the</TOKEN>
<TOKEN end_char="3542" id="token-28-14" morph="none" pos="word" start_char="3536">seventh</TOKEN>
<TOKEN end_char="3547" id="token-28-15" morph="none" pos="word" start_char="3544">most</TOKEN>
<TOKEN end_char="3554" id="token-28-16" morph="none" pos="word" start_char="3549">common</TOKEN>
<TOKEN end_char="3562" id="token-28-17" morph="none" pos="word" start_char="3556">symptom</TOKEN>
<TOKEN end_char="3563" id="token-28-18" morph="none" pos="punct" start_char="3563">,</TOKEN>
<TOKEN end_char="3568" id="token-28-19" morph="none" pos="word" start_char="3565">well</TOKEN>
<TOKEN end_char="3574" id="token-28-20" morph="none" pos="word" start_char="3570">below</TOKEN>
<TOKEN end_char="3578" id="token-28-21" morph="none" pos="word" start_char="3576">the</TOKEN>
<TOKEN end_char="3582" id="token-28-22" morph="none" pos="word" start_char="3580">top</TOKEN>
<TOKEN end_char="3588" id="token-28-23" morph="none" pos="word" start_char="3584">three</TOKEN>
<TOKEN end_char="3589" id="token-28-24" morph="none" pos="punct" start_char="3589">:</TOKEN>
<TOKEN end_char="3595" id="token-28-25" morph="none" pos="word" start_char="3591">cough</TOKEN>
<TOKEN end_char="3596" id="token-28-26" morph="none" pos="punct" start_char="3596">,</TOKEN>
<TOKEN end_char="3602" id="token-28-27" morph="none" pos="word" start_char="3598">fever</TOKEN>
<TOKEN end_char="3606" id="token-28-28" morph="none" pos="word" start_char="3604">and</TOKEN>
<TOKEN end_char="3616" id="token-28-29" morph="none" pos="word" start_char="3608">shortness</TOKEN>
<TOKEN end_char="3619" id="token-28-30" morph="none" pos="word" start_char="3618">of</TOKEN>
<TOKEN end_char="3626" id="token-28-31" morph="none" pos="word" start_char="3621">breath</TOKEN>
<TOKEN end_char="3627" id="token-28-32" morph="none" pos="punct" start_char="3627">.</TOKEN>
</SEG>
<SEG end_char="3659" id="segment-29" start_char="3630">
<ORIGINAL_TEXT>What about the number of cars?</ORIGINAL_TEXT>
<TOKEN end_char="3633" id="token-29-0" morph="none" pos="word" start_char="3630">What</TOKEN>
<TOKEN end_char="3639" id="token-29-1" morph="none" pos="word" start_char="3635">about</TOKEN>
<TOKEN end_char="3643" id="token-29-2" morph="none" pos="word" start_char="3641">the</TOKEN>
<TOKEN end_char="3650" id="token-29-3" morph="none" pos="word" start_char="3645">number</TOKEN>
<TOKEN end_char="3653" id="token-29-4" morph="none" pos="word" start_char="3652">of</TOKEN>
<TOKEN end_char="3658" id="token-29-5" morph="none" pos="word" start_char="3655">cars</TOKEN>
<TOKEN end_char="3659" id="token-29-6" morph="none" pos="punct" start_char="3659">?</TOKEN>
</SEG>
<SEG end_char="3784" id="segment-30" start_char="3663">
<ORIGINAL_TEXT>Across the six hospitals, the Harvard study reported a rise in cars in hospital parking lots from August to December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="3668" id="token-30-0" morph="none" pos="word" start_char="3663">Across</TOKEN>
<TOKEN end_char="3672" id="token-30-1" morph="none" pos="word" start_char="3670">the</TOKEN>
<TOKEN end_char="3676" id="token-30-2" morph="none" pos="word" start_char="3674">six</TOKEN>
<TOKEN end_char="3686" id="token-30-3" morph="none" pos="word" start_char="3678">hospitals</TOKEN>
<TOKEN end_char="3687" id="token-30-4" morph="none" pos="punct" start_char="3687">,</TOKEN>
<TOKEN end_char="3691" id="token-30-5" morph="none" pos="word" start_char="3689">the</TOKEN>
<TOKEN end_char="3699" id="token-30-6" morph="none" pos="word" start_char="3693">Harvard</TOKEN>
<TOKEN end_char="3705" id="token-30-7" morph="none" pos="word" start_char="3701">study</TOKEN>
<TOKEN end_char="3714" id="token-30-8" morph="none" pos="word" start_char="3707">reported</TOKEN>
<TOKEN end_char="3716" id="token-30-9" morph="none" pos="word" start_char="3716">a</TOKEN>
<TOKEN end_char="3721" id="token-30-10" morph="none" pos="word" start_char="3718">rise</TOKEN>
<TOKEN end_char="3724" id="token-30-11" morph="none" pos="word" start_char="3723">in</TOKEN>
<TOKEN end_char="3729" id="token-30-12" morph="none" pos="word" start_char="3726">cars</TOKEN>
<TOKEN end_char="3732" id="token-30-13" morph="none" pos="word" start_char="3731">in</TOKEN>
<TOKEN end_char="3741" id="token-30-14" morph="none" pos="word" start_char="3734">hospital</TOKEN>
<TOKEN end_char="3749" id="token-30-15" morph="none" pos="word" start_char="3743">parking</TOKEN>
<TOKEN end_char="3754" id="token-30-16" morph="none" pos="word" start_char="3751">lots</TOKEN>
<TOKEN end_char="3759" id="token-30-17" morph="none" pos="word" start_char="3756">from</TOKEN>
<TOKEN end_char="3766" id="token-30-18" morph="none" pos="word" start_char="3761">August</TOKEN>
<TOKEN end_char="3769" id="token-30-19" morph="none" pos="word" start_char="3768">to</TOKEN>
<TOKEN end_char="3778" id="token-30-20" morph="none" pos="word" start_char="3771">December</TOKEN>
<TOKEN end_char="3783" id="token-30-21" morph="none" pos="word" start_char="3780">2019</TOKEN>
<TOKEN end_char="3784" id="token-30-22" morph="none" pos="punct" start_char="3784">.</TOKEN>
</SEG>
<SEG end_char="3844" id="segment-31" start_char="3787">
<ORIGINAL_TEXT>However, we've found some serious flaws in their analysis.</ORIGINAL_TEXT>
<TOKEN end_char="3793" id="token-31-0" morph="none" pos="word" start_char="3787">However</TOKEN>
<TOKEN end_char="3794" id="token-31-1" morph="none" pos="punct" start_char="3794">,</TOKEN>
<TOKEN end_char="3800" id="token-31-2" morph="none" pos="word" start_char="3796">we've</TOKEN>
<TOKEN end_char="3806" id="token-31-3" morph="none" pos="word" start_char="3802">found</TOKEN>
<TOKEN end_char="3811" id="token-31-4" morph="none" pos="word" start_char="3808">some</TOKEN>
<TOKEN end_char="3819" id="token-31-5" morph="none" pos="word" start_char="3813">serious</TOKEN>
<TOKEN end_char="3825" id="token-31-6" morph="none" pos="word" start_char="3821">flaws</TOKEN>
<TOKEN end_char="3828" id="token-31-7" morph="none" pos="word" start_char="3827">in</TOKEN>
<TOKEN end_char="3834" id="token-31-8" morph="none" pos="word" start_char="3830">their</TOKEN>
<TOKEN end_char="3843" id="token-31-9" morph="none" pos="word" start_char="3836">analysis</TOKEN>
<TOKEN end_char="3844" id="token-31-10" morph="none" pos="punct" start_char="3844">.</TOKEN>
</SEG>
<SEG end_char="3864" id="segment-32" start_char="3847">
<ORIGINAL_TEXT>Harvard University</ORIGINAL_TEXT>
<TOKEN end_char="3853" id="token-32-0" morph="none" pos="word" start_char="3847">Harvard</TOKEN>
<TOKEN end_char="3864" id="token-32-1" morph="none" pos="word" start_char="3855">University</TOKEN>
</SEG>
<SEG end_char="3944" id="segment-33" start_char="3867">
<ORIGINAL_TEXT>The researchers studied and annotated satellite images showing Wuhan hospitals</ORIGINAL_TEXT>
<TOKEN end_char="3869" id="token-33-0" morph="none" pos="word" start_char="3867">The</TOKEN>
<TOKEN end_char="3881" id="token-33-1" morph="none" pos="word" start_char="3871">researchers</TOKEN>
<TOKEN end_char="3889" id="token-33-2" morph="none" pos="word" start_char="3883">studied</TOKEN>
<TOKEN end_char="3893" id="token-33-3" morph="none" pos="word" start_char="3891">and</TOKEN>
<TOKEN end_char="3903" id="token-33-4" morph="none" pos="word" start_char="3895">annotated</TOKEN>
<TOKEN end_char="3913" id="token-33-5" morph="none" pos="word" start_char="3905">satellite</TOKEN>
<TOKEN end_char="3920" id="token-33-6" morph="none" pos="word" start_char="3915">images</TOKEN>
<TOKEN end_char="3928" id="token-33-7" morph="none" pos="word" start_char="3922">showing</TOKEN>
<TOKEN end_char="3934" id="token-33-8" morph="none" pos="word" start_char="3930">Wuhan</TOKEN>
<TOKEN end_char="3944" id="token-33-9" morph="none" pos="word" start_char="3936">hospitals</TOKEN>
</SEG>
<SEG end_char="4072" id="segment-34" start_char="3948">
<ORIGINAL_TEXT>The report states that images with tree cover and building shadows were excluded to avoid over or under-counting of vehicles.</ORIGINAL_TEXT>
<TOKEN end_char="3950" id="token-34-0" morph="none" pos="word" start_char="3948">The</TOKEN>
<TOKEN end_char="3957" id="token-34-1" morph="none" pos="word" start_char="3952">report</TOKEN>
<TOKEN end_char="3964" id="token-34-2" morph="none" pos="word" start_char="3959">states</TOKEN>
<TOKEN end_char="3969" id="token-34-3" morph="none" pos="word" start_char="3966">that</TOKEN>
<TOKEN end_char="3976" id="token-34-4" morph="none" pos="word" start_char="3971">images</TOKEN>
<TOKEN end_char="3981" id="token-34-5" morph="none" pos="word" start_char="3978">with</TOKEN>
<TOKEN end_char="3986" id="token-34-6" morph="none" pos="word" start_char="3983">tree</TOKEN>
<TOKEN end_char="3992" id="token-34-7" morph="none" pos="word" start_char="3988">cover</TOKEN>
<TOKEN end_char="3996" id="token-34-8" morph="none" pos="word" start_char="3994">and</TOKEN>
<TOKEN end_char="4005" id="token-34-9" morph="none" pos="word" start_char="3998">building</TOKEN>
<TOKEN end_char="4013" id="token-34-10" morph="none" pos="word" start_char="4007">shadows</TOKEN>
<TOKEN end_char="4018" id="token-34-11" morph="none" pos="word" start_char="4015">were</TOKEN>
<TOKEN end_char="4027" id="token-34-12" morph="none" pos="word" start_char="4020">excluded</TOKEN>
<TOKEN end_char="4030" id="token-34-13" morph="none" pos="word" start_char="4029">to</TOKEN>
<TOKEN end_char="4036" id="token-34-14" morph="none" pos="word" start_char="4032">avoid</TOKEN>
<TOKEN end_char="4041" id="token-34-15" morph="none" pos="word" start_char="4038">over</TOKEN>
<TOKEN end_char="4044" id="token-34-16" morph="none" pos="word" start_char="4043">or</TOKEN>
<TOKEN end_char="4059" id="token-34-17" morph="none" pos="unknown" start_char="4046">under-counting</TOKEN>
<TOKEN end_char="4062" id="token-34-18" morph="none" pos="word" start_char="4061">of</TOKEN>
<TOKEN end_char="4071" id="token-34-19" morph="none" pos="word" start_char="4064">vehicles</TOKEN>
<TOKEN end_char="4072" id="token-34-20" morph="none" pos="punct" start_char="4072">.</TOKEN>
</SEG>
<SEG end_char="4270" id="segment-35" start_char="4075">
<ORIGINAL_TEXT>However, satellite images released to the media show large areas of hospital car parks blocked by tall buildings which means that it's not possible to accurately assess the number of cars present.</ORIGINAL_TEXT>
<TOKEN end_char="4081" id="token-35-0" morph="none" pos="word" start_char="4075">However</TOKEN>
<TOKEN end_char="4082" id="token-35-1" morph="none" pos="punct" start_char="4082">,</TOKEN>
<TOKEN end_char="4092" id="token-35-2" morph="none" pos="word" start_char="4084">satellite</TOKEN>
<TOKEN end_char="4099" id="token-35-3" morph="none" pos="word" start_char="4094">images</TOKEN>
<TOKEN end_char="4108" id="token-35-4" morph="none" pos="word" start_char="4101">released</TOKEN>
<TOKEN end_char="4111" id="token-35-5" morph="none" pos="word" start_char="4110">to</TOKEN>
<TOKEN end_char="4115" id="token-35-6" morph="none" pos="word" start_char="4113">the</TOKEN>
<TOKEN end_char="4121" id="token-35-7" morph="none" pos="word" start_char="4117">media</TOKEN>
<TOKEN end_char="4126" id="token-35-8" morph="none" pos="word" start_char="4123">show</TOKEN>
<TOKEN end_char="4132" id="token-35-9" morph="none" pos="word" start_char="4128">large</TOKEN>
<TOKEN end_char="4138" id="token-35-10" morph="none" pos="word" start_char="4134">areas</TOKEN>
<TOKEN end_char="4141" id="token-35-11" morph="none" pos="word" start_char="4140">of</TOKEN>
<TOKEN end_char="4150" id="token-35-12" morph="none" pos="word" start_char="4143">hospital</TOKEN>
<TOKEN end_char="4154" id="token-35-13" morph="none" pos="word" start_char="4152">car</TOKEN>
<TOKEN end_char="4160" id="token-35-14" morph="none" pos="word" start_char="4156">parks</TOKEN>
<TOKEN end_char="4168" id="token-35-15" morph="none" pos="word" start_char="4162">blocked</TOKEN>
<TOKEN end_char="4171" id="token-35-16" morph="none" pos="word" start_char="4170">by</TOKEN>
<TOKEN end_char="4176" id="token-35-17" morph="none" pos="word" start_char="4173">tall</TOKEN>
<TOKEN end_char="4186" id="token-35-18" morph="none" pos="word" start_char="4178">buildings</TOKEN>
<TOKEN end_char="4192" id="token-35-19" morph="none" pos="word" start_char="4188">which</TOKEN>
<TOKEN end_char="4198" id="token-35-20" morph="none" pos="word" start_char="4194">means</TOKEN>
<TOKEN end_char="4203" id="token-35-21" morph="none" pos="word" start_char="4200">that</TOKEN>
<TOKEN end_char="4208" id="token-35-22" morph="none" pos="word" start_char="4205">it's</TOKEN>
<TOKEN end_char="4212" id="token-35-23" morph="none" pos="word" start_char="4210">not</TOKEN>
<TOKEN end_char="4221" id="token-35-24" morph="none" pos="word" start_char="4214">possible</TOKEN>
<TOKEN end_char="4224" id="token-35-25" morph="none" pos="word" start_char="4223">to</TOKEN>
<TOKEN end_char="4235" id="token-35-26" morph="none" pos="word" start_char="4226">accurately</TOKEN>
<TOKEN end_char="4242" id="token-35-27" morph="none" pos="word" start_char="4237">assess</TOKEN>
<TOKEN end_char="4246" id="token-35-28" morph="none" pos="word" start_char="4244">the</TOKEN>
<TOKEN end_char="4253" id="token-35-29" morph="none" pos="word" start_char="4248">number</TOKEN>
<TOKEN end_char="4256" id="token-35-30" morph="none" pos="word" start_char="4255">of</TOKEN>
<TOKEN end_char="4261" id="token-35-31" morph="none" pos="word" start_char="4258">cars</TOKEN>
<TOKEN end_char="4269" id="token-35-32" morph="none" pos="word" start_char="4263">present</TOKEN>
<TOKEN end_char="4270" id="token-35-33" morph="none" pos="punct" start_char="4270">.</TOKEN>
</SEG>
<SEG end_char="4387" id="segment-36" start_char="4273">
<ORIGINAL_TEXT>To show this, we've selected two of these images taken at different times of the Hubei Women and Children Hospital.</ORIGINAL_TEXT>
<TOKEN end_char="4274" id="token-36-0" morph="none" pos="word" start_char="4273">To</TOKEN>
<TOKEN end_char="4279" id="token-36-1" morph="none" pos="word" start_char="4276">show</TOKEN>
<TOKEN end_char="4284" id="token-36-2" morph="none" pos="word" start_char="4281">this</TOKEN>
<TOKEN end_char="4285" id="token-36-3" morph="none" pos="punct" start_char="4285">,</TOKEN>
<TOKEN end_char="4291" id="token-36-4" morph="none" pos="word" start_char="4287">we've</TOKEN>
<TOKEN end_char="4300" id="token-36-5" morph="none" pos="word" start_char="4293">selected</TOKEN>
<TOKEN end_char="4304" id="token-36-6" morph="none" pos="word" start_char="4302">two</TOKEN>
<TOKEN end_char="4307" id="token-36-7" morph="none" pos="word" start_char="4306">of</TOKEN>
<TOKEN end_char="4313" id="token-36-8" morph="none" pos="word" start_char="4309">these</TOKEN>
<TOKEN end_char="4320" id="token-36-9" morph="none" pos="word" start_char="4315">images</TOKEN>
<TOKEN end_char="4326" id="token-36-10" morph="none" pos="word" start_char="4322">taken</TOKEN>
<TOKEN end_char="4329" id="token-36-11" morph="none" pos="word" start_char="4328">at</TOKEN>
<TOKEN end_char="4339" id="token-36-12" morph="none" pos="word" start_char="4331">different</TOKEN>
<TOKEN end_char="4345" id="token-36-13" morph="none" pos="word" start_char="4341">times</TOKEN>
<TOKEN end_char="4348" id="token-36-14" morph="none" pos="word" start_char="4347">of</TOKEN>
<TOKEN end_char="4352" id="token-36-15" morph="none" pos="word" start_char="4350">the</TOKEN>
<TOKEN end_char="4358" id="token-36-16" morph="none" pos="word" start_char="4354">Hubei</TOKEN>
<TOKEN end_char="4364" id="token-36-17" morph="none" pos="word" start_char="4360">Women</TOKEN>
<TOKEN end_char="4368" id="token-36-18" morph="none" pos="word" start_char="4366">and</TOKEN>
<TOKEN end_char="4377" id="token-36-19" morph="none" pos="word" start_char="4370">Children</TOKEN>
<TOKEN end_char="4386" id="token-36-20" morph="none" pos="word" start_char="4379">Hospital</TOKEN>
<TOKEN end_char="4387" id="token-36-21" morph="none" pos="punct" start_char="4387">.</TOKEN>
</SEG>
<SEG end_char="4518" id="segment-37" start_char="4390">
<ORIGINAL_TEXT>The first is of the healthcare centre in October 2018 - and we've marked with white boxes the parking area obscured by buildings.</ORIGINAL_TEXT>
<TOKEN end_char="4392" id="token-37-0" morph="none" pos="word" start_char="4390">The</TOKEN>
<TOKEN end_char="4398" id="token-37-1" morph="none" pos="word" start_char="4394">first</TOKEN>
<TOKEN end_char="4401" id="token-37-2" morph="none" pos="word" start_char="4400">is</TOKEN>
<TOKEN end_char="4404" id="token-37-3" morph="none" pos="word" start_char="4403">of</TOKEN>
<TOKEN end_char="4408" id="token-37-4" morph="none" pos="word" start_char="4406">the</TOKEN>
<TOKEN end_char="4419" id="token-37-5" morph="none" pos="word" start_char="4410">healthcare</TOKEN>
<TOKEN end_char="4426" id="token-37-6" morph="none" pos="word" start_char="4421">centre</TOKEN>
<TOKEN end_char="4429" id="token-37-7" morph="none" pos="word" start_char="4428">in</TOKEN>
<TOKEN end_char="4437" id="token-37-8" morph="none" pos="word" start_char="4431">October</TOKEN>
<TOKEN end_char="4442" id="token-37-9" morph="none" pos="word" start_char="4439">2018</TOKEN>
<TOKEN end_char="4444" id="token-37-10" morph="none" pos="punct" start_char="4444">-</TOKEN>
<TOKEN end_char="4448" id="token-37-11" morph="none" pos="word" start_char="4446">and</TOKEN>
<TOKEN end_char="4454" id="token-37-12" morph="none" pos="word" start_char="4450">we've</TOKEN>
<TOKEN end_char="4461" id="token-37-13" morph="none" pos="word" start_char="4456">marked</TOKEN>
<TOKEN end_char="4466" id="token-37-14" morph="none" pos="word" start_char="4463">with</TOKEN>
<TOKEN end_char="4472" id="token-37-15" morph="none" pos="word" start_char="4468">white</TOKEN>
<TOKEN end_char="4478" id="token-37-16" morph="none" pos="word" start_char="4474">boxes</TOKEN>
<TOKEN end_char="4482" id="token-37-17" morph="none" pos="word" start_char="4480">the</TOKEN>
<TOKEN end_char="4490" id="token-37-18" morph="none" pos="word" start_char="4484">parking</TOKEN>
<TOKEN end_char="4495" id="token-37-19" morph="none" pos="word" start_char="4492">area</TOKEN>
<TOKEN end_char="4504" id="token-37-20" morph="none" pos="word" start_char="4497">obscured</TOKEN>
<TOKEN end_char="4507" id="token-37-21" morph="none" pos="word" start_char="4506">by</TOKEN>
<TOKEN end_char="4517" id="token-37-22" morph="none" pos="word" start_char="4509">buildings</TOKEN>
<TOKEN end_char="4518" id="token-37-23" morph="none" pos="punct" start_char="4518">.</TOKEN>
</SEG>
<SEG end_char="4530" id="segment-38" start_char="4521">
<ORIGINAL_TEXT>RS Metrics</ORIGINAL_TEXT>
<TOKEN end_char="4522" id="token-38-0" morph="none" pos="word" start_char="4521">RS</TOKEN>
<TOKEN end_char="4530" id="token-38-1" morph="none" pos="word" start_char="4524">Metrics</TOKEN>
</SEG>
<SEG end_char="4628" id="segment-39" start_char="4534">
<ORIGINAL_TEXT>This second image below - taken in October 2019 - shows the same hospital at a different angle.</ORIGINAL_TEXT>
<TOKEN end_char="4537" id="token-39-0" morph="none" pos="word" start_char="4534">This</TOKEN>
<TOKEN end_char="4544" id="token-39-1" morph="none" pos="word" start_char="4539">second</TOKEN>
<TOKEN end_char="4550" id="token-39-2" morph="none" pos="word" start_char="4546">image</TOKEN>
<TOKEN end_char="4556" id="token-39-3" morph="none" pos="word" start_char="4552">below</TOKEN>
<TOKEN end_char="4558" id="token-39-4" morph="none" pos="punct" start_char="4558">-</TOKEN>
<TOKEN end_char="4564" id="token-39-5" morph="none" pos="word" start_char="4560">taken</TOKEN>
<TOKEN end_char="4567" id="token-39-6" morph="none" pos="word" start_char="4566">in</TOKEN>
<TOKEN end_char="4575" id="token-39-7" morph="none" pos="word" start_char="4569">October</TOKEN>
<TOKEN end_char="4580" id="token-39-8" morph="none" pos="word" start_char="4577">2019</TOKEN>
<TOKEN end_char="4582" id="token-39-9" morph="none" pos="punct" start_char="4582">-</TOKEN>
<TOKEN end_char="4588" id="token-39-10" morph="none" pos="word" start_char="4584">shows</TOKEN>
<TOKEN end_char="4592" id="token-39-11" morph="none" pos="word" start_char="4590">the</TOKEN>
<TOKEN end_char="4597" id="token-39-12" morph="none" pos="word" start_char="4594">same</TOKEN>
<TOKEN end_char="4606" id="token-39-13" morph="none" pos="word" start_char="4599">hospital</TOKEN>
<TOKEN end_char="4609" id="token-39-14" morph="none" pos="word" start_char="4608">at</TOKEN>
<TOKEN end_char="4611" id="token-39-15" morph="none" pos="word" start_char="4611">a</TOKEN>
<TOKEN end_char="4621" id="token-39-16" morph="none" pos="word" start_char="4613">different</TOKEN>
<TOKEN end_char="4627" id="token-39-17" morph="none" pos="word" start_char="4623">angle</TOKEN>
<TOKEN end_char="4628" id="token-39-18" morph="none" pos="punct" start_char="4628">.</TOKEN>
</SEG>
<SEG end_char="4712" id="segment-40" start_char="4630">
<ORIGINAL_TEXT>In this image there's a full view of the spaces previously hidden by the buildings.</ORIGINAL_TEXT>
<TOKEN end_char="4631" id="token-40-0" morph="none" pos="word" start_char="4630">In</TOKEN>
<TOKEN end_char="4636" id="token-40-1" morph="none" pos="word" start_char="4633">this</TOKEN>
<TOKEN end_char="4642" id="token-40-2" morph="none" pos="word" start_char="4638">image</TOKEN>
<TOKEN end_char="4650" id="token-40-3" morph="none" pos="word" start_char="4644">there's</TOKEN>
<TOKEN end_char="4652" id="token-40-4" morph="none" pos="word" start_char="4652">a</TOKEN>
<TOKEN end_char="4657" id="token-40-5" morph="none" pos="word" start_char="4654">full</TOKEN>
<TOKEN end_char="4662" id="token-40-6" morph="none" pos="word" start_char="4659">view</TOKEN>
<TOKEN end_char="4665" id="token-40-7" morph="none" pos="word" start_char="4664">of</TOKEN>
<TOKEN end_char="4669" id="token-40-8" morph="none" pos="word" start_char="4667">the</TOKEN>
<TOKEN end_char="4676" id="token-40-9" morph="none" pos="word" start_char="4671">spaces</TOKEN>
<TOKEN end_char="4687" id="token-40-10" morph="none" pos="word" start_char="4678">previously</TOKEN>
<TOKEN end_char="4694" id="token-40-11" morph="none" pos="word" start_char="4689">hidden</TOKEN>
<TOKEN end_char="4697" id="token-40-12" morph="none" pos="word" start_char="4696">by</TOKEN>
<TOKEN end_char="4701" id="token-40-13" morph="none" pos="word" start_char="4699">the</TOKEN>
<TOKEN end_char="4711" id="token-40-14" morph="none" pos="word" start_char="4703">buildings</TOKEN>
<TOKEN end_char="4712" id="token-40-15" morph="none" pos="punct" start_char="4712">.</TOKEN>
</SEG>
<SEG end_char="4724" id="segment-41" start_char="4715">
<ORIGINAL_TEXT>RS Metrics</ORIGINAL_TEXT>
<TOKEN end_char="4716" id="token-41-0" morph="none" pos="word" start_char="4715">RS</TOKEN>
<TOKEN end_char="4724" id="token-41-1" morph="none" pos="word" start_char="4718">Metrics</TOKEN>
</SEG>
<SEG end_char="4853" id="segment-42" start_char="4728">
<ORIGINAL_TEXT>Looking at other satellite images used in the Harvard report, we found car parks in other hospitals obscured in a similar way.</ORIGINAL_TEXT>
<TOKEN end_char="4734" id="token-42-0" morph="none" pos="word" start_char="4728">Looking</TOKEN>
<TOKEN end_char="4737" id="token-42-1" morph="none" pos="word" start_char="4736">at</TOKEN>
<TOKEN end_char="4743" id="token-42-2" morph="none" pos="word" start_char="4739">other</TOKEN>
<TOKEN end_char="4753" id="token-42-3" morph="none" pos="word" start_char="4745">satellite</TOKEN>
<TOKEN end_char="4760" id="token-42-4" morph="none" pos="word" start_char="4755">images</TOKEN>
<TOKEN end_char="4765" id="token-42-5" morph="none" pos="word" start_char="4762">used</TOKEN>
<TOKEN end_char="4768" id="token-42-6" morph="none" pos="word" start_char="4767">in</TOKEN>
<TOKEN end_char="4772" id="token-42-7" morph="none" pos="word" start_char="4770">the</TOKEN>
<TOKEN end_char="4780" id="token-42-8" morph="none" pos="word" start_char="4774">Harvard</TOKEN>
<TOKEN end_char="4787" id="token-42-9" morph="none" pos="word" start_char="4782">report</TOKEN>
<TOKEN end_char="4788" id="token-42-10" morph="none" pos="punct" start_char="4788">,</TOKEN>
<TOKEN end_char="4791" id="token-42-11" morph="none" pos="word" start_char="4790">we</TOKEN>
<TOKEN end_char="4797" id="token-42-12" morph="none" pos="word" start_char="4793">found</TOKEN>
<TOKEN end_char="4801" id="token-42-13" morph="none" pos="word" start_char="4799">car</TOKEN>
<TOKEN end_char="4807" id="token-42-14" morph="none" pos="word" start_char="4803">parks</TOKEN>
<TOKEN end_char="4810" id="token-42-15" morph="none" pos="word" start_char="4809">in</TOKEN>
<TOKEN end_char="4816" id="token-42-16" morph="none" pos="word" start_char="4812">other</TOKEN>
<TOKEN end_char="4826" id="token-42-17" morph="none" pos="word" start_char="4818">hospitals</TOKEN>
<TOKEN end_char="4835" id="token-42-18" morph="none" pos="word" start_char="4828">obscured</TOKEN>
<TOKEN end_char="4838" id="token-42-19" morph="none" pos="word" start_char="4837">in</TOKEN>
<TOKEN end_char="4840" id="token-42-20" morph="none" pos="word" start_char="4840">a</TOKEN>
<TOKEN end_char="4848" id="token-42-21" morph="none" pos="word" start_char="4842">similar</TOKEN>
<TOKEN end_char="4852" id="token-42-22" morph="none" pos="word" start_char="4850">way</TOKEN>
<TOKEN end_char="4853" id="token-42-23" morph="none" pos="punct" start_char="4853">.</TOKEN>
</SEG>
<SEG end_char="5054" id="segment-43" start_char="4856">
<ORIGINAL_TEXT>There's also an underground car park at Tianyou Hospital, which is visible on Baidu's street view function, but only the entrance is in view on satellite imagery - not the cars underneath the ground.</ORIGINAL_TEXT>
<TOKEN end_char="4862" id="token-43-0" morph="none" pos="word" start_char="4856">There's</TOKEN>
<TOKEN end_char="4867" id="token-43-1" morph="none" pos="word" start_char="4864">also</TOKEN>
<TOKEN end_char="4870" id="token-43-2" morph="none" pos="word" start_char="4869">an</TOKEN>
<TOKEN end_char="4882" id="token-43-3" morph="none" pos="word" start_char="4872">underground</TOKEN>
<TOKEN end_char="4886" id="token-43-4" morph="none" pos="word" start_char="4884">car</TOKEN>
<TOKEN end_char="4891" id="token-43-5" morph="none" pos="word" start_char="4888">park</TOKEN>
<TOKEN end_char="4894" id="token-43-6" morph="none" pos="word" start_char="4893">at</TOKEN>
<TOKEN end_char="4902" id="token-43-7" morph="none" pos="word" start_char="4896">Tianyou</TOKEN>
<TOKEN end_char="4911" id="token-43-8" morph="none" pos="word" start_char="4904">Hospital</TOKEN>
<TOKEN end_char="4912" id="token-43-9" morph="none" pos="punct" start_char="4912">,</TOKEN>
<TOKEN end_char="4918" id="token-43-10" morph="none" pos="word" start_char="4914">which</TOKEN>
<TOKEN end_char="4921" id="token-43-11" morph="none" pos="word" start_char="4920">is</TOKEN>
<TOKEN end_char="4929" id="token-43-12" morph="none" pos="word" start_char="4923">visible</TOKEN>
<TOKEN end_char="4932" id="token-43-13" morph="none" pos="word" start_char="4931">on</TOKEN>
<TOKEN end_char="4940" id="token-43-14" morph="none" pos="word" start_char="4934">Baidu's</TOKEN>
<TOKEN end_char="4947" id="token-43-15" morph="none" pos="word" start_char="4942">street</TOKEN>
<TOKEN end_char="4952" id="token-43-16" morph="none" pos="word" start_char="4949">view</TOKEN>
<TOKEN end_char="4961" id="token-43-17" morph="none" pos="word" start_char="4954">function</TOKEN>
<TOKEN end_char="4962" id="token-43-18" morph="none" pos="punct" start_char="4962">,</TOKEN>
<TOKEN end_char="4966" id="token-43-19" morph="none" pos="word" start_char="4964">but</TOKEN>
<TOKEN end_char="4971" id="token-43-20" morph="none" pos="word" start_char="4968">only</TOKEN>
<TOKEN end_char="4975" id="token-43-21" morph="none" pos="word" start_char="4973">the</TOKEN>
<TOKEN end_char="4984" id="token-43-22" morph="none" pos="word" start_char="4977">entrance</TOKEN>
<TOKEN end_char="4987" id="token-43-23" morph="none" pos="word" start_char="4986">is</TOKEN>
<TOKEN end_char="4990" id="token-43-24" morph="none" pos="word" start_char="4989">in</TOKEN>
<TOKEN end_char="4995" id="token-43-25" morph="none" pos="word" start_char="4992">view</TOKEN>
<TOKEN end_char="4998" id="token-43-26" morph="none" pos="word" start_char="4997">on</TOKEN>
<TOKEN end_char="5008" id="token-43-27" morph="none" pos="word" start_char="5000">satellite</TOKEN>
<TOKEN end_char="5016" id="token-43-28" morph="none" pos="word" start_char="5010">imagery</TOKEN>
<TOKEN end_char="5018" id="token-43-29" morph="none" pos="punct" start_char="5018">-</TOKEN>
<TOKEN end_char="5022" id="token-43-30" morph="none" pos="word" start_char="5020">not</TOKEN>
<TOKEN end_char="5026" id="token-43-31" morph="none" pos="word" start_char="5024">the</TOKEN>
<TOKEN end_char="5031" id="token-43-32" morph="none" pos="word" start_char="5028">cars</TOKEN>
<TOKEN end_char="5042" id="token-43-33" morph="none" pos="word" start_char="5033">underneath</TOKEN>
<TOKEN end_char="5046" id="token-43-34" morph="none" pos="word" start_char="5044">the</TOKEN>
<TOKEN end_char="5053" id="token-43-35" morph="none" pos="word" start_char="5048">ground</TOKEN>
<TOKEN end_char="5054" id="token-43-36" morph="none" pos="punct" start_char="5054">.</TOKEN>
</SEG>
<SEG end_char="5235" id="segment-44" start_char="5056">
<ORIGINAL_TEXT>Report author Benjamin Rader said "we definitely can't account for underground parking in any time period of the study and this is one of the limitations of this type of research."</ORIGINAL_TEXT>
<TOKEN end_char="5061" id="token-44-0" morph="none" pos="word" start_char="5056">Report</TOKEN>
<TOKEN end_char="5068" id="token-44-1" morph="none" pos="word" start_char="5063">author</TOKEN>
<TOKEN end_char="5077" id="token-44-2" morph="none" pos="word" start_char="5070">Benjamin</TOKEN>
<TOKEN end_char="5083" id="token-44-3" morph="none" pos="word" start_char="5079">Rader</TOKEN>
<TOKEN end_char="5088" id="token-44-4" morph="none" pos="word" start_char="5085">said</TOKEN>
<TOKEN end_char="5090" id="token-44-5" morph="none" pos="punct" start_char="5090">"</TOKEN>
<TOKEN end_char="5092" id="token-44-6" morph="none" pos="word" start_char="5091">we</TOKEN>
<TOKEN end_char="5103" id="token-44-7" morph="none" pos="word" start_char="5094">definitely</TOKEN>
<TOKEN end_char="5109" id="token-44-8" morph="none" pos="word" start_char="5105">can't</TOKEN>
<TOKEN end_char="5117" id="token-44-9" morph="none" pos="word" start_char="5111">account</TOKEN>
<TOKEN end_char="5121" id="token-44-10" morph="none" pos="word" start_char="5119">for</TOKEN>
<TOKEN end_char="5133" id="token-44-11" morph="none" pos="word" start_char="5123">underground</TOKEN>
<TOKEN end_char="5141" id="token-44-12" morph="none" pos="word" start_char="5135">parking</TOKEN>
<TOKEN end_char="5144" id="token-44-13" morph="none" pos="word" start_char="5143">in</TOKEN>
<TOKEN end_char="5148" id="token-44-14" morph="none" pos="word" start_char="5146">any</TOKEN>
<TOKEN end_char="5153" id="token-44-15" morph="none" pos="word" start_char="5150">time</TOKEN>
<TOKEN end_char="5160" id="token-44-16" morph="none" pos="word" start_char="5155">period</TOKEN>
<TOKEN end_char="5163" id="token-44-17" morph="none" pos="word" start_char="5162">of</TOKEN>
<TOKEN end_char="5167" id="token-44-18" morph="none" pos="word" start_char="5165">the</TOKEN>
<TOKEN end_char="5173" id="token-44-19" morph="none" pos="word" start_char="5169">study</TOKEN>
<TOKEN end_char="5177" id="token-44-20" morph="none" pos="word" start_char="5175">and</TOKEN>
<TOKEN end_char="5182" id="token-44-21" morph="none" pos="word" start_char="5179">this</TOKEN>
<TOKEN end_char="5185" id="token-44-22" morph="none" pos="word" start_char="5184">is</TOKEN>
<TOKEN end_char="5189" id="token-44-23" morph="none" pos="word" start_char="5187">one</TOKEN>
<TOKEN end_char="5192" id="token-44-24" morph="none" pos="word" start_char="5191">of</TOKEN>
<TOKEN end_char="5196" id="token-44-25" morph="none" pos="word" start_char="5194">the</TOKEN>
<TOKEN end_char="5208" id="token-44-26" morph="none" pos="word" start_char="5198">limitations</TOKEN>
<TOKEN end_char="5211" id="token-44-27" morph="none" pos="word" start_char="5210">of</TOKEN>
<TOKEN end_char="5216" id="token-44-28" morph="none" pos="word" start_char="5213">this</TOKEN>
<TOKEN end_char="5221" id="token-44-29" morph="none" pos="word" start_char="5218">type</TOKEN>
<TOKEN end_char="5224" id="token-44-30" morph="none" pos="word" start_char="5223">of</TOKEN>
<TOKEN end_char="5233" id="token-44-31" morph="none" pos="word" start_char="5226">research</TOKEN>
<TOKEN end_char="5235" id="token-44-32" morph="none" pos="punct" start_char="5234">."</TOKEN>
</SEG>
<SEG end_char="5486" id="segment-45" start_char="5238">
<ORIGINAL_TEXT>The researchers could have compared their data with other Chinese cities to see if the rises in hospital traffic and search queries were specific to Wuhan, where the outbreak first came to light, or whether similar patterns where observed elsewhere.</ORIGINAL_TEXT>
<TOKEN end_char="5240" id="token-45-0" morph="none" pos="word" start_char="5238">The</TOKEN>
<TOKEN end_char="5252" id="token-45-1" morph="none" pos="word" start_char="5242">researchers</TOKEN>
<TOKEN end_char="5258" id="token-45-2" morph="none" pos="word" start_char="5254">could</TOKEN>
<TOKEN end_char="5263" id="token-45-3" morph="none" pos="word" start_char="5260">have</TOKEN>
<TOKEN end_char="5272" id="token-45-4" morph="none" pos="word" start_char="5265">compared</TOKEN>
<TOKEN end_char="5278" id="token-45-5" morph="none" pos="word" start_char="5274">their</TOKEN>
<TOKEN end_char="5283" id="token-45-6" morph="none" pos="word" start_char="5280">data</TOKEN>
<TOKEN end_char="5288" id="token-45-7" morph="none" pos="word" start_char="5285">with</TOKEN>
<TOKEN end_char="5294" id="token-45-8" morph="none" pos="word" start_char="5290">other</TOKEN>
<TOKEN end_char="5302" id="token-45-9" morph="none" pos="word" start_char="5296">Chinese</TOKEN>
<TOKEN end_char="5309" id="token-45-10" morph="none" pos="word" start_char="5304">cities</TOKEN>
<TOKEN end_char="5312" id="token-45-11" morph="none" pos="word" start_char="5311">to</TOKEN>
<TOKEN end_char="5316" id="token-45-12" morph="none" pos="word" start_char="5314">see</TOKEN>
<TOKEN end_char="5319" id="token-45-13" morph="none" pos="word" start_char="5318">if</TOKEN>
<TOKEN end_char="5323" id="token-45-14" morph="none" pos="word" start_char="5321">the</TOKEN>
<TOKEN end_char="5329" id="token-45-15" morph="none" pos="word" start_char="5325">rises</TOKEN>
<TOKEN end_char="5332" id="token-45-16" morph="none" pos="word" start_char="5331">in</TOKEN>
<TOKEN end_char="5341" id="token-45-17" morph="none" pos="word" start_char="5334">hospital</TOKEN>
<TOKEN end_char="5349" id="token-45-18" morph="none" pos="word" start_char="5343">traffic</TOKEN>
<TOKEN end_char="5353" id="token-45-19" morph="none" pos="word" start_char="5351">and</TOKEN>
<TOKEN end_char="5360" id="token-45-20" morph="none" pos="word" start_char="5355">search</TOKEN>
<TOKEN end_char="5368" id="token-45-21" morph="none" pos="word" start_char="5362">queries</TOKEN>
<TOKEN end_char="5373" id="token-45-22" morph="none" pos="word" start_char="5370">were</TOKEN>
<TOKEN end_char="5382" id="token-45-23" morph="none" pos="word" start_char="5375">specific</TOKEN>
<TOKEN end_char="5385" id="token-45-24" morph="none" pos="word" start_char="5384">to</TOKEN>
<TOKEN end_char="5391" id="token-45-25" morph="none" pos="word" start_char="5387">Wuhan</TOKEN>
<TOKEN end_char="5392" id="token-45-26" morph="none" pos="punct" start_char="5392">,</TOKEN>
<TOKEN end_char="5398" id="token-45-27" morph="none" pos="word" start_char="5394">where</TOKEN>
<TOKEN end_char="5402" id="token-45-28" morph="none" pos="word" start_char="5400">the</TOKEN>
<TOKEN end_char="5411" id="token-45-29" morph="none" pos="word" start_char="5404">outbreak</TOKEN>
<TOKEN end_char="5417" id="token-45-30" morph="none" pos="word" start_char="5413">first</TOKEN>
<TOKEN end_char="5422" id="token-45-31" morph="none" pos="word" start_char="5419">came</TOKEN>
<TOKEN end_char="5425" id="token-45-32" morph="none" pos="word" start_char="5424">to</TOKEN>
<TOKEN end_char="5431" id="token-45-33" morph="none" pos="word" start_char="5427">light</TOKEN>
<TOKEN end_char="5432" id="token-45-34" morph="none" pos="punct" start_char="5432">,</TOKEN>
<TOKEN end_char="5435" id="token-45-35" morph="none" pos="word" start_char="5434">or</TOKEN>
<TOKEN end_char="5443" id="token-45-36" morph="none" pos="word" start_char="5437">whether</TOKEN>
<TOKEN end_char="5451" id="token-45-37" morph="none" pos="word" start_char="5445">similar</TOKEN>
<TOKEN end_char="5460" id="token-45-38" morph="none" pos="word" start_char="5453">patterns</TOKEN>
<TOKEN end_char="5466" id="token-45-39" morph="none" pos="word" start_char="5462">where</TOKEN>
<TOKEN end_char="5475" id="token-45-40" morph="none" pos="word" start_char="5468">observed</TOKEN>
<TOKEN end_char="5485" id="token-45-41" morph="none" pos="word" start_char="5477">elsewhere</TOKEN>
<TOKEN end_char="5486" id="token-45-42" morph="none" pos="punct" start_char="5486">.</TOKEN>
</SEG>
<SEG end_char="5644" id="segment-46" start_char="5489">
<ORIGINAL_TEXT>The study findings - that coronavirus may have been present in Wuhan as far back as last August - are for the reasons we've highlighted, highly problematic.</ORIGINAL_TEXT>
<TOKEN end_char="5491" id="token-46-0" morph="none" pos="word" start_char="5489">The</TOKEN>
<TOKEN end_char="5497" id="token-46-1" morph="none" pos="word" start_char="5493">study</TOKEN>
<TOKEN end_char="5506" id="token-46-2" morph="none" pos="word" start_char="5499">findings</TOKEN>
<TOKEN end_char="5508" id="token-46-3" morph="none" pos="punct" start_char="5508">-</TOKEN>
<TOKEN end_char="5513" id="token-46-4" morph="none" pos="word" start_char="5510">that</TOKEN>
<TOKEN end_char="5525" id="token-46-5" morph="none" pos="word" start_char="5515">coronavirus</TOKEN>
<TOKEN end_char="5529" id="token-46-6" morph="none" pos="word" start_char="5527">may</TOKEN>
<TOKEN end_char="5534" id="token-46-7" morph="none" pos="word" start_char="5531">have</TOKEN>
<TOKEN end_char="5539" id="token-46-8" morph="none" pos="word" start_char="5536">been</TOKEN>
<TOKEN end_char="5547" id="token-46-9" morph="none" pos="word" start_char="5541">present</TOKEN>
<TOKEN end_char="5550" id="token-46-10" morph="none" pos="word" start_char="5549">in</TOKEN>
<TOKEN end_char="5556" id="token-46-11" morph="none" pos="word" start_char="5552">Wuhan</TOKEN>
<TOKEN end_char="5559" id="token-46-12" morph="none" pos="word" start_char="5558">as</TOKEN>
<TOKEN end_char="5563" id="token-46-13" morph="none" pos="word" start_char="5561">far</TOKEN>
<TOKEN end_char="5568" id="token-46-14" morph="none" pos="word" start_char="5565">back</TOKEN>
<TOKEN end_char="5571" id="token-46-15" morph="none" pos="word" start_char="5570">as</TOKEN>
<TOKEN end_char="5576" id="token-46-16" morph="none" pos="word" start_char="5573">last</TOKEN>
<TOKEN end_char="5583" id="token-46-17" morph="none" pos="word" start_char="5578">August</TOKEN>
<TOKEN end_char="5585" id="token-46-18" morph="none" pos="punct" start_char="5585">-</TOKEN>
<TOKEN end_char="5589" id="token-46-19" morph="none" pos="word" start_char="5587">are</TOKEN>
<TOKEN end_char="5593" id="token-46-20" morph="none" pos="word" start_char="5591">for</TOKEN>
<TOKEN end_char="5597" id="token-46-21" morph="none" pos="word" start_char="5595">the</TOKEN>
<TOKEN end_char="5605" id="token-46-22" morph="none" pos="word" start_char="5599">reasons</TOKEN>
<TOKEN end_char="5611" id="token-46-23" morph="none" pos="word" start_char="5607">we've</TOKEN>
<TOKEN end_char="5623" id="token-46-24" morph="none" pos="word" start_char="5613">highlighted</TOKEN>
<TOKEN end_char="5624" id="token-46-25" morph="none" pos="punct" start_char="5624">,</TOKEN>
<TOKEN end_char="5631" id="token-46-26" morph="none" pos="word" start_char="5626">highly</TOKEN>
<TOKEN end_char="5643" id="token-46-27" morph="none" pos="word" start_char="5633">problematic</TOKEN>
<TOKEN end_char="5644" id="token-46-28" morph="none" pos="punct" start_char="5644">.</TOKEN>
</SEG>
<SEG end_char="5778" id="segment-47" start_char="5647">
<ORIGINAL_TEXT>There is, however, still much we don't know about the early onset of the virus in China - both when and how the first cases emerged.</ORIGINAL_TEXT>
<TOKEN end_char="5651" id="token-47-0" morph="none" pos="word" start_char="5647">There</TOKEN>
<TOKEN end_char="5654" id="token-47-1" morph="none" pos="word" start_char="5653">is</TOKEN>
<TOKEN end_char="5655" id="token-47-2" morph="none" pos="punct" start_char="5655">,</TOKEN>
<TOKEN end_char="5663" id="token-47-3" morph="none" pos="word" start_char="5657">however</TOKEN>
<TOKEN end_char="5664" id="token-47-4" morph="none" pos="punct" start_char="5664">,</TOKEN>
<TOKEN end_char="5670" id="token-47-5" morph="none" pos="word" start_char="5666">still</TOKEN>
<TOKEN end_char="5675" id="token-47-6" morph="none" pos="word" start_char="5672">much</TOKEN>
<TOKEN end_char="5678" id="token-47-7" morph="none" pos="word" start_char="5677">we</TOKEN>
<TOKEN end_char="5684" id="token-47-8" morph="none" pos="word" start_char="5680">don't</TOKEN>
<TOKEN end_char="5689" id="token-47-9" morph="none" pos="word" start_char="5686">know</TOKEN>
<TOKEN end_char="5695" id="token-47-10" morph="none" pos="word" start_char="5691">about</TOKEN>
<TOKEN end_char="5699" id="token-47-11" morph="none" pos="word" start_char="5697">the</TOKEN>
<TOKEN end_char="5705" id="token-47-12" morph="none" pos="word" start_char="5701">early</TOKEN>
<TOKEN end_char="5711" id="token-47-13" morph="none" pos="word" start_char="5707">onset</TOKEN>
<TOKEN end_char="5714" id="token-47-14" morph="none" pos="word" start_char="5713">of</TOKEN>
<TOKEN end_char="5718" id="token-47-15" morph="none" pos="word" start_char="5716">the</TOKEN>
<TOKEN end_char="5724" id="token-47-16" morph="none" pos="word" start_char="5720">virus</TOKEN>
<TOKEN end_char="5727" id="token-47-17" morph="none" pos="word" start_char="5726">in</TOKEN>
<TOKEN end_char="5733" id="token-47-18" morph="none" pos="word" start_char="5729">China</TOKEN>
<TOKEN end_char="5735" id="token-47-19" morph="none" pos="punct" start_char="5735">-</TOKEN>
<TOKEN end_char="5740" id="token-47-20" morph="none" pos="word" start_char="5737">both</TOKEN>
<TOKEN end_char="5745" id="token-47-21" morph="none" pos="word" start_char="5742">when</TOKEN>
<TOKEN end_char="5749" id="token-47-22" morph="none" pos="word" start_char="5747">and</TOKEN>
<TOKEN end_char="5753" id="token-47-23" morph="none" pos="word" start_char="5751">how</TOKEN>
<TOKEN end_char="5757" id="token-47-24" morph="none" pos="word" start_char="5755">the</TOKEN>
<TOKEN end_char="5763" id="token-47-25" morph="none" pos="word" start_char="5759">first</TOKEN>
<TOKEN end_char="5769" id="token-47-26" morph="none" pos="word" start_char="5765">cases</TOKEN>
<TOKEN end_char="5777" id="token-47-27" morph="none" pos="word" start_char="5771">emerged</TOKEN>
<TOKEN end_char="5778" id="token-47-28" morph="none" pos="punct" start_char="5778">.</TOKEN>
</SEG>
<SEG end_char="5821" id="segment-48" start_char="5782">
<ORIGINAL_TEXT>A SIMPLE GUIDE: How do I protect myself?</ORIGINAL_TEXT>
<TOKEN end_char="5782" id="token-48-0" morph="none" pos="word" start_char="5782">A</TOKEN>
<TOKEN end_char="5789" id="token-48-1" morph="none" pos="word" start_char="5784">SIMPLE</TOKEN>
<TOKEN end_char="5795" id="token-48-2" morph="none" pos="word" start_char="5791">GUIDE</TOKEN>
<TOKEN end_char="5796" id="token-48-3" morph="none" pos="punct" start_char="5796">:</TOKEN>
<TOKEN end_char="5800" id="token-48-4" morph="none" pos="word" start_char="5798">How</TOKEN>
<TOKEN end_char="5803" id="token-48-5" morph="none" pos="word" start_char="5802">do</TOKEN>
<TOKEN end_char="5805" id="token-48-6" morph="none" pos="word" start_char="5805">I</TOKEN>
<TOKEN end_char="5813" id="token-48-7" morph="none" pos="word" start_char="5807">protect</TOKEN>
<TOKEN end_char="5820" id="token-48-8" morph="none" pos="word" start_char="5815">myself</TOKEN>
<TOKEN end_char="5821" id="token-48-9" morph="none" pos="punct" start_char="5821">?</TOKEN>
</SEG>
<SEG end_char="5862" id="segment-49" start_char="5824">
<ORIGINAL_TEXT>IMPACT: What the virus does to the body</ORIGINAL_TEXT>
<TOKEN end_char="5829" id="token-49-0" morph="none" pos="word" start_char="5824">IMPACT</TOKEN>
<TOKEN end_char="5830" id="token-49-1" morph="none" pos="punct" start_char="5830">:</TOKEN>
<TOKEN end_char="5835" id="token-49-2" morph="none" pos="word" start_char="5832">What</TOKEN>
<TOKEN end_char="5839" id="token-49-3" morph="none" pos="word" start_char="5837">the</TOKEN>
<TOKEN end_char="5845" id="token-49-4" morph="none" pos="word" start_char="5841">virus</TOKEN>
<TOKEN end_char="5850" id="token-49-5" morph="none" pos="word" start_char="5847">does</TOKEN>
<TOKEN end_char="5853" id="token-49-6" morph="none" pos="word" start_char="5852">to</TOKEN>
<TOKEN end_char="5857" id="token-49-7" morph="none" pos="word" start_char="5855">the</TOKEN>
<TOKEN end_char="5862" id="token-49-8" morph="none" pos="word" start_char="5859">body</TOKEN>
</SEG>
<SEG end_char="5896" id="segment-50" start_char="5865">
<ORIGINAL_TEXT>RECOVERY: How long does it take?</ORIGINAL_TEXT>
<TOKEN end_char="5872" id="token-50-0" morph="none" pos="word" start_char="5865">RECOVERY</TOKEN>
<TOKEN end_char="5873" id="token-50-1" morph="none" pos="punct" start_char="5873">:</TOKEN>
<TOKEN end_char="5877" id="token-50-2" morph="none" pos="word" start_char="5875">How</TOKEN>
<TOKEN end_char="5882" id="token-50-3" morph="none" pos="word" start_char="5879">long</TOKEN>
<TOKEN end_char="5887" id="token-50-4" morph="none" pos="word" start_char="5884">does</TOKEN>
<TOKEN end_char="5890" id="token-50-5" morph="none" pos="word" start_char="5889">it</TOKEN>
<TOKEN end_char="5895" id="token-50-6" morph="none" pos="word" start_char="5892">take</TOKEN>
<TOKEN end_char="5896" id="token-50-7" morph="none" pos="punct" start_char="5896">?</TOKEN>
</SEG>
<SEG end_char="5937" id="segment-51" start_char="5899">
<ORIGINAL_TEXT>LOCKDOWN: How can we lift restrictions?</ORIGINAL_TEXT>
<TOKEN end_char="5906" id="token-51-0" morph="none" pos="word" start_char="5899">LOCKDOWN</TOKEN>
<TOKEN end_char="5907" id="token-51-1" morph="none" pos="punct" start_char="5907">:</TOKEN>
<TOKEN end_char="5911" id="token-51-2" morph="none" pos="word" start_char="5909">How</TOKEN>
<TOKEN end_char="5915" id="token-51-3" morph="none" pos="word" start_char="5913">can</TOKEN>
<TOKEN end_char="5918" id="token-51-4" morph="none" pos="word" start_char="5917">we</TOKEN>
<TOKEN end_char="5923" id="token-51-5" morph="none" pos="word" start_char="5920">lift</TOKEN>
<TOKEN end_char="5936" id="token-51-6" morph="none" pos="word" start_char="5925">restrictions</TOKEN>
<TOKEN end_char="5937" id="token-51-7" morph="none" pos="punct" start_char="5937">?</TOKEN>
</SEG>
<SEG end_char="5979" id="segment-52" start_char="5940">
<ORIGINAL_TEXT>ENDGAME: How do we get out of this mess?</ORIGINAL_TEXT>
<TOKEN end_char="5946" id="token-52-0" morph="none" pos="word" start_char="5940">ENDGAME</TOKEN>
<TOKEN end_char="5947" id="token-52-1" morph="none" pos="punct" start_char="5947">:</TOKEN>
<TOKEN end_char="5951" id="token-52-2" morph="none" pos="word" start_char="5949">How</TOKEN>
<TOKEN end_char="5954" id="token-52-3" morph="none" pos="word" start_char="5953">do</TOKEN>
<TOKEN end_char="5957" id="token-52-4" morph="none" pos="word" start_char="5956">we</TOKEN>
<TOKEN end_char="5961" id="token-52-5" morph="none" pos="word" start_char="5959">get</TOKEN>
<TOKEN end_char="5965" id="token-52-6" morph="none" pos="word" start_char="5963">out</TOKEN>
<TOKEN end_char="5968" id="token-52-7" morph="none" pos="word" start_char="5967">of</TOKEN>
<TOKEN end_char="5973" id="token-52-8" morph="none" pos="word" start_char="5970">this</TOKEN>
<TOKEN end_char="5978" id="token-52-9" morph="none" pos="word" start_char="5975">mess</TOKEN>
<TOKEN end_char="5979" id="token-52-10" morph="none" pos="punct" start_char="5979">?</TOKEN>
</SEG>
<SEG end_char="6012" id="segment-53" start_char="5985">
<ORIGINAL_TEXT>Read more from Reality Check</ORIGINAL_TEXT>
<TOKEN end_char="5988" id="token-53-0" morph="none" pos="word" start_char="5985">Read</TOKEN>
<TOKEN end_char="5993" id="token-53-1" morph="none" pos="word" start_char="5990">more</TOKEN>
<TOKEN end_char="5998" id="token-53-2" morph="none" pos="word" start_char="5995">from</TOKEN>
<TOKEN end_char="6006" id="token-53-3" morph="none" pos="word" start_char="6000">Reality</TOKEN>
<TOKEN end_char="6012" id="token-53-4" morph="none" pos="word" start_char="6008">Check</TOKEN>
</SEG>
<SEG end_char="6036" id="segment-54" start_char="6015">
<ORIGINAL_TEXT>Send us your questions</ORIGINAL_TEXT>
<TOKEN end_char="6018" id="token-54-0" morph="none" pos="word" start_char="6015">Send</TOKEN>
<TOKEN end_char="6021" id="token-54-1" morph="none" pos="word" start_char="6020">us</TOKEN>
<TOKEN end_char="6026" id="token-54-2" morph="none" pos="word" start_char="6023">your</TOKEN>
<TOKEN end_char="6036" id="token-54-3" morph="none" pos="word" start_char="6028">questions</TOKEN>
</SEG>
<SEG end_char="6058" id="segment-55" start_char="6039">
<ORIGINAL_TEXT>Follow us on Twitter</ORIGINAL_TEXT>
<TOKEN end_char="6044" id="token-55-0" morph="none" pos="word" start_char="6039">Follow</TOKEN>
<TOKEN end_char="6047" id="token-55-1" morph="none" pos="word" start_char="6046">us</TOKEN>
<TOKEN end_char="6050" id="token-55-2" morph="none" pos="word" start_char="6049">on</TOKEN>
<TOKEN end_char="6058" id="token-55-3" morph="none" pos="word" start_char="6052">Twitter</TOKEN>
</SEG>
<SEG end_char="6075" id="segment-56" start_char="6062">
<ORIGINAL_TEXT>Related Topics</ORIGINAL_TEXT>
<TOKEN end_char="6068" id="token-56-0" morph="none" pos="word" start_char="6062">Related</TOKEN>
<TOKEN end_char="6075" id="token-56-1" morph="none" pos="word" start_char="6070">Topics</TOKEN>
</SEG>
<SEG end_char="6091" id="segment-57" start_char="6079">
<ORIGINAL_TEXT>Reality Check</ORIGINAL_TEXT>
<TOKEN end_char="6085" id="token-57-0" morph="none" pos="word" start_char="6079">Reality</TOKEN>
<TOKEN end_char="6091" id="token-57-1" morph="none" pos="word" start_char="6087">Check</TOKEN>
</SEG>
<SEG end_char="6098" id="segment-58" start_char="6094">
<ORIGINAL_TEXT>China</ORIGINAL_TEXT>
<TOKEN end_char="6098" id="token-58-0" morph="none" pos="word" start_char="6094">China</TOKEN>
<TRANSLATED_TEXT>Kiina</TRANSLATED_TEXT><DETECTED_LANGUAGE>sw</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>