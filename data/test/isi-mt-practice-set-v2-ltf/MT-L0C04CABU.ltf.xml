<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CABU" lang="spa" raw_text_char_length="3919" raw_text_md5="09861ad1009b01089d7a5ab266b29e6c" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="88" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Novel coronavirus likely circulated undetected for two months before first case in Wuhan</ORIGINAL_TEXT>
<TOKEN end_char="5" id="token-0-0" morph="none" pos="word" start_char="1">Novel</TOKEN>
<TOKEN end_char="17" id="token-0-1" morph="none" pos="word" start_char="7">coronavirus</TOKEN>
<TOKEN end_char="24" id="token-0-2" morph="none" pos="word" start_char="19">likely</TOKEN>
<TOKEN end_char="35" id="token-0-3" morph="none" pos="word" start_char="26">circulated</TOKEN>
<TOKEN end_char="46" id="token-0-4" morph="none" pos="word" start_char="37">undetected</TOKEN>
<TOKEN end_char="50" id="token-0-5" morph="none" pos="word" start_char="48">for</TOKEN>
<TOKEN end_char="54" id="token-0-6" morph="none" pos="word" start_char="52">two</TOKEN>
<TOKEN end_char="61" id="token-0-7" morph="none" pos="word" start_char="56">months</TOKEN>
<TOKEN end_char="68" id="token-0-8" morph="none" pos="word" start_char="63">before</TOKEN>
<TOKEN end_char="74" id="token-0-9" morph="none" pos="word" start_char="70">first</TOKEN>
<TOKEN end_char="79" id="token-0-10" morph="none" pos="word" start_char="76">case</TOKEN>
<TOKEN end_char="82" id="token-0-11" morph="none" pos="word" start_char="81">in</TOKEN>
<TOKEN end_char="88" id="token-0-12" morph="none" pos="word" start_char="84">Wuhan</TOKEN>
</SEG>
<SEG end_char="112" id="segment-1" start_char="93">
<ORIGINAL_TEXT>MORE FROM THE AUTHOR</ORIGINAL_TEXT>
<TOKEN end_char="96" id="token-1-0" morph="none" pos="word" start_char="93">MORE</TOKEN>
<TOKEN end_char="101" id="token-1-1" morph="none" pos="word" start_char="98">FROM</TOKEN>
<TOKEN end_char="105" id="token-1-2" morph="none" pos="word" start_char="103">THE</TOKEN>
<TOKEN end_char="112" id="token-1-3" morph="none" pos="word" start_char="107">AUTHOR</TOKEN>
</SEG>
<SEG end_char="310" id="segment-2" start_char="116">
<ORIGINAL_TEXT>The novel coronavirus was likely circulating undetected for at most two months before the first human cases of COVID-19 were described in Wuhan, China in late-December 2019, according to a study.</ORIGINAL_TEXT>
<TOKEN end_char="118" id="token-2-0" morph="none" pos="word" start_char="116">The</TOKEN>
<TOKEN end_char="124" id="token-2-1" morph="none" pos="word" start_char="120">novel</TOKEN>
<TOKEN end_char="136" id="token-2-2" morph="none" pos="word" start_char="126">coronavirus</TOKEN>
<TOKEN end_char="140" id="token-2-3" morph="none" pos="word" start_char="138">was</TOKEN>
<TOKEN end_char="147" id="token-2-4" morph="none" pos="word" start_char="142">likely</TOKEN>
<TOKEN end_char="159" id="token-2-5" morph="none" pos="word" start_char="149">circulating</TOKEN>
<TOKEN end_char="170" id="token-2-6" morph="none" pos="word" start_char="161">undetected</TOKEN>
<TOKEN end_char="174" id="token-2-7" morph="none" pos="word" start_char="172">for</TOKEN>
<TOKEN end_char="177" id="token-2-8" morph="none" pos="word" start_char="176">at</TOKEN>
<TOKEN end_char="182" id="token-2-9" morph="none" pos="word" start_char="179">most</TOKEN>
<TOKEN end_char="186" id="token-2-10" morph="none" pos="word" start_char="184">two</TOKEN>
<TOKEN end_char="193" id="token-2-11" morph="none" pos="word" start_char="188">months</TOKEN>
<TOKEN end_char="200" id="token-2-12" morph="none" pos="word" start_char="195">before</TOKEN>
<TOKEN end_char="204" id="token-2-13" morph="none" pos="word" start_char="202">the</TOKEN>
<TOKEN end_char="210" id="token-2-14" morph="none" pos="word" start_char="206">first</TOKEN>
<TOKEN end_char="216" id="token-2-15" morph="none" pos="word" start_char="212">human</TOKEN>
<TOKEN end_char="222" id="token-2-16" morph="none" pos="word" start_char="218">cases</TOKEN>
<TOKEN end_char="225" id="token-2-17" morph="none" pos="word" start_char="224">of</TOKEN>
<TOKEN end_char="234" id="token-2-18" morph="none" pos="unknown" start_char="227">COVID-19</TOKEN>
<TOKEN end_char="239" id="token-2-19" morph="none" pos="word" start_char="236">were</TOKEN>
<TOKEN end_char="249" id="token-2-20" morph="none" pos="word" start_char="241">described</TOKEN>
<TOKEN end_char="252" id="token-2-21" morph="none" pos="word" start_char="251">in</TOKEN>
<TOKEN end_char="258" id="token-2-22" morph="none" pos="word" start_char="254">Wuhan</TOKEN>
<TOKEN end_char="259" id="token-2-23" morph="none" pos="punct" start_char="259">,</TOKEN>
<TOKEN end_char="265" id="token-2-24" morph="none" pos="word" start_char="261">China</TOKEN>
<TOKEN end_char="268" id="token-2-25" morph="none" pos="word" start_char="267">in</TOKEN>
<TOKEN end_char="282" id="token-2-26" morph="none" pos="unknown" start_char="270">late-December</TOKEN>
<TOKEN end_char="287" id="token-2-27" morph="none" pos="word" start_char="284">2019</TOKEN>
<TOKEN end_char="288" id="token-2-28" morph="none" pos="punct" start_char="288">,</TOKEN>
<TOKEN end_char="298" id="token-2-29" morph="none" pos="word" start_char="290">according</TOKEN>
<TOKEN end_char="301" id="token-2-30" morph="none" pos="word" start_char="300">to</TOKEN>
<TOKEN end_char="303" id="token-2-31" morph="none" pos="word" start_char="303">a</TOKEN>
<TOKEN end_char="309" id="token-2-32" morph="none" pos="word" start_char="305">study</TOKEN>
<TOKEN end_char="310" id="token-2-33" morph="none" pos="punct" start_char="310">.</TOKEN>
</SEG>
<SEG end_char="482" id="segment-3" start_char="312">
<ORIGINAL_TEXT>The research, published in the journal Science, used molecular dating tools and epidemiological simulations to date the emergence of the virus to as early as October 2019.</ORIGINAL_TEXT>
<TOKEN end_char="314" id="token-3-0" morph="none" pos="word" start_char="312">The</TOKEN>
<TOKEN end_char="323" id="token-3-1" morph="none" pos="word" start_char="316">research</TOKEN>
<TOKEN end_char="324" id="token-3-2" morph="none" pos="punct" start_char="324">,</TOKEN>
<TOKEN end_char="334" id="token-3-3" morph="none" pos="word" start_char="326">published</TOKEN>
<TOKEN end_char="337" id="token-3-4" morph="none" pos="word" start_char="336">in</TOKEN>
<TOKEN end_char="341" id="token-3-5" morph="none" pos="word" start_char="339">the</TOKEN>
<TOKEN end_char="349" id="token-3-6" morph="none" pos="word" start_char="343">journal</TOKEN>
<TOKEN end_char="357" id="token-3-7" morph="none" pos="word" start_char="351">Science</TOKEN>
<TOKEN end_char="358" id="token-3-8" morph="none" pos="punct" start_char="358">,</TOKEN>
<TOKEN end_char="363" id="token-3-9" morph="none" pos="word" start_char="360">used</TOKEN>
<TOKEN end_char="373" id="token-3-10" morph="none" pos="word" start_char="365">molecular</TOKEN>
<TOKEN end_char="380" id="token-3-11" morph="none" pos="word" start_char="375">dating</TOKEN>
<TOKEN end_char="386" id="token-3-12" morph="none" pos="word" start_char="382">tools</TOKEN>
<TOKEN end_char="390" id="token-3-13" morph="none" pos="word" start_char="388">and</TOKEN>
<TOKEN end_char="406" id="token-3-14" morph="none" pos="word" start_char="392">epidemiological</TOKEN>
<TOKEN end_char="418" id="token-3-15" morph="none" pos="word" start_char="408">simulations</TOKEN>
<TOKEN end_char="421" id="token-3-16" morph="none" pos="word" start_char="420">to</TOKEN>
<TOKEN end_char="426" id="token-3-17" morph="none" pos="word" start_char="423">date</TOKEN>
<TOKEN end_char="430" id="token-3-18" morph="none" pos="word" start_char="428">the</TOKEN>
<TOKEN end_char="440" id="token-3-19" morph="none" pos="word" start_char="432">emergence</TOKEN>
<TOKEN end_char="443" id="token-3-20" morph="none" pos="word" start_char="442">of</TOKEN>
<TOKEN end_char="447" id="token-3-21" morph="none" pos="word" start_char="445">the</TOKEN>
<TOKEN end_char="453" id="token-3-22" morph="none" pos="word" start_char="449">virus</TOKEN>
<TOKEN end_char="456" id="token-3-23" morph="none" pos="word" start_char="455">to</TOKEN>
<TOKEN end_char="459" id="token-3-24" morph="none" pos="word" start_char="458">as</TOKEN>
<TOKEN end_char="465" id="token-3-25" morph="none" pos="word" start_char="461">early</TOKEN>
<TOKEN end_char="468" id="token-3-26" morph="none" pos="word" start_char="467">as</TOKEN>
<TOKEN end_char="476" id="token-3-27" morph="none" pos="word" start_char="470">October</TOKEN>
<TOKEN end_char="481" id="token-3-28" morph="none" pos="word" start_char="478">2019</TOKEN>
<TOKEN end_char="482" id="token-3-29" morph="none" pos="punct" start_char="482">.</TOKEN>
</SEG>
<SEG end_char="711" id="segment-4" start_char="485">
<ORIGINAL_TEXT>The team, including researchers from University of California San Diego in the US, note that their simulations suggest that the mutating virus dies out naturally more than three-quarters of the time without causing an epidemic.</ORIGINAL_TEXT>
<TOKEN end_char="487" id="token-4-0" morph="none" pos="word" start_char="485">The</TOKEN>
<TOKEN end_char="492" id="token-4-1" morph="none" pos="word" start_char="489">team</TOKEN>
<TOKEN end_char="493" id="token-4-2" morph="none" pos="punct" start_char="493">,</TOKEN>
<TOKEN end_char="503" id="token-4-3" morph="none" pos="word" start_char="495">including</TOKEN>
<TOKEN end_char="515" id="token-4-4" morph="none" pos="word" start_char="505">researchers</TOKEN>
<TOKEN end_char="520" id="token-4-5" morph="none" pos="word" start_char="517">from</TOKEN>
<TOKEN end_char="531" id="token-4-6" morph="none" pos="word" start_char="522">University</TOKEN>
<TOKEN end_char="534" id="token-4-7" morph="none" pos="word" start_char="533">of</TOKEN>
<TOKEN end_char="545" id="token-4-8" morph="none" pos="word" start_char="536">California</TOKEN>
<TOKEN end_char="549" id="token-4-9" morph="none" pos="word" start_char="547">San</TOKEN>
<TOKEN end_char="555" id="token-4-10" morph="none" pos="word" start_char="551">Diego</TOKEN>
<TOKEN end_char="558" id="token-4-11" morph="none" pos="word" start_char="557">in</TOKEN>
<TOKEN end_char="562" id="token-4-12" morph="none" pos="word" start_char="560">the</TOKEN>
<TOKEN end_char="565" id="token-4-13" morph="none" pos="word" start_char="564">US</TOKEN>
<TOKEN end_char="566" id="token-4-14" morph="none" pos="punct" start_char="566">,</TOKEN>
<TOKEN end_char="571" id="token-4-15" morph="none" pos="word" start_char="568">note</TOKEN>
<TOKEN end_char="576" id="token-4-16" morph="none" pos="word" start_char="573">that</TOKEN>
<TOKEN end_char="582" id="token-4-17" morph="none" pos="word" start_char="578">their</TOKEN>
<TOKEN end_char="594" id="token-4-18" morph="none" pos="word" start_char="584">simulations</TOKEN>
<TOKEN end_char="602" id="token-4-19" morph="none" pos="word" start_char="596">suggest</TOKEN>
<TOKEN end_char="607" id="token-4-20" morph="none" pos="word" start_char="604">that</TOKEN>
<TOKEN end_char="611" id="token-4-21" morph="none" pos="word" start_char="609">the</TOKEN>
<TOKEN end_char="620" id="token-4-22" morph="none" pos="word" start_char="613">mutating</TOKEN>
<TOKEN end_char="626" id="token-4-23" morph="none" pos="word" start_char="622">virus</TOKEN>
<TOKEN end_char="631" id="token-4-24" morph="none" pos="word" start_char="628">dies</TOKEN>
<TOKEN end_char="635" id="token-4-25" morph="none" pos="word" start_char="633">out</TOKEN>
<TOKEN end_char="645" id="token-4-26" morph="none" pos="word" start_char="637">naturally</TOKEN>
<TOKEN end_char="650" id="token-4-27" morph="none" pos="word" start_char="647">more</TOKEN>
<TOKEN end_char="655" id="token-4-28" morph="none" pos="word" start_char="652">than</TOKEN>
<TOKEN end_char="670" id="token-4-29" morph="none" pos="unknown" start_char="657">three-quarters</TOKEN>
<TOKEN end_char="673" id="token-4-30" morph="none" pos="word" start_char="672">of</TOKEN>
<TOKEN end_char="677" id="token-4-31" morph="none" pos="word" start_char="675">the</TOKEN>
<TOKEN end_char="682" id="token-4-32" morph="none" pos="word" start_char="679">time</TOKEN>
<TOKEN end_char="690" id="token-4-33" morph="none" pos="word" start_char="684">without</TOKEN>
<TOKEN end_char="698" id="token-4-34" morph="none" pos="word" start_char="692">causing</TOKEN>
<TOKEN end_char="701" id="token-4-35" morph="none" pos="word" start_char="700">an</TOKEN>
<TOKEN end_char="710" id="token-4-36" morph="none" pos="word" start_char="703">epidemic</TOKEN>
<TOKEN end_char="711" id="token-4-37" morph="none" pos="punct" start_char="711">.</TOKEN>
</SEG>
<SEG end_char="940" id="segment-5" start_char="713">
<ORIGINAL_TEXT>"Our study was designed to answer the question of how long could SARS-CoV-2 have circulated in China before it was discovered," said senior study author Joel O Wertheim, an associate professor at UC San Diego School of Medicine.</ORIGINAL_TEXT>
<TOKEN end_char="713" id="token-5-0" morph="none" pos="punct" start_char="713">"</TOKEN>
<TOKEN end_char="716" id="token-5-1" morph="none" pos="word" start_char="714">Our</TOKEN>
<TOKEN end_char="722" id="token-5-2" morph="none" pos="word" start_char="718">study</TOKEN>
<TOKEN end_char="726" id="token-5-3" morph="none" pos="word" start_char="724">was</TOKEN>
<TOKEN end_char="735" id="token-5-4" morph="none" pos="word" start_char="728">designed</TOKEN>
<TOKEN end_char="738" id="token-5-5" morph="none" pos="word" start_char="737">to</TOKEN>
<TOKEN end_char="745" id="token-5-6" morph="none" pos="word" start_char="740">answer</TOKEN>
<TOKEN end_char="749" id="token-5-7" morph="none" pos="word" start_char="747">the</TOKEN>
<TOKEN end_char="758" id="token-5-8" morph="none" pos="word" start_char="751">question</TOKEN>
<TOKEN end_char="761" id="token-5-9" morph="none" pos="word" start_char="760">of</TOKEN>
<TOKEN end_char="765" id="token-5-10" morph="none" pos="word" start_char="763">how</TOKEN>
<TOKEN end_char="770" id="token-5-11" morph="none" pos="word" start_char="767">long</TOKEN>
<TOKEN end_char="776" id="token-5-12" morph="none" pos="word" start_char="772">could</TOKEN>
<TOKEN end_char="787" id="token-5-13" morph="none" pos="unknown" start_char="778">SARS-CoV-2</TOKEN>
<TOKEN end_char="792" id="token-5-14" morph="none" pos="word" start_char="789">have</TOKEN>
<TOKEN end_char="803" id="token-5-15" morph="none" pos="word" start_char="794">circulated</TOKEN>
<TOKEN end_char="806" id="token-5-16" morph="none" pos="word" start_char="805">in</TOKEN>
<TOKEN end_char="812" id="token-5-17" morph="none" pos="word" start_char="808">China</TOKEN>
<TOKEN end_char="819" id="token-5-18" morph="none" pos="word" start_char="814">before</TOKEN>
<TOKEN end_char="822" id="token-5-19" morph="none" pos="word" start_char="821">it</TOKEN>
<TOKEN end_char="826" id="token-5-20" morph="none" pos="word" start_char="824">was</TOKEN>
<TOKEN end_char="837" id="token-5-21" morph="none" pos="word" start_char="828">discovered</TOKEN>
<TOKEN end_char="839" id="token-5-22" morph="none" pos="punct" start_char="838">,"</TOKEN>
<TOKEN end_char="844" id="token-5-23" morph="none" pos="word" start_char="841">said</TOKEN>
<TOKEN end_char="851" id="token-5-24" morph="none" pos="word" start_char="846">senior</TOKEN>
<TOKEN end_char="857" id="token-5-25" morph="none" pos="word" start_char="853">study</TOKEN>
<TOKEN end_char="864" id="token-5-26" morph="none" pos="word" start_char="859">author</TOKEN>
<TOKEN end_char="869" id="token-5-27" morph="none" pos="word" start_char="866">Joel</TOKEN>
<TOKEN end_char="871" id="token-5-28" morph="none" pos="word" start_char="871">O</TOKEN>
<TOKEN end_char="880" id="token-5-29" morph="none" pos="word" start_char="873">Wertheim</TOKEN>
<TOKEN end_char="881" id="token-5-30" morph="none" pos="punct" start_char="881">,</TOKEN>
<TOKEN end_char="884" id="token-5-31" morph="none" pos="word" start_char="883">an</TOKEN>
<TOKEN end_char="894" id="token-5-32" morph="none" pos="word" start_char="886">associate</TOKEN>
<TOKEN end_char="904" id="token-5-33" morph="none" pos="word" start_char="896">professor</TOKEN>
<TOKEN end_char="907" id="token-5-34" morph="none" pos="word" start_char="906">at</TOKEN>
<TOKEN end_char="910" id="token-5-35" morph="none" pos="word" start_char="909">UC</TOKEN>
<TOKEN end_char="914" id="token-5-36" morph="none" pos="word" start_char="912">San</TOKEN>
<TOKEN end_char="920" id="token-5-37" morph="none" pos="word" start_char="916">Diego</TOKEN>
<TOKEN end_char="927" id="token-5-38" morph="none" pos="word" start_char="922">School</TOKEN>
<TOKEN end_char="930" id="token-5-39" morph="none" pos="word" start_char="929">of</TOKEN>
<TOKEN end_char="939" id="token-5-40" morph="none" pos="word" start_char="932">Medicine</TOKEN>
<TOKEN end_char="940" id="token-5-41" morph="none" pos="punct" start_char="940">.</TOKEN>
</SEG>
<SEG end_char="1214" id="segment-6" start_char="943">
<ORIGINAL_TEXT>"To answer this question, we combined three important pieces of information: a detailed understanding of how SARS-CoV-2 spread in Wuhan before the lockdown, the genetic diversity of the virus in China and reports of the earliest cases of COVID-19 in China," Wertheim said.</ORIGINAL_TEXT>
<TOKEN end_char="943" id="token-6-0" morph="none" pos="punct" start_char="943">"</TOKEN>
<TOKEN end_char="945" id="token-6-1" morph="none" pos="word" start_char="944">To</TOKEN>
<TOKEN end_char="952" id="token-6-2" morph="none" pos="word" start_char="947">answer</TOKEN>
<TOKEN end_char="957" id="token-6-3" morph="none" pos="word" start_char="954">this</TOKEN>
<TOKEN end_char="966" id="token-6-4" morph="none" pos="word" start_char="959">question</TOKEN>
<TOKEN end_char="967" id="token-6-5" morph="none" pos="punct" start_char="967">,</TOKEN>
<TOKEN end_char="970" id="token-6-6" morph="none" pos="word" start_char="969">we</TOKEN>
<TOKEN end_char="979" id="token-6-7" morph="none" pos="word" start_char="972">combined</TOKEN>
<TOKEN end_char="985" id="token-6-8" morph="none" pos="word" start_char="981">three</TOKEN>
<TOKEN end_char="995" id="token-6-9" morph="none" pos="word" start_char="987">important</TOKEN>
<TOKEN end_char="1002" id="token-6-10" morph="none" pos="word" start_char="997">pieces</TOKEN>
<TOKEN end_char="1005" id="token-6-11" morph="none" pos="word" start_char="1004">of</TOKEN>
<TOKEN end_char="1017" id="token-6-12" morph="none" pos="word" start_char="1007">information</TOKEN>
<TOKEN end_char="1018" id="token-6-13" morph="none" pos="punct" start_char="1018">:</TOKEN>
<TOKEN end_char="1020" id="token-6-14" morph="none" pos="word" start_char="1020">a</TOKEN>
<TOKEN end_char="1029" id="token-6-15" morph="none" pos="word" start_char="1022">detailed</TOKEN>
<TOKEN end_char="1043" id="token-6-16" morph="none" pos="word" start_char="1031">understanding</TOKEN>
<TOKEN end_char="1046" id="token-6-17" morph="none" pos="word" start_char="1045">of</TOKEN>
<TOKEN end_char="1050" id="token-6-18" morph="none" pos="word" start_char="1048">how</TOKEN>
<TOKEN end_char="1061" id="token-6-19" morph="none" pos="unknown" start_char="1052">SARS-CoV-2</TOKEN>
<TOKEN end_char="1068" id="token-6-20" morph="none" pos="word" start_char="1063">spread</TOKEN>
<TOKEN end_char="1071" id="token-6-21" morph="none" pos="word" start_char="1070">in</TOKEN>
<TOKEN end_char="1077" id="token-6-22" morph="none" pos="word" start_char="1073">Wuhan</TOKEN>
<TOKEN end_char="1084" id="token-6-23" morph="none" pos="word" start_char="1079">before</TOKEN>
<TOKEN end_char="1088" id="token-6-24" morph="none" pos="word" start_char="1086">the</TOKEN>
<TOKEN end_char="1097" id="token-6-25" morph="none" pos="word" start_char="1090">lockdown</TOKEN>
<TOKEN end_char="1098" id="token-6-26" morph="none" pos="punct" start_char="1098">,</TOKEN>
<TOKEN end_char="1102" id="token-6-27" morph="none" pos="word" start_char="1100">the</TOKEN>
<TOKEN end_char="1110" id="token-6-28" morph="none" pos="word" start_char="1104">genetic</TOKEN>
<TOKEN end_char="1120" id="token-6-29" morph="none" pos="word" start_char="1112">diversity</TOKEN>
<TOKEN end_char="1123" id="token-6-30" morph="none" pos="word" start_char="1122">of</TOKEN>
<TOKEN end_char="1127" id="token-6-31" morph="none" pos="word" start_char="1125">the</TOKEN>
<TOKEN end_char="1133" id="token-6-32" morph="none" pos="word" start_char="1129">virus</TOKEN>
<TOKEN end_char="1136" id="token-6-33" morph="none" pos="word" start_char="1135">in</TOKEN>
<TOKEN end_char="1142" id="token-6-34" morph="none" pos="word" start_char="1138">China</TOKEN>
<TOKEN end_char="1146" id="token-6-35" morph="none" pos="word" start_char="1144">and</TOKEN>
<TOKEN end_char="1154" id="token-6-36" morph="none" pos="word" start_char="1148">reports</TOKEN>
<TOKEN end_char="1157" id="token-6-37" morph="none" pos="word" start_char="1156">of</TOKEN>
<TOKEN end_char="1161" id="token-6-38" morph="none" pos="word" start_char="1159">the</TOKEN>
<TOKEN end_char="1170" id="token-6-39" morph="none" pos="word" start_char="1163">earliest</TOKEN>
<TOKEN end_char="1176" id="token-6-40" morph="none" pos="word" start_char="1172">cases</TOKEN>
<TOKEN end_char="1179" id="token-6-41" morph="none" pos="word" start_char="1178">of</TOKEN>
<TOKEN end_char="1188" id="token-6-42" morph="none" pos="unknown" start_char="1181">COVID-19</TOKEN>
<TOKEN end_char="1191" id="token-6-43" morph="none" pos="word" start_char="1190">in</TOKEN>
<TOKEN end_char="1197" id="token-6-44" morph="none" pos="word" start_char="1193">China</TOKEN>
<TOKEN end_char="1199" id="token-6-45" morph="none" pos="punct" start_char="1198">,"</TOKEN>
<TOKEN end_char="1208" id="token-6-46" morph="none" pos="word" start_char="1201">Wertheim</TOKEN>
<TOKEN end_char="1213" id="token-6-47" morph="none" pos="word" start_char="1210">said</TOKEN>
<TOKEN end_char="1214" id="token-6-48" morph="none" pos="punct" start_char="1214">.</TOKEN>
</SEG>
<SEG end_char="1390" id="segment-7" start_char="1217">
<ORIGINAL_TEXT>By combining these disparate lines of evidence, the researchers were able to put an upper limit of mid-October 2019 for when SARS-CoV-2 started circulating in Hubei province.</ORIGINAL_TEXT>
<TOKEN end_char="1218" id="token-7-0" morph="none" pos="word" start_char="1217">By</TOKEN>
<TOKEN end_char="1228" id="token-7-1" morph="none" pos="word" start_char="1220">combining</TOKEN>
<TOKEN end_char="1234" id="token-7-2" morph="none" pos="word" start_char="1230">these</TOKEN>
<TOKEN end_char="1244" id="token-7-3" morph="none" pos="word" start_char="1236">disparate</TOKEN>
<TOKEN end_char="1250" id="token-7-4" morph="none" pos="word" start_char="1246">lines</TOKEN>
<TOKEN end_char="1253" id="token-7-5" morph="none" pos="word" start_char="1252">of</TOKEN>
<TOKEN end_char="1262" id="token-7-6" morph="none" pos="word" start_char="1255">evidence</TOKEN>
<TOKEN end_char="1263" id="token-7-7" morph="none" pos="punct" start_char="1263">,</TOKEN>
<TOKEN end_char="1267" id="token-7-8" morph="none" pos="word" start_char="1265">the</TOKEN>
<TOKEN end_char="1279" id="token-7-9" morph="none" pos="word" start_char="1269">researchers</TOKEN>
<TOKEN end_char="1284" id="token-7-10" morph="none" pos="word" start_char="1281">were</TOKEN>
<TOKEN end_char="1289" id="token-7-11" morph="none" pos="word" start_char="1286">able</TOKEN>
<TOKEN end_char="1292" id="token-7-12" morph="none" pos="word" start_char="1291">to</TOKEN>
<TOKEN end_char="1296" id="token-7-13" morph="none" pos="word" start_char="1294">put</TOKEN>
<TOKEN end_char="1299" id="token-7-14" morph="none" pos="word" start_char="1298">an</TOKEN>
<TOKEN end_char="1305" id="token-7-15" morph="none" pos="word" start_char="1301">upper</TOKEN>
<TOKEN end_char="1311" id="token-7-16" morph="none" pos="word" start_char="1307">limit</TOKEN>
<TOKEN end_char="1314" id="token-7-17" morph="none" pos="word" start_char="1313">of</TOKEN>
<TOKEN end_char="1326" id="token-7-18" morph="none" pos="unknown" start_char="1316">mid-October</TOKEN>
<TOKEN end_char="1331" id="token-7-19" morph="none" pos="word" start_char="1328">2019</TOKEN>
<TOKEN end_char="1335" id="token-7-20" morph="none" pos="word" start_char="1333">for</TOKEN>
<TOKEN end_char="1340" id="token-7-21" morph="none" pos="word" start_char="1337">when</TOKEN>
<TOKEN end_char="1351" id="token-7-22" morph="none" pos="unknown" start_char="1342">SARS-CoV-2</TOKEN>
<TOKEN end_char="1359" id="token-7-23" morph="none" pos="word" start_char="1353">started</TOKEN>
<TOKEN end_char="1371" id="token-7-24" morph="none" pos="word" start_char="1361">circulating</TOKEN>
<TOKEN end_char="1374" id="token-7-25" morph="none" pos="word" start_char="1373">in</TOKEN>
<TOKEN end_char="1380" id="token-7-26" morph="none" pos="word" start_char="1376">Hubei</TOKEN>
<TOKEN end_char="1389" id="token-7-27" morph="none" pos="word" start_char="1382">province</TOKEN>
<TOKEN end_char="1390" id="token-7-28" morph="none" pos="punct" start_char="1390">.</TOKEN>
</SEG>
<SEG end_char="1508" id="segment-8" start_char="1392">
<ORIGINAL_TEXT>Cases of COVID-19 were first reported in late-December 2019 in Wuhan, located in the Hubei province of central China.</ORIGINAL_TEXT>
<TOKEN end_char="1396" id="token-8-0" morph="none" pos="word" start_char="1392">Cases</TOKEN>
<TOKEN end_char="1399" id="token-8-1" morph="none" pos="word" start_char="1398">of</TOKEN>
<TOKEN end_char="1408" id="token-8-2" morph="none" pos="unknown" start_char="1401">COVID-19</TOKEN>
<TOKEN end_char="1413" id="token-8-3" morph="none" pos="word" start_char="1410">were</TOKEN>
<TOKEN end_char="1419" id="token-8-4" morph="none" pos="word" start_char="1415">first</TOKEN>
<TOKEN end_char="1428" id="token-8-5" morph="none" pos="word" start_char="1421">reported</TOKEN>
<TOKEN end_char="1431" id="token-8-6" morph="none" pos="word" start_char="1430">in</TOKEN>
<TOKEN end_char="1445" id="token-8-7" morph="none" pos="unknown" start_char="1433">late-December</TOKEN>
<TOKEN end_char="1450" id="token-8-8" morph="none" pos="word" start_char="1447">2019</TOKEN>
<TOKEN end_char="1453" id="token-8-9" morph="none" pos="word" start_char="1452">in</TOKEN>
<TOKEN end_char="1459" id="token-8-10" morph="none" pos="word" start_char="1455">Wuhan</TOKEN>
<TOKEN end_char="1460" id="token-8-11" morph="none" pos="punct" start_char="1460">,</TOKEN>
<TOKEN end_char="1468" id="token-8-12" morph="none" pos="word" start_char="1462">located</TOKEN>
<TOKEN end_char="1471" id="token-8-13" morph="none" pos="word" start_char="1470">in</TOKEN>
<TOKEN end_char="1475" id="token-8-14" morph="none" pos="word" start_char="1473">the</TOKEN>
<TOKEN end_char="1481" id="token-8-15" morph="none" pos="word" start_char="1477">Hubei</TOKEN>
<TOKEN end_char="1490" id="token-8-16" morph="none" pos="word" start_char="1483">province</TOKEN>
<TOKEN end_char="1493" id="token-8-17" morph="none" pos="word" start_char="1492">of</TOKEN>
<TOKEN end_char="1501" id="token-8-18" morph="none" pos="word" start_char="1495">central</TOKEN>
<TOKEN end_char="1507" id="token-8-19" morph="none" pos="word" start_char="1503">China</TOKEN>
<TOKEN end_char="1508" id="token-8-20" morph="none" pos="punct" start_char="1508">.</TOKEN>
</SEG>
<SEG end_char="1645" id="segment-9" start_char="1511">
<ORIGINAL_TEXT>The researchers used molecular clock evolutionary analyses to try to home in on when the first , or index, case of SARS-CoV-2 occurred.</ORIGINAL_TEXT>
<TOKEN end_char="1513" id="token-9-0" morph="none" pos="word" start_char="1511">The</TOKEN>
<TOKEN end_char="1525" id="token-9-1" morph="none" pos="word" start_char="1515">researchers</TOKEN>
<TOKEN end_char="1530" id="token-9-2" morph="none" pos="word" start_char="1527">used</TOKEN>
<TOKEN end_char="1540" id="token-9-3" morph="none" pos="word" start_char="1532">molecular</TOKEN>
<TOKEN end_char="1546" id="token-9-4" morph="none" pos="word" start_char="1542">clock</TOKEN>
<TOKEN end_char="1559" id="token-9-5" morph="none" pos="word" start_char="1548">evolutionary</TOKEN>
<TOKEN end_char="1568" id="token-9-6" morph="none" pos="word" start_char="1561">analyses</TOKEN>
<TOKEN end_char="1571" id="token-9-7" morph="none" pos="word" start_char="1570">to</TOKEN>
<TOKEN end_char="1575" id="token-9-8" morph="none" pos="word" start_char="1573">try</TOKEN>
<TOKEN end_char="1578" id="token-9-9" morph="none" pos="word" start_char="1577">to</TOKEN>
<TOKEN end_char="1583" id="token-9-10" morph="none" pos="word" start_char="1580">home</TOKEN>
<TOKEN end_char="1586" id="token-9-11" morph="none" pos="word" start_char="1585">in</TOKEN>
<TOKEN end_char="1589" id="token-9-12" morph="none" pos="word" start_char="1588">on</TOKEN>
<TOKEN end_char="1594" id="token-9-13" morph="none" pos="word" start_char="1591">when</TOKEN>
<TOKEN end_char="1598" id="token-9-14" morph="none" pos="word" start_char="1596">the</TOKEN>
<TOKEN end_char="1604" id="token-9-15" morph="none" pos="word" start_char="1600">first</TOKEN>
<TOKEN end_char="1606" id="token-9-16" morph="none" pos="punct" start_char="1606">,</TOKEN>
<TOKEN end_char="1609" id="token-9-17" morph="none" pos="word" start_char="1608">or</TOKEN>
<TOKEN end_char="1615" id="token-9-18" morph="none" pos="word" start_char="1611">index</TOKEN>
<TOKEN end_char="1616" id="token-9-19" morph="none" pos="punct" start_char="1616">,</TOKEN>
<TOKEN end_char="1621" id="token-9-20" morph="none" pos="word" start_char="1618">case</TOKEN>
<TOKEN end_char="1624" id="token-9-21" morph="none" pos="word" start_char="1623">of</TOKEN>
<TOKEN end_char="1635" id="token-9-22" morph="none" pos="unknown" start_char="1626">SARS-CoV-2</TOKEN>
<TOKEN end_char="1644" id="token-9-23" morph="none" pos="word" start_char="1637">occurred</TOKEN>
<TOKEN end_char="1645" id="token-9-24" morph="none" pos="punct" start_char="1645">.</TOKEN>
</SEG>
<SEG end_char="1913" id="segment-10" start_char="1648">
<ORIGINAL_TEXT>"Molecular clock" is a term for a technique that uses the mutation rate of genes to deduce when two or more life forms diverged -- in this case, when the common ancestor of all variants of SARS-CoV-2 existed, estimated in this study to as early as mid-November 2019.</ORIGINAL_TEXT>
<TOKEN end_char="1648" id="token-10-0" morph="none" pos="punct" start_char="1648">"</TOKEN>
<TOKEN end_char="1657" id="token-10-1" morph="none" pos="word" start_char="1649">Molecular</TOKEN>
<TOKEN end_char="1663" id="token-10-2" morph="none" pos="word" start_char="1659">clock</TOKEN>
<TOKEN end_char="1664" id="token-10-3" morph="none" pos="punct" start_char="1664">"</TOKEN>
<TOKEN end_char="1667" id="token-10-4" morph="none" pos="word" start_char="1666">is</TOKEN>
<TOKEN end_char="1669" id="token-10-5" morph="none" pos="word" start_char="1669">a</TOKEN>
<TOKEN end_char="1674" id="token-10-6" morph="none" pos="word" start_char="1671">term</TOKEN>
<TOKEN end_char="1678" id="token-10-7" morph="none" pos="word" start_char="1676">for</TOKEN>
<TOKEN end_char="1680" id="token-10-8" morph="none" pos="word" start_char="1680">a</TOKEN>
<TOKEN end_char="1690" id="token-10-9" morph="none" pos="word" start_char="1682">technique</TOKEN>
<TOKEN end_char="1695" id="token-10-10" morph="none" pos="word" start_char="1692">that</TOKEN>
<TOKEN end_char="1700" id="token-10-11" morph="none" pos="word" start_char="1697">uses</TOKEN>
<TOKEN end_char="1704" id="token-10-12" morph="none" pos="word" start_char="1702">the</TOKEN>
<TOKEN end_char="1713" id="token-10-13" morph="none" pos="word" start_char="1706">mutation</TOKEN>
<TOKEN end_char="1718" id="token-10-14" morph="none" pos="word" start_char="1715">rate</TOKEN>
<TOKEN end_char="1721" id="token-10-15" morph="none" pos="word" start_char="1720">of</TOKEN>
<TOKEN end_char="1727" id="token-10-16" morph="none" pos="word" start_char="1723">genes</TOKEN>
<TOKEN end_char="1730" id="token-10-17" morph="none" pos="word" start_char="1729">to</TOKEN>
<TOKEN end_char="1737" id="token-10-18" morph="none" pos="word" start_char="1732">deduce</TOKEN>
<TOKEN end_char="1742" id="token-10-19" morph="none" pos="word" start_char="1739">when</TOKEN>
<TOKEN end_char="1746" id="token-10-20" morph="none" pos="word" start_char="1744">two</TOKEN>
<TOKEN end_char="1749" id="token-10-21" morph="none" pos="word" start_char="1748">or</TOKEN>
<TOKEN end_char="1754" id="token-10-22" morph="none" pos="word" start_char="1751">more</TOKEN>
<TOKEN end_char="1759" id="token-10-23" morph="none" pos="word" start_char="1756">life</TOKEN>
<TOKEN end_char="1765" id="token-10-24" morph="none" pos="word" start_char="1761">forms</TOKEN>
<TOKEN end_char="1774" id="token-10-25" morph="none" pos="word" start_char="1767">diverged</TOKEN>
<TOKEN end_char="1777" id="token-10-26" morph="none" pos="punct" start_char="1776">--</TOKEN>
<TOKEN end_char="1780" id="token-10-27" morph="none" pos="word" start_char="1779">in</TOKEN>
<TOKEN end_char="1785" id="token-10-28" morph="none" pos="word" start_char="1782">this</TOKEN>
<TOKEN end_char="1790" id="token-10-29" morph="none" pos="word" start_char="1787">case</TOKEN>
<TOKEN end_char="1791" id="token-10-30" morph="none" pos="punct" start_char="1791">,</TOKEN>
<TOKEN end_char="1796" id="token-10-31" morph="none" pos="word" start_char="1793">when</TOKEN>
<TOKEN end_char="1800" id="token-10-32" morph="none" pos="word" start_char="1798">the</TOKEN>
<TOKEN end_char="1807" id="token-10-33" morph="none" pos="word" start_char="1802">common</TOKEN>
<TOKEN end_char="1816" id="token-10-34" morph="none" pos="word" start_char="1809">ancestor</TOKEN>
<TOKEN end_char="1819" id="token-10-35" morph="none" pos="word" start_char="1818">of</TOKEN>
<TOKEN end_char="1823" id="token-10-36" morph="none" pos="word" start_char="1821">all</TOKEN>
<TOKEN end_char="1832" id="token-10-37" morph="none" pos="word" start_char="1825">variants</TOKEN>
<TOKEN end_char="1835" id="token-10-38" morph="none" pos="word" start_char="1834">of</TOKEN>
<TOKEN end_char="1846" id="token-10-39" morph="none" pos="unknown" start_char="1837">SARS-CoV-2</TOKEN>
<TOKEN end_char="1854" id="token-10-40" morph="none" pos="word" start_char="1848">existed</TOKEN>
<TOKEN end_char="1855" id="token-10-41" morph="none" pos="punct" start_char="1855">,</TOKEN>
<TOKEN end_char="1865" id="token-10-42" morph="none" pos="word" start_char="1857">estimated</TOKEN>
<TOKEN end_char="1868" id="token-10-43" morph="none" pos="word" start_char="1867">in</TOKEN>
<TOKEN end_char="1873" id="token-10-44" morph="none" pos="word" start_char="1870">this</TOKEN>
<TOKEN end_char="1879" id="token-10-45" morph="none" pos="word" start_char="1875">study</TOKEN>
<TOKEN end_char="1882" id="token-10-46" morph="none" pos="word" start_char="1881">to</TOKEN>
<TOKEN end_char="1885" id="token-10-47" morph="none" pos="word" start_char="1884">as</TOKEN>
<TOKEN end_char="1891" id="token-10-48" morph="none" pos="word" start_char="1887">early</TOKEN>
<TOKEN end_char="1894" id="token-10-49" morph="none" pos="word" start_char="1893">as</TOKEN>
<TOKEN end_char="1907" id="token-10-50" morph="none" pos="unknown" start_char="1896">mid-November</TOKEN>
<TOKEN end_char="1912" id="token-10-51" morph="none" pos="word" start_char="1909">2019</TOKEN>
<TOKEN end_char="1913" id="token-10-52" morph="none" pos="punct" start_char="1913">.</TOKEN>
</SEG>
<SEG end_char="2042" id="segment-11" start_char="1916">
<ORIGINAL_TEXT>Molecular dating of the most recent common ancestor is often taken to be synonymous with the index case of an emerging disease.</ORIGINAL_TEXT>
<TOKEN end_char="1924" id="token-11-0" morph="none" pos="word" start_char="1916">Molecular</TOKEN>
<TOKEN end_char="1931" id="token-11-1" morph="none" pos="word" start_char="1926">dating</TOKEN>
<TOKEN end_char="1934" id="token-11-2" morph="none" pos="word" start_char="1933">of</TOKEN>
<TOKEN end_char="1938" id="token-11-3" morph="none" pos="word" start_char="1936">the</TOKEN>
<TOKEN end_char="1943" id="token-11-4" morph="none" pos="word" start_char="1940">most</TOKEN>
<TOKEN end_char="1950" id="token-11-5" morph="none" pos="word" start_char="1945">recent</TOKEN>
<TOKEN end_char="1957" id="token-11-6" morph="none" pos="word" start_char="1952">common</TOKEN>
<TOKEN end_char="1966" id="token-11-7" morph="none" pos="word" start_char="1959">ancestor</TOKEN>
<TOKEN end_char="1969" id="token-11-8" morph="none" pos="word" start_char="1968">is</TOKEN>
<TOKEN end_char="1975" id="token-11-9" morph="none" pos="word" start_char="1971">often</TOKEN>
<TOKEN end_char="1981" id="token-11-10" morph="none" pos="word" start_char="1977">taken</TOKEN>
<TOKEN end_char="1984" id="token-11-11" morph="none" pos="word" start_char="1983">to</TOKEN>
<TOKEN end_char="1987" id="token-11-12" morph="none" pos="word" start_char="1986">be</TOKEN>
<TOKEN end_char="1998" id="token-11-13" morph="none" pos="word" start_char="1989">synonymous</TOKEN>
<TOKEN end_char="2003" id="token-11-14" morph="none" pos="word" start_char="2000">with</TOKEN>
<TOKEN end_char="2007" id="token-11-15" morph="none" pos="word" start_char="2005">the</TOKEN>
<TOKEN end_char="2013" id="token-11-16" morph="none" pos="word" start_char="2009">index</TOKEN>
<TOKEN end_char="2018" id="token-11-17" morph="none" pos="word" start_char="2015">case</TOKEN>
<TOKEN end_char="2021" id="token-11-18" morph="none" pos="word" start_char="2020">of</TOKEN>
<TOKEN end_char="2024" id="token-11-19" morph="none" pos="word" start_char="2023">an</TOKEN>
<TOKEN end_char="2033" id="token-11-20" morph="none" pos="word" start_char="2026">emerging</TOKEN>
<TOKEN end_char="2041" id="token-11-21" morph="none" pos="word" start_char="2035">disease</TOKEN>
<TOKEN end_char="2042" id="token-11-22" morph="none" pos="punct" start_char="2042">.</TOKEN>
</SEG>
<SEG end_char="2313" id="segment-12" start_char="2044">
<ORIGINAL_TEXT>"The index case can conceivably predate the common ancestor -- the actual first case of this outbreak may have occurred days, weeks or even many months before the estimated common ancestor," said study co-author Michael Worobey, a professor at the University of Arizona.</ORIGINAL_TEXT>
<TOKEN end_char="2044" id="token-12-0" morph="none" pos="punct" start_char="2044">"</TOKEN>
<TOKEN end_char="2047" id="token-12-1" morph="none" pos="word" start_char="2045">The</TOKEN>
<TOKEN end_char="2053" id="token-12-2" morph="none" pos="word" start_char="2049">index</TOKEN>
<TOKEN end_char="2058" id="token-12-3" morph="none" pos="word" start_char="2055">case</TOKEN>
<TOKEN end_char="2062" id="token-12-4" morph="none" pos="word" start_char="2060">can</TOKEN>
<TOKEN end_char="2074" id="token-12-5" morph="none" pos="word" start_char="2064">conceivably</TOKEN>
<TOKEN end_char="2082" id="token-12-6" morph="none" pos="word" start_char="2076">predate</TOKEN>
<TOKEN end_char="2086" id="token-12-7" morph="none" pos="word" start_char="2084">the</TOKEN>
<TOKEN end_char="2093" id="token-12-8" morph="none" pos="word" start_char="2088">common</TOKEN>
<TOKEN end_char="2102" id="token-12-9" morph="none" pos="word" start_char="2095">ancestor</TOKEN>
<TOKEN end_char="2105" id="token-12-10" morph="none" pos="punct" start_char="2104">--</TOKEN>
<TOKEN end_char="2109" id="token-12-11" morph="none" pos="word" start_char="2107">the</TOKEN>
<TOKEN end_char="2116" id="token-12-12" morph="none" pos="word" start_char="2111">actual</TOKEN>
<TOKEN end_char="2122" id="token-12-13" morph="none" pos="word" start_char="2118">first</TOKEN>
<TOKEN end_char="2127" id="token-12-14" morph="none" pos="word" start_char="2124">case</TOKEN>
<TOKEN end_char="2130" id="token-12-15" morph="none" pos="word" start_char="2129">of</TOKEN>
<TOKEN end_char="2135" id="token-12-16" morph="none" pos="word" start_char="2132">this</TOKEN>
<TOKEN end_char="2144" id="token-12-17" morph="none" pos="word" start_char="2137">outbreak</TOKEN>
<TOKEN end_char="2148" id="token-12-18" morph="none" pos="word" start_char="2146">may</TOKEN>
<TOKEN end_char="2153" id="token-12-19" morph="none" pos="word" start_char="2150">have</TOKEN>
<TOKEN end_char="2162" id="token-12-20" morph="none" pos="word" start_char="2155">occurred</TOKEN>
<TOKEN end_char="2167" id="token-12-21" morph="none" pos="word" start_char="2164">days</TOKEN>
<TOKEN end_char="2168" id="token-12-22" morph="none" pos="punct" start_char="2168">,</TOKEN>
<TOKEN end_char="2174" id="token-12-23" morph="none" pos="word" start_char="2170">weeks</TOKEN>
<TOKEN end_char="2177" id="token-12-24" morph="none" pos="word" start_char="2176">or</TOKEN>
<TOKEN end_char="2182" id="token-12-25" morph="none" pos="word" start_char="2179">even</TOKEN>
<TOKEN end_char="2187" id="token-12-26" morph="none" pos="word" start_char="2184">many</TOKEN>
<TOKEN end_char="2194" id="token-12-27" morph="none" pos="word" start_char="2189">months</TOKEN>
<TOKEN end_char="2201" id="token-12-28" morph="none" pos="word" start_char="2196">before</TOKEN>
<TOKEN end_char="2205" id="token-12-29" morph="none" pos="word" start_char="2203">the</TOKEN>
<TOKEN end_char="2215" id="token-12-30" morph="none" pos="word" start_char="2207">estimated</TOKEN>
<TOKEN end_char="2222" id="token-12-31" morph="none" pos="word" start_char="2217">common</TOKEN>
<TOKEN end_char="2231" id="token-12-32" morph="none" pos="word" start_char="2224">ancestor</TOKEN>
<TOKEN end_char="2233" id="token-12-33" morph="none" pos="punct" start_char="2232">,"</TOKEN>
<TOKEN end_char="2238" id="token-12-34" morph="none" pos="word" start_char="2235">said</TOKEN>
<TOKEN end_char="2244" id="token-12-35" morph="none" pos="word" start_char="2240">study</TOKEN>
<TOKEN end_char="2254" id="token-12-36" morph="none" pos="unknown" start_char="2246">co-author</TOKEN>
<TOKEN end_char="2262" id="token-12-37" morph="none" pos="word" start_char="2256">Michael</TOKEN>
<TOKEN end_char="2270" id="token-12-38" morph="none" pos="word" start_char="2264">Worobey</TOKEN>
<TOKEN end_char="2271" id="token-12-39" morph="none" pos="punct" start_char="2271">,</TOKEN>
<TOKEN end_char="2273" id="token-12-40" morph="none" pos="word" start_char="2273">a</TOKEN>
<TOKEN end_char="2283" id="token-12-41" morph="none" pos="word" start_char="2275">professor</TOKEN>
<TOKEN end_char="2286" id="token-12-42" morph="none" pos="word" start_char="2285">at</TOKEN>
<TOKEN end_char="2290" id="token-12-43" morph="none" pos="word" start_char="2288">the</TOKEN>
<TOKEN end_char="2301" id="token-12-44" morph="none" pos="word" start_char="2292">University</TOKEN>
<TOKEN end_char="2304" id="token-12-45" morph="none" pos="word" start_char="2303">of</TOKEN>
<TOKEN end_char="2312" id="token-12-46" morph="none" pos="word" start_char="2306">Arizona</TOKEN>
<TOKEN end_char="2313" id="token-12-47" morph="none" pos="punct" start_char="2313">.</TOKEN>
</SEG>
<SEG end_char="2420" id="segment-13" start_char="2316">
<ORIGINAL_TEXT>"Determining the length of that 'phylogenetic fuse' was at the heart of our investigation," Worobey said.</ORIGINAL_TEXT>
<TOKEN end_char="2316" id="token-13-0" morph="none" pos="punct" start_char="2316">"</TOKEN>
<TOKEN end_char="2327" id="token-13-1" morph="none" pos="word" start_char="2317">Determining</TOKEN>
<TOKEN end_char="2331" id="token-13-2" morph="none" pos="word" start_char="2329">the</TOKEN>
<TOKEN end_char="2338" id="token-13-3" morph="none" pos="word" start_char="2333">length</TOKEN>
<TOKEN end_char="2341" id="token-13-4" morph="none" pos="word" start_char="2340">of</TOKEN>
<TOKEN end_char="2346" id="token-13-5" morph="none" pos="word" start_char="2343">that</TOKEN>
<TOKEN end_char="2348" id="token-13-6" morph="none" pos="punct" start_char="2348">'</TOKEN>
<TOKEN end_char="2360" id="token-13-7" morph="none" pos="word" start_char="2349">phylogenetic</TOKEN>
<TOKEN end_char="2365" id="token-13-8" morph="none" pos="word" start_char="2362">fuse</TOKEN>
<TOKEN end_char="2366" id="token-13-9" morph="none" pos="punct" start_char="2366">'</TOKEN>
<TOKEN end_char="2370" id="token-13-10" morph="none" pos="word" start_char="2368">was</TOKEN>
<TOKEN end_char="2373" id="token-13-11" morph="none" pos="word" start_char="2372">at</TOKEN>
<TOKEN end_char="2377" id="token-13-12" morph="none" pos="word" start_char="2375">the</TOKEN>
<TOKEN end_char="2383" id="token-13-13" morph="none" pos="word" start_char="2379">heart</TOKEN>
<TOKEN end_char="2386" id="token-13-14" morph="none" pos="word" start_char="2385">of</TOKEN>
<TOKEN end_char="2390" id="token-13-15" morph="none" pos="word" start_char="2388">our</TOKEN>
<TOKEN end_char="2404" id="token-13-16" morph="none" pos="word" start_char="2392">investigation</TOKEN>
<TOKEN end_char="2406" id="token-13-17" morph="none" pos="punct" start_char="2405">,"</TOKEN>
<TOKEN end_char="2414" id="token-13-18" morph="none" pos="word" start_char="2408">Worobey</TOKEN>
<TOKEN end_char="2419" id="token-13-19" morph="none" pos="word" start_char="2416">said</TOKEN>
<TOKEN end_char="2420" id="token-13-20" morph="none" pos="punct" start_char="2420">.</TOKEN>
</SEG>
<SEG end_char="2575" id="segment-14" start_char="2422">
<ORIGINAL_TEXT>Based on this work, the researchers estimate that the median number of persons infected with SARS-CoV-2 in China was less than one until November 4, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2426" id="token-14-0" morph="none" pos="word" start_char="2422">Based</TOKEN>
<TOKEN end_char="2429" id="token-14-1" morph="none" pos="word" start_char="2428">on</TOKEN>
<TOKEN end_char="2434" id="token-14-2" morph="none" pos="word" start_char="2431">this</TOKEN>
<TOKEN end_char="2439" id="token-14-3" morph="none" pos="word" start_char="2436">work</TOKEN>
<TOKEN end_char="2440" id="token-14-4" morph="none" pos="punct" start_char="2440">,</TOKEN>
<TOKEN end_char="2444" id="token-14-5" morph="none" pos="word" start_char="2442">the</TOKEN>
<TOKEN end_char="2456" id="token-14-6" morph="none" pos="word" start_char="2446">researchers</TOKEN>
<TOKEN end_char="2465" id="token-14-7" morph="none" pos="word" start_char="2458">estimate</TOKEN>
<TOKEN end_char="2470" id="token-14-8" morph="none" pos="word" start_char="2467">that</TOKEN>
<TOKEN end_char="2474" id="token-14-9" morph="none" pos="word" start_char="2472">the</TOKEN>
<TOKEN end_char="2481" id="token-14-10" morph="none" pos="word" start_char="2476">median</TOKEN>
<TOKEN end_char="2488" id="token-14-11" morph="none" pos="word" start_char="2483">number</TOKEN>
<TOKEN end_char="2491" id="token-14-12" morph="none" pos="word" start_char="2490">of</TOKEN>
<TOKEN end_char="2499" id="token-14-13" morph="none" pos="word" start_char="2493">persons</TOKEN>
<TOKEN end_char="2508" id="token-14-14" morph="none" pos="word" start_char="2501">infected</TOKEN>
<TOKEN end_char="2513" id="token-14-15" morph="none" pos="word" start_char="2510">with</TOKEN>
<TOKEN end_char="2524" id="token-14-16" morph="none" pos="unknown" start_char="2515">SARS-CoV-2</TOKEN>
<TOKEN end_char="2527" id="token-14-17" morph="none" pos="word" start_char="2526">in</TOKEN>
<TOKEN end_char="2533" id="token-14-18" morph="none" pos="word" start_char="2529">China</TOKEN>
<TOKEN end_char="2537" id="token-14-19" morph="none" pos="word" start_char="2535">was</TOKEN>
<TOKEN end_char="2542" id="token-14-20" morph="none" pos="word" start_char="2539">less</TOKEN>
<TOKEN end_char="2547" id="token-14-21" morph="none" pos="word" start_char="2544">than</TOKEN>
<TOKEN end_char="2551" id="token-14-22" morph="none" pos="word" start_char="2549">one</TOKEN>
<TOKEN end_char="2557" id="token-14-23" morph="none" pos="word" start_char="2553">until</TOKEN>
<TOKEN end_char="2566" id="token-14-24" morph="none" pos="word" start_char="2559">November</TOKEN>
<TOKEN end_char="2568" id="token-14-25" morph="none" pos="word" start_char="2568">4</TOKEN>
<TOKEN end_char="2569" id="token-14-26" morph="none" pos="punct" start_char="2569">,</TOKEN>
<TOKEN end_char="2574" id="token-14-27" morph="none" pos="word" start_char="2571">2019</TOKEN>
<TOKEN end_char="2575" id="token-14-28" morph="none" pos="punct" start_char="2575">.</TOKEN>
</SEG>
<SEG end_char="2668" id="segment-15" start_char="2578">
<ORIGINAL_TEXT>Thirteen days later, it was four individuals, and just nine on December 1, 2019, they said.</ORIGINAL_TEXT>
<TOKEN end_char="2585" id="token-15-0" morph="none" pos="word" start_char="2578">Thirteen</TOKEN>
<TOKEN end_char="2590" id="token-15-1" morph="none" pos="word" start_char="2587">days</TOKEN>
<TOKEN end_char="2596" id="token-15-2" morph="none" pos="word" start_char="2592">later</TOKEN>
<TOKEN end_char="2597" id="token-15-3" morph="none" pos="punct" start_char="2597">,</TOKEN>
<TOKEN end_char="2600" id="token-15-4" morph="none" pos="word" start_char="2599">it</TOKEN>
<TOKEN end_char="2604" id="token-15-5" morph="none" pos="word" start_char="2602">was</TOKEN>
<TOKEN end_char="2609" id="token-15-6" morph="none" pos="word" start_char="2606">four</TOKEN>
<TOKEN end_char="2621" id="token-15-7" morph="none" pos="word" start_char="2611">individuals</TOKEN>
<TOKEN end_char="2622" id="token-15-8" morph="none" pos="punct" start_char="2622">,</TOKEN>
<TOKEN end_char="2626" id="token-15-9" morph="none" pos="word" start_char="2624">and</TOKEN>
<TOKEN end_char="2631" id="token-15-10" morph="none" pos="word" start_char="2628">just</TOKEN>
<TOKEN end_char="2636" id="token-15-11" morph="none" pos="word" start_char="2633">nine</TOKEN>
<TOKEN end_char="2639" id="token-15-12" morph="none" pos="word" start_char="2638">on</TOKEN>
<TOKEN end_char="2648" id="token-15-13" morph="none" pos="word" start_char="2641">December</TOKEN>
<TOKEN end_char="2650" id="token-15-14" morph="none" pos="word" start_char="2650">1</TOKEN>
<TOKEN end_char="2651" id="token-15-15" morph="none" pos="punct" start_char="2651">,</TOKEN>
<TOKEN end_char="2656" id="token-15-16" morph="none" pos="word" start_char="2653">2019</TOKEN>
<TOKEN end_char="2657" id="token-15-17" morph="none" pos="punct" start_char="2657">,</TOKEN>
<TOKEN end_char="2662" id="token-15-18" morph="none" pos="word" start_char="2659">they</TOKEN>
<TOKEN end_char="2667" id="token-15-19" morph="none" pos="word" start_char="2664">said</TOKEN>
<TOKEN end_char="2668" id="token-15-20" morph="none" pos="punct" start_char="2668">.</TOKEN>
</SEG>
<SEG end_char="2776" id="segment-16" start_char="2670">
<ORIGINAL_TEXT>The first hospitalisations in Wuhan with a condition later identified as COVID-19 occurred in mid-December.</ORIGINAL_TEXT>
<TOKEN end_char="2672" id="token-16-0" morph="none" pos="word" start_char="2670">The</TOKEN>
<TOKEN end_char="2678" id="token-16-1" morph="none" pos="word" start_char="2674">first</TOKEN>
<TOKEN end_char="2695" id="token-16-2" morph="none" pos="word" start_char="2680">hospitalisations</TOKEN>
<TOKEN end_char="2698" id="token-16-3" morph="none" pos="word" start_char="2697">in</TOKEN>
<TOKEN end_char="2704" id="token-16-4" morph="none" pos="word" start_char="2700">Wuhan</TOKEN>
<TOKEN end_char="2709" id="token-16-5" morph="none" pos="word" start_char="2706">with</TOKEN>
<TOKEN end_char="2711" id="token-16-6" morph="none" pos="word" start_char="2711">a</TOKEN>
<TOKEN end_char="2721" id="token-16-7" morph="none" pos="word" start_char="2713">condition</TOKEN>
<TOKEN end_char="2727" id="token-16-8" morph="none" pos="word" start_char="2723">later</TOKEN>
<TOKEN end_char="2738" id="token-16-9" morph="none" pos="word" start_char="2729">identified</TOKEN>
<TOKEN end_char="2741" id="token-16-10" morph="none" pos="word" start_char="2740">as</TOKEN>
<TOKEN end_char="2750" id="token-16-11" morph="none" pos="unknown" start_char="2743">COVID-19</TOKEN>
<TOKEN end_char="2759" id="token-16-12" morph="none" pos="word" start_char="2752">occurred</TOKEN>
<TOKEN end_char="2762" id="token-16-13" morph="none" pos="word" start_char="2761">in</TOKEN>
<TOKEN end_char="2775" id="token-16-14" morph="none" pos="unknown" start_char="2764">mid-December</TOKEN>
<TOKEN end_char="2776" id="token-16-15" morph="none" pos="punct" start_char="2776">.</TOKEN>
</SEG>
<SEG end_char="2976" id="segment-17" start_char="2778">
<ORIGINAL_TEXT>The researchers used a variety of analytical tools to model how the SARS-CoV-2 virus may have behaved during the initial outbreak and early days of the pandemic when it was largely an unknown entity.</ORIGINAL_TEXT>
<TOKEN end_char="2780" id="token-17-0" morph="none" pos="word" start_char="2778">The</TOKEN>
<TOKEN end_char="2792" id="token-17-1" morph="none" pos="word" start_char="2782">researchers</TOKEN>
<TOKEN end_char="2797" id="token-17-2" morph="none" pos="word" start_char="2794">used</TOKEN>
<TOKEN end_char="2799" id="token-17-3" morph="none" pos="word" start_char="2799">a</TOKEN>
<TOKEN end_char="2807" id="token-17-4" morph="none" pos="word" start_char="2801">variety</TOKEN>
<TOKEN end_char="2810" id="token-17-5" morph="none" pos="word" start_char="2809">of</TOKEN>
<TOKEN end_char="2821" id="token-17-6" morph="none" pos="word" start_char="2812">analytical</TOKEN>
<TOKEN end_char="2827" id="token-17-7" morph="none" pos="word" start_char="2823">tools</TOKEN>
<TOKEN end_char="2830" id="token-17-8" morph="none" pos="word" start_char="2829">to</TOKEN>
<TOKEN end_char="2836" id="token-17-9" morph="none" pos="word" start_char="2832">model</TOKEN>
<TOKEN end_char="2840" id="token-17-10" morph="none" pos="word" start_char="2838">how</TOKEN>
<TOKEN end_char="2844" id="token-17-11" morph="none" pos="word" start_char="2842">the</TOKEN>
<TOKEN end_char="2855" id="token-17-12" morph="none" pos="unknown" start_char="2846">SARS-CoV-2</TOKEN>
<TOKEN end_char="2861" id="token-17-13" morph="none" pos="word" start_char="2857">virus</TOKEN>
<TOKEN end_char="2865" id="token-17-14" morph="none" pos="word" start_char="2863">may</TOKEN>
<TOKEN end_char="2870" id="token-17-15" morph="none" pos="word" start_char="2867">have</TOKEN>
<TOKEN end_char="2878" id="token-17-16" morph="none" pos="word" start_char="2872">behaved</TOKEN>
<TOKEN end_char="2885" id="token-17-17" morph="none" pos="word" start_char="2880">during</TOKEN>
<TOKEN end_char="2889" id="token-17-18" morph="none" pos="word" start_char="2887">the</TOKEN>
<TOKEN end_char="2897" id="token-17-19" morph="none" pos="word" start_char="2891">initial</TOKEN>
<TOKEN end_char="2906" id="token-17-20" morph="none" pos="word" start_char="2899">outbreak</TOKEN>
<TOKEN end_char="2910" id="token-17-21" morph="none" pos="word" start_char="2908">and</TOKEN>
<TOKEN end_char="2916" id="token-17-22" morph="none" pos="word" start_char="2912">early</TOKEN>
<TOKEN end_char="2921" id="token-17-23" morph="none" pos="word" start_char="2918">days</TOKEN>
<TOKEN end_char="2924" id="token-17-24" morph="none" pos="word" start_char="2923">of</TOKEN>
<TOKEN end_char="2928" id="token-17-25" morph="none" pos="word" start_char="2926">the</TOKEN>
<TOKEN end_char="2937" id="token-17-26" morph="none" pos="word" start_char="2930">pandemic</TOKEN>
<TOKEN end_char="2942" id="token-17-27" morph="none" pos="word" start_char="2939">when</TOKEN>
<TOKEN end_char="2945" id="token-17-28" morph="none" pos="word" start_char="2944">it</TOKEN>
<TOKEN end_char="2949" id="token-17-29" morph="none" pos="word" start_char="2947">was</TOKEN>
<TOKEN end_char="2957" id="token-17-30" morph="none" pos="word" start_char="2951">largely</TOKEN>
<TOKEN end_char="2960" id="token-17-31" morph="none" pos="word" start_char="2959">an</TOKEN>
<TOKEN end_char="2968" id="token-17-32" morph="none" pos="word" start_char="2962">unknown</TOKEN>
<TOKEN end_char="2975" id="token-17-33" morph="none" pos="word" start_char="2970">entity</TOKEN>
<TOKEN end_char="2976" id="token-17-34" morph="none" pos="punct" start_char="2976">.</TOKEN>
</SEG>
<SEG end_char="3103" id="segment-18" start_char="2979">
<ORIGINAL_TEXT>These tools included epidemic simulations based on the virus's known biology, such as its transmissibility and other factors.</ORIGINAL_TEXT>
<TOKEN end_char="2983" id="token-18-0" morph="none" pos="word" start_char="2979">These</TOKEN>
<TOKEN end_char="2989" id="token-18-1" morph="none" pos="word" start_char="2985">tools</TOKEN>
<TOKEN end_char="2998" id="token-18-2" morph="none" pos="word" start_char="2991">included</TOKEN>
<TOKEN end_char="3007" id="token-18-3" morph="none" pos="word" start_char="3000">epidemic</TOKEN>
<TOKEN end_char="3019" id="token-18-4" morph="none" pos="word" start_char="3009">simulations</TOKEN>
<TOKEN end_char="3025" id="token-18-5" morph="none" pos="word" start_char="3021">based</TOKEN>
<TOKEN end_char="3028" id="token-18-6" morph="none" pos="word" start_char="3027">on</TOKEN>
<TOKEN end_char="3032" id="token-18-7" morph="none" pos="word" start_char="3030">the</TOKEN>
<TOKEN end_char="3040" id="token-18-8" morph="none" pos="word" start_char="3034">virus's</TOKEN>
<TOKEN end_char="3046" id="token-18-9" morph="none" pos="word" start_char="3042">known</TOKEN>
<TOKEN end_char="3054" id="token-18-10" morph="none" pos="word" start_char="3048">biology</TOKEN>
<TOKEN end_char="3055" id="token-18-11" morph="none" pos="punct" start_char="3055">,</TOKEN>
<TOKEN end_char="3060" id="token-18-12" morph="none" pos="word" start_char="3057">such</TOKEN>
<TOKEN end_char="3063" id="token-18-13" morph="none" pos="word" start_char="3062">as</TOKEN>
<TOKEN end_char="3067" id="token-18-14" morph="none" pos="word" start_char="3065">its</TOKEN>
<TOKEN end_char="3084" id="token-18-15" morph="none" pos="word" start_char="3069">transmissibility</TOKEN>
<TOKEN end_char="3088" id="token-18-16" morph="none" pos="word" start_char="3086">and</TOKEN>
<TOKEN end_char="3094" id="token-18-17" morph="none" pos="word" start_char="3090">other</TOKEN>
<TOKEN end_char="3102" id="token-18-18" morph="none" pos="word" start_char="3096">factors</TOKEN>
<TOKEN end_char="3103" id="token-18-19" morph="none" pos="punct" start_char="3103">.</TOKEN>
</SEG>
<SEG end_char="3202" id="segment-19" start_char="3105">
<ORIGINAL_TEXT>In just 29.7 per cent of these simulations was the virus able to create self-sustaining epidemics.</ORIGINAL_TEXT>
<TOKEN end_char="3106" id="token-19-0" morph="none" pos="word" start_char="3105">In</TOKEN>
<TOKEN end_char="3111" id="token-19-1" morph="none" pos="word" start_char="3108">just</TOKEN>
<TOKEN end_char="3116" id="token-19-2" morph="none" pos="word" start_char="3113">29.7</TOKEN>
<TOKEN end_char="3120" id="token-19-3" morph="none" pos="word" start_char="3118">per</TOKEN>
<TOKEN end_char="3125" id="token-19-4" morph="none" pos="word" start_char="3122">cent</TOKEN>
<TOKEN end_char="3128" id="token-19-5" morph="none" pos="word" start_char="3127">of</TOKEN>
<TOKEN end_char="3134" id="token-19-6" morph="none" pos="word" start_char="3130">these</TOKEN>
<TOKEN end_char="3146" id="token-19-7" morph="none" pos="word" start_char="3136">simulations</TOKEN>
<TOKEN end_char="3150" id="token-19-8" morph="none" pos="word" start_char="3148">was</TOKEN>
<TOKEN end_char="3154" id="token-19-9" morph="none" pos="word" start_char="3152">the</TOKEN>
<TOKEN end_char="3160" id="token-19-10" morph="none" pos="word" start_char="3156">virus</TOKEN>
<TOKEN end_char="3165" id="token-19-11" morph="none" pos="word" start_char="3162">able</TOKEN>
<TOKEN end_char="3168" id="token-19-12" morph="none" pos="word" start_char="3167">to</TOKEN>
<TOKEN end_char="3175" id="token-19-13" morph="none" pos="word" start_char="3170">create</TOKEN>
<TOKEN end_char="3191" id="token-19-14" morph="none" pos="unknown" start_char="3177">self-sustaining</TOKEN>
<TOKEN end_char="3201" id="token-19-15" morph="none" pos="word" start_char="3193">epidemics</TOKEN>
<TOKEN end_char="3202" id="token-19-16" morph="none" pos="punct" start_char="3202">.</TOKEN>
</SEG>
<SEG end_char="3290" id="segment-20" start_char="3204">
<ORIGINAL_TEXT>In the other 70.3 per cent, the virus infected relatively few persons before dying out.</ORIGINAL_TEXT>
<TOKEN end_char="3205" id="token-20-0" morph="none" pos="word" start_char="3204">In</TOKEN>
<TOKEN end_char="3209" id="token-20-1" morph="none" pos="word" start_char="3207">the</TOKEN>
<TOKEN end_char="3215" id="token-20-2" morph="none" pos="word" start_char="3211">other</TOKEN>
<TOKEN end_char="3220" id="token-20-3" morph="none" pos="word" start_char="3217">70.3</TOKEN>
<TOKEN end_char="3224" id="token-20-4" morph="none" pos="word" start_char="3222">per</TOKEN>
<TOKEN end_char="3229" id="token-20-5" morph="none" pos="word" start_char="3226">cent</TOKEN>
<TOKEN end_char="3230" id="token-20-6" morph="none" pos="punct" start_char="3230">,</TOKEN>
<TOKEN end_char="3234" id="token-20-7" morph="none" pos="word" start_char="3232">the</TOKEN>
<TOKEN end_char="3240" id="token-20-8" morph="none" pos="word" start_char="3236">virus</TOKEN>
<TOKEN end_char="3249" id="token-20-9" morph="none" pos="word" start_char="3242">infected</TOKEN>
<TOKEN end_char="3260" id="token-20-10" morph="none" pos="word" start_char="3251">relatively</TOKEN>
<TOKEN end_char="3264" id="token-20-11" morph="none" pos="word" start_char="3262">few</TOKEN>
<TOKEN end_char="3272" id="token-20-12" morph="none" pos="word" start_char="3266">persons</TOKEN>
<TOKEN end_char="3279" id="token-20-13" morph="none" pos="word" start_char="3274">before</TOKEN>
<TOKEN end_char="3285" id="token-20-14" morph="none" pos="word" start_char="3281">dying</TOKEN>
<TOKEN end_char="3289" id="token-20-15" morph="none" pos="word" start_char="3287">out</TOKEN>
<TOKEN end_char="3290" id="token-20-16" morph="none" pos="punct" start_char="3290">.</TOKEN>
</SEG>
<SEG end_char="3362" id="segment-21" start_char="3292">
<ORIGINAL_TEXT>The average failed epidemic ended just eight days after the index case.</ORIGINAL_TEXT>
<TOKEN end_char="3294" id="token-21-0" morph="none" pos="word" start_char="3292">The</TOKEN>
<TOKEN end_char="3302" id="token-21-1" morph="none" pos="word" start_char="3296">average</TOKEN>
<TOKEN end_char="3309" id="token-21-2" morph="none" pos="word" start_char="3304">failed</TOKEN>
<TOKEN end_char="3318" id="token-21-3" morph="none" pos="word" start_char="3311">epidemic</TOKEN>
<TOKEN end_char="3324" id="token-21-4" morph="none" pos="word" start_char="3320">ended</TOKEN>
<TOKEN end_char="3329" id="token-21-5" morph="none" pos="word" start_char="3326">just</TOKEN>
<TOKEN end_char="3335" id="token-21-6" morph="none" pos="word" start_char="3331">eight</TOKEN>
<TOKEN end_char="3340" id="token-21-7" morph="none" pos="word" start_char="3337">days</TOKEN>
<TOKEN end_char="3346" id="token-21-8" morph="none" pos="word" start_char="3342">after</TOKEN>
<TOKEN end_char="3350" id="token-21-9" morph="none" pos="word" start_char="3348">the</TOKEN>
<TOKEN end_char="3356" id="token-21-10" morph="none" pos="word" start_char="3352">index</TOKEN>
<TOKEN end_char="3361" id="token-21-11" morph="none" pos="word" start_char="3358">case</TOKEN>
<TOKEN end_char="3362" id="token-21-12" morph="none" pos="punct" start_char="3362">.</TOKEN>
</SEG>
<SEG end_char="3448" id="segment-22" start_char="3365">
<ORIGINAL_TEXT>"We saw that over two-thirds of the epidemics we attempted to simulate went extinct.</ORIGINAL_TEXT>
<TOKEN end_char="3365" id="token-22-0" morph="none" pos="punct" start_char="3365">"</TOKEN>
<TOKEN end_char="3367" id="token-22-1" morph="none" pos="word" start_char="3366">We</TOKEN>
<TOKEN end_char="3371" id="token-22-2" morph="none" pos="word" start_char="3369">saw</TOKEN>
<TOKEN end_char="3376" id="token-22-3" morph="none" pos="word" start_char="3373">that</TOKEN>
<TOKEN end_char="3381" id="token-22-4" morph="none" pos="word" start_char="3378">over</TOKEN>
<TOKEN end_char="3392" id="token-22-5" morph="none" pos="unknown" start_char="3383">two-thirds</TOKEN>
<TOKEN end_char="3395" id="token-22-6" morph="none" pos="word" start_char="3394">of</TOKEN>
<TOKEN end_char="3399" id="token-22-7" morph="none" pos="word" start_char="3397">the</TOKEN>
<TOKEN end_char="3409" id="token-22-8" morph="none" pos="word" start_char="3401">epidemics</TOKEN>
<TOKEN end_char="3412" id="token-22-9" morph="none" pos="word" start_char="3411">we</TOKEN>
<TOKEN end_char="3422" id="token-22-10" morph="none" pos="word" start_char="3414">attempted</TOKEN>
<TOKEN end_char="3425" id="token-22-11" morph="none" pos="word" start_char="3424">to</TOKEN>
<TOKEN end_char="3434" id="token-22-12" morph="none" pos="word" start_char="3427">simulate</TOKEN>
<TOKEN end_char="3439" id="token-22-13" morph="none" pos="word" start_char="3436">went</TOKEN>
<TOKEN end_char="3447" id="token-22-14" morph="none" pos="word" start_char="3441">extinct</TOKEN>
<TOKEN end_char="3448" id="token-22-15" morph="none" pos="punct" start_char="3448">.</TOKEN>
</SEG>
<SEG end_char="3639" id="segment-23" start_char="3450">
<ORIGINAL_TEXT>That means that if we could go back in time and repeat 2019 one hundred times, two out of three times, COVID-19 would have fizzled out on its own without igniting a pandemic, Wertheim noted.</ORIGINAL_TEXT>
<TOKEN end_char="3453" id="token-23-0" morph="none" pos="word" start_char="3450">That</TOKEN>
<TOKEN end_char="3459" id="token-23-1" morph="none" pos="word" start_char="3455">means</TOKEN>
<TOKEN end_char="3464" id="token-23-2" morph="none" pos="word" start_char="3461">that</TOKEN>
<TOKEN end_char="3467" id="token-23-3" morph="none" pos="word" start_char="3466">if</TOKEN>
<TOKEN end_char="3470" id="token-23-4" morph="none" pos="word" start_char="3469">we</TOKEN>
<TOKEN end_char="3476" id="token-23-5" morph="none" pos="word" start_char="3472">could</TOKEN>
<TOKEN end_char="3479" id="token-23-6" morph="none" pos="word" start_char="3478">go</TOKEN>
<TOKEN end_char="3484" id="token-23-7" morph="none" pos="word" start_char="3481">back</TOKEN>
<TOKEN end_char="3487" id="token-23-8" morph="none" pos="word" start_char="3486">in</TOKEN>
<TOKEN end_char="3492" id="token-23-9" morph="none" pos="word" start_char="3489">time</TOKEN>
<TOKEN end_char="3496" id="token-23-10" morph="none" pos="word" start_char="3494">and</TOKEN>
<TOKEN end_char="3503" id="token-23-11" morph="none" pos="word" start_char="3498">repeat</TOKEN>
<TOKEN end_char="3508" id="token-23-12" morph="none" pos="word" start_char="3505">2019</TOKEN>
<TOKEN end_char="3512" id="token-23-13" morph="none" pos="word" start_char="3510">one</TOKEN>
<TOKEN end_char="3520" id="token-23-14" morph="none" pos="word" start_char="3514">hundred</TOKEN>
<TOKEN end_char="3526" id="token-23-15" morph="none" pos="word" start_char="3522">times</TOKEN>
<TOKEN end_char="3527" id="token-23-16" morph="none" pos="punct" start_char="3527">,</TOKEN>
<TOKEN end_char="3531" id="token-23-17" morph="none" pos="word" start_char="3529">two</TOKEN>
<TOKEN end_char="3535" id="token-23-18" morph="none" pos="word" start_char="3533">out</TOKEN>
<TOKEN end_char="3538" id="token-23-19" morph="none" pos="word" start_char="3537">of</TOKEN>
<TOKEN end_char="3544" id="token-23-20" morph="none" pos="word" start_char="3540">three</TOKEN>
<TOKEN end_char="3550" id="token-23-21" morph="none" pos="word" start_char="3546">times</TOKEN>
<TOKEN end_char="3551" id="token-23-22" morph="none" pos="punct" start_char="3551">,</TOKEN>
<TOKEN end_char="3560" id="token-23-23" morph="none" pos="unknown" start_char="3553">COVID-19</TOKEN>
<TOKEN end_char="3566" id="token-23-24" morph="none" pos="word" start_char="3562">would</TOKEN>
<TOKEN end_char="3571" id="token-23-25" morph="none" pos="word" start_char="3568">have</TOKEN>
<TOKEN end_char="3579" id="token-23-26" morph="none" pos="word" start_char="3573">fizzled</TOKEN>
<TOKEN end_char="3583" id="token-23-27" morph="none" pos="word" start_char="3581">out</TOKEN>
<TOKEN end_char="3586" id="token-23-28" morph="none" pos="word" start_char="3585">on</TOKEN>
<TOKEN end_char="3590" id="token-23-29" morph="none" pos="word" start_char="3588">its</TOKEN>
<TOKEN end_char="3594" id="token-23-30" morph="none" pos="word" start_char="3592">own</TOKEN>
<TOKEN end_char="3602" id="token-23-31" morph="none" pos="word" start_char="3596">without</TOKEN>
<TOKEN end_char="3611" id="token-23-32" morph="none" pos="word" start_char="3604">igniting</TOKEN>
<TOKEN end_char="3613" id="token-23-33" morph="none" pos="word" start_char="3613">a</TOKEN>
<TOKEN end_char="3622" id="token-23-34" morph="none" pos="word" start_char="3615">pandemic</TOKEN>
<TOKEN end_char="3623" id="token-23-35" morph="none" pos="punct" start_char="3623">,</TOKEN>
<TOKEN end_char="3632" id="token-23-36" morph="none" pos="word" start_char="3625">Wertheim</TOKEN>
<TOKEN end_char="3638" id="token-23-37" morph="none" pos="word" start_char="3634">noted</TOKEN>
<TOKEN end_char="3639" id="token-23-38" morph="none" pos="punct" start_char="3639">.</TOKEN>
</SEG>
<SEG end_char="3753" id="segment-24" start_char="3642">
<ORIGINAL_TEXT>"This finding supports the notion that humans are constantly being bombarded with zoonotic pathogens," he added.</ORIGINAL_TEXT>
<TOKEN end_char="3642" id="token-24-0" morph="none" pos="punct" start_char="3642">"</TOKEN>
<TOKEN end_char="3646" id="token-24-1" morph="none" pos="word" start_char="3643">This</TOKEN>
<TOKEN end_char="3654" id="token-24-2" morph="none" pos="word" start_char="3648">finding</TOKEN>
<TOKEN end_char="3663" id="token-24-3" morph="none" pos="word" start_char="3656">supports</TOKEN>
<TOKEN end_char="3667" id="token-24-4" morph="none" pos="word" start_char="3665">the</TOKEN>
<TOKEN end_char="3674" id="token-24-5" morph="none" pos="word" start_char="3669">notion</TOKEN>
<TOKEN end_char="3679" id="token-24-6" morph="none" pos="word" start_char="3676">that</TOKEN>
<TOKEN end_char="3686" id="token-24-7" morph="none" pos="word" start_char="3681">humans</TOKEN>
<TOKEN end_char="3690" id="token-24-8" morph="none" pos="word" start_char="3688">are</TOKEN>
<TOKEN end_char="3701" id="token-24-9" morph="none" pos="word" start_char="3692">constantly</TOKEN>
<TOKEN end_char="3707" id="token-24-10" morph="none" pos="word" start_char="3703">being</TOKEN>
<TOKEN end_char="3717" id="token-24-11" morph="none" pos="word" start_char="3709">bombarded</TOKEN>
<TOKEN end_char="3722" id="token-24-12" morph="none" pos="word" start_char="3719">with</TOKEN>
<TOKEN end_char="3731" id="token-24-13" morph="none" pos="word" start_char="3724">zoonotic</TOKEN>
<TOKEN end_char="3741" id="token-24-14" morph="none" pos="word" start_char="3733">pathogens</TOKEN>
<TOKEN end_char="3743" id="token-24-15" morph="none" pos="punct" start_char="3742">,"</TOKEN>
<TOKEN end_char="3746" id="token-24-16" morph="none" pos="word" start_char="3745">he</TOKEN>
<TOKEN end_char="3752" id="token-24-17" morph="none" pos="word" start_char="3748">added</TOKEN>
<TOKEN end_char="3753" id="token-24-18" morph="none" pos="punct" start_char="3753">.</TOKEN>
</SEG>
<SEG end_char="3833" id="segment-25" start_char="3756">
<ORIGINAL_TEXT>Also read: In a first, woman delivers baby with antibodies against coronavirus</ORIGINAL_TEXT>
<TOKEN end_char="3759" id="token-25-0" morph="none" pos="word" start_char="3756">Also</TOKEN>
<TOKEN end_char="3764" id="token-25-1" morph="none" pos="word" start_char="3761">read</TOKEN>
<TOKEN end_char="3765" id="token-25-2" morph="none" pos="punct" start_char="3765">:</TOKEN>
<TOKEN end_char="3768" id="token-25-3" morph="none" pos="word" start_char="3767">In</TOKEN>
<TOKEN end_char="3770" id="token-25-4" morph="none" pos="word" start_char="3770">a</TOKEN>
<TOKEN end_char="3776" id="token-25-5" morph="none" pos="word" start_char="3772">first</TOKEN>
<TOKEN end_char="3777" id="token-25-6" morph="none" pos="punct" start_char="3777">,</TOKEN>
<TOKEN end_char="3783" id="token-25-7" morph="none" pos="word" start_char="3779">woman</TOKEN>
<TOKEN end_char="3792" id="token-25-8" morph="none" pos="word" start_char="3785">delivers</TOKEN>
<TOKEN end_char="3797" id="token-25-9" morph="none" pos="word" start_char="3794">baby</TOKEN>
<TOKEN end_char="3802" id="token-25-10" morph="none" pos="word" start_char="3799">with</TOKEN>
<TOKEN end_char="3813" id="token-25-11" morph="none" pos="word" start_char="3804">antibodies</TOKEN>
<TOKEN end_char="3821" id="token-25-12" morph="none" pos="word" start_char="3815">against</TOKEN>
<TOKEN end_char="3833" id="token-25-13" morph="none" pos="word" start_char="3823">coronavirus</TOKEN>
</SEG>
<SEG end_char="3915" id="segment-26" start_char="3836">
<ORIGINAL_TEXT>Also read: IndiGo, AirAsia crack the whip against flyers flouting COVID-19 norms</ORIGINAL_TEXT>
<TOKEN end_char="3839" id="token-26-0" morph="none" pos="word" start_char="3836">Also</TOKEN>
<TOKEN end_char="3844" id="token-26-1" morph="none" pos="word" start_char="3841">read</TOKEN>
<TOKEN end_char="3845" id="token-26-2" morph="none" pos="punct" start_char="3845">:</TOKEN>
<TOKEN end_char="3852" id="token-26-3" morph="none" pos="word" start_char="3847">IndiGo</TOKEN>
<TOKEN end_char="3853" id="token-26-4" morph="none" pos="punct" start_char="3853">,</TOKEN>
<TOKEN end_char="3861" id="token-26-5" morph="none" pos="word" start_char="3855">AirAsia</TOKEN>
<TOKEN end_char="3867" id="token-26-6" morph="none" pos="word" start_char="3863">crack</TOKEN>
<TOKEN end_char="3871" id="token-26-7" morph="none" pos="word" start_char="3869">the</TOKEN>
<TOKEN end_char="3876" id="token-26-8" morph="none" pos="word" start_char="3873">whip</TOKEN>
<TOKEN end_char="3884" id="token-26-9" morph="none" pos="word" start_char="3878">against</TOKEN>
<TOKEN end_char="3891" id="token-26-10" morph="none" pos="word" start_char="3886">flyers</TOKEN>
<TOKEN end_char="3900" id="token-26-11" morph="none" pos="word" start_char="3893">flouting</TOKEN>
<TOKEN end_char="3909" id="token-26-12" morph="none" pos="unknown" start_char="3902">COVID-19</TOKEN>
<TOKEN end_char="3915" id="token-26-13" morph="none" pos="word" start_char="3911">norms</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>