<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C04CA24" lang="eng" raw_text_char_length="3767" raw_text_md5="51c43544e4955195927281d58573a5e0" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="92" id="segment-0" start_char="1">
<ORIGINAL_TEXT>China, scientists dismiss Harvard study suggesting COVID-19 was spreading in Wuhan in August</ORIGINAL_TEXT>
<TOKEN end_char="5" id="token-0-0" morph="none" pos="word" start_char="1">China</TOKEN>
<TOKEN end_char="6" id="token-0-1" morph="none" pos="punct" start_char="6">,</TOKEN>
<TOKEN end_char="17" id="token-0-2" morph="none" pos="word" start_char="8">scientists</TOKEN>
<TOKEN end_char="25" id="token-0-3" morph="none" pos="word" start_char="19">dismiss</TOKEN>
<TOKEN end_char="33" id="token-0-4" morph="none" pos="word" start_char="27">Harvard</TOKEN>
<TOKEN end_char="39" id="token-0-5" morph="none" pos="word" start_char="35">study</TOKEN>
<TOKEN end_char="50" id="token-0-6" morph="none" pos="word" start_char="41">suggesting</TOKEN>
<TOKEN end_char="59" id="token-0-7" morph="none" pos="unknown" start_char="52">COVID-19</TOKEN>
<TOKEN end_char="63" id="token-0-8" morph="none" pos="word" start_char="61">was</TOKEN>
<TOKEN end_char="73" id="token-0-9" morph="none" pos="word" start_char="65">spreading</TOKEN>
<TOKEN end_char="76" id="token-0-10" morph="none" pos="word" start_char="75">in</TOKEN>
<TOKEN end_char="82" id="token-0-11" morph="none" pos="word" start_char="78">Wuhan</TOKEN>
<TOKEN end_char="85" id="token-0-12" morph="none" pos="word" start_char="84">in</TOKEN>
<TOKEN end_char="92" id="token-0-13" morph="none" pos="word" start_char="87">August</TOKEN>
</SEG>
<SEG end_char="392" id="segment-1" start_char="96">
<ORIGINAL_TEXT>LONDON (Reuters) - Beijing dismissed as "ridiculous" a Harvard Medical School study of hospital traffic and search engine data that suggested the new coronavirus may already have been spreading in China last August, and scientists said it offered no convincing evidence of when the outbreak began.</ORIGINAL_TEXT>
<TOKEN end_char="101" id="token-1-0" morph="none" pos="word" start_char="96">LONDON</TOKEN>
<TOKEN end_char="103" id="token-1-1" morph="none" pos="punct" start_char="103">(</TOKEN>
<TOKEN end_char="110" id="token-1-2" morph="none" pos="word" start_char="104">Reuters</TOKEN>
<TOKEN end_char="111" id="token-1-3" morph="none" pos="punct" start_char="111">)</TOKEN>
<TOKEN end_char="113" id="token-1-4" morph="none" pos="punct" start_char="113">-</TOKEN>
<TOKEN end_char="121" id="token-1-5" morph="none" pos="word" start_char="115">Beijing</TOKEN>
<TOKEN end_char="131" id="token-1-6" morph="none" pos="word" start_char="123">dismissed</TOKEN>
<TOKEN end_char="134" id="token-1-7" morph="none" pos="word" start_char="133">as</TOKEN>
<TOKEN end_char="136" id="token-1-8" morph="none" pos="punct" start_char="136">"</TOKEN>
<TOKEN end_char="146" id="token-1-9" morph="none" pos="word" start_char="137">ridiculous</TOKEN>
<TOKEN end_char="147" id="token-1-10" morph="none" pos="punct" start_char="147">"</TOKEN>
<TOKEN end_char="149" id="token-1-11" morph="none" pos="word" start_char="149">a</TOKEN>
<TOKEN end_char="157" id="token-1-12" morph="none" pos="word" start_char="151">Harvard</TOKEN>
<TOKEN end_char="165" id="token-1-13" morph="none" pos="word" start_char="159">Medical</TOKEN>
<TOKEN end_char="172" id="token-1-14" morph="none" pos="word" start_char="167">School</TOKEN>
<TOKEN end_char="178" id="token-1-15" morph="none" pos="word" start_char="174">study</TOKEN>
<TOKEN end_char="181" id="token-1-16" morph="none" pos="word" start_char="180">of</TOKEN>
<TOKEN end_char="190" id="token-1-17" morph="none" pos="word" start_char="183">hospital</TOKEN>
<TOKEN end_char="198" id="token-1-18" morph="none" pos="word" start_char="192">traffic</TOKEN>
<TOKEN end_char="202" id="token-1-19" morph="none" pos="word" start_char="200">and</TOKEN>
<TOKEN end_char="209" id="token-1-20" morph="none" pos="word" start_char="204">search</TOKEN>
<TOKEN end_char="216" id="token-1-21" morph="none" pos="word" start_char="211">engine</TOKEN>
<TOKEN end_char="221" id="token-1-22" morph="none" pos="word" start_char="218">data</TOKEN>
<TOKEN end_char="226" id="token-1-23" morph="none" pos="word" start_char="223">that</TOKEN>
<TOKEN end_char="236" id="token-1-24" morph="none" pos="word" start_char="228">suggested</TOKEN>
<TOKEN end_char="240" id="token-1-25" morph="none" pos="word" start_char="238">the</TOKEN>
<TOKEN end_char="244" id="token-1-26" morph="none" pos="word" start_char="242">new</TOKEN>
<TOKEN end_char="256" id="token-1-27" morph="none" pos="word" start_char="246">coronavirus</TOKEN>
<TOKEN end_char="260" id="token-1-28" morph="none" pos="word" start_char="258">may</TOKEN>
<TOKEN end_char="268" id="token-1-29" morph="none" pos="word" start_char="262">already</TOKEN>
<TOKEN end_char="273" id="token-1-30" morph="none" pos="word" start_char="270">have</TOKEN>
<TOKEN end_char="278" id="token-1-31" morph="none" pos="word" start_char="275">been</TOKEN>
<TOKEN end_char="288" id="token-1-32" morph="none" pos="word" start_char="280">spreading</TOKEN>
<TOKEN end_char="291" id="token-1-33" morph="none" pos="word" start_char="290">in</TOKEN>
<TOKEN end_char="297" id="token-1-34" morph="none" pos="word" start_char="293">China</TOKEN>
<TOKEN end_char="302" id="token-1-35" morph="none" pos="word" start_char="299">last</TOKEN>
<TOKEN end_char="309" id="token-1-36" morph="none" pos="word" start_char="304">August</TOKEN>
<TOKEN end_char="310" id="token-1-37" morph="none" pos="punct" start_char="310">,</TOKEN>
<TOKEN end_char="314" id="token-1-38" morph="none" pos="word" start_char="312">and</TOKEN>
<TOKEN end_char="325" id="token-1-39" morph="none" pos="word" start_char="316">scientists</TOKEN>
<TOKEN end_char="330" id="token-1-40" morph="none" pos="word" start_char="327">said</TOKEN>
<TOKEN end_char="333" id="token-1-41" morph="none" pos="word" start_char="332">it</TOKEN>
<TOKEN end_char="341" id="token-1-42" morph="none" pos="word" start_char="335">offered</TOKEN>
<TOKEN end_char="344" id="token-1-43" morph="none" pos="word" start_char="343">no</TOKEN>
<TOKEN end_char="355" id="token-1-44" morph="none" pos="word" start_char="346">convincing</TOKEN>
<TOKEN end_char="364" id="token-1-45" morph="none" pos="word" start_char="357">evidence</TOKEN>
<TOKEN end_char="367" id="token-1-46" morph="none" pos="word" start_char="366">of</TOKEN>
<TOKEN end_char="372" id="token-1-47" morph="none" pos="word" start_char="369">when</TOKEN>
<TOKEN end_char="376" id="token-1-48" morph="none" pos="word" start_char="374">the</TOKEN>
<TOKEN end_char="385" id="token-1-49" morph="none" pos="word" start_char="378">outbreak</TOKEN>
<TOKEN end_char="391" id="token-1-50" morph="none" pos="word" start_char="387">began</TOKEN>
<TOKEN end_char="392" id="token-1-51" morph="none" pos="punct" start_char="392">.</TOKEN>
</SEG>
<SEG end_char="674" id="segment-2" start_char="396">
<ORIGINAL_TEXT>The research, which has not been peer-reviewed by other scientists, used satellite imagery of hospital parking lots in Wuhan - where the disease was first identified in late 2019 - and data for symptom-related queries on search engines for things such as "cough" and "diarrhoea".</ORIGINAL_TEXT>
<TOKEN end_char="398" id="token-2-0" morph="none" pos="word" start_char="396">The</TOKEN>
<TOKEN end_char="407" id="token-2-1" morph="none" pos="word" start_char="400">research</TOKEN>
<TOKEN end_char="408" id="token-2-2" morph="none" pos="punct" start_char="408">,</TOKEN>
<TOKEN end_char="414" id="token-2-3" morph="none" pos="word" start_char="410">which</TOKEN>
<TOKEN end_char="418" id="token-2-4" morph="none" pos="word" start_char="416">has</TOKEN>
<TOKEN end_char="422" id="token-2-5" morph="none" pos="word" start_char="420">not</TOKEN>
<TOKEN end_char="427" id="token-2-6" morph="none" pos="word" start_char="424">been</TOKEN>
<TOKEN end_char="441" id="token-2-7" morph="none" pos="unknown" start_char="429">peer-reviewed</TOKEN>
<TOKEN end_char="444" id="token-2-8" morph="none" pos="word" start_char="443">by</TOKEN>
<TOKEN end_char="450" id="token-2-9" morph="none" pos="word" start_char="446">other</TOKEN>
<TOKEN end_char="461" id="token-2-10" morph="none" pos="word" start_char="452">scientists</TOKEN>
<TOKEN end_char="462" id="token-2-11" morph="none" pos="punct" start_char="462">,</TOKEN>
<TOKEN end_char="467" id="token-2-12" morph="none" pos="word" start_char="464">used</TOKEN>
<TOKEN end_char="477" id="token-2-13" morph="none" pos="word" start_char="469">satellite</TOKEN>
<TOKEN end_char="485" id="token-2-14" morph="none" pos="word" start_char="479">imagery</TOKEN>
<TOKEN end_char="488" id="token-2-15" morph="none" pos="word" start_char="487">of</TOKEN>
<TOKEN end_char="497" id="token-2-16" morph="none" pos="word" start_char="490">hospital</TOKEN>
<TOKEN end_char="505" id="token-2-17" morph="none" pos="word" start_char="499">parking</TOKEN>
<TOKEN end_char="510" id="token-2-18" morph="none" pos="word" start_char="507">lots</TOKEN>
<TOKEN end_char="513" id="token-2-19" morph="none" pos="word" start_char="512">in</TOKEN>
<TOKEN end_char="519" id="token-2-20" morph="none" pos="word" start_char="515">Wuhan</TOKEN>
<TOKEN end_char="521" id="token-2-21" morph="none" pos="punct" start_char="521">-</TOKEN>
<TOKEN end_char="527" id="token-2-22" morph="none" pos="word" start_char="523">where</TOKEN>
<TOKEN end_char="531" id="token-2-23" morph="none" pos="word" start_char="529">the</TOKEN>
<TOKEN end_char="539" id="token-2-24" morph="none" pos="word" start_char="533">disease</TOKEN>
<TOKEN end_char="543" id="token-2-25" morph="none" pos="word" start_char="541">was</TOKEN>
<TOKEN end_char="549" id="token-2-26" morph="none" pos="word" start_char="545">first</TOKEN>
<TOKEN end_char="560" id="token-2-27" morph="none" pos="word" start_char="551">identified</TOKEN>
<TOKEN end_char="563" id="token-2-28" morph="none" pos="word" start_char="562">in</TOKEN>
<TOKEN end_char="568" id="token-2-29" morph="none" pos="word" start_char="565">late</TOKEN>
<TOKEN end_char="573" id="token-2-30" morph="none" pos="word" start_char="570">2019</TOKEN>
<TOKEN end_char="575" id="token-2-31" morph="none" pos="punct" start_char="575">-</TOKEN>
<TOKEN end_char="579" id="token-2-32" morph="none" pos="word" start_char="577">and</TOKEN>
<TOKEN end_char="584" id="token-2-33" morph="none" pos="word" start_char="581">data</TOKEN>
<TOKEN end_char="588" id="token-2-34" morph="none" pos="word" start_char="586">for</TOKEN>
<TOKEN end_char="604" id="token-2-35" morph="none" pos="unknown" start_char="590">symptom-related</TOKEN>
<TOKEN end_char="612" id="token-2-36" morph="none" pos="word" start_char="606">queries</TOKEN>
<TOKEN end_char="615" id="token-2-37" morph="none" pos="word" start_char="614">on</TOKEN>
<TOKEN end_char="622" id="token-2-38" morph="none" pos="word" start_char="617">search</TOKEN>
<TOKEN end_char="630" id="token-2-39" morph="none" pos="word" start_char="624">engines</TOKEN>
<TOKEN end_char="634" id="token-2-40" morph="none" pos="word" start_char="632">for</TOKEN>
<TOKEN end_char="641" id="token-2-41" morph="none" pos="word" start_char="636">things</TOKEN>
<TOKEN end_char="646" id="token-2-42" morph="none" pos="word" start_char="643">such</TOKEN>
<TOKEN end_char="649" id="token-2-43" morph="none" pos="word" start_char="648">as</TOKEN>
<TOKEN end_char="651" id="token-2-44" morph="none" pos="punct" start_char="651">"</TOKEN>
<TOKEN end_char="656" id="token-2-45" morph="none" pos="word" start_char="652">cough</TOKEN>
<TOKEN end_char="657" id="token-2-46" morph="none" pos="punct" start_char="657">"</TOKEN>
<TOKEN end_char="661" id="token-2-47" morph="none" pos="word" start_char="659">and</TOKEN>
<TOKEN end_char="663" id="token-2-48" morph="none" pos="punct" start_char="663">"</TOKEN>
<TOKEN end_char="672" id="token-2-49" morph="none" pos="word" start_char="664">diarrhoea</TOKEN>
<TOKEN end_char="674" id="token-2-50" morph="none" pos="punct" start_char="673">".</TOKEN>
</SEG>
<SEG end_char="836" id="segment-3" start_char="677">
<ORIGINAL_TEXT>The study’s authors said increased hospital traffic and symptom search data in Wuhan preceded the documented start of the coronavirus pandemic in December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="679" id="token-3-0" morph="none" pos="word" start_char="677">The</TOKEN>
<TOKEN end_char="687" id="token-3-1" morph="none" pos="word" start_char="681">study’s</TOKEN>
<TOKEN end_char="695" id="token-3-2" morph="none" pos="word" start_char="689">authors</TOKEN>
<TOKEN end_char="700" id="token-3-3" morph="none" pos="word" start_char="697">said</TOKEN>
<TOKEN end_char="710" id="token-3-4" morph="none" pos="word" start_char="702">increased</TOKEN>
<TOKEN end_char="719" id="token-3-5" morph="none" pos="word" start_char="712">hospital</TOKEN>
<TOKEN end_char="727" id="token-3-6" morph="none" pos="word" start_char="721">traffic</TOKEN>
<TOKEN end_char="731" id="token-3-7" morph="none" pos="word" start_char="729">and</TOKEN>
<TOKEN end_char="739" id="token-3-8" morph="none" pos="word" start_char="733">symptom</TOKEN>
<TOKEN end_char="746" id="token-3-9" morph="none" pos="word" start_char="741">search</TOKEN>
<TOKEN end_char="751" id="token-3-10" morph="none" pos="word" start_char="748">data</TOKEN>
<TOKEN end_char="754" id="token-3-11" morph="none" pos="word" start_char="753">in</TOKEN>
<TOKEN end_char="760" id="token-3-12" morph="none" pos="word" start_char="756">Wuhan</TOKEN>
<TOKEN end_char="769" id="token-3-13" morph="none" pos="word" start_char="762">preceded</TOKEN>
<TOKEN end_char="773" id="token-3-14" morph="none" pos="word" start_char="771">the</TOKEN>
<TOKEN end_char="784" id="token-3-15" morph="none" pos="word" start_char="775">documented</TOKEN>
<TOKEN end_char="790" id="token-3-16" morph="none" pos="word" start_char="786">start</TOKEN>
<TOKEN end_char="793" id="token-3-17" morph="none" pos="word" start_char="792">of</TOKEN>
<TOKEN end_char="797" id="token-3-18" morph="none" pos="word" start_char="795">the</TOKEN>
<TOKEN end_char="809" id="token-3-19" morph="none" pos="word" start_char="799">coronavirus</TOKEN>
<TOKEN end_char="818" id="token-3-20" morph="none" pos="word" start_char="811">pandemic</TOKEN>
<TOKEN end_char="821" id="token-3-21" morph="none" pos="word" start_char="820">in</TOKEN>
<TOKEN end_char="830" id="token-3-22" morph="none" pos="word" start_char="823">December</TOKEN>
<TOKEN end_char="835" id="token-3-23" morph="none" pos="word" start_char="832">2019</TOKEN>
<TOKEN end_char="836" id="token-3-24" morph="none" pos="punct" start_char="836">.</TOKEN>
</SEG>
<SEG end_char="1072" id="segment-4" start_char="839">
<ORIGINAL_TEXT>"While we cannot confirm if the increased volume was directly related to the new virus, our evidence supports other recent work showing that emergence happened before identification at the Huanan Seafood market (in Wuhan)," they said.</ORIGINAL_TEXT>
<TOKEN end_char="839" id="token-4-0" morph="none" pos="punct" start_char="839">"</TOKEN>
<TOKEN end_char="844" id="token-4-1" morph="none" pos="word" start_char="840">While</TOKEN>
<TOKEN end_char="847" id="token-4-2" morph="none" pos="word" start_char="846">we</TOKEN>
<TOKEN end_char="854" id="token-4-3" morph="none" pos="word" start_char="849">cannot</TOKEN>
<TOKEN end_char="862" id="token-4-4" morph="none" pos="word" start_char="856">confirm</TOKEN>
<TOKEN end_char="865" id="token-4-5" morph="none" pos="word" start_char="864">if</TOKEN>
<TOKEN end_char="869" id="token-4-6" morph="none" pos="word" start_char="867">the</TOKEN>
<TOKEN end_char="879" id="token-4-7" morph="none" pos="word" start_char="871">increased</TOKEN>
<TOKEN end_char="886" id="token-4-8" morph="none" pos="word" start_char="881">volume</TOKEN>
<TOKEN end_char="890" id="token-4-9" morph="none" pos="word" start_char="888">was</TOKEN>
<TOKEN end_char="899" id="token-4-10" morph="none" pos="word" start_char="892">directly</TOKEN>
<TOKEN end_char="907" id="token-4-11" morph="none" pos="word" start_char="901">related</TOKEN>
<TOKEN end_char="910" id="token-4-12" morph="none" pos="word" start_char="909">to</TOKEN>
<TOKEN end_char="914" id="token-4-13" morph="none" pos="word" start_char="912">the</TOKEN>
<TOKEN end_char="918" id="token-4-14" morph="none" pos="word" start_char="916">new</TOKEN>
<TOKEN end_char="924" id="token-4-15" morph="none" pos="word" start_char="920">virus</TOKEN>
<TOKEN end_char="925" id="token-4-16" morph="none" pos="punct" start_char="925">,</TOKEN>
<TOKEN end_char="929" id="token-4-17" morph="none" pos="word" start_char="927">our</TOKEN>
<TOKEN end_char="938" id="token-4-18" morph="none" pos="word" start_char="931">evidence</TOKEN>
<TOKEN end_char="947" id="token-4-19" morph="none" pos="word" start_char="940">supports</TOKEN>
<TOKEN end_char="953" id="token-4-20" morph="none" pos="word" start_char="949">other</TOKEN>
<TOKEN end_char="960" id="token-4-21" morph="none" pos="word" start_char="955">recent</TOKEN>
<TOKEN end_char="965" id="token-4-22" morph="none" pos="word" start_char="962">work</TOKEN>
<TOKEN end_char="973" id="token-4-23" morph="none" pos="word" start_char="967">showing</TOKEN>
<TOKEN end_char="978" id="token-4-24" morph="none" pos="word" start_char="975">that</TOKEN>
<TOKEN end_char="988" id="token-4-25" morph="none" pos="word" start_char="980">emergence</TOKEN>
<TOKEN end_char="997" id="token-4-26" morph="none" pos="word" start_char="990">happened</TOKEN>
<TOKEN end_char="1004" id="token-4-27" morph="none" pos="word" start_char="999">before</TOKEN>
<TOKEN end_char="1019" id="token-4-28" morph="none" pos="word" start_char="1006">identification</TOKEN>
<TOKEN end_char="1022" id="token-4-29" morph="none" pos="word" start_char="1021">at</TOKEN>
<TOKEN end_char="1026" id="token-4-30" morph="none" pos="word" start_char="1024">the</TOKEN>
<TOKEN end_char="1033" id="token-4-31" morph="none" pos="word" start_char="1028">Huanan</TOKEN>
<TOKEN end_char="1041" id="token-4-32" morph="none" pos="word" start_char="1035">Seafood</TOKEN>
<TOKEN end_char="1048" id="token-4-33" morph="none" pos="word" start_char="1043">market</TOKEN>
<TOKEN end_char="1050" id="token-4-34" morph="none" pos="punct" start_char="1050">(</TOKEN>
<TOKEN end_char="1052" id="token-4-35" morph="none" pos="word" start_char="1051">in</TOKEN>
<TOKEN end_char="1058" id="token-4-36" morph="none" pos="word" start_char="1054">Wuhan</TOKEN>
<TOKEN end_char="1061" id="token-4-37" morph="none" pos="punct" start_char="1059">),"</TOKEN>
<TOKEN end_char="1066" id="token-4-38" morph="none" pos="word" start_char="1063">they</TOKEN>
<TOKEN end_char="1071" id="token-4-39" morph="none" pos="word" start_char="1068">said</TOKEN>
<TOKEN end_char="1072" id="token-4-40" morph="none" pos="punct" start_char="1072">.</TOKEN>
</SEG>
<SEG end_char="1290" id="segment-5" start_char="1075">
<ORIGINAL_TEXT>Paul Digard, an expert in virology at the University of Edinburgh, said that using search engine data and satellite imagery of hospital traffic to detect disease outbreaks "is an interesting idea with some validity."</ORIGINAL_TEXT>
<TOKEN end_char="1078" id="token-5-0" morph="none" pos="word" start_char="1075">Paul</TOKEN>
<TOKEN end_char="1085" id="token-5-1" morph="none" pos="word" start_char="1080">Digard</TOKEN>
<TOKEN end_char="1086" id="token-5-2" morph="none" pos="punct" start_char="1086">,</TOKEN>
<TOKEN end_char="1089" id="token-5-3" morph="none" pos="word" start_char="1088">an</TOKEN>
<TOKEN end_char="1096" id="token-5-4" morph="none" pos="word" start_char="1091">expert</TOKEN>
<TOKEN end_char="1099" id="token-5-5" morph="none" pos="word" start_char="1098">in</TOKEN>
<TOKEN end_char="1108" id="token-5-6" morph="none" pos="word" start_char="1101">virology</TOKEN>
<TOKEN end_char="1111" id="token-5-7" morph="none" pos="word" start_char="1110">at</TOKEN>
<TOKEN end_char="1115" id="token-5-8" morph="none" pos="word" start_char="1113">the</TOKEN>
<TOKEN end_char="1126" id="token-5-9" morph="none" pos="word" start_char="1117">University</TOKEN>
<TOKEN end_char="1129" id="token-5-10" morph="none" pos="word" start_char="1128">of</TOKEN>
<TOKEN end_char="1139" id="token-5-11" morph="none" pos="word" start_char="1131">Edinburgh</TOKEN>
<TOKEN end_char="1140" id="token-5-12" morph="none" pos="punct" start_char="1140">,</TOKEN>
<TOKEN end_char="1145" id="token-5-13" morph="none" pos="word" start_char="1142">said</TOKEN>
<TOKEN end_char="1150" id="token-5-14" morph="none" pos="word" start_char="1147">that</TOKEN>
<TOKEN end_char="1156" id="token-5-15" morph="none" pos="word" start_char="1152">using</TOKEN>
<TOKEN end_char="1163" id="token-5-16" morph="none" pos="word" start_char="1158">search</TOKEN>
<TOKEN end_char="1170" id="token-5-17" morph="none" pos="word" start_char="1165">engine</TOKEN>
<TOKEN end_char="1175" id="token-5-18" morph="none" pos="word" start_char="1172">data</TOKEN>
<TOKEN end_char="1179" id="token-5-19" morph="none" pos="word" start_char="1177">and</TOKEN>
<TOKEN end_char="1189" id="token-5-20" morph="none" pos="word" start_char="1181">satellite</TOKEN>
<TOKEN end_char="1197" id="token-5-21" morph="none" pos="word" start_char="1191">imagery</TOKEN>
<TOKEN end_char="1200" id="token-5-22" morph="none" pos="word" start_char="1199">of</TOKEN>
<TOKEN end_char="1209" id="token-5-23" morph="none" pos="word" start_char="1202">hospital</TOKEN>
<TOKEN end_char="1217" id="token-5-24" morph="none" pos="word" start_char="1211">traffic</TOKEN>
<TOKEN end_char="1220" id="token-5-25" morph="none" pos="word" start_char="1219">to</TOKEN>
<TOKEN end_char="1227" id="token-5-26" morph="none" pos="word" start_char="1222">detect</TOKEN>
<TOKEN end_char="1235" id="token-5-27" morph="none" pos="word" start_char="1229">disease</TOKEN>
<TOKEN end_char="1245" id="token-5-28" morph="none" pos="word" start_char="1237">outbreaks</TOKEN>
<TOKEN end_char="1247" id="token-5-29" morph="none" pos="punct" start_char="1247">"</TOKEN>
<TOKEN end_char="1249" id="token-5-30" morph="none" pos="word" start_char="1248">is</TOKEN>
<TOKEN end_char="1252" id="token-5-31" morph="none" pos="word" start_char="1251">an</TOKEN>
<TOKEN end_char="1264" id="token-5-32" morph="none" pos="word" start_char="1254">interesting</TOKEN>
<TOKEN end_char="1269" id="token-5-33" morph="none" pos="word" start_char="1266">idea</TOKEN>
<TOKEN end_char="1274" id="token-5-34" morph="none" pos="word" start_char="1271">with</TOKEN>
<TOKEN end_char="1279" id="token-5-35" morph="none" pos="word" start_char="1276">some</TOKEN>
<TOKEN end_char="1288" id="token-5-36" morph="none" pos="word" start_char="1281">validity</TOKEN>
<TOKEN end_char="1290" id="token-5-37" morph="none" pos="punct" start_char="1289">."</TOKEN>
</SEG>
<SEG end_char="1397" id="segment-6" start_char="1293">
<ORIGINAL_TEXT>But he said the data were only correlative and - as the Harvard scientists noted - cannot identify cause.</ORIGINAL_TEXT>
<TOKEN end_char="1295" id="token-6-0" morph="none" pos="word" start_char="1293">But</TOKEN>
<TOKEN end_char="1298" id="token-6-1" morph="none" pos="word" start_char="1297">he</TOKEN>
<TOKEN end_char="1303" id="token-6-2" morph="none" pos="word" start_char="1300">said</TOKEN>
<TOKEN end_char="1307" id="token-6-3" morph="none" pos="word" start_char="1305">the</TOKEN>
<TOKEN end_char="1312" id="token-6-4" morph="none" pos="word" start_char="1309">data</TOKEN>
<TOKEN end_char="1317" id="token-6-5" morph="none" pos="word" start_char="1314">were</TOKEN>
<TOKEN end_char="1322" id="token-6-6" morph="none" pos="word" start_char="1319">only</TOKEN>
<TOKEN end_char="1334" id="token-6-7" morph="none" pos="word" start_char="1324">correlative</TOKEN>
<TOKEN end_char="1338" id="token-6-8" morph="none" pos="word" start_char="1336">and</TOKEN>
<TOKEN end_char="1340" id="token-6-9" morph="none" pos="punct" start_char="1340">-</TOKEN>
<TOKEN end_char="1343" id="token-6-10" morph="none" pos="word" start_char="1342">as</TOKEN>
<TOKEN end_char="1347" id="token-6-11" morph="none" pos="word" start_char="1345">the</TOKEN>
<TOKEN end_char="1355" id="token-6-12" morph="none" pos="word" start_char="1349">Harvard</TOKEN>
<TOKEN end_char="1366" id="token-6-13" morph="none" pos="word" start_char="1357">scientists</TOKEN>
<TOKEN end_char="1372" id="token-6-14" morph="none" pos="word" start_char="1368">noted</TOKEN>
<TOKEN end_char="1374" id="token-6-15" morph="none" pos="punct" start_char="1374">-</TOKEN>
<TOKEN end_char="1381" id="token-6-16" morph="none" pos="word" start_char="1376">cannot</TOKEN>
<TOKEN end_char="1390" id="token-6-17" morph="none" pos="word" start_char="1383">identify</TOKEN>
<TOKEN end_char="1396" id="token-6-18" morph="none" pos="word" start_char="1392">cause</TOKEN>
<TOKEN end_char="1397" id="token-6-19" morph="none" pos="punct" start_char="1397">.</TOKEN>
</SEG>
<SEG end_char="1594" id="segment-7" start_char="1400">
<ORIGINAL_TEXT>"It’s an interesting piece of work, but I’m not sure it takes us much further forward," said Keith Neal, a professor of the epidemiology of infectious diseases at Britain’s Nottingham University.</ORIGINAL_TEXT>
<TOKEN end_char="1400" id="token-7-0" morph="none" pos="punct" start_char="1400">"</TOKEN>
<TOKEN end_char="1404" id="token-7-1" morph="none" pos="word" start_char="1401">It’s</TOKEN>
<TOKEN end_char="1407" id="token-7-2" morph="none" pos="word" start_char="1406">an</TOKEN>
<TOKEN end_char="1419" id="token-7-3" morph="none" pos="word" start_char="1409">interesting</TOKEN>
<TOKEN end_char="1425" id="token-7-4" morph="none" pos="word" start_char="1421">piece</TOKEN>
<TOKEN end_char="1428" id="token-7-5" morph="none" pos="word" start_char="1427">of</TOKEN>
<TOKEN end_char="1433" id="token-7-6" morph="none" pos="word" start_char="1430">work</TOKEN>
<TOKEN end_char="1434" id="token-7-7" morph="none" pos="punct" start_char="1434">,</TOKEN>
<TOKEN end_char="1438" id="token-7-8" morph="none" pos="word" start_char="1436">but</TOKEN>
<TOKEN end_char="1442" id="token-7-9" morph="none" pos="word" start_char="1440">I’m</TOKEN>
<TOKEN end_char="1446" id="token-7-10" morph="none" pos="word" start_char="1444">not</TOKEN>
<TOKEN end_char="1451" id="token-7-11" morph="none" pos="word" start_char="1448">sure</TOKEN>
<TOKEN end_char="1454" id="token-7-12" morph="none" pos="word" start_char="1453">it</TOKEN>
<TOKEN end_char="1460" id="token-7-13" morph="none" pos="word" start_char="1456">takes</TOKEN>
<TOKEN end_char="1463" id="token-7-14" morph="none" pos="word" start_char="1462">us</TOKEN>
<TOKEN end_char="1468" id="token-7-15" morph="none" pos="word" start_char="1465">much</TOKEN>
<TOKEN end_char="1476" id="token-7-16" morph="none" pos="word" start_char="1470">further</TOKEN>
<TOKEN end_char="1484" id="token-7-17" morph="none" pos="word" start_char="1478">forward</TOKEN>
<TOKEN end_char="1486" id="token-7-18" morph="none" pos="punct" start_char="1485">,"</TOKEN>
<TOKEN end_char="1491" id="token-7-19" morph="none" pos="word" start_char="1488">said</TOKEN>
<TOKEN end_char="1497" id="token-7-20" morph="none" pos="word" start_char="1493">Keith</TOKEN>
<TOKEN end_char="1502" id="token-7-21" morph="none" pos="word" start_char="1499">Neal</TOKEN>
<TOKEN end_char="1503" id="token-7-22" morph="none" pos="punct" start_char="1503">,</TOKEN>
<TOKEN end_char="1505" id="token-7-23" morph="none" pos="word" start_char="1505">a</TOKEN>
<TOKEN end_char="1515" id="token-7-24" morph="none" pos="word" start_char="1507">professor</TOKEN>
<TOKEN end_char="1518" id="token-7-25" morph="none" pos="word" start_char="1517">of</TOKEN>
<TOKEN end_char="1522" id="token-7-26" morph="none" pos="word" start_char="1520">the</TOKEN>
<TOKEN end_char="1535" id="token-7-27" morph="none" pos="word" start_char="1524">epidemiology</TOKEN>
<TOKEN end_char="1538" id="token-7-28" morph="none" pos="word" start_char="1537">of</TOKEN>
<TOKEN end_char="1549" id="token-7-29" morph="none" pos="word" start_char="1540">infectious</TOKEN>
<TOKEN end_char="1558" id="token-7-30" morph="none" pos="word" start_char="1551">diseases</TOKEN>
<TOKEN end_char="1561" id="token-7-31" morph="none" pos="word" start_char="1560">at</TOKEN>
<TOKEN end_char="1571" id="token-7-32" morph="none" pos="word" start_char="1563">Britain’s</TOKEN>
<TOKEN end_char="1582" id="token-7-33" morph="none" pos="word" start_char="1573">Nottingham</TOKEN>
<TOKEN end_char="1593" id="token-7-34" morph="none" pos="word" start_char="1584">University</TOKEN>
<TOKEN end_char="1594" id="token-7-35" morph="none" pos="punct" start_char="1594">.</TOKEN>
</SEG>
<SEG end_char="1849" id="segment-8" start_char="1597">
<ORIGINAL_TEXT>Chinese Foreign Ministry spokeswoman Hua Chunying, asked about the research at a news briefing on Tuesday, said: "I think it is ridiculous, incredibly ridiculous, to come up with this conclusion based on superficial observations such as traffic volume."</ORIGINAL_TEXT>
<TOKEN end_char="1603" id="token-8-0" morph="none" pos="word" start_char="1597">Chinese</TOKEN>
<TOKEN end_char="1611" id="token-8-1" morph="none" pos="word" start_char="1605">Foreign</TOKEN>
<TOKEN end_char="1620" id="token-8-2" morph="none" pos="word" start_char="1613">Ministry</TOKEN>
<TOKEN end_char="1632" id="token-8-3" morph="none" pos="word" start_char="1622">spokeswoman</TOKEN>
<TOKEN end_char="1636" id="token-8-4" morph="none" pos="word" start_char="1634">Hua</TOKEN>
<TOKEN end_char="1645" id="token-8-5" morph="none" pos="word" start_char="1638">Chunying</TOKEN>
<TOKEN end_char="1646" id="token-8-6" morph="none" pos="punct" start_char="1646">,</TOKEN>
<TOKEN end_char="1652" id="token-8-7" morph="none" pos="word" start_char="1648">asked</TOKEN>
<TOKEN end_char="1658" id="token-8-8" morph="none" pos="word" start_char="1654">about</TOKEN>
<TOKEN end_char="1662" id="token-8-9" morph="none" pos="word" start_char="1660">the</TOKEN>
<TOKEN end_char="1671" id="token-8-10" morph="none" pos="word" start_char="1664">research</TOKEN>
<TOKEN end_char="1674" id="token-8-11" morph="none" pos="word" start_char="1673">at</TOKEN>
<TOKEN end_char="1676" id="token-8-12" morph="none" pos="word" start_char="1676">a</TOKEN>
<TOKEN end_char="1681" id="token-8-13" morph="none" pos="word" start_char="1678">news</TOKEN>
<TOKEN end_char="1690" id="token-8-14" morph="none" pos="word" start_char="1683">briefing</TOKEN>
<TOKEN end_char="1693" id="token-8-15" morph="none" pos="word" start_char="1692">on</TOKEN>
<TOKEN end_char="1701" id="token-8-16" morph="none" pos="word" start_char="1695">Tuesday</TOKEN>
<TOKEN end_char="1702" id="token-8-17" morph="none" pos="punct" start_char="1702">,</TOKEN>
<TOKEN end_char="1707" id="token-8-18" morph="none" pos="word" start_char="1704">said</TOKEN>
<TOKEN end_char="1708" id="token-8-19" morph="none" pos="punct" start_char="1708">:</TOKEN>
<TOKEN end_char="1710" id="token-8-20" morph="none" pos="punct" start_char="1710">"</TOKEN>
<TOKEN end_char="1711" id="token-8-21" morph="none" pos="word" start_char="1711">I</TOKEN>
<TOKEN end_char="1717" id="token-8-22" morph="none" pos="word" start_char="1713">think</TOKEN>
<TOKEN end_char="1720" id="token-8-23" morph="none" pos="word" start_char="1719">it</TOKEN>
<TOKEN end_char="1723" id="token-8-24" morph="none" pos="word" start_char="1722">is</TOKEN>
<TOKEN end_char="1734" id="token-8-25" morph="none" pos="word" start_char="1725">ridiculous</TOKEN>
<TOKEN end_char="1735" id="token-8-26" morph="none" pos="punct" start_char="1735">,</TOKEN>
<TOKEN end_char="1746" id="token-8-27" morph="none" pos="word" start_char="1737">incredibly</TOKEN>
<TOKEN end_char="1757" id="token-8-28" morph="none" pos="word" start_char="1748">ridiculous</TOKEN>
<TOKEN end_char="1758" id="token-8-29" morph="none" pos="punct" start_char="1758">,</TOKEN>
<TOKEN end_char="1761" id="token-8-30" morph="none" pos="word" start_char="1760">to</TOKEN>
<TOKEN end_char="1766" id="token-8-31" morph="none" pos="word" start_char="1763">come</TOKEN>
<TOKEN end_char="1769" id="token-8-32" morph="none" pos="word" start_char="1768">up</TOKEN>
<TOKEN end_char="1774" id="token-8-33" morph="none" pos="word" start_char="1771">with</TOKEN>
<TOKEN end_char="1779" id="token-8-34" morph="none" pos="word" start_char="1776">this</TOKEN>
<TOKEN end_char="1790" id="token-8-35" morph="none" pos="word" start_char="1781">conclusion</TOKEN>
<TOKEN end_char="1796" id="token-8-36" morph="none" pos="word" start_char="1792">based</TOKEN>
<TOKEN end_char="1799" id="token-8-37" morph="none" pos="word" start_char="1798">on</TOKEN>
<TOKEN end_char="1811" id="token-8-38" morph="none" pos="word" start_char="1801">superficial</TOKEN>
<TOKEN end_char="1824" id="token-8-39" morph="none" pos="word" start_char="1813">observations</TOKEN>
<TOKEN end_char="1829" id="token-8-40" morph="none" pos="word" start_char="1826">such</TOKEN>
<TOKEN end_char="1832" id="token-8-41" morph="none" pos="word" start_char="1831">as</TOKEN>
<TOKEN end_char="1840" id="token-8-42" morph="none" pos="word" start_char="1834">traffic</TOKEN>
<TOKEN end_char="1847" id="token-8-43" morph="none" pos="word" start_char="1842">volume</TOKEN>
<TOKEN end_char="1849" id="token-8-44" morph="none" pos="punct" start_char="1848">."</TOKEN>
</SEG>
<SEG end_char="1992" id="segment-9" start_char="1852">
<ORIGINAL_TEXT>The Harvard research, which was posted online as a so-called preprint, showed a steep increase in hospital car park occupancy in August 2019.</ORIGINAL_TEXT>
<TOKEN end_char="1854" id="token-9-0" morph="none" pos="word" start_char="1852">The</TOKEN>
<TOKEN end_char="1862" id="token-9-1" morph="none" pos="word" start_char="1856">Harvard</TOKEN>
<TOKEN end_char="1871" id="token-9-2" morph="none" pos="word" start_char="1864">research</TOKEN>
<TOKEN end_char="1872" id="token-9-3" morph="none" pos="punct" start_char="1872">,</TOKEN>
<TOKEN end_char="1878" id="token-9-4" morph="none" pos="word" start_char="1874">which</TOKEN>
<TOKEN end_char="1882" id="token-9-5" morph="none" pos="word" start_char="1880">was</TOKEN>
<TOKEN end_char="1889" id="token-9-6" morph="none" pos="word" start_char="1884">posted</TOKEN>
<TOKEN end_char="1896" id="token-9-7" morph="none" pos="word" start_char="1891">online</TOKEN>
<TOKEN end_char="1899" id="token-9-8" morph="none" pos="word" start_char="1898">as</TOKEN>
<TOKEN end_char="1901" id="token-9-9" morph="none" pos="word" start_char="1901">a</TOKEN>
<TOKEN end_char="1911" id="token-9-10" morph="none" pos="unknown" start_char="1903">so-called</TOKEN>
<TOKEN end_char="1920" id="token-9-11" morph="none" pos="word" start_char="1913">preprint</TOKEN>
<TOKEN end_char="1921" id="token-9-12" morph="none" pos="punct" start_char="1921">,</TOKEN>
<TOKEN end_char="1928" id="token-9-13" morph="none" pos="word" start_char="1923">showed</TOKEN>
<TOKEN end_char="1930" id="token-9-14" morph="none" pos="word" start_char="1930">a</TOKEN>
<TOKEN end_char="1936" id="token-9-15" morph="none" pos="word" start_char="1932">steep</TOKEN>
<TOKEN end_char="1945" id="token-9-16" morph="none" pos="word" start_char="1938">increase</TOKEN>
<TOKEN end_char="1948" id="token-9-17" morph="none" pos="word" start_char="1947">in</TOKEN>
<TOKEN end_char="1957" id="token-9-18" morph="none" pos="word" start_char="1950">hospital</TOKEN>
<TOKEN end_char="1961" id="token-9-19" morph="none" pos="word" start_char="1959">car</TOKEN>
<TOKEN end_char="1966" id="token-9-20" morph="none" pos="word" start_char="1963">park</TOKEN>
<TOKEN end_char="1976" id="token-9-21" morph="none" pos="word" start_char="1968">occupancy</TOKEN>
<TOKEN end_char="1979" id="token-9-22" morph="none" pos="word" start_char="1978">in</TOKEN>
<TOKEN end_char="1986" id="token-9-23" morph="none" pos="word" start_char="1981">August</TOKEN>
<TOKEN end_char="1991" id="token-9-24" morph="none" pos="word" start_char="1988">2019</TOKEN>
<TOKEN end_char="1992" id="token-9-25" morph="none" pos="punct" start_char="1992">.</TOKEN>
</SEG>
<SEG end_char="2329" id="segment-10" start_char="1995">
<ORIGINAL_TEXT>FILE PHOTO: The ultrastructural morphology exhibited by the 2019 Novel Coronavirus (2019-nCoV), which was identified as the cause of an outbreak of respiratory illness first detected in Wuhan, China, is seen in an illustration released by the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, U.S. January 29, 2020.</ORIGINAL_TEXT>
<TOKEN end_char="1998" id="token-10-0" morph="none" pos="word" start_char="1995">FILE</TOKEN>
<TOKEN end_char="2004" id="token-10-1" morph="none" pos="word" start_char="2000">PHOTO</TOKEN>
<TOKEN end_char="2005" id="token-10-2" morph="none" pos="punct" start_char="2005">:</TOKEN>
<TOKEN end_char="2009" id="token-10-3" morph="none" pos="word" start_char="2007">The</TOKEN>
<TOKEN end_char="2025" id="token-10-4" morph="none" pos="word" start_char="2011">ultrastructural</TOKEN>
<TOKEN end_char="2036" id="token-10-5" morph="none" pos="word" start_char="2027">morphology</TOKEN>
<TOKEN end_char="2046" id="token-10-6" morph="none" pos="word" start_char="2038">exhibited</TOKEN>
<TOKEN end_char="2049" id="token-10-7" morph="none" pos="word" start_char="2048">by</TOKEN>
<TOKEN end_char="2053" id="token-10-8" morph="none" pos="word" start_char="2051">the</TOKEN>
<TOKEN end_char="2058" id="token-10-9" morph="none" pos="word" start_char="2055">2019</TOKEN>
<TOKEN end_char="2064" id="token-10-10" morph="none" pos="word" start_char="2060">Novel</TOKEN>
<TOKEN end_char="2076" id="token-10-11" morph="none" pos="word" start_char="2066">Coronavirus</TOKEN>
<TOKEN end_char="2078" id="token-10-12" morph="none" pos="punct" start_char="2078">(</TOKEN>
<TOKEN end_char="2087" id="token-10-13" morph="none" pos="unknown" start_char="2079">2019-nCoV</TOKEN>
<TOKEN end_char="2089" id="token-10-14" morph="none" pos="punct" start_char="2088">),</TOKEN>
<TOKEN end_char="2095" id="token-10-15" morph="none" pos="word" start_char="2091">which</TOKEN>
<TOKEN end_char="2099" id="token-10-16" morph="none" pos="word" start_char="2097">was</TOKEN>
<TOKEN end_char="2110" id="token-10-17" morph="none" pos="word" start_char="2101">identified</TOKEN>
<TOKEN end_char="2113" id="token-10-18" morph="none" pos="word" start_char="2112">as</TOKEN>
<TOKEN end_char="2117" id="token-10-19" morph="none" pos="word" start_char="2115">the</TOKEN>
<TOKEN end_char="2123" id="token-10-20" morph="none" pos="word" start_char="2119">cause</TOKEN>
<TOKEN end_char="2126" id="token-10-21" morph="none" pos="word" start_char="2125">of</TOKEN>
<TOKEN end_char="2129" id="token-10-22" morph="none" pos="word" start_char="2128">an</TOKEN>
<TOKEN end_char="2138" id="token-10-23" morph="none" pos="word" start_char="2131">outbreak</TOKEN>
<TOKEN end_char="2141" id="token-10-24" morph="none" pos="word" start_char="2140">of</TOKEN>
<TOKEN end_char="2153" id="token-10-25" morph="none" pos="word" start_char="2143">respiratory</TOKEN>
<TOKEN end_char="2161" id="token-10-26" morph="none" pos="word" start_char="2155">illness</TOKEN>
<TOKEN end_char="2167" id="token-10-27" morph="none" pos="word" start_char="2163">first</TOKEN>
<TOKEN end_char="2176" id="token-10-28" morph="none" pos="word" start_char="2169">detected</TOKEN>
<TOKEN end_char="2179" id="token-10-29" morph="none" pos="word" start_char="2178">in</TOKEN>
<TOKEN end_char="2185" id="token-10-30" morph="none" pos="word" start_char="2181">Wuhan</TOKEN>
<TOKEN end_char="2186" id="token-10-31" morph="none" pos="punct" start_char="2186">,</TOKEN>
<TOKEN end_char="2192" id="token-10-32" morph="none" pos="word" start_char="2188">China</TOKEN>
<TOKEN end_char="2193" id="token-10-33" morph="none" pos="punct" start_char="2193">,</TOKEN>
<TOKEN end_char="2196" id="token-10-34" morph="none" pos="word" start_char="2195">is</TOKEN>
<TOKEN end_char="2201" id="token-10-35" morph="none" pos="word" start_char="2198">seen</TOKEN>
<TOKEN end_char="2204" id="token-10-36" morph="none" pos="word" start_char="2203">in</TOKEN>
<TOKEN end_char="2207" id="token-10-37" morph="none" pos="word" start_char="2206">an</TOKEN>
<TOKEN end_char="2220" id="token-10-38" morph="none" pos="word" start_char="2209">illustration</TOKEN>
<TOKEN end_char="2229" id="token-10-39" morph="none" pos="word" start_char="2222">released</TOKEN>
<TOKEN end_char="2232" id="token-10-40" morph="none" pos="word" start_char="2231">by</TOKEN>
<TOKEN end_char="2236" id="token-10-41" morph="none" pos="word" start_char="2234">the</TOKEN>
<TOKEN end_char="2244" id="token-10-42" morph="none" pos="word" start_char="2238">Centers</TOKEN>
<TOKEN end_char="2248" id="token-10-43" morph="none" pos="word" start_char="2246">for</TOKEN>
<TOKEN end_char="2256" id="token-10-44" morph="none" pos="word" start_char="2250">Disease</TOKEN>
<TOKEN end_char="2264" id="token-10-45" morph="none" pos="word" start_char="2258">Control</TOKEN>
<TOKEN end_char="2268" id="token-10-46" morph="none" pos="word" start_char="2266">and</TOKEN>
<TOKEN end_char="2279" id="token-10-47" morph="none" pos="word" start_char="2270">Prevention</TOKEN>
<TOKEN end_char="2281" id="token-10-48" morph="none" pos="punct" start_char="2281">(</TOKEN>
<TOKEN end_char="2284" id="token-10-49" morph="none" pos="word" start_char="2282">CDC</TOKEN>
<TOKEN end_char="2285" id="token-10-50" morph="none" pos="punct" start_char="2285">)</TOKEN>
<TOKEN end_char="2288" id="token-10-51" morph="none" pos="word" start_char="2287">in</TOKEN>
<TOKEN end_char="2296" id="token-10-52" morph="none" pos="word" start_char="2290">Atlanta</TOKEN>
<TOKEN end_char="2297" id="token-10-53" morph="none" pos="punct" start_char="2297">,</TOKEN>
<TOKEN end_char="2305" id="token-10-54" morph="none" pos="word" start_char="2299">Georgia</TOKEN>
<TOKEN end_char="2306" id="token-10-55" morph="none" pos="punct" start_char="2306">,</TOKEN>
<TOKEN end_char="2310" id="token-10-56" morph="none" pos="unknown" start_char="2308">U.S</TOKEN>
<TOKEN end_char="2311" id="token-10-57" morph="none" pos="punct" start_char="2311">.</TOKEN>
<TOKEN end_char="2319" id="token-10-58" morph="none" pos="word" start_char="2313">January</TOKEN>
<TOKEN end_char="2322" id="token-10-59" morph="none" pos="word" start_char="2321">29</TOKEN>
<TOKEN end_char="2323" id="token-10-60" morph="none" pos="punct" start_char="2323">,</TOKEN>
<TOKEN end_char="2328" id="token-10-61" morph="none" pos="word" start_char="2325">2020</TOKEN>
<TOKEN end_char="2329" id="token-10-62" morph="none" pos="punct" start_char="2329">.</TOKEN>
</SEG>
<SEG end_char="2389" id="segment-11" start_char="2331">
<ORIGINAL_TEXT>Alissa Eckert, MS; Dan Higgins, MAM/CDC/Handout via REUTERS</ORIGINAL_TEXT>
<TOKEN end_char="2336" id="token-11-0" morph="none" pos="word" start_char="2331">Alissa</TOKEN>
<TOKEN end_char="2343" id="token-11-1" morph="none" pos="word" start_char="2338">Eckert</TOKEN>
<TOKEN end_char="2344" id="token-11-2" morph="none" pos="punct" start_char="2344">,</TOKEN>
<TOKEN end_char="2347" id="token-11-3" morph="none" pos="word" start_char="2346">MS</TOKEN>
<TOKEN end_char="2348" id="token-11-4" morph="none" pos="punct" start_char="2348">;</TOKEN>
<TOKEN end_char="2352" id="token-11-5" morph="none" pos="word" start_char="2350">Dan</TOKEN>
<TOKEN end_char="2360" id="token-11-6" morph="none" pos="word" start_char="2354">Higgins</TOKEN>
<TOKEN end_char="2361" id="token-11-7" morph="none" pos="punct" start_char="2361">,</TOKEN>
<TOKEN end_char="2377" id="token-11-8" morph="none" pos="unknown" start_char="2363">MAM/CDC/Handout</TOKEN>
<TOKEN end_char="2381" id="token-11-9" morph="none" pos="word" start_char="2379">via</TOKEN>
<TOKEN end_char="2389" id="token-11-10" morph="none" pos="word" start_char="2383">REUTERS</TOKEN>
<TRANSLATED_TEXT>Alissa Eckert, MS; Dan Higgins, MAM / CDC / Handout via REUTERS</TRANSLATED_TEXT><DETECTED_LANGUAGE>sv</DETECTED_LANGUAGE></SEG>
<SEG end_char="2554" id="segment-12" start_char="2393">
<ORIGINAL_TEXT>"In August, we identify a unique increase in searches for diarrhoea which was neither seen in previous flu seasons or mirrored in the cough search data," it said.</ORIGINAL_TEXT>
<TOKEN end_char="2393" id="token-12-0" morph="none" pos="punct" start_char="2393">"</TOKEN>
<TOKEN end_char="2395" id="token-12-1" morph="none" pos="word" start_char="2394">In</TOKEN>
<TOKEN end_char="2402" id="token-12-2" morph="none" pos="word" start_char="2397">August</TOKEN>
<TOKEN end_char="2403" id="token-12-3" morph="none" pos="punct" start_char="2403">,</TOKEN>
<TOKEN end_char="2406" id="token-12-4" morph="none" pos="word" start_char="2405">we</TOKEN>
<TOKEN end_char="2415" id="token-12-5" morph="none" pos="word" start_char="2408">identify</TOKEN>
<TOKEN end_char="2417" id="token-12-6" morph="none" pos="word" start_char="2417">a</TOKEN>
<TOKEN end_char="2424" id="token-12-7" morph="none" pos="word" start_char="2419">unique</TOKEN>
<TOKEN end_char="2433" id="token-12-8" morph="none" pos="word" start_char="2426">increase</TOKEN>
<TOKEN end_char="2436" id="token-12-9" morph="none" pos="word" start_char="2435">in</TOKEN>
<TOKEN end_char="2445" id="token-12-10" morph="none" pos="word" start_char="2438">searches</TOKEN>
<TOKEN end_char="2449" id="token-12-11" morph="none" pos="word" start_char="2447">for</TOKEN>
<TOKEN end_char="2459" id="token-12-12" morph="none" pos="word" start_char="2451">diarrhoea</TOKEN>
<TOKEN end_char="2465" id="token-12-13" morph="none" pos="word" start_char="2461">which</TOKEN>
<TOKEN end_char="2469" id="token-12-14" morph="none" pos="word" start_char="2467">was</TOKEN>
<TOKEN end_char="2477" id="token-12-15" morph="none" pos="word" start_char="2471">neither</TOKEN>
<TOKEN end_char="2482" id="token-12-16" morph="none" pos="word" start_char="2479">seen</TOKEN>
<TOKEN end_char="2485" id="token-12-17" morph="none" pos="word" start_char="2484">in</TOKEN>
<TOKEN end_char="2494" id="token-12-18" morph="none" pos="word" start_char="2487">previous</TOKEN>
<TOKEN end_char="2498" id="token-12-19" morph="none" pos="word" start_char="2496">flu</TOKEN>
<TOKEN end_char="2506" id="token-12-20" morph="none" pos="word" start_char="2500">seasons</TOKEN>
<TOKEN end_char="2509" id="token-12-21" morph="none" pos="word" start_char="2508">or</TOKEN>
<TOKEN end_char="2518" id="token-12-22" morph="none" pos="word" start_char="2511">mirrored</TOKEN>
<TOKEN end_char="2521" id="token-12-23" morph="none" pos="word" start_char="2520">in</TOKEN>
<TOKEN end_char="2525" id="token-12-24" morph="none" pos="word" start_char="2523">the</TOKEN>
<TOKEN end_char="2531" id="token-12-25" morph="none" pos="word" start_char="2527">cough</TOKEN>
<TOKEN end_char="2538" id="token-12-26" morph="none" pos="word" start_char="2533">search</TOKEN>
<TOKEN end_char="2543" id="token-12-27" morph="none" pos="word" start_char="2540">data</TOKEN>
<TOKEN end_char="2545" id="token-12-28" morph="none" pos="punct" start_char="2544">,"</TOKEN>
<TOKEN end_char="2548" id="token-12-29" morph="none" pos="word" start_char="2547">it</TOKEN>
<TOKEN end_char="2553" id="token-12-30" morph="none" pos="word" start_char="2550">said</TOKEN>
<TOKEN end_char="2554" id="token-12-31" morph="none" pos="punct" start_char="2554">.</TOKEN>
</SEG>
<SEG end_char="2721" id="segment-13" start_char="2557">
<ORIGINAL_TEXT>Neal said the study included traffic around at least one children’s hospital and that while children do get ill with flu, they do not tend to get sick with COVID-19.</ORIGINAL_TEXT>
<TOKEN end_char="2560" id="token-13-0" morph="none" pos="word" start_char="2557">Neal</TOKEN>
<TOKEN end_char="2565" id="token-13-1" morph="none" pos="word" start_char="2562">said</TOKEN>
<TOKEN end_char="2569" id="token-13-2" morph="none" pos="word" start_char="2567">the</TOKEN>
<TOKEN end_char="2575" id="token-13-3" morph="none" pos="word" start_char="2571">study</TOKEN>
<TOKEN end_char="2584" id="token-13-4" morph="none" pos="word" start_char="2577">included</TOKEN>
<TOKEN end_char="2592" id="token-13-5" morph="none" pos="word" start_char="2586">traffic</TOKEN>
<TOKEN end_char="2599" id="token-13-6" morph="none" pos="word" start_char="2594">around</TOKEN>
<TOKEN end_char="2602" id="token-13-7" morph="none" pos="word" start_char="2601">at</TOKEN>
<TOKEN end_char="2608" id="token-13-8" morph="none" pos="word" start_char="2604">least</TOKEN>
<TOKEN end_char="2612" id="token-13-9" morph="none" pos="word" start_char="2610">one</TOKEN>
<TOKEN end_char="2623" id="token-13-10" morph="none" pos="word" start_char="2614">children’s</TOKEN>
<TOKEN end_char="2632" id="token-13-11" morph="none" pos="word" start_char="2625">hospital</TOKEN>
<TOKEN end_char="2636" id="token-13-12" morph="none" pos="word" start_char="2634">and</TOKEN>
<TOKEN end_char="2641" id="token-13-13" morph="none" pos="word" start_char="2638">that</TOKEN>
<TOKEN end_char="2647" id="token-13-14" morph="none" pos="word" start_char="2643">while</TOKEN>
<TOKEN end_char="2656" id="token-13-15" morph="none" pos="word" start_char="2649">children</TOKEN>
<TOKEN end_char="2659" id="token-13-16" morph="none" pos="word" start_char="2658">do</TOKEN>
<TOKEN end_char="2663" id="token-13-17" morph="none" pos="word" start_char="2661">get</TOKEN>
<TOKEN end_char="2667" id="token-13-18" morph="none" pos="word" start_char="2665">ill</TOKEN>
<TOKEN end_char="2672" id="token-13-19" morph="none" pos="word" start_char="2669">with</TOKEN>
<TOKEN end_char="2676" id="token-13-20" morph="none" pos="word" start_char="2674">flu</TOKEN>
<TOKEN end_char="2677" id="token-13-21" morph="none" pos="punct" start_char="2677">,</TOKEN>
<TOKEN end_char="2682" id="token-13-22" morph="none" pos="word" start_char="2679">they</TOKEN>
<TOKEN end_char="2685" id="token-13-23" morph="none" pos="word" start_char="2684">do</TOKEN>
<TOKEN end_char="2689" id="token-13-24" morph="none" pos="word" start_char="2687">not</TOKEN>
<TOKEN end_char="2694" id="token-13-25" morph="none" pos="word" start_char="2691">tend</TOKEN>
<TOKEN end_char="2697" id="token-13-26" morph="none" pos="word" start_char="2696">to</TOKEN>
<TOKEN end_char="2701" id="token-13-27" morph="none" pos="word" start_char="2699">get</TOKEN>
<TOKEN end_char="2706" id="token-13-28" morph="none" pos="word" start_char="2703">sick</TOKEN>
<TOKEN end_char="2711" id="token-13-29" morph="none" pos="word" start_char="2708">with</TOKEN>
<TOKEN end_char="2720" id="token-13-30" morph="none" pos="unknown" start_char="2713">COVID-19</TOKEN>
<TOKEN end_char="2721" id="token-13-31" morph="none" pos="punct" start_char="2721">.</TOKEN>
</SEG>
<SEG end_char="2871" id="segment-14" start_char="2724">
<ORIGINAL_TEXT>Digard cautioned that by focusing only on hospitals in Wuhan, already known to be the epicentre of the outbreak, "the study forces the correlation."</ORIGINAL_TEXT>
<TOKEN end_char="2729" id="token-14-0" morph="none" pos="word" start_char="2724">Digard</TOKEN>
<TOKEN end_char="2739" id="token-14-1" morph="none" pos="word" start_char="2731">cautioned</TOKEN>
<TOKEN end_char="2744" id="token-14-2" morph="none" pos="word" start_char="2741">that</TOKEN>
<TOKEN end_char="2747" id="token-14-3" morph="none" pos="word" start_char="2746">by</TOKEN>
<TOKEN end_char="2756" id="token-14-4" morph="none" pos="word" start_char="2749">focusing</TOKEN>
<TOKEN end_char="2761" id="token-14-5" morph="none" pos="word" start_char="2758">only</TOKEN>
<TOKEN end_char="2764" id="token-14-6" morph="none" pos="word" start_char="2763">on</TOKEN>
<TOKEN end_char="2774" id="token-14-7" morph="none" pos="word" start_char="2766">hospitals</TOKEN>
<TOKEN end_char="2777" id="token-14-8" morph="none" pos="word" start_char="2776">in</TOKEN>
<TOKEN end_char="2783" id="token-14-9" morph="none" pos="word" start_char="2779">Wuhan</TOKEN>
<TOKEN end_char="2784" id="token-14-10" morph="none" pos="punct" start_char="2784">,</TOKEN>
<TOKEN end_char="2792" id="token-14-11" morph="none" pos="word" start_char="2786">already</TOKEN>
<TOKEN end_char="2798" id="token-14-12" morph="none" pos="word" start_char="2794">known</TOKEN>
<TOKEN end_char="2801" id="token-14-13" morph="none" pos="word" start_char="2800">to</TOKEN>
<TOKEN end_char="2804" id="token-14-14" morph="none" pos="word" start_char="2803">be</TOKEN>
<TOKEN end_char="2808" id="token-14-15" morph="none" pos="word" start_char="2806">the</TOKEN>
<TOKEN end_char="2818" id="token-14-16" morph="none" pos="word" start_char="2810">epicentre</TOKEN>
<TOKEN end_char="2821" id="token-14-17" morph="none" pos="word" start_char="2820">of</TOKEN>
<TOKEN end_char="2825" id="token-14-18" morph="none" pos="word" start_char="2823">the</TOKEN>
<TOKEN end_char="2834" id="token-14-19" morph="none" pos="word" start_char="2827">outbreak</TOKEN>
<TOKEN end_char="2835" id="token-14-20" morph="none" pos="punct" start_char="2835">,</TOKEN>
<TOKEN end_char="2837" id="token-14-21" morph="none" pos="punct" start_char="2837">"</TOKEN>
<TOKEN end_char="2840" id="token-14-22" morph="none" pos="word" start_char="2838">the</TOKEN>
<TOKEN end_char="2846" id="token-14-23" morph="none" pos="word" start_char="2842">study</TOKEN>
<TOKEN end_char="2853" id="token-14-24" morph="none" pos="word" start_char="2848">forces</TOKEN>
<TOKEN end_char="2857" id="token-14-25" morph="none" pos="word" start_char="2855">the</TOKEN>
<TOKEN end_char="2869" id="token-14-26" morph="none" pos="word" start_char="2859">correlation</TOKEN>
<TOKEN end_char="2871" id="token-14-27" morph="none" pos="punct" start_char="2870">."</TOKEN>
</SEG>
<SEG end_char="3035" id="segment-15" start_char="2874">
<ORIGINAL_TEXT>"It would have been interesting - and possibly much more convincing - to have seen control analyses of other Chinese cities outside of the Hubei region," he said.</ORIGINAL_TEXT>
<TOKEN end_char="2874" id="token-15-0" morph="none" pos="punct" start_char="2874">"</TOKEN>
<TOKEN end_char="2876" id="token-15-1" morph="none" pos="word" start_char="2875">It</TOKEN>
<TOKEN end_char="2882" id="token-15-2" morph="none" pos="word" start_char="2878">would</TOKEN>
<TOKEN end_char="2887" id="token-15-3" morph="none" pos="word" start_char="2884">have</TOKEN>
<TOKEN end_char="2892" id="token-15-4" morph="none" pos="word" start_char="2889">been</TOKEN>
<TOKEN end_char="2904" id="token-15-5" morph="none" pos="word" start_char="2894">interesting</TOKEN>
<TOKEN end_char="2906" id="token-15-6" morph="none" pos="punct" start_char="2906">-</TOKEN>
<TOKEN end_char="2910" id="token-15-7" morph="none" pos="word" start_char="2908">and</TOKEN>
<TOKEN end_char="2919" id="token-15-8" morph="none" pos="word" start_char="2912">possibly</TOKEN>
<TOKEN end_char="2924" id="token-15-9" morph="none" pos="word" start_char="2921">much</TOKEN>
<TOKEN end_char="2929" id="token-15-10" morph="none" pos="word" start_char="2926">more</TOKEN>
<TOKEN end_char="2940" id="token-15-11" morph="none" pos="word" start_char="2931">convincing</TOKEN>
<TOKEN end_char="2942" id="token-15-12" morph="none" pos="punct" start_char="2942">-</TOKEN>
<TOKEN end_char="2945" id="token-15-13" morph="none" pos="word" start_char="2944">to</TOKEN>
<TOKEN end_char="2950" id="token-15-14" morph="none" pos="word" start_char="2947">have</TOKEN>
<TOKEN end_char="2955" id="token-15-15" morph="none" pos="word" start_char="2952">seen</TOKEN>
<TOKEN end_char="2963" id="token-15-16" morph="none" pos="word" start_char="2957">control</TOKEN>
<TOKEN end_char="2972" id="token-15-17" morph="none" pos="word" start_char="2965">analyses</TOKEN>
<TOKEN end_char="2975" id="token-15-18" morph="none" pos="word" start_char="2974">of</TOKEN>
<TOKEN end_char="2981" id="token-15-19" morph="none" pos="word" start_char="2977">other</TOKEN>
<TOKEN end_char="2989" id="token-15-20" morph="none" pos="word" start_char="2983">Chinese</TOKEN>
<TOKEN end_char="2996" id="token-15-21" morph="none" pos="word" start_char="2991">cities</TOKEN>
<TOKEN end_char="3004" id="token-15-22" morph="none" pos="word" start_char="2998">outside</TOKEN>
<TOKEN end_char="3007" id="token-15-23" morph="none" pos="word" start_char="3006">of</TOKEN>
<TOKEN end_char="3011" id="token-15-24" morph="none" pos="word" start_char="3009">the</TOKEN>
<TOKEN end_char="3017" id="token-15-25" morph="none" pos="word" start_char="3013">Hubei</TOKEN>
<TOKEN end_char="3024" id="token-15-26" morph="none" pos="word" start_char="3019">region</TOKEN>
<TOKEN end_char="3026" id="token-15-27" morph="none" pos="punct" start_char="3025">,"</TOKEN>
<TOKEN end_char="3029" id="token-15-28" morph="none" pos="word" start_char="3028">he</TOKEN>
<TOKEN end_char="3034" id="token-15-29" morph="none" pos="word" start_char="3031">said</TOKEN>
<TOKEN end_char="3035" id="token-15-30" morph="none" pos="punct" start_char="3035">.</TOKEN>
</SEG>
<SEG end_char="3040" id="segment-16" start_char="3038">
<ORIGINAL_TEXT>Dr.</ORIGINAL_TEXT>
<TOKEN end_char="3039" id="token-16-0" morph="none" pos="word" start_char="3038">Dr</TOKEN>
<TOKEN end_char="3040" id="token-16-1" morph="none" pos="punct" start_char="3040">.</TOKEN>
<TRANSLATED_TEXT>dr.</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="3190" id="segment-17" start_char="3042">
<ORIGINAL_TEXT>Eric Topol, director of the Scripps Research Translational Institute, said the research method is not validated and is "very indirect and imprecise."</ORIGINAL_TEXT>
<TOKEN end_char="3045" id="token-17-0" morph="none" pos="word" start_char="3042">Eric</TOKEN>
<TOKEN end_char="3051" id="token-17-1" morph="none" pos="word" start_char="3047">Topol</TOKEN>
<TOKEN end_char="3052" id="token-17-2" morph="none" pos="punct" start_char="3052">,</TOKEN>
<TOKEN end_char="3061" id="token-17-3" morph="none" pos="word" start_char="3054">director</TOKEN>
<TOKEN end_char="3064" id="token-17-4" morph="none" pos="word" start_char="3063">of</TOKEN>
<TOKEN end_char="3068" id="token-17-5" morph="none" pos="word" start_char="3066">the</TOKEN>
<TOKEN end_char="3076" id="token-17-6" morph="none" pos="word" start_char="3070">Scripps</TOKEN>
<TOKEN end_char="3085" id="token-17-7" morph="none" pos="word" start_char="3078">Research</TOKEN>
<TOKEN end_char="3099" id="token-17-8" morph="none" pos="word" start_char="3087">Translational</TOKEN>
<TOKEN end_char="3109" id="token-17-9" morph="none" pos="word" start_char="3101">Institute</TOKEN>
<TOKEN end_char="3110" id="token-17-10" morph="none" pos="punct" start_char="3110">,</TOKEN>
<TOKEN end_char="3115" id="token-17-11" morph="none" pos="word" start_char="3112">said</TOKEN>
<TOKEN end_char="3119" id="token-17-12" morph="none" pos="word" start_char="3117">the</TOKEN>
<TOKEN end_char="3128" id="token-17-13" morph="none" pos="word" start_char="3121">research</TOKEN>
<TOKEN end_char="3135" id="token-17-14" morph="none" pos="word" start_char="3130">method</TOKEN>
<TOKEN end_char="3138" id="token-17-15" morph="none" pos="word" start_char="3137">is</TOKEN>
<TOKEN end_char="3142" id="token-17-16" morph="none" pos="word" start_char="3140">not</TOKEN>
<TOKEN end_char="3152" id="token-17-17" morph="none" pos="word" start_char="3144">validated</TOKEN>
<TOKEN end_char="3156" id="token-17-18" morph="none" pos="word" start_char="3154">and</TOKEN>
<TOKEN end_char="3159" id="token-17-19" morph="none" pos="word" start_char="3158">is</TOKEN>
<TOKEN end_char="3161" id="token-17-20" morph="none" pos="punct" start_char="3161">"</TOKEN>
<TOKEN end_char="3165" id="token-17-21" morph="none" pos="word" start_char="3162">very</TOKEN>
<TOKEN end_char="3174" id="token-17-22" morph="none" pos="word" start_char="3167">indirect</TOKEN>
<TOKEN end_char="3178" id="token-17-23" morph="none" pos="word" start_char="3176">and</TOKEN>
<TOKEN end_char="3188" id="token-17-24" morph="none" pos="word" start_char="3180">imprecise</TOKEN>
<TOKEN end_char="3190" id="token-17-25" morph="none" pos="punct" start_char="3189">."</TOKEN>
</SEG>
<SEG end_char="3325" id="segment-18" start_char="3193">
<ORIGINAL_TEXT>Topol, who was not involved with the research, said he doubts the outbreak began in August, based on the evidence he has seen so far.</ORIGINAL_TEXT>
<TOKEN end_char="3197" id="token-18-0" morph="none" pos="word" start_char="3193">Topol</TOKEN>
<TOKEN end_char="3198" id="token-18-1" morph="none" pos="punct" start_char="3198">,</TOKEN>
<TOKEN end_char="3202" id="token-18-2" morph="none" pos="word" start_char="3200">who</TOKEN>
<TOKEN end_char="3206" id="token-18-3" morph="none" pos="word" start_char="3204">was</TOKEN>
<TOKEN end_char="3210" id="token-18-4" morph="none" pos="word" start_char="3208">not</TOKEN>
<TOKEN end_char="3219" id="token-18-5" morph="none" pos="word" start_char="3212">involved</TOKEN>
<TOKEN end_char="3224" id="token-18-6" morph="none" pos="word" start_char="3221">with</TOKEN>
<TOKEN end_char="3228" id="token-18-7" morph="none" pos="word" start_char="3226">the</TOKEN>
<TOKEN end_char="3237" id="token-18-8" morph="none" pos="word" start_char="3230">research</TOKEN>
<TOKEN end_char="3238" id="token-18-9" morph="none" pos="punct" start_char="3238">,</TOKEN>
<TOKEN end_char="3243" id="token-18-10" morph="none" pos="word" start_char="3240">said</TOKEN>
<TOKEN end_char="3246" id="token-18-11" morph="none" pos="word" start_char="3245">he</TOKEN>
<TOKEN end_char="3253" id="token-18-12" morph="none" pos="word" start_char="3248">doubts</TOKEN>
<TOKEN end_char="3257" id="token-18-13" morph="none" pos="word" start_char="3255">the</TOKEN>
<TOKEN end_char="3266" id="token-18-14" morph="none" pos="word" start_char="3259">outbreak</TOKEN>
<TOKEN end_char="3272" id="token-18-15" morph="none" pos="word" start_char="3268">began</TOKEN>
<TOKEN end_char="3275" id="token-18-16" morph="none" pos="word" start_char="3274">in</TOKEN>
<TOKEN end_char="3282" id="token-18-17" morph="none" pos="word" start_char="3277">August</TOKEN>
<TOKEN end_char="3283" id="token-18-18" morph="none" pos="punct" start_char="3283">,</TOKEN>
<TOKEN end_char="3289" id="token-18-19" morph="none" pos="word" start_char="3285">based</TOKEN>
<TOKEN end_char="3292" id="token-18-20" morph="none" pos="word" start_char="3291">on</TOKEN>
<TOKEN end_char="3296" id="token-18-21" morph="none" pos="word" start_char="3294">the</TOKEN>
<TOKEN end_char="3305" id="token-18-22" morph="none" pos="word" start_char="3298">evidence</TOKEN>
<TOKEN end_char="3308" id="token-18-23" morph="none" pos="word" start_char="3307">he</TOKEN>
<TOKEN end_char="3312" id="token-18-24" morph="none" pos="word" start_char="3310">has</TOKEN>
<TOKEN end_char="3317" id="token-18-25" morph="none" pos="word" start_char="3314">seen</TOKEN>
<TOKEN end_char="3320" id="token-18-26" morph="none" pos="word" start_char="3319">so</TOKEN>
<TOKEN end_char="3324" id="token-18-27" morph="none" pos="word" start_char="3322">far</TOKEN>
<TOKEN end_char="3325" id="token-18-28" morph="none" pos="punct" start_char="3325">.</TOKEN>
</SEG>
<SEG end_char="3452" id="segment-19" start_char="3327">
<ORIGINAL_TEXT>He and others pointed to genetic evidence suggesting the virus made the leap from animal host to humans some time in the fall.</ORIGINAL_TEXT>
<TOKEN end_char="3328" id="token-19-0" morph="none" pos="word" start_char="3327">He</TOKEN>
<TOKEN end_char="3332" id="token-19-1" morph="none" pos="word" start_char="3330">and</TOKEN>
<TOKEN end_char="3339" id="token-19-2" morph="none" pos="word" start_char="3334">others</TOKEN>
<TOKEN end_char="3347" id="token-19-3" morph="none" pos="word" start_char="3341">pointed</TOKEN>
<TOKEN end_char="3350" id="token-19-4" morph="none" pos="word" start_char="3349">to</TOKEN>
<TOKEN end_char="3358" id="token-19-5" morph="none" pos="word" start_char="3352">genetic</TOKEN>
<TOKEN end_char="3367" id="token-19-6" morph="none" pos="word" start_char="3360">evidence</TOKEN>
<TOKEN end_char="3378" id="token-19-7" morph="none" pos="word" start_char="3369">suggesting</TOKEN>
<TOKEN end_char="3382" id="token-19-8" morph="none" pos="word" start_char="3380">the</TOKEN>
<TOKEN end_char="3388" id="token-19-9" morph="none" pos="word" start_char="3384">virus</TOKEN>
<TOKEN end_char="3393" id="token-19-10" morph="none" pos="word" start_char="3390">made</TOKEN>
<TOKEN end_char="3397" id="token-19-11" morph="none" pos="word" start_char="3395">the</TOKEN>
<TOKEN end_char="3402" id="token-19-12" morph="none" pos="word" start_char="3399">leap</TOKEN>
<TOKEN end_char="3407" id="token-19-13" morph="none" pos="word" start_char="3404">from</TOKEN>
<TOKEN end_char="3414" id="token-19-14" morph="none" pos="word" start_char="3409">animal</TOKEN>
<TOKEN end_char="3419" id="token-19-15" morph="none" pos="word" start_char="3416">host</TOKEN>
<TOKEN end_char="3422" id="token-19-16" morph="none" pos="word" start_char="3421">to</TOKEN>
<TOKEN end_char="3429" id="token-19-17" morph="none" pos="word" start_char="3424">humans</TOKEN>
<TOKEN end_char="3434" id="token-19-18" morph="none" pos="word" start_char="3431">some</TOKEN>
<TOKEN end_char="3439" id="token-19-19" morph="none" pos="word" start_char="3436">time</TOKEN>
<TOKEN end_char="3442" id="token-19-20" morph="none" pos="word" start_char="3441">in</TOKEN>
<TOKEN end_char="3446" id="token-19-21" morph="none" pos="word" start_char="3444">the</TOKEN>
<TOKEN end_char="3451" id="token-19-22" morph="none" pos="word" start_char="3448">fall</TOKEN>
<TOKEN end_char="3452" id="token-19-23" morph="none" pos="punct" start_char="3452">.</TOKEN>
</SEG>
<SEG end_char="3627" id="segment-20" start_char="3455">
<ORIGINAL_TEXT>"I don’t know about the August start," said Dr. Amesh Adalja, an infectious disease expert at the Johns Hopkins Center for Health Security who was not involved in the study.</ORIGINAL_TEXT>
<TOKEN end_char="3455" id="token-20-0" morph="none" pos="punct" start_char="3455">"</TOKEN>
<TOKEN end_char="3456" id="token-20-1" morph="none" pos="word" start_char="3456">I</TOKEN>
<TOKEN end_char="3462" id="token-20-2" morph="none" pos="word" start_char="3458">don’t</TOKEN>
<TOKEN end_char="3467" id="token-20-3" morph="none" pos="word" start_char="3464">know</TOKEN>
<TOKEN end_char="3473" id="token-20-4" morph="none" pos="word" start_char="3469">about</TOKEN>
<TOKEN end_char="3477" id="token-20-5" morph="none" pos="word" start_char="3475">the</TOKEN>
<TOKEN end_char="3484" id="token-20-6" morph="none" pos="word" start_char="3479">August</TOKEN>
<TOKEN end_char="3490" id="token-20-7" morph="none" pos="word" start_char="3486">start</TOKEN>
<TOKEN end_char="3492" id="token-20-8" morph="none" pos="punct" start_char="3491">,"</TOKEN>
<TOKEN end_char="3497" id="token-20-9" morph="none" pos="word" start_char="3494">said</TOKEN>
<TOKEN end_char="3500" id="token-20-10" morph="none" pos="word" start_char="3499">Dr</TOKEN>
<TOKEN end_char="3501" id="token-20-11" morph="none" pos="punct" start_char="3501">.</TOKEN>
<TOKEN end_char="3507" id="token-20-12" morph="none" pos="word" start_char="3503">Amesh</TOKEN>
<TOKEN end_char="3514" id="token-20-13" morph="none" pos="word" start_char="3509">Adalja</TOKEN>
<TOKEN end_char="3515" id="token-20-14" morph="none" pos="punct" start_char="3515">,</TOKEN>
<TOKEN end_char="3518" id="token-20-15" morph="none" pos="word" start_char="3517">an</TOKEN>
<TOKEN end_char="3529" id="token-20-16" morph="none" pos="word" start_char="3520">infectious</TOKEN>
<TOKEN end_char="3537" id="token-20-17" morph="none" pos="word" start_char="3531">disease</TOKEN>
<TOKEN end_char="3544" id="token-20-18" morph="none" pos="word" start_char="3539">expert</TOKEN>
<TOKEN end_char="3547" id="token-20-19" morph="none" pos="word" start_char="3546">at</TOKEN>
<TOKEN end_char="3551" id="token-20-20" morph="none" pos="word" start_char="3549">the</TOKEN>
<TOKEN end_char="3557" id="token-20-21" morph="none" pos="word" start_char="3553">Johns</TOKEN>
<TOKEN end_char="3565" id="token-20-22" morph="none" pos="word" start_char="3559">Hopkins</TOKEN>
<TOKEN end_char="3572" id="token-20-23" morph="none" pos="word" start_char="3567">Center</TOKEN>
<TOKEN end_char="3576" id="token-20-24" morph="none" pos="word" start_char="3574">for</TOKEN>
<TOKEN end_char="3583" id="token-20-25" morph="none" pos="word" start_char="3578">Health</TOKEN>
<TOKEN end_char="3592" id="token-20-26" morph="none" pos="word" start_char="3585">Security</TOKEN>
<TOKEN end_char="3596" id="token-20-27" morph="none" pos="word" start_char="3594">who</TOKEN>
<TOKEN end_char="3600" id="token-20-28" morph="none" pos="word" start_char="3598">was</TOKEN>
<TOKEN end_char="3604" id="token-20-29" morph="none" pos="word" start_char="3602">not</TOKEN>
<TOKEN end_char="3613" id="token-20-30" morph="none" pos="word" start_char="3606">involved</TOKEN>
<TOKEN end_char="3616" id="token-20-31" morph="none" pos="word" start_char="3615">in</TOKEN>
<TOKEN end_char="3620" id="token-20-32" morph="none" pos="word" start_char="3618">the</TOKEN>
<TOKEN end_char="3626" id="token-20-33" morph="none" pos="word" start_char="3622">study</TOKEN>
<TOKEN end_char="3627" id="token-20-34" morph="none" pos="punct" start_char="3627">.</TOKEN>
</SEG>
<SEG end_char="3763" id="segment-21" start_char="3630">
<ORIGINAL_TEXT>"It’s clear this had been spreading for some time before it was recognized and reported to the World Health Organization in December."</ORIGINAL_TEXT>
<TOKEN end_char="3630" id="token-21-0" morph="none" pos="punct" start_char="3630">"</TOKEN>
<TOKEN end_char="3634" id="token-21-1" morph="none" pos="word" start_char="3631">It’s</TOKEN>
<TOKEN end_char="3640" id="token-21-2" morph="none" pos="word" start_char="3636">clear</TOKEN>
<TOKEN end_char="3645" id="token-21-3" morph="none" pos="word" start_char="3642">this</TOKEN>
<TOKEN end_char="3649" id="token-21-4" morph="none" pos="word" start_char="3647">had</TOKEN>
<TOKEN end_char="3654" id="token-21-5" morph="none" pos="word" start_char="3651">been</TOKEN>
<TOKEN end_char="3664" id="token-21-6" morph="none" pos="word" start_char="3656">spreading</TOKEN>
<TOKEN end_char="3668" id="token-21-7" morph="none" pos="word" start_char="3666">for</TOKEN>
<TOKEN end_char="3673" id="token-21-8" morph="none" pos="word" start_char="3670">some</TOKEN>
<TOKEN end_char="3678" id="token-21-9" morph="none" pos="word" start_char="3675">time</TOKEN>
<TOKEN end_char="3685" id="token-21-10" morph="none" pos="word" start_char="3680">before</TOKEN>
<TOKEN end_char="3688" id="token-21-11" morph="none" pos="word" start_char="3687">it</TOKEN>
<TOKEN end_char="3692" id="token-21-12" morph="none" pos="word" start_char="3690">was</TOKEN>
<TOKEN end_char="3703" id="token-21-13" morph="none" pos="word" start_char="3694">recognized</TOKEN>
<TOKEN end_char="3707" id="token-21-14" morph="none" pos="word" start_char="3705">and</TOKEN>
<TOKEN end_char="3716" id="token-21-15" morph="none" pos="word" start_char="3709">reported</TOKEN>
<TOKEN end_char="3719" id="token-21-16" morph="none" pos="word" start_char="3718">to</TOKEN>
<TOKEN end_char="3723" id="token-21-17" morph="none" pos="word" start_char="3721">the</TOKEN>
<TOKEN end_char="3729" id="token-21-18" morph="none" pos="word" start_char="3725">World</TOKEN>
<TOKEN end_char="3736" id="token-21-19" morph="none" pos="word" start_char="3731">Health</TOKEN>
<TOKEN end_char="3749" id="token-21-20" morph="none" pos="word" start_char="3738">Organization</TOKEN>
<TOKEN end_char="3752" id="token-21-21" morph="none" pos="word" start_char="3751">in</TOKEN>
<TOKEN end_char="3761" id="token-21-22" morph="none" pos="word" start_char="3754">December</TOKEN>
<TOKEN end_char="3763" id="token-21-23" morph="none" pos="punct" start_char="3762">."</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>