<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CABD" lang="spa" raw_text_char_length="2945" raw_text_md5="366d6e7c3947120815104caf9adfb436" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="81" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Coronavirus circulated for months before first case was reported: modelling study</ORIGINAL_TEXT>
<TOKEN end_char="11" id="token-0-0" morph="none" pos="word" start_char="1">Coronavirus</TOKEN>
<TOKEN end_char="22" id="token-0-1" morph="none" pos="word" start_char="13">circulated</TOKEN>
<TOKEN end_char="26" id="token-0-2" morph="none" pos="word" start_char="24">for</TOKEN>
<TOKEN end_char="33" id="token-0-3" morph="none" pos="word" start_char="28">months</TOKEN>
<TOKEN end_char="40" id="token-0-4" morph="none" pos="word" start_char="35">before</TOKEN>
<TOKEN end_char="46" id="token-0-5" morph="none" pos="word" start_char="42">first</TOKEN>
<TOKEN end_char="51" id="token-0-6" morph="none" pos="word" start_char="48">case</TOKEN>
<TOKEN end_char="55" id="token-0-7" morph="none" pos="word" start_char="53">was</TOKEN>
<TOKEN end_char="64" id="token-0-8" morph="none" pos="word" start_char="57">reported</TOKEN>
<TOKEN end_char="65" id="token-0-9" morph="none" pos="punct" start_char="65">:</TOKEN>
<TOKEN end_char="75" id="token-0-10" morph="none" pos="word" start_char="67">modelling</TOKEN>
<TOKEN end_char="81" id="token-0-11" morph="none" pos="word" start_char="77">study</TOKEN>
</SEG>
<SEG end_char="204" id="segment-1" start_char="85">
<ORIGINAL_TEXT>A woman wearing a face mask to protect against the coronavirus uses an exercise machine at a public park in Beijing (AP)</ORIGINAL_TEXT>
<TOKEN end_char="85" id="token-1-0" morph="none" pos="word" start_char="85">A</TOKEN>
<TOKEN end_char="91" id="token-1-1" morph="none" pos="word" start_char="87">woman</TOKEN>
<TOKEN end_char="99" id="token-1-2" morph="none" pos="word" start_char="93">wearing</TOKEN>
<TOKEN end_char="101" id="token-1-3" morph="none" pos="word" start_char="101">a</TOKEN>
<TOKEN end_char="106" id="token-1-4" morph="none" pos="word" start_char="103">face</TOKEN>
<TOKEN end_char="111" id="token-1-5" morph="none" pos="word" start_char="108">mask</TOKEN>
<TOKEN end_char="114" id="token-1-6" morph="none" pos="word" start_char="113">to</TOKEN>
<TOKEN end_char="122" id="token-1-7" morph="none" pos="word" start_char="116">protect</TOKEN>
<TOKEN end_char="130" id="token-1-8" morph="none" pos="word" start_char="124">against</TOKEN>
<TOKEN end_char="134" id="token-1-9" morph="none" pos="word" start_char="132">the</TOKEN>
<TOKEN end_char="146" id="token-1-10" morph="none" pos="word" start_char="136">coronavirus</TOKEN>
<TOKEN end_char="151" id="token-1-11" morph="none" pos="word" start_char="148">uses</TOKEN>
<TOKEN end_char="154" id="token-1-12" morph="none" pos="word" start_char="153">an</TOKEN>
<TOKEN end_char="163" id="token-1-13" morph="none" pos="word" start_char="156">exercise</TOKEN>
<TOKEN end_char="171" id="token-1-14" morph="none" pos="word" start_char="165">machine</TOKEN>
<TOKEN end_char="174" id="token-1-15" morph="none" pos="word" start_char="173">at</TOKEN>
<TOKEN end_char="176" id="token-1-16" morph="none" pos="word" start_char="176">a</TOKEN>
<TOKEN end_char="183" id="token-1-17" morph="none" pos="word" start_char="178">public</TOKEN>
<TOKEN end_char="188" id="token-1-18" morph="none" pos="word" start_char="185">park</TOKEN>
<TOKEN end_char="191" id="token-1-19" morph="none" pos="word" start_char="190">in</TOKEN>
<TOKEN end_char="199" id="token-1-20" morph="none" pos="word" start_char="193">Beijing</TOKEN>
<TOKEN end_char="201" id="token-1-21" morph="none" pos="punct" start_char="201">(</TOKEN>
<TOKEN end_char="203" id="token-1-22" morph="none" pos="word" start_char="202">AP</TOKEN>
<TOKEN end_char="204" id="token-1-23" morph="none" pos="punct" start_char="204">)</TOKEN>
</SEG>
<SEG end_char="473" id="segment-2" start_char="208">
<ORIGINAL_TEXT>Using molecular dating tools and epidemiological simulations, researchers have estimated that the SARS-CoV-2 virus was likely circulating undetected for at most two months before the first human cases of Covid-19 were described in Wuhan, China in late December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="212" id="token-2-0" morph="none" pos="word" start_char="208">Using</TOKEN>
<TOKEN end_char="222" id="token-2-1" morph="none" pos="word" start_char="214">molecular</TOKEN>
<TOKEN end_char="229" id="token-2-2" morph="none" pos="word" start_char="224">dating</TOKEN>
<TOKEN end_char="235" id="token-2-3" morph="none" pos="word" start_char="231">tools</TOKEN>
<TOKEN end_char="239" id="token-2-4" morph="none" pos="word" start_char="237">and</TOKEN>
<TOKEN end_char="255" id="token-2-5" morph="none" pos="word" start_char="241">epidemiological</TOKEN>
<TOKEN end_char="267" id="token-2-6" morph="none" pos="word" start_char="257">simulations</TOKEN>
<TOKEN end_char="268" id="token-2-7" morph="none" pos="punct" start_char="268">,</TOKEN>
<TOKEN end_char="280" id="token-2-8" morph="none" pos="word" start_char="270">researchers</TOKEN>
<TOKEN end_char="285" id="token-2-9" morph="none" pos="word" start_char="282">have</TOKEN>
<TOKEN end_char="295" id="token-2-10" morph="none" pos="word" start_char="287">estimated</TOKEN>
<TOKEN end_char="300" id="token-2-11" morph="none" pos="word" start_char="297">that</TOKEN>
<TOKEN end_char="304" id="token-2-12" morph="none" pos="word" start_char="302">the</TOKEN>
<TOKEN end_char="315" id="token-2-13" morph="none" pos="unknown" start_char="306">SARS-CoV-2</TOKEN>
<TOKEN end_char="321" id="token-2-14" morph="none" pos="word" start_char="317">virus</TOKEN>
<TOKEN end_char="325" id="token-2-15" morph="none" pos="word" start_char="323">was</TOKEN>
<TOKEN end_char="332" id="token-2-16" morph="none" pos="word" start_char="327">likely</TOKEN>
<TOKEN end_char="344" id="token-2-17" morph="none" pos="word" start_char="334">circulating</TOKEN>
<TOKEN end_char="355" id="token-2-18" morph="none" pos="word" start_char="346">undetected</TOKEN>
<TOKEN end_char="359" id="token-2-19" morph="none" pos="word" start_char="357">for</TOKEN>
<TOKEN end_char="362" id="token-2-20" morph="none" pos="word" start_char="361">at</TOKEN>
<TOKEN end_char="367" id="token-2-21" morph="none" pos="word" start_char="364">most</TOKEN>
<TOKEN end_char="371" id="token-2-22" morph="none" pos="word" start_char="369">two</TOKEN>
<TOKEN end_char="378" id="token-2-23" morph="none" pos="word" start_char="373">months</TOKEN>
<TOKEN end_char="385" id="token-2-24" morph="none" pos="word" start_char="380">before</TOKEN>
<TOKEN end_char="389" id="token-2-25" morph="none" pos="word" start_char="387">the</TOKEN>
<TOKEN end_char="395" id="token-2-26" morph="none" pos="word" start_char="391">first</TOKEN>
<TOKEN end_char="401" id="token-2-27" morph="none" pos="word" start_char="397">human</TOKEN>
<TOKEN end_char="407" id="token-2-28" morph="none" pos="word" start_char="403">cases</TOKEN>
<TOKEN end_char="410" id="token-2-29" morph="none" pos="word" start_char="409">of</TOKEN>
<TOKEN end_char="419" id="token-2-30" morph="none" pos="unknown" start_char="412">Covid-19</TOKEN>
<TOKEN end_char="424" id="token-2-31" morph="none" pos="word" start_char="421">were</TOKEN>
<TOKEN end_char="434" id="token-2-32" morph="none" pos="word" start_char="426">described</TOKEN>
<TOKEN end_char="437" id="token-2-33" morph="none" pos="word" start_char="436">in</TOKEN>
<TOKEN end_char="443" id="token-2-34" morph="none" pos="word" start_char="439">Wuhan</TOKEN>
<TOKEN end_char="444" id="token-2-35" morph="none" pos="punct" start_char="444">,</TOKEN>
<TOKEN end_char="450" id="token-2-36" morph="none" pos="word" start_char="446">China</TOKEN>
<TOKEN end_char="453" id="token-2-37" morph="none" pos="word" start_char="452">in</TOKEN>
<TOKEN end_char="458" id="token-2-38" morph="none" pos="word" start_char="455">late</TOKEN>
<TOKEN end_char="467" id="token-2-39" morph="none" pos="word" start_char="460">December</TOKEN>
<TOKEN end_char="472" id="token-2-40" morph="none" pos="word" start_char="469">2019</TOKEN>
<TOKEN end_char="473" id="token-2-41" morph="none" pos="punct" start_char="473">.</TOKEN>
</SEG>
<SEG end_char="672" id="segment-3" start_char="475">
<ORIGINAL_TEXT>In a study published in Science, the researchers also note that their simulations suggest that the mutating virus dies out naturally more than three-quarters of the time without causing an epidemic.</ORIGINAL_TEXT>
<TOKEN end_char="476" id="token-3-0" morph="none" pos="word" start_char="475">In</TOKEN>
<TOKEN end_char="478" id="token-3-1" morph="none" pos="word" start_char="478">a</TOKEN>
<TOKEN end_char="484" id="token-3-2" morph="none" pos="word" start_char="480">study</TOKEN>
<TOKEN end_char="494" id="token-3-3" morph="none" pos="word" start_char="486">published</TOKEN>
<TOKEN end_char="497" id="token-3-4" morph="none" pos="word" start_char="496">in</TOKEN>
<TOKEN end_char="505" id="token-3-5" morph="none" pos="word" start_char="499">Science</TOKEN>
<TOKEN end_char="506" id="token-3-6" morph="none" pos="punct" start_char="506">,</TOKEN>
<TOKEN end_char="510" id="token-3-7" morph="none" pos="word" start_char="508">the</TOKEN>
<TOKEN end_char="522" id="token-3-8" morph="none" pos="word" start_char="512">researchers</TOKEN>
<TOKEN end_char="527" id="token-3-9" morph="none" pos="word" start_char="524">also</TOKEN>
<TOKEN end_char="532" id="token-3-10" morph="none" pos="word" start_char="529">note</TOKEN>
<TOKEN end_char="537" id="token-3-11" morph="none" pos="word" start_char="534">that</TOKEN>
<TOKEN end_char="543" id="token-3-12" morph="none" pos="word" start_char="539">their</TOKEN>
<TOKEN end_char="555" id="token-3-13" morph="none" pos="word" start_char="545">simulations</TOKEN>
<TOKEN end_char="563" id="token-3-14" morph="none" pos="word" start_char="557">suggest</TOKEN>
<TOKEN end_char="568" id="token-3-15" morph="none" pos="word" start_char="565">that</TOKEN>
<TOKEN end_char="572" id="token-3-16" morph="none" pos="word" start_char="570">the</TOKEN>
<TOKEN end_char="581" id="token-3-17" morph="none" pos="word" start_char="574">mutating</TOKEN>
<TOKEN end_char="587" id="token-3-18" morph="none" pos="word" start_char="583">virus</TOKEN>
<TOKEN end_char="592" id="token-3-19" morph="none" pos="word" start_char="589">dies</TOKEN>
<TOKEN end_char="596" id="token-3-20" morph="none" pos="word" start_char="594">out</TOKEN>
<TOKEN end_char="606" id="token-3-21" morph="none" pos="word" start_char="598">naturally</TOKEN>
<TOKEN end_char="611" id="token-3-22" morph="none" pos="word" start_char="608">more</TOKEN>
<TOKEN end_char="616" id="token-3-23" morph="none" pos="word" start_char="613">than</TOKEN>
<TOKEN end_char="631" id="token-3-24" morph="none" pos="unknown" start_char="618">three-quarters</TOKEN>
<TOKEN end_char="634" id="token-3-25" morph="none" pos="word" start_char="633">of</TOKEN>
<TOKEN end_char="638" id="token-3-26" morph="none" pos="word" start_char="636">the</TOKEN>
<TOKEN end_char="643" id="token-3-27" morph="none" pos="word" start_char="640">time</TOKEN>
<TOKEN end_char="651" id="token-3-28" morph="none" pos="word" start_char="645">without</TOKEN>
<TOKEN end_char="659" id="token-3-29" morph="none" pos="word" start_char="653">causing</TOKEN>
<TOKEN end_char="662" id="token-3-30" morph="none" pos="word" start_char="661">an</TOKEN>
<TOKEN end_char="671" id="token-3-31" morph="none" pos="word" start_char="664">epidemic</TOKEN>
<TOKEN end_char="672" id="token-3-32" morph="none" pos="punct" start_char="672">.</TOKEN>
</SEG>
<SEG end_char="824" id="segment-4" start_char="675">
<ORIGINAL_TEXT>The study was conducted by researchers from University of California San Diego (UCSD) School of Medicine, the University of Arizona, and Illumina Inc.</ORIGINAL_TEXT>
<TOKEN end_char="677" id="token-4-0" morph="none" pos="word" start_char="675">The</TOKEN>
<TOKEN end_char="683" id="token-4-1" morph="none" pos="word" start_char="679">study</TOKEN>
<TOKEN end_char="687" id="token-4-2" morph="none" pos="word" start_char="685">was</TOKEN>
<TOKEN end_char="697" id="token-4-3" morph="none" pos="word" start_char="689">conducted</TOKEN>
<TOKEN end_char="700" id="token-4-4" morph="none" pos="word" start_char="699">by</TOKEN>
<TOKEN end_char="712" id="token-4-5" morph="none" pos="word" start_char="702">researchers</TOKEN>
<TOKEN end_char="717" id="token-4-6" morph="none" pos="word" start_char="714">from</TOKEN>
<TOKEN end_char="728" id="token-4-7" morph="none" pos="word" start_char="719">University</TOKEN>
<TOKEN end_char="731" id="token-4-8" morph="none" pos="word" start_char="730">of</TOKEN>
<TOKEN end_char="742" id="token-4-9" morph="none" pos="word" start_char="733">California</TOKEN>
<TOKEN end_char="746" id="token-4-10" morph="none" pos="word" start_char="744">San</TOKEN>
<TOKEN end_char="752" id="token-4-11" morph="none" pos="word" start_char="748">Diego</TOKEN>
<TOKEN end_char="754" id="token-4-12" morph="none" pos="punct" start_char="754">(</TOKEN>
<TOKEN end_char="758" id="token-4-13" morph="none" pos="word" start_char="755">UCSD</TOKEN>
<TOKEN end_char="759" id="token-4-14" morph="none" pos="punct" start_char="759">)</TOKEN>
<TOKEN end_char="766" id="token-4-15" morph="none" pos="word" start_char="761">School</TOKEN>
<TOKEN end_char="769" id="token-4-16" morph="none" pos="word" start_char="768">of</TOKEN>
<TOKEN end_char="778" id="token-4-17" morph="none" pos="word" start_char="771">Medicine</TOKEN>
<TOKEN end_char="779" id="token-4-18" morph="none" pos="punct" start_char="779">,</TOKEN>
<TOKEN end_char="783" id="token-4-19" morph="none" pos="word" start_char="781">the</TOKEN>
<TOKEN end_char="794" id="token-4-20" morph="none" pos="word" start_char="785">University</TOKEN>
<TOKEN end_char="797" id="token-4-21" morph="none" pos="word" start_char="796">of</TOKEN>
<TOKEN end_char="805" id="token-4-22" morph="none" pos="word" start_char="799">Arizona</TOKEN>
<TOKEN end_char="806" id="token-4-23" morph="none" pos="punct" start_char="806">,</TOKEN>
<TOKEN end_char="810" id="token-4-24" morph="none" pos="word" start_char="808">and</TOKEN>
<TOKEN end_char="819" id="token-4-25" morph="none" pos="word" start_char="812">Illumina</TOKEN>
<TOKEN end_char="823" id="token-4-26" morph="none" pos="word" start_char="821">Inc</TOKEN>
<TOKEN end_char="824" id="token-4-27" morph="none" pos="punct" start_char="824">.</TOKEN>
</SEG>
<SEG end_char="951" id="segment-5" start_char="826">
<ORIGINAL_TEXT>"Our study was designed to answer the question of how long could SARS-CoV-2 have circulated in China before it was discovered.</ORIGINAL_TEXT>
<TOKEN end_char="826" id="token-5-0" morph="none" pos="punct" start_char="826">"</TOKEN>
<TOKEN end_char="829" id="token-5-1" morph="none" pos="word" start_char="827">Our</TOKEN>
<TOKEN end_char="835" id="token-5-2" morph="none" pos="word" start_char="831">study</TOKEN>
<TOKEN end_char="839" id="token-5-3" morph="none" pos="word" start_char="837">was</TOKEN>
<TOKEN end_char="848" id="token-5-4" morph="none" pos="word" start_char="841">designed</TOKEN>
<TOKEN end_char="851" id="token-5-5" morph="none" pos="word" start_char="850">to</TOKEN>
<TOKEN end_char="858" id="token-5-6" morph="none" pos="word" start_char="853">answer</TOKEN>
<TOKEN end_char="862" id="token-5-7" morph="none" pos="word" start_char="860">the</TOKEN>
<TOKEN end_char="871" id="token-5-8" morph="none" pos="word" start_char="864">question</TOKEN>
<TOKEN end_char="874" id="token-5-9" morph="none" pos="word" start_char="873">of</TOKEN>
<TOKEN end_char="878" id="token-5-10" morph="none" pos="word" start_char="876">how</TOKEN>
<TOKEN end_char="883" id="token-5-11" morph="none" pos="word" start_char="880">long</TOKEN>
<TOKEN end_char="889" id="token-5-12" morph="none" pos="word" start_char="885">could</TOKEN>
<TOKEN end_char="900" id="token-5-13" morph="none" pos="unknown" start_char="891">SARS-CoV-2</TOKEN>
<TOKEN end_char="905" id="token-5-14" morph="none" pos="word" start_char="902">have</TOKEN>
<TOKEN end_char="916" id="token-5-15" morph="none" pos="word" start_char="907">circulated</TOKEN>
<TOKEN end_char="919" id="token-5-16" morph="none" pos="word" start_char="918">in</TOKEN>
<TOKEN end_char="925" id="token-5-17" morph="none" pos="word" start_char="921">China</TOKEN>
<TOKEN end_char="932" id="token-5-18" morph="none" pos="word" start_char="927">before</TOKEN>
<TOKEN end_char="935" id="token-5-19" morph="none" pos="word" start_char="934">it</TOKEN>
<TOKEN end_char="939" id="token-5-20" morph="none" pos="word" start_char="937">was</TOKEN>
<TOKEN end_char="950" id="token-5-21" morph="none" pos="word" start_char="941">discovered</TOKEN>
<TOKEN end_char="951" id="token-5-22" morph="none" pos="punct" start_char="951">.</TOKEN>
</SEG>
<SEG end_char="1208" id="segment-6" start_char="953">
<ORIGINAL_TEXT>To answer this question, we combined three important pieces of information: a detailed understanding of how SARS-CoV-2 spread in Wuhan before the lockdown, the genetic diversity of the virus in China, and reports of the earliest cases of Covid-19 in China.</ORIGINAL_TEXT>
<TOKEN end_char="954" id="token-6-0" morph="none" pos="word" start_char="953">To</TOKEN>
<TOKEN end_char="961" id="token-6-1" morph="none" pos="word" start_char="956">answer</TOKEN>
<TOKEN end_char="966" id="token-6-2" morph="none" pos="word" start_char="963">this</TOKEN>
<TOKEN end_char="975" id="token-6-3" morph="none" pos="word" start_char="968">question</TOKEN>
<TOKEN end_char="976" id="token-6-4" morph="none" pos="punct" start_char="976">,</TOKEN>
<TOKEN end_char="979" id="token-6-5" morph="none" pos="word" start_char="978">we</TOKEN>
<TOKEN end_char="988" id="token-6-6" morph="none" pos="word" start_char="981">combined</TOKEN>
<TOKEN end_char="994" id="token-6-7" morph="none" pos="word" start_char="990">three</TOKEN>
<TOKEN end_char="1004" id="token-6-8" morph="none" pos="word" start_char="996">important</TOKEN>
<TOKEN end_char="1011" id="token-6-9" morph="none" pos="word" start_char="1006">pieces</TOKEN>
<TOKEN end_char="1014" id="token-6-10" morph="none" pos="word" start_char="1013">of</TOKEN>
<TOKEN end_char="1026" id="token-6-11" morph="none" pos="word" start_char="1016">information</TOKEN>
<TOKEN end_char="1027" id="token-6-12" morph="none" pos="punct" start_char="1027">:</TOKEN>
<TOKEN end_char="1029" id="token-6-13" morph="none" pos="word" start_char="1029">a</TOKEN>
<TOKEN end_char="1038" id="token-6-14" morph="none" pos="word" start_char="1031">detailed</TOKEN>
<TOKEN end_char="1052" id="token-6-15" morph="none" pos="word" start_char="1040">understanding</TOKEN>
<TOKEN end_char="1055" id="token-6-16" morph="none" pos="word" start_char="1054">of</TOKEN>
<TOKEN end_char="1059" id="token-6-17" morph="none" pos="word" start_char="1057">how</TOKEN>
<TOKEN end_char="1070" id="token-6-18" morph="none" pos="unknown" start_char="1061">SARS-CoV-2</TOKEN>
<TOKEN end_char="1077" id="token-6-19" morph="none" pos="word" start_char="1072">spread</TOKEN>
<TOKEN end_char="1080" id="token-6-20" morph="none" pos="word" start_char="1079">in</TOKEN>
<TOKEN end_char="1086" id="token-6-21" morph="none" pos="word" start_char="1082">Wuhan</TOKEN>
<TOKEN end_char="1093" id="token-6-22" morph="none" pos="word" start_char="1088">before</TOKEN>
<TOKEN end_char="1097" id="token-6-23" morph="none" pos="word" start_char="1095">the</TOKEN>
<TOKEN end_char="1106" id="token-6-24" morph="none" pos="word" start_char="1099">lockdown</TOKEN>
<TOKEN end_char="1107" id="token-6-25" morph="none" pos="punct" start_char="1107">,</TOKEN>
<TOKEN end_char="1111" id="token-6-26" morph="none" pos="word" start_char="1109">the</TOKEN>
<TOKEN end_char="1119" id="token-6-27" morph="none" pos="word" start_char="1113">genetic</TOKEN>
<TOKEN end_char="1129" id="token-6-28" morph="none" pos="word" start_char="1121">diversity</TOKEN>
<TOKEN end_char="1132" id="token-6-29" morph="none" pos="word" start_char="1131">of</TOKEN>
<TOKEN end_char="1136" id="token-6-30" morph="none" pos="word" start_char="1134">the</TOKEN>
<TOKEN end_char="1142" id="token-6-31" morph="none" pos="word" start_char="1138">virus</TOKEN>
<TOKEN end_char="1145" id="token-6-32" morph="none" pos="word" start_char="1144">in</TOKEN>
<TOKEN end_char="1151" id="token-6-33" morph="none" pos="word" start_char="1147">China</TOKEN>
<TOKEN end_char="1152" id="token-6-34" morph="none" pos="punct" start_char="1152">,</TOKEN>
<TOKEN end_char="1156" id="token-6-35" morph="none" pos="word" start_char="1154">and</TOKEN>
<TOKEN end_char="1164" id="token-6-36" morph="none" pos="word" start_char="1158">reports</TOKEN>
<TOKEN end_char="1167" id="token-6-37" morph="none" pos="word" start_char="1166">of</TOKEN>
<TOKEN end_char="1171" id="token-6-38" morph="none" pos="word" start_char="1169">the</TOKEN>
<TOKEN end_char="1180" id="token-6-39" morph="none" pos="word" start_char="1173">earliest</TOKEN>
<TOKEN end_char="1186" id="token-6-40" morph="none" pos="word" start_char="1182">cases</TOKEN>
<TOKEN end_char="1189" id="token-6-41" morph="none" pos="word" start_char="1188">of</TOKEN>
<TOKEN end_char="1198" id="token-6-42" morph="none" pos="unknown" start_char="1191">Covid-19</TOKEN>
<TOKEN end_char="1201" id="token-6-43" morph="none" pos="word" start_char="1200">in</TOKEN>
<TOKEN end_char="1207" id="token-6-44" morph="none" pos="word" start_char="1203">China</TOKEN>
<TOKEN end_char="1208" id="token-6-45" morph="none" pos="punct" start_char="1208">.</TOKEN>
</SEG>
<SEG end_char="1451" id="segment-7" start_char="1210">
<ORIGINAL_TEXT>By combining these disparate lines of evidence, we were able to put an upper limit of mid-October 2019 for when SARS-CoV-2 started circulating in Hubei province," senior author Joel O Wertheim said in a statement from UCSD School of Medicine.</ORIGINAL_TEXT>
<TOKEN end_char="1211" id="token-7-0" morph="none" pos="word" start_char="1210">By</TOKEN>
<TOKEN end_char="1221" id="token-7-1" morph="none" pos="word" start_char="1213">combining</TOKEN>
<TOKEN end_char="1227" id="token-7-2" morph="none" pos="word" start_char="1223">these</TOKEN>
<TOKEN end_char="1237" id="token-7-3" morph="none" pos="word" start_char="1229">disparate</TOKEN>
<TOKEN end_char="1243" id="token-7-4" morph="none" pos="word" start_char="1239">lines</TOKEN>
<TOKEN end_char="1246" id="token-7-5" morph="none" pos="word" start_char="1245">of</TOKEN>
<TOKEN end_char="1255" id="token-7-6" morph="none" pos="word" start_char="1248">evidence</TOKEN>
<TOKEN end_char="1256" id="token-7-7" morph="none" pos="punct" start_char="1256">,</TOKEN>
<TOKEN end_char="1259" id="token-7-8" morph="none" pos="word" start_char="1258">we</TOKEN>
<TOKEN end_char="1264" id="token-7-9" morph="none" pos="word" start_char="1261">were</TOKEN>
<TOKEN end_char="1269" id="token-7-10" morph="none" pos="word" start_char="1266">able</TOKEN>
<TOKEN end_char="1272" id="token-7-11" morph="none" pos="word" start_char="1271">to</TOKEN>
<TOKEN end_char="1276" id="token-7-12" morph="none" pos="word" start_char="1274">put</TOKEN>
<TOKEN end_char="1279" id="token-7-13" morph="none" pos="word" start_char="1278">an</TOKEN>
<TOKEN end_char="1285" id="token-7-14" morph="none" pos="word" start_char="1281">upper</TOKEN>
<TOKEN end_char="1291" id="token-7-15" morph="none" pos="word" start_char="1287">limit</TOKEN>
<TOKEN end_char="1294" id="token-7-16" morph="none" pos="word" start_char="1293">of</TOKEN>
<TOKEN end_char="1306" id="token-7-17" morph="none" pos="unknown" start_char="1296">mid-October</TOKEN>
<TOKEN end_char="1311" id="token-7-18" morph="none" pos="word" start_char="1308">2019</TOKEN>
<TOKEN end_char="1315" id="token-7-19" morph="none" pos="word" start_char="1313">for</TOKEN>
<TOKEN end_char="1320" id="token-7-20" morph="none" pos="word" start_char="1317">when</TOKEN>
<TOKEN end_char="1331" id="token-7-21" morph="none" pos="unknown" start_char="1322">SARS-CoV-2</TOKEN>
<TOKEN end_char="1339" id="token-7-22" morph="none" pos="word" start_char="1333">started</TOKEN>
<TOKEN end_char="1351" id="token-7-23" morph="none" pos="word" start_char="1341">circulating</TOKEN>
<TOKEN end_char="1354" id="token-7-24" morph="none" pos="word" start_char="1353">in</TOKEN>
<TOKEN end_char="1360" id="token-7-25" morph="none" pos="word" start_char="1356">Hubei</TOKEN>
<TOKEN end_char="1369" id="token-7-26" morph="none" pos="word" start_char="1362">province</TOKEN>
<TOKEN end_char="1371" id="token-7-27" morph="none" pos="punct" start_char="1370">,"</TOKEN>
<TOKEN end_char="1378" id="token-7-28" morph="none" pos="word" start_char="1373">senior</TOKEN>
<TOKEN end_char="1385" id="token-7-29" morph="none" pos="word" start_char="1380">author</TOKEN>
<TOKEN end_char="1390" id="token-7-30" morph="none" pos="word" start_char="1387">Joel</TOKEN>
<TOKEN end_char="1392" id="token-7-31" morph="none" pos="word" start_char="1392">O</TOKEN>
<TOKEN end_char="1401" id="token-7-32" morph="none" pos="word" start_char="1394">Wertheim</TOKEN>
<TOKEN end_char="1406" id="token-7-33" morph="none" pos="word" start_char="1403">said</TOKEN>
<TOKEN end_char="1409" id="token-7-34" morph="none" pos="word" start_char="1408">in</TOKEN>
<TOKEN end_char="1411" id="token-7-35" morph="none" pos="word" start_char="1411">a</TOKEN>
<TOKEN end_char="1421" id="token-7-36" morph="none" pos="word" start_char="1413">statement</TOKEN>
<TOKEN end_char="1426" id="token-7-37" morph="none" pos="word" start_char="1423">from</TOKEN>
<TOKEN end_char="1431" id="token-7-38" morph="none" pos="word" start_char="1428">UCSD</TOKEN>
<TOKEN end_char="1438" id="token-7-39" morph="none" pos="word" start_char="1433">School</TOKEN>
<TOKEN end_char="1441" id="token-7-40" morph="none" pos="word" start_char="1440">of</TOKEN>
<TOKEN end_char="1450" id="token-7-41" morph="none" pos="word" start_char="1443">Medicine</TOKEN>
<TOKEN end_char="1451" id="token-7-42" morph="none" pos="punct" start_char="1451">.</TOKEN>
</SEG>
<SEG end_char="1518" id="segment-8" start_char="1455">
<ORIGINAL_TEXT>Cases of Covid-19 were first reported in December 2019 in Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="1459" id="token-8-0" morph="none" pos="word" start_char="1455">Cases</TOKEN>
<TOKEN end_char="1462" id="token-8-1" morph="none" pos="word" start_char="1461">of</TOKEN>
<TOKEN end_char="1471" id="token-8-2" morph="none" pos="unknown" start_char="1464">Covid-19</TOKEN>
<TOKEN end_char="1476" id="token-8-3" morph="none" pos="word" start_char="1473">were</TOKEN>
<TOKEN end_char="1482" id="token-8-4" morph="none" pos="word" start_char="1478">first</TOKEN>
<TOKEN end_char="1491" id="token-8-5" morph="none" pos="word" start_char="1484">reported</TOKEN>
<TOKEN end_char="1494" id="token-8-6" morph="none" pos="word" start_char="1493">in</TOKEN>
<TOKEN end_char="1503" id="token-8-7" morph="none" pos="word" start_char="1496">December</TOKEN>
<TOKEN end_char="1508" id="token-8-8" morph="none" pos="word" start_char="1505">2019</TOKEN>
<TOKEN end_char="1511" id="token-8-9" morph="none" pos="word" start_char="1510">in</TOKEN>
<TOKEN end_char="1517" id="token-8-10" morph="none" pos="word" start_char="1513">Wuhan</TOKEN>
<TOKEN end_char="1518" id="token-8-11" morph="none" pos="punct" start_char="1518">.</TOKEN>
</SEG>
<SEG end_char="1774" id="segment-9" start_char="1520">
<ORIGINAL_TEXT>The UCSD statement also referred to regional newspaper reports that suggest Covid-19 diagnoses in Hubei date back to at least November 17, 2019, suggesting the virus was already actively circulating when Chinese authorities enacted public health measures.</ORIGINAL_TEXT>
<TOKEN end_char="1522" id="token-9-0" morph="none" pos="word" start_char="1520">The</TOKEN>
<TOKEN end_char="1527" id="token-9-1" morph="none" pos="word" start_char="1524">UCSD</TOKEN>
<TOKEN end_char="1537" id="token-9-2" morph="none" pos="word" start_char="1529">statement</TOKEN>
<TOKEN end_char="1542" id="token-9-3" morph="none" pos="word" start_char="1539">also</TOKEN>
<TOKEN end_char="1551" id="token-9-4" morph="none" pos="word" start_char="1544">referred</TOKEN>
<TOKEN end_char="1554" id="token-9-5" morph="none" pos="word" start_char="1553">to</TOKEN>
<TOKEN end_char="1563" id="token-9-6" morph="none" pos="word" start_char="1556">regional</TOKEN>
<TOKEN end_char="1573" id="token-9-7" morph="none" pos="word" start_char="1565">newspaper</TOKEN>
<TOKEN end_char="1581" id="token-9-8" morph="none" pos="word" start_char="1575">reports</TOKEN>
<TOKEN end_char="1586" id="token-9-9" morph="none" pos="word" start_char="1583">that</TOKEN>
<TOKEN end_char="1594" id="token-9-10" morph="none" pos="word" start_char="1588">suggest</TOKEN>
<TOKEN end_char="1603" id="token-9-11" morph="none" pos="unknown" start_char="1596">Covid-19</TOKEN>
<TOKEN end_char="1613" id="token-9-12" morph="none" pos="word" start_char="1605">diagnoses</TOKEN>
<TOKEN end_char="1616" id="token-9-13" morph="none" pos="word" start_char="1615">in</TOKEN>
<TOKEN end_char="1622" id="token-9-14" morph="none" pos="word" start_char="1618">Hubei</TOKEN>
<TOKEN end_char="1627" id="token-9-15" morph="none" pos="word" start_char="1624">date</TOKEN>
<TOKEN end_char="1632" id="token-9-16" morph="none" pos="word" start_char="1629">back</TOKEN>
<TOKEN end_char="1635" id="token-9-17" morph="none" pos="word" start_char="1634">to</TOKEN>
<TOKEN end_char="1638" id="token-9-18" morph="none" pos="word" start_char="1637">at</TOKEN>
<TOKEN end_char="1644" id="token-9-19" morph="none" pos="word" start_char="1640">least</TOKEN>
<TOKEN end_char="1653" id="token-9-20" morph="none" pos="word" start_char="1646">November</TOKEN>
<TOKEN end_char="1656" id="token-9-21" morph="none" pos="word" start_char="1655">17</TOKEN>
<TOKEN end_char="1657" id="token-9-22" morph="none" pos="punct" start_char="1657">,</TOKEN>
<TOKEN end_char="1662" id="token-9-23" morph="none" pos="word" start_char="1659">2019</TOKEN>
<TOKEN end_char="1663" id="token-9-24" morph="none" pos="punct" start_char="1663">,</TOKEN>
<TOKEN end_char="1674" id="token-9-25" morph="none" pos="word" start_char="1665">suggesting</TOKEN>
<TOKEN end_char="1678" id="token-9-26" morph="none" pos="word" start_char="1676">the</TOKEN>
<TOKEN end_char="1684" id="token-9-27" morph="none" pos="word" start_char="1680">virus</TOKEN>
<TOKEN end_char="1688" id="token-9-28" morph="none" pos="word" start_char="1686">was</TOKEN>
<TOKEN end_char="1696" id="token-9-29" morph="none" pos="word" start_char="1690">already</TOKEN>
<TOKEN end_char="1705" id="token-9-30" morph="none" pos="word" start_char="1698">actively</TOKEN>
<TOKEN end_char="1717" id="token-9-31" morph="none" pos="word" start_char="1707">circulating</TOKEN>
<TOKEN end_char="1722" id="token-9-32" morph="none" pos="word" start_char="1719">when</TOKEN>
<TOKEN end_char="1730" id="token-9-33" morph="none" pos="word" start_char="1724">Chinese</TOKEN>
<TOKEN end_char="1742" id="token-9-34" morph="none" pos="word" start_char="1732">authorities</TOKEN>
<TOKEN end_char="1750" id="token-9-35" morph="none" pos="word" start_char="1744">enacted</TOKEN>
<TOKEN end_char="1757" id="token-9-36" morph="none" pos="word" start_char="1752">public</TOKEN>
<TOKEN end_char="1764" id="token-9-37" morph="none" pos="word" start_char="1759">health</TOKEN>
<TOKEN end_char="1773" id="token-9-38" morph="none" pos="word" start_char="1766">measures</TOKEN>
<TOKEN end_char="1774" id="token-9-39" morph="none" pos="punct" start_char="1774">.</TOKEN>
</SEG>
<SEG end_char="1924" id="segment-10" start_char="1777">
<ORIGINAL_TEXT>In the new study, researchers used molecular clock evolutionary analyses to try to home in on when the first, or index, case of SARS-CoV-2 occurred.</ORIGINAL_TEXT>
<TOKEN end_char="1778" id="token-10-0" morph="none" pos="word" start_char="1777">In</TOKEN>
<TOKEN end_char="1782" id="token-10-1" morph="none" pos="word" start_char="1780">the</TOKEN>
<TOKEN end_char="1786" id="token-10-2" morph="none" pos="word" start_char="1784">new</TOKEN>
<TOKEN end_char="1792" id="token-10-3" morph="none" pos="word" start_char="1788">study</TOKEN>
<TOKEN end_char="1793" id="token-10-4" morph="none" pos="punct" start_char="1793">,</TOKEN>
<TOKEN end_char="1805" id="token-10-5" morph="none" pos="word" start_char="1795">researchers</TOKEN>
<TOKEN end_char="1810" id="token-10-6" morph="none" pos="word" start_char="1807">used</TOKEN>
<TOKEN end_char="1820" id="token-10-7" morph="none" pos="word" start_char="1812">molecular</TOKEN>
<TOKEN end_char="1826" id="token-10-8" morph="none" pos="word" start_char="1822">clock</TOKEN>
<TOKEN end_char="1839" id="token-10-9" morph="none" pos="word" start_char="1828">evolutionary</TOKEN>
<TOKEN end_char="1848" id="token-10-10" morph="none" pos="word" start_char="1841">analyses</TOKEN>
<TOKEN end_char="1851" id="token-10-11" morph="none" pos="word" start_char="1850">to</TOKEN>
<TOKEN end_char="1855" id="token-10-12" morph="none" pos="word" start_char="1853">try</TOKEN>
<TOKEN end_char="1858" id="token-10-13" morph="none" pos="word" start_char="1857">to</TOKEN>
<TOKEN end_char="1863" id="token-10-14" morph="none" pos="word" start_char="1860">home</TOKEN>
<TOKEN end_char="1866" id="token-10-15" morph="none" pos="word" start_char="1865">in</TOKEN>
<TOKEN end_char="1869" id="token-10-16" morph="none" pos="word" start_char="1868">on</TOKEN>
<TOKEN end_char="1874" id="token-10-17" morph="none" pos="word" start_char="1871">when</TOKEN>
<TOKEN end_char="1878" id="token-10-18" morph="none" pos="word" start_char="1876">the</TOKEN>
<TOKEN end_char="1884" id="token-10-19" morph="none" pos="word" start_char="1880">first</TOKEN>
<TOKEN end_char="1885" id="token-10-20" morph="none" pos="punct" start_char="1885">,</TOKEN>
<TOKEN end_char="1888" id="token-10-21" morph="none" pos="word" start_char="1887">or</TOKEN>
<TOKEN end_char="1894" id="token-10-22" morph="none" pos="word" start_char="1890">index</TOKEN>
<TOKEN end_char="1895" id="token-10-23" morph="none" pos="punct" start_char="1895">,</TOKEN>
<TOKEN end_char="1900" id="token-10-24" morph="none" pos="word" start_char="1897">case</TOKEN>
<TOKEN end_char="1903" id="token-10-25" morph="none" pos="word" start_char="1902">of</TOKEN>
<TOKEN end_char="1914" id="token-10-26" morph="none" pos="unknown" start_char="1905">SARS-CoV-2</TOKEN>
<TOKEN end_char="1923" id="token-10-27" morph="none" pos="word" start_char="1916">occurred</TOKEN>
<TOKEN end_char="1924" id="token-10-28" morph="none" pos="punct" start_char="1924">.</TOKEN>
</SEG>
<SEG end_char="2190" id="segment-11" start_char="1926">
<ORIGINAL_TEXT>"Molecular clock" is a term for a technique that uses the mutation rate of genes to deduce when two or more life forms diverged — in this case, when the common ancestor of all variants of SARS-CoV-2 existed, estimated in this study to as early as mid-November 2019.</ORIGINAL_TEXT>
<TOKEN end_char="1926" id="token-11-0" morph="none" pos="punct" start_char="1926">"</TOKEN>
<TOKEN end_char="1935" id="token-11-1" morph="none" pos="word" start_char="1927">Molecular</TOKEN>
<TOKEN end_char="1941" id="token-11-2" morph="none" pos="word" start_char="1937">clock</TOKEN>
<TOKEN end_char="1942" id="token-11-3" morph="none" pos="punct" start_char="1942">"</TOKEN>
<TOKEN end_char="1945" id="token-11-4" morph="none" pos="word" start_char="1944">is</TOKEN>
<TOKEN end_char="1947" id="token-11-5" morph="none" pos="word" start_char="1947">a</TOKEN>
<TOKEN end_char="1952" id="token-11-6" morph="none" pos="word" start_char="1949">term</TOKEN>
<TOKEN end_char="1956" id="token-11-7" morph="none" pos="word" start_char="1954">for</TOKEN>
<TOKEN end_char="1958" id="token-11-8" morph="none" pos="word" start_char="1958">a</TOKEN>
<TOKEN end_char="1968" id="token-11-9" morph="none" pos="word" start_char="1960">technique</TOKEN>
<TOKEN end_char="1973" id="token-11-10" morph="none" pos="word" start_char="1970">that</TOKEN>
<TOKEN end_char="1978" id="token-11-11" morph="none" pos="word" start_char="1975">uses</TOKEN>
<TOKEN end_char="1982" id="token-11-12" morph="none" pos="word" start_char="1980">the</TOKEN>
<TOKEN end_char="1991" id="token-11-13" morph="none" pos="word" start_char="1984">mutation</TOKEN>
<TOKEN end_char="1996" id="token-11-14" morph="none" pos="word" start_char="1993">rate</TOKEN>
<TOKEN end_char="1999" id="token-11-15" morph="none" pos="word" start_char="1998">of</TOKEN>
<TOKEN end_char="2005" id="token-11-16" morph="none" pos="word" start_char="2001">genes</TOKEN>
<TOKEN end_char="2008" id="token-11-17" morph="none" pos="word" start_char="2007">to</TOKEN>
<TOKEN end_char="2015" id="token-11-18" morph="none" pos="word" start_char="2010">deduce</TOKEN>
<TOKEN end_char="2020" id="token-11-19" morph="none" pos="word" start_char="2017">when</TOKEN>
<TOKEN end_char="2024" id="token-11-20" morph="none" pos="word" start_char="2022">two</TOKEN>
<TOKEN end_char="2027" id="token-11-21" morph="none" pos="word" start_char="2026">or</TOKEN>
<TOKEN end_char="2032" id="token-11-22" morph="none" pos="word" start_char="2029">more</TOKEN>
<TOKEN end_char="2037" id="token-11-23" morph="none" pos="word" start_char="2034">life</TOKEN>
<TOKEN end_char="2043" id="token-11-24" morph="none" pos="word" start_char="2039">forms</TOKEN>
<TOKEN end_char="2052" id="token-11-25" morph="none" pos="word" start_char="2045">diverged</TOKEN>
<TOKEN end_char="2054" id="token-11-26" morph="none" pos="punct" start_char="2054">—</TOKEN>
<TOKEN end_char="2057" id="token-11-27" morph="none" pos="word" start_char="2056">in</TOKEN>
<TOKEN end_char="2062" id="token-11-28" morph="none" pos="word" start_char="2059">this</TOKEN>
<TOKEN end_char="2067" id="token-11-29" morph="none" pos="word" start_char="2064">case</TOKEN>
<TOKEN end_char="2068" id="token-11-30" morph="none" pos="punct" start_char="2068">,</TOKEN>
<TOKEN end_char="2073" id="token-11-31" morph="none" pos="word" start_char="2070">when</TOKEN>
<TOKEN end_char="2077" id="token-11-32" morph="none" pos="word" start_char="2075">the</TOKEN>
<TOKEN end_char="2084" id="token-11-33" morph="none" pos="word" start_char="2079">common</TOKEN>
<TOKEN end_char="2093" id="token-11-34" morph="none" pos="word" start_char="2086">ancestor</TOKEN>
<TOKEN end_char="2096" id="token-11-35" morph="none" pos="word" start_char="2095">of</TOKEN>
<TOKEN end_char="2100" id="token-11-36" morph="none" pos="word" start_char="2098">all</TOKEN>
<TOKEN end_char="2109" id="token-11-37" morph="none" pos="word" start_char="2102">variants</TOKEN>
<TOKEN end_char="2112" id="token-11-38" morph="none" pos="word" start_char="2111">of</TOKEN>
<TOKEN end_char="2123" id="token-11-39" morph="none" pos="unknown" start_char="2114">SARS-CoV-2</TOKEN>
<TOKEN end_char="2131" id="token-11-40" morph="none" pos="word" start_char="2125">existed</TOKEN>
<TOKEN end_char="2132" id="token-11-41" morph="none" pos="punct" start_char="2132">,</TOKEN>
<TOKEN end_char="2142" id="token-11-42" morph="none" pos="word" start_char="2134">estimated</TOKEN>
<TOKEN end_char="2145" id="token-11-43" morph="none" pos="word" start_char="2144">in</TOKEN>
<TOKEN end_char="2150" id="token-11-44" morph="none" pos="word" start_char="2147">this</TOKEN>
<TOKEN end_char="2156" id="token-11-45" morph="none" pos="word" start_char="2152">study</TOKEN>
<TOKEN end_char="2159" id="token-11-46" morph="none" pos="word" start_char="2158">to</TOKEN>
<TOKEN end_char="2162" id="token-11-47" morph="none" pos="word" start_char="2161">as</TOKEN>
<TOKEN end_char="2168" id="token-11-48" morph="none" pos="word" start_char="2164">early</TOKEN>
<TOKEN end_char="2171" id="token-11-49" morph="none" pos="word" start_char="2170">as</TOKEN>
<TOKEN end_char="2184" id="token-11-50" morph="none" pos="unknown" start_char="2173">mid-November</TOKEN>
<TOKEN end_char="2189" id="token-11-51" morph="none" pos="word" start_char="2186">2019</TOKEN>
<TOKEN end_char="2190" id="token-11-52" morph="none" pos="punct" start_char="2190">.</TOKEN>
</SEG>
<SEG end_char="2346" id="segment-12" start_char="2193">
<ORIGINAL_TEXT>Based on this work, the researchers estimate that the median number of persons infected with SARS-CoV-2 in China was less than one until November 4, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2197" id="token-12-0" morph="none" pos="word" start_char="2193">Based</TOKEN>
<TOKEN end_char="2200" id="token-12-1" morph="none" pos="word" start_char="2199">on</TOKEN>
<TOKEN end_char="2205" id="token-12-2" morph="none" pos="word" start_char="2202">this</TOKEN>
<TOKEN end_char="2210" id="token-12-3" morph="none" pos="word" start_char="2207">work</TOKEN>
<TOKEN end_char="2211" id="token-12-4" morph="none" pos="punct" start_char="2211">,</TOKEN>
<TOKEN end_char="2215" id="token-12-5" morph="none" pos="word" start_char="2213">the</TOKEN>
<TOKEN end_char="2227" id="token-12-6" morph="none" pos="word" start_char="2217">researchers</TOKEN>
<TOKEN end_char="2236" id="token-12-7" morph="none" pos="word" start_char="2229">estimate</TOKEN>
<TOKEN end_char="2241" id="token-12-8" morph="none" pos="word" start_char="2238">that</TOKEN>
<TOKEN end_char="2245" id="token-12-9" morph="none" pos="word" start_char="2243">the</TOKEN>
<TOKEN end_char="2252" id="token-12-10" morph="none" pos="word" start_char="2247">median</TOKEN>
<TOKEN end_char="2259" id="token-12-11" morph="none" pos="word" start_char="2254">number</TOKEN>
<TOKEN end_char="2262" id="token-12-12" morph="none" pos="word" start_char="2261">of</TOKEN>
<TOKEN end_char="2270" id="token-12-13" morph="none" pos="word" start_char="2264">persons</TOKEN>
<TOKEN end_char="2279" id="token-12-14" morph="none" pos="word" start_char="2272">infected</TOKEN>
<TOKEN end_char="2284" id="token-12-15" morph="none" pos="word" start_char="2281">with</TOKEN>
<TOKEN end_char="2295" id="token-12-16" morph="none" pos="unknown" start_char="2286">SARS-CoV-2</TOKEN>
<TOKEN end_char="2298" id="token-12-17" morph="none" pos="word" start_char="2297">in</TOKEN>
<TOKEN end_char="2304" id="token-12-18" morph="none" pos="word" start_char="2300">China</TOKEN>
<TOKEN end_char="2308" id="token-12-19" morph="none" pos="word" start_char="2306">was</TOKEN>
<TOKEN end_char="2313" id="token-12-20" morph="none" pos="word" start_char="2310">less</TOKEN>
<TOKEN end_char="2318" id="token-12-21" morph="none" pos="word" start_char="2315">than</TOKEN>
<TOKEN end_char="2322" id="token-12-22" morph="none" pos="word" start_char="2320">one</TOKEN>
<TOKEN end_char="2328" id="token-12-23" morph="none" pos="word" start_char="2324">until</TOKEN>
<TOKEN end_char="2337" id="token-12-24" morph="none" pos="word" start_char="2330">November</TOKEN>
<TOKEN end_char="2339" id="token-12-25" morph="none" pos="word" start_char="2339">4</TOKEN>
<TOKEN end_char="2340" id="token-12-26" morph="none" pos="punct" start_char="2340">,</TOKEN>
<TOKEN end_char="2345" id="token-12-27" morph="none" pos="word" start_char="2342">2019</TOKEN>
<TOKEN end_char="2346" id="token-12-28" morph="none" pos="punct" start_char="2346">.</TOKEN>
</SEG>
<SEG end_char="2427" id="segment-13" start_char="2348">
<ORIGINAL_TEXT>Thirteen days later, it was four individuals, and just nine on December 1, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2355" id="token-13-0" morph="none" pos="word" start_char="2348">Thirteen</TOKEN>
<TOKEN end_char="2360" id="token-13-1" morph="none" pos="word" start_char="2357">days</TOKEN>
<TOKEN end_char="2366" id="token-13-2" morph="none" pos="word" start_char="2362">later</TOKEN>
<TOKEN end_char="2367" id="token-13-3" morph="none" pos="punct" start_char="2367">,</TOKEN>
<TOKEN end_char="2370" id="token-13-4" morph="none" pos="word" start_char="2369">it</TOKEN>
<TOKEN end_char="2374" id="token-13-5" morph="none" pos="word" start_char="2372">was</TOKEN>
<TOKEN end_char="2379" id="token-13-6" morph="none" pos="word" start_char="2376">four</TOKEN>
<TOKEN end_char="2391" id="token-13-7" morph="none" pos="word" start_char="2381">individuals</TOKEN>
<TOKEN end_char="2392" id="token-13-8" morph="none" pos="punct" start_char="2392">,</TOKEN>
<TOKEN end_char="2396" id="token-13-9" morph="none" pos="word" start_char="2394">and</TOKEN>
<TOKEN end_char="2401" id="token-13-10" morph="none" pos="word" start_char="2398">just</TOKEN>
<TOKEN end_char="2406" id="token-13-11" morph="none" pos="word" start_char="2403">nine</TOKEN>
<TOKEN end_char="2409" id="token-13-12" morph="none" pos="word" start_char="2408">on</TOKEN>
<TOKEN end_char="2418" id="token-13-13" morph="none" pos="word" start_char="2411">December</TOKEN>
<TOKEN end_char="2420" id="token-13-14" morph="none" pos="word" start_char="2420">1</TOKEN>
<TOKEN end_char="2421" id="token-13-15" morph="none" pos="punct" start_char="2421">,</TOKEN>
<TOKEN end_char="2426" id="token-13-16" morph="none" pos="word" start_char="2423">2019</TOKEN>
<TOKEN end_char="2427" id="token-13-17" morph="none" pos="punct" start_char="2427">.</TOKEN>
</SEG>
<SEG end_char="2654" id="segment-14" start_char="2430">
<ORIGINAL_TEXT>The authors modelled how the SARS-CoV-2 virus may have behaved during the initial outbreak and early days of the pandemic when it was largely an unknown entity and the scope of the public health threat not yet fully realised.</ORIGINAL_TEXT>
<TOKEN end_char="2432" id="token-14-0" morph="none" pos="word" start_char="2430">The</TOKEN>
<TOKEN end_char="2440" id="token-14-1" morph="none" pos="word" start_char="2434">authors</TOKEN>
<TOKEN end_char="2449" id="token-14-2" morph="none" pos="word" start_char="2442">modelled</TOKEN>
<TOKEN end_char="2453" id="token-14-3" morph="none" pos="word" start_char="2451">how</TOKEN>
<TOKEN end_char="2457" id="token-14-4" morph="none" pos="word" start_char="2455">the</TOKEN>
<TOKEN end_char="2468" id="token-14-5" morph="none" pos="unknown" start_char="2459">SARS-CoV-2</TOKEN>
<TOKEN end_char="2474" id="token-14-6" morph="none" pos="word" start_char="2470">virus</TOKEN>
<TOKEN end_char="2478" id="token-14-7" morph="none" pos="word" start_char="2476">may</TOKEN>
<TOKEN end_char="2483" id="token-14-8" morph="none" pos="word" start_char="2480">have</TOKEN>
<TOKEN end_char="2491" id="token-14-9" morph="none" pos="word" start_char="2485">behaved</TOKEN>
<TOKEN end_char="2498" id="token-14-10" morph="none" pos="word" start_char="2493">during</TOKEN>
<TOKEN end_char="2502" id="token-14-11" morph="none" pos="word" start_char="2500">the</TOKEN>
<TOKEN end_char="2510" id="token-14-12" morph="none" pos="word" start_char="2504">initial</TOKEN>
<TOKEN end_char="2519" id="token-14-13" morph="none" pos="word" start_char="2512">outbreak</TOKEN>
<TOKEN end_char="2523" id="token-14-14" morph="none" pos="word" start_char="2521">and</TOKEN>
<TOKEN end_char="2529" id="token-14-15" morph="none" pos="word" start_char="2525">early</TOKEN>
<TOKEN end_char="2534" id="token-14-16" morph="none" pos="word" start_char="2531">days</TOKEN>
<TOKEN end_char="2537" id="token-14-17" morph="none" pos="word" start_char="2536">of</TOKEN>
<TOKEN end_char="2541" id="token-14-18" morph="none" pos="word" start_char="2539">the</TOKEN>
<TOKEN end_char="2550" id="token-14-19" morph="none" pos="word" start_char="2543">pandemic</TOKEN>
<TOKEN end_char="2555" id="token-14-20" morph="none" pos="word" start_char="2552">when</TOKEN>
<TOKEN end_char="2558" id="token-14-21" morph="none" pos="word" start_char="2557">it</TOKEN>
<TOKEN end_char="2562" id="token-14-22" morph="none" pos="word" start_char="2560">was</TOKEN>
<TOKEN end_char="2570" id="token-14-23" morph="none" pos="word" start_char="2564">largely</TOKEN>
<TOKEN end_char="2573" id="token-14-24" morph="none" pos="word" start_char="2572">an</TOKEN>
<TOKEN end_char="2581" id="token-14-25" morph="none" pos="word" start_char="2575">unknown</TOKEN>
<TOKEN end_char="2588" id="token-14-26" morph="none" pos="word" start_char="2583">entity</TOKEN>
<TOKEN end_char="2592" id="token-14-27" morph="none" pos="word" start_char="2590">and</TOKEN>
<TOKEN end_char="2596" id="token-14-28" morph="none" pos="word" start_char="2594">the</TOKEN>
<TOKEN end_char="2602" id="token-14-29" morph="none" pos="word" start_char="2598">scope</TOKEN>
<TOKEN end_char="2605" id="token-14-30" morph="none" pos="word" start_char="2604">of</TOKEN>
<TOKEN end_char="2609" id="token-14-31" morph="none" pos="word" start_char="2607">the</TOKEN>
<TOKEN end_char="2616" id="token-14-32" morph="none" pos="word" start_char="2611">public</TOKEN>
<TOKEN end_char="2623" id="token-14-33" morph="none" pos="word" start_char="2618">health</TOKEN>
<TOKEN end_char="2630" id="token-14-34" morph="none" pos="word" start_char="2625">threat</TOKEN>
<TOKEN end_char="2634" id="token-14-35" morph="none" pos="word" start_char="2632">not</TOKEN>
<TOKEN end_char="2638" id="token-14-36" morph="none" pos="word" start_char="2636">yet</TOKEN>
<TOKEN end_char="2644" id="token-14-37" morph="none" pos="word" start_char="2640">fully</TOKEN>
<TOKEN end_char="2653" id="token-14-38" morph="none" pos="word" start_char="2646">realised</TOKEN>
<TOKEN end_char="2654" id="token-14-39" morph="none" pos="punct" start_char="2654">.</TOKEN>
</SEG>
<SEG end_char="2745" id="segment-15" start_char="2656">
<ORIGINAL_TEXT>In just 29.7% of these simulations was the virus able to create self-sustaining epidemics.</ORIGINAL_TEXT>
<TOKEN end_char="2657" id="token-15-0" morph="none" pos="word" start_char="2656">In</TOKEN>
<TOKEN end_char="2662" id="token-15-1" morph="none" pos="word" start_char="2659">just</TOKEN>
<TOKEN end_char="2667" id="token-15-2" morph="none" pos="unknown" start_char="2664">29.7</TOKEN>
<TOKEN end_char="2668" id="token-15-3" morph="none" pos="punct" start_char="2668">%</TOKEN>
<TOKEN end_char="2671" id="token-15-4" morph="none" pos="word" start_char="2670">of</TOKEN>
<TOKEN end_char="2677" id="token-15-5" morph="none" pos="word" start_char="2673">these</TOKEN>
<TOKEN end_char="2689" id="token-15-6" morph="none" pos="word" start_char="2679">simulations</TOKEN>
<TOKEN end_char="2693" id="token-15-7" morph="none" pos="word" start_char="2691">was</TOKEN>
<TOKEN end_char="2697" id="token-15-8" morph="none" pos="word" start_char="2695">the</TOKEN>
<TOKEN end_char="2703" id="token-15-9" morph="none" pos="word" start_char="2699">virus</TOKEN>
<TOKEN end_char="2708" id="token-15-10" morph="none" pos="word" start_char="2705">able</TOKEN>
<TOKEN end_char="2711" id="token-15-11" morph="none" pos="word" start_char="2710">to</TOKEN>
<TOKEN end_char="2718" id="token-15-12" morph="none" pos="word" start_char="2713">create</TOKEN>
<TOKEN end_char="2734" id="token-15-13" morph="none" pos="unknown" start_char="2720">self-sustaining</TOKEN>
<TOKEN end_char="2744" id="token-15-14" morph="none" pos="word" start_char="2736">epidemics</TOKEN>
<TOKEN end_char="2745" id="token-15-15" morph="none" pos="punct" start_char="2745">.</TOKEN>
</SEG>
<SEG end_char="2825" id="segment-16" start_char="2747">
<ORIGINAL_TEXT>In the other 70.3%, the virus infected relatively few persons before dying out.</ORIGINAL_TEXT>
<TOKEN end_char="2748" id="token-16-0" morph="none" pos="word" start_char="2747">In</TOKEN>
<TOKEN end_char="2752" id="token-16-1" morph="none" pos="word" start_char="2750">the</TOKEN>
<TOKEN end_char="2758" id="token-16-2" morph="none" pos="word" start_char="2754">other</TOKEN>
<TOKEN end_char="2763" id="token-16-3" morph="none" pos="unknown" start_char="2760">70.3</TOKEN>
<TOKEN end_char="2765" id="token-16-4" morph="none" pos="punct" start_char="2764">%,</TOKEN>
<TOKEN end_char="2769" id="token-16-5" morph="none" pos="word" start_char="2767">the</TOKEN>
<TOKEN end_char="2775" id="token-16-6" morph="none" pos="word" start_char="2771">virus</TOKEN>
<TOKEN end_char="2784" id="token-16-7" morph="none" pos="word" start_char="2777">infected</TOKEN>
<TOKEN end_char="2795" id="token-16-8" morph="none" pos="word" start_char="2786">relatively</TOKEN>
<TOKEN end_char="2799" id="token-16-9" morph="none" pos="word" start_char="2797">few</TOKEN>
<TOKEN end_char="2807" id="token-16-10" morph="none" pos="word" start_char="2801">persons</TOKEN>
<TOKEN end_char="2814" id="token-16-11" morph="none" pos="word" start_char="2809">before</TOKEN>
<TOKEN end_char="2820" id="token-16-12" morph="none" pos="word" start_char="2816">dying</TOKEN>
<TOKEN end_char="2824" id="token-16-13" morph="none" pos="word" start_char="2822">out</TOKEN>
<TOKEN end_char="2825" id="token-16-14" morph="none" pos="punct" start_char="2825">.</TOKEN>
</SEG>
<SEG end_char="2897" id="segment-17" start_char="2827">
<ORIGINAL_TEXT>The average failed epidemic ended just eight days after the index case.</ORIGINAL_TEXT>
<TOKEN end_char="2829" id="token-17-0" morph="none" pos="word" start_char="2827">The</TOKEN>
<TOKEN end_char="2837" id="token-17-1" morph="none" pos="word" start_char="2831">average</TOKEN>
<TOKEN end_char="2844" id="token-17-2" morph="none" pos="word" start_char="2839">failed</TOKEN>
<TOKEN end_char="2853" id="token-17-3" morph="none" pos="word" start_char="2846">epidemic</TOKEN>
<TOKEN end_char="2859" id="token-17-4" morph="none" pos="word" start_char="2855">ended</TOKEN>
<TOKEN end_char="2864" id="token-17-5" morph="none" pos="word" start_char="2861">just</TOKEN>
<TOKEN end_char="2870" id="token-17-6" morph="none" pos="word" start_char="2866">eight</TOKEN>
<TOKEN end_char="2875" id="token-17-7" morph="none" pos="word" start_char="2872">days</TOKEN>
<TOKEN end_char="2881" id="token-17-8" morph="none" pos="word" start_char="2877">after</TOKEN>
<TOKEN end_char="2885" id="token-17-9" morph="none" pos="word" start_char="2883">the</TOKEN>
<TOKEN end_char="2891" id="token-17-10" morph="none" pos="word" start_char="2887">index</TOKEN>
<TOKEN end_char="2896" id="token-17-11" morph="none" pos="word" start_char="2893">case</TOKEN>
<TOKEN end_char="2897" id="token-17-12" morph="none" pos="punct" start_char="2897">.</TOKEN>
</SEG>
<SEG end_char="2941" id="segment-18" start_char="2900">
<ORIGINAL_TEXT>Source: University of California San Diego</ORIGINAL_TEXT>
<TOKEN end_char="2905" id="token-18-0" morph="none" pos="word" start_char="2900">Source</TOKEN>
<TOKEN end_char="2906" id="token-18-1" morph="none" pos="punct" start_char="2906">:</TOKEN>
<TOKEN end_char="2917" id="token-18-2" morph="none" pos="word" start_char="2908">University</TOKEN>
<TOKEN end_char="2920" id="token-18-3" morph="none" pos="word" start_char="2919">of</TOKEN>
<TOKEN end_char="2931" id="token-18-4" morph="none" pos="word" start_char="2922">California</TOKEN>
<TOKEN end_char="2935" id="token-18-5" morph="none" pos="word" start_char="2933">San</TOKEN>
<TOKEN end_char="2941" id="token-18-6" morph="none" pos="word" start_char="2937">Diego</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>