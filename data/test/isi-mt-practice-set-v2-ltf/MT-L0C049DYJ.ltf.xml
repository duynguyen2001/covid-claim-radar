<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C049DYJ" lang="eng" raw_text_char_length="7577" raw_text_md5="78a258a0b7475e81ca26615715262fed" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="89" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Dr. Fauci Backed Controversial Wuhan Lab with U.S. Dollars for Risky Coronavirus Research</ORIGINAL_TEXT>
<TOKEN end_char="2" id="token-0-0" morph="none" pos="word" start_char="1">Dr</TOKEN>
<TOKEN end_char="3" id="token-0-1" morph="none" pos="punct" start_char="3">.</TOKEN>
<TOKEN end_char="9" id="token-0-2" morph="none" pos="word" start_char="5">Fauci</TOKEN>
<TOKEN end_char="16" id="token-0-3" morph="none" pos="word" start_char="11">Backed</TOKEN>
<TOKEN end_char="30" id="token-0-4" morph="none" pos="word" start_char="18">Controversial</TOKEN>
<TOKEN end_char="36" id="token-0-5" morph="none" pos="word" start_char="32">Wuhan</TOKEN>
<TOKEN end_char="40" id="token-0-6" morph="none" pos="word" start_char="38">Lab</TOKEN>
<TOKEN end_char="45" id="token-0-7" morph="none" pos="word" start_char="42">with</TOKEN>
<TOKEN end_char="49" id="token-0-8" morph="none" pos="unknown" start_char="47">U.S</TOKEN>
<TOKEN end_char="50" id="token-0-9" morph="none" pos="punct" start_char="50">.</TOKEN>
<TOKEN end_char="58" id="token-0-10" morph="none" pos="word" start_char="52">Dollars</TOKEN>
<TOKEN end_char="62" id="token-0-11" morph="none" pos="word" start_char="60">for</TOKEN>
<TOKEN end_char="68" id="token-0-12" morph="none" pos="word" start_char="64">Risky</TOKEN>
<TOKEN end_char="80" id="token-0-13" morph="none" pos="word" start_char="70">Coronavirus</TOKEN>
<TOKEN end_char="89" id="token-0-14" morph="none" pos="word" start_char="82">Research</TOKEN>
</SEG>
<SEG end_char="156" id="segment-1" start_char="94">
<ORIGINAL_TEXT>Biomedical research ultimately protects public health, said Dr.</ORIGINAL_TEXT>
<TOKEN end_char="103" id="token-1-0" morph="none" pos="word" start_char="94">Biomedical</TOKEN>
<TOKEN end_char="112" id="token-1-1" morph="none" pos="word" start_char="105">research</TOKEN>
<TOKEN end_char="123" id="token-1-2" morph="none" pos="word" start_char="114">ultimately</TOKEN>
<TOKEN end_char="132" id="token-1-3" morph="none" pos="word" start_char="125">protects</TOKEN>
<TOKEN end_char="139" id="token-1-4" morph="none" pos="word" start_char="134">public</TOKEN>
<TOKEN end_char="146" id="token-1-5" morph="none" pos="word" start_char="141">health</TOKEN>
<TOKEN end_char="147" id="token-1-6" morph="none" pos="punct" start_char="147">,</TOKEN>
<TOKEN end_char="152" id="token-1-7" morph="none" pos="word" start_char="149">said</TOKEN>
<TOKEN end_char="155" id="token-1-8" morph="none" pos="word" start_char="154">Dr</TOKEN>
<TOKEN end_char="156" id="token-1-9" morph="none" pos="punct" start_char="156">.</TOKEN>
</SEG>
<SEG end_char="225" id="segment-2" start_char="158">
<ORIGINAL_TEXT>Anthony Fauci, in explaining his support for controversial research.</ORIGINAL_TEXT>
<TOKEN end_char="164" id="token-2-0" morph="none" pos="word" start_char="158">Anthony</TOKEN>
<TOKEN end_char="170" id="token-2-1" morph="none" pos="word" start_char="166">Fauci</TOKEN>
<TOKEN end_char="171" id="token-2-2" morph="none" pos="punct" start_char="171">,</TOKEN>
<TOKEN end_char="174" id="token-2-3" morph="none" pos="word" start_char="173">in</TOKEN>
<TOKEN end_char="185" id="token-2-4" morph="none" pos="word" start_char="176">explaining</TOKEN>
<TOKEN end_char="189" id="token-2-5" morph="none" pos="word" start_char="187">his</TOKEN>
<TOKEN end_char="197" id="token-2-6" morph="none" pos="word" start_char="191">support</TOKEN>
<TOKEN end_char="201" id="token-2-7" morph="none" pos="word" start_char="199">for</TOKEN>
<TOKEN end_char="215" id="token-2-8" morph="none" pos="word" start_char="203">controversial</TOKEN>
<TOKEN end_char="224" id="token-2-9" morph="none" pos="word" start_char="217">research</TOKEN>
<TOKEN end_char="225" id="token-2-10" morph="none" pos="punct" start_char="225">.</TOKEN>
</SEG>
<SEG end_char="231" id="segment-3" start_char="229">
<ORIGINAL_TEXT>Dr.</ORIGINAL_TEXT>
<TOKEN end_char="230" id="token-3-0" morph="none" pos="word" start_char="229">Dr</TOKEN>
<TOKEN end_char="231" id="token-3-1" morph="none" pos="punct" start_char="231">.</TOKEN>
<TRANSLATED_TEXT>dr.</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="384" id="segment-4" start_char="233">
<ORIGINAL_TEXT>Anthony Fauci is an adviser to President Donald Trump and something of an American folk hero for his steady, calm leadership during the pandemic crisis.</ORIGINAL_TEXT>
<TOKEN end_char="239" id="token-4-0" morph="none" pos="word" start_char="233">Anthony</TOKEN>
<TOKEN end_char="245" id="token-4-1" morph="none" pos="word" start_char="241">Fauci</TOKEN>
<TOKEN end_char="248" id="token-4-2" morph="none" pos="word" start_char="247">is</TOKEN>
<TOKEN end_char="251" id="token-4-3" morph="none" pos="word" start_char="250">an</TOKEN>
<TOKEN end_char="259" id="token-4-4" morph="none" pos="word" start_char="253">adviser</TOKEN>
<TOKEN end_char="262" id="token-4-5" morph="none" pos="word" start_char="261">to</TOKEN>
<TOKEN end_char="272" id="token-4-6" morph="none" pos="word" start_char="264">President</TOKEN>
<TOKEN end_char="279" id="token-4-7" morph="none" pos="word" start_char="274">Donald</TOKEN>
<TOKEN end_char="285" id="token-4-8" morph="none" pos="word" start_char="281">Trump</TOKEN>
<TOKEN end_char="289" id="token-4-9" morph="none" pos="word" start_char="287">and</TOKEN>
<TOKEN end_char="299" id="token-4-10" morph="none" pos="word" start_char="291">something</TOKEN>
<TOKEN end_char="302" id="token-4-11" morph="none" pos="word" start_char="301">of</TOKEN>
<TOKEN end_char="305" id="token-4-12" morph="none" pos="word" start_char="304">an</TOKEN>
<TOKEN end_char="314" id="token-4-13" morph="none" pos="word" start_char="307">American</TOKEN>
<TOKEN end_char="319" id="token-4-14" morph="none" pos="word" start_char="316">folk</TOKEN>
<TOKEN end_char="324" id="token-4-15" morph="none" pos="word" start_char="321">hero</TOKEN>
<TOKEN end_char="328" id="token-4-16" morph="none" pos="word" start_char="326">for</TOKEN>
<TOKEN end_char="332" id="token-4-17" morph="none" pos="word" start_char="330">his</TOKEN>
<TOKEN end_char="339" id="token-4-18" morph="none" pos="word" start_char="334">steady</TOKEN>
<TOKEN end_char="340" id="token-4-19" morph="none" pos="punct" start_char="340">,</TOKEN>
<TOKEN end_char="345" id="token-4-20" morph="none" pos="word" start_char="342">calm</TOKEN>
<TOKEN end_char="356" id="token-4-21" morph="none" pos="word" start_char="347">leadership</TOKEN>
<TOKEN end_char="363" id="token-4-22" morph="none" pos="word" start_char="358">during</TOKEN>
<TOKEN end_char="367" id="token-4-23" morph="none" pos="word" start_char="365">the</TOKEN>
<TOKEN end_char="376" id="token-4-24" morph="none" pos="word" start_char="369">pandemic</TOKEN>
<TOKEN end_char="383" id="token-4-25" morph="none" pos="word" start_char="378">crisis</TOKEN>
<TOKEN end_char="384" id="token-4-26" morph="none" pos="punct" start_char="384">.</TOKEN>
</SEG>
<SEG end_char="532" id="segment-5" start_char="386">
<ORIGINAL_TEXT>At least one poll shows that Americans trust Fauci more than Trump on the coronavirus pandemic—and few scientists are portrayed on TV by Brad Pitt.</ORIGINAL_TEXT>
<TOKEN end_char="387" id="token-5-0" morph="none" pos="word" start_char="386">At</TOKEN>
<TOKEN end_char="393" id="token-5-1" morph="none" pos="word" start_char="389">least</TOKEN>
<TOKEN end_char="397" id="token-5-2" morph="none" pos="word" start_char="395">one</TOKEN>
<TOKEN end_char="402" id="token-5-3" morph="none" pos="word" start_char="399">poll</TOKEN>
<TOKEN end_char="408" id="token-5-4" morph="none" pos="word" start_char="404">shows</TOKEN>
<TOKEN end_char="413" id="token-5-5" morph="none" pos="word" start_char="410">that</TOKEN>
<TOKEN end_char="423" id="token-5-6" morph="none" pos="word" start_char="415">Americans</TOKEN>
<TOKEN end_char="429" id="token-5-7" morph="none" pos="word" start_char="425">trust</TOKEN>
<TOKEN end_char="435" id="token-5-8" morph="none" pos="word" start_char="431">Fauci</TOKEN>
<TOKEN end_char="440" id="token-5-9" morph="none" pos="word" start_char="437">more</TOKEN>
<TOKEN end_char="445" id="token-5-10" morph="none" pos="word" start_char="442">than</TOKEN>
<TOKEN end_char="451" id="token-5-11" morph="none" pos="word" start_char="447">Trump</TOKEN>
<TOKEN end_char="454" id="token-5-12" morph="none" pos="word" start_char="453">on</TOKEN>
<TOKEN end_char="458" id="token-5-13" morph="none" pos="word" start_char="456">the</TOKEN>
<TOKEN end_char="470" id="token-5-14" morph="none" pos="word" start_char="460">coronavirus</TOKEN>
<TOKEN end_char="483" id="token-5-15" morph="none" pos="unknown" start_char="472">pandemic—and</TOKEN>
<TOKEN end_char="487" id="token-5-16" morph="none" pos="word" start_char="485">few</TOKEN>
<TOKEN end_char="498" id="token-5-17" morph="none" pos="word" start_char="489">scientists</TOKEN>
<TOKEN end_char="502" id="token-5-18" morph="none" pos="word" start_char="500">are</TOKEN>
<TOKEN end_char="512" id="token-5-19" morph="none" pos="word" start_char="504">portrayed</TOKEN>
<TOKEN end_char="515" id="token-5-20" morph="none" pos="word" start_char="514">on</TOKEN>
<TOKEN end_char="518" id="token-5-21" morph="none" pos="word" start_char="517">TV</TOKEN>
<TOKEN end_char="521" id="token-5-22" morph="none" pos="word" start_char="520">by</TOKEN>
<TOKEN end_char="526" id="token-5-23" morph="none" pos="word" start_char="523">Brad</TOKEN>
<TOKEN end_char="531" id="token-5-24" morph="none" pos="word" start_char="528">Pitt</TOKEN>
<TOKEN end_char="532" id="token-5-25" morph="none" pos="punct" start_char="532">.</TOKEN>
</SEG>
<SEG end_char="784" id="segment-6" start_char="535">
<ORIGINAL_TEXT>But just last year, the National Institute for Allergy and Infectious Diseases, the organization led by Dr. Fauci, funded scientists at the Wuhan Institute of Virology and other institutions for work on gain-of-function research on bat coronaviruses.</ORIGINAL_TEXT>
<TOKEN end_char="537" id="token-6-0" morph="none" pos="word" start_char="535">But</TOKEN>
<TOKEN end_char="542" id="token-6-1" morph="none" pos="word" start_char="539">just</TOKEN>
<TOKEN end_char="547" id="token-6-2" morph="none" pos="word" start_char="544">last</TOKEN>
<TOKEN end_char="552" id="token-6-3" morph="none" pos="word" start_char="549">year</TOKEN>
<TOKEN end_char="553" id="token-6-4" morph="none" pos="punct" start_char="553">,</TOKEN>
<TOKEN end_char="557" id="token-6-5" morph="none" pos="word" start_char="555">the</TOKEN>
<TOKEN end_char="566" id="token-6-6" morph="none" pos="word" start_char="559">National</TOKEN>
<TOKEN end_char="576" id="token-6-7" morph="none" pos="word" start_char="568">Institute</TOKEN>
<TOKEN end_char="580" id="token-6-8" morph="none" pos="word" start_char="578">for</TOKEN>
<TOKEN end_char="588" id="token-6-9" morph="none" pos="word" start_char="582">Allergy</TOKEN>
<TOKEN end_char="592" id="token-6-10" morph="none" pos="word" start_char="590">and</TOKEN>
<TOKEN end_char="603" id="token-6-11" morph="none" pos="word" start_char="594">Infectious</TOKEN>
<TOKEN end_char="612" id="token-6-12" morph="none" pos="word" start_char="605">Diseases</TOKEN>
<TOKEN end_char="613" id="token-6-13" morph="none" pos="punct" start_char="613">,</TOKEN>
<TOKEN end_char="617" id="token-6-14" morph="none" pos="word" start_char="615">the</TOKEN>
<TOKEN end_char="630" id="token-6-15" morph="none" pos="word" start_char="619">organization</TOKEN>
<TOKEN end_char="634" id="token-6-16" morph="none" pos="word" start_char="632">led</TOKEN>
<TOKEN end_char="637" id="token-6-17" morph="none" pos="word" start_char="636">by</TOKEN>
<TOKEN end_char="640" id="token-6-18" morph="none" pos="word" start_char="639">Dr</TOKEN>
<TOKEN end_char="641" id="token-6-19" morph="none" pos="punct" start_char="641">.</TOKEN>
<TOKEN end_char="647" id="token-6-20" morph="none" pos="word" start_char="643">Fauci</TOKEN>
<TOKEN end_char="648" id="token-6-21" morph="none" pos="punct" start_char="648">,</TOKEN>
<TOKEN end_char="655" id="token-6-22" morph="none" pos="word" start_char="650">funded</TOKEN>
<TOKEN end_char="666" id="token-6-23" morph="none" pos="word" start_char="657">scientists</TOKEN>
<TOKEN end_char="669" id="token-6-24" morph="none" pos="word" start_char="668">at</TOKEN>
<TOKEN end_char="673" id="token-6-25" morph="none" pos="word" start_char="671">the</TOKEN>
<TOKEN end_char="679" id="token-6-26" morph="none" pos="word" start_char="675">Wuhan</TOKEN>
<TOKEN end_char="689" id="token-6-27" morph="none" pos="word" start_char="681">Institute</TOKEN>
<TOKEN end_char="692" id="token-6-28" morph="none" pos="word" start_char="691">of</TOKEN>
<TOKEN end_char="701" id="token-6-29" morph="none" pos="word" start_char="694">Virology</TOKEN>
<TOKEN end_char="705" id="token-6-30" morph="none" pos="word" start_char="703">and</TOKEN>
<TOKEN end_char="711" id="token-6-31" morph="none" pos="word" start_char="707">other</TOKEN>
<TOKEN end_char="724" id="token-6-32" morph="none" pos="word" start_char="713">institutions</TOKEN>
<TOKEN end_char="728" id="token-6-33" morph="none" pos="word" start_char="726">for</TOKEN>
<TOKEN end_char="733" id="token-6-34" morph="none" pos="word" start_char="730">work</TOKEN>
<TOKEN end_char="736" id="token-6-35" morph="none" pos="word" start_char="735">on</TOKEN>
<TOKEN end_char="753" id="token-6-36" morph="none" pos="unknown" start_char="738">gain-of-function</TOKEN>
<TOKEN end_char="762" id="token-6-37" morph="none" pos="word" start_char="755">research</TOKEN>
<TOKEN end_char="765" id="token-6-38" morph="none" pos="word" start_char="764">on</TOKEN>
<TOKEN end_char="769" id="token-6-39" morph="none" pos="word" start_char="767">bat</TOKEN>
<TOKEN end_char="783" id="token-6-40" morph="none" pos="word" start_char="771">coronaviruses</TOKEN>
<TOKEN end_char="784" id="token-6-41" morph="none" pos="punct" start_char="784">.</TOKEN>
</SEG>
<SEG end_char="948" id="segment-7" start_char="787">
<ORIGINAL_TEXT>In 2019, with the backing of NIAID, the National Institutes of Health committed $3.7 million over six years for research that included some gain-of-function work.</ORIGINAL_TEXT>
<TOKEN end_char="788" id="token-7-0" morph="none" pos="word" start_char="787">In</TOKEN>
<TOKEN end_char="793" id="token-7-1" morph="none" pos="word" start_char="790">2019</TOKEN>
<TOKEN end_char="794" id="token-7-2" morph="none" pos="punct" start_char="794">,</TOKEN>
<TOKEN end_char="799" id="token-7-3" morph="none" pos="word" start_char="796">with</TOKEN>
<TOKEN end_char="803" id="token-7-4" morph="none" pos="word" start_char="801">the</TOKEN>
<TOKEN end_char="811" id="token-7-5" morph="none" pos="word" start_char="805">backing</TOKEN>
<TOKEN end_char="814" id="token-7-6" morph="none" pos="word" start_char="813">of</TOKEN>
<TOKEN end_char="820" id="token-7-7" morph="none" pos="word" start_char="816">NIAID</TOKEN>
<TOKEN end_char="821" id="token-7-8" morph="none" pos="punct" start_char="821">,</TOKEN>
<TOKEN end_char="825" id="token-7-9" morph="none" pos="word" start_char="823">the</TOKEN>
<TOKEN end_char="834" id="token-7-10" morph="none" pos="word" start_char="827">National</TOKEN>
<TOKEN end_char="845" id="token-7-11" morph="none" pos="word" start_char="836">Institutes</TOKEN>
<TOKEN end_char="848" id="token-7-12" morph="none" pos="word" start_char="847">of</TOKEN>
<TOKEN end_char="855" id="token-7-13" morph="none" pos="word" start_char="850">Health</TOKEN>
<TOKEN end_char="865" id="token-7-14" morph="none" pos="word" start_char="857">committed</TOKEN>
<TOKEN end_char="870" id="token-7-15" morph="none" pos="unknown" start_char="867">$3.7</TOKEN>
<TOKEN end_char="878" id="token-7-16" morph="none" pos="word" start_char="872">million</TOKEN>
<TOKEN end_char="883" id="token-7-17" morph="none" pos="word" start_char="880">over</TOKEN>
<TOKEN end_char="887" id="token-7-18" morph="none" pos="word" start_char="885">six</TOKEN>
<TOKEN end_char="893" id="token-7-19" morph="none" pos="word" start_char="889">years</TOKEN>
<TOKEN end_char="897" id="token-7-20" morph="none" pos="word" start_char="895">for</TOKEN>
<TOKEN end_char="906" id="token-7-21" morph="none" pos="word" start_char="899">research</TOKEN>
<TOKEN end_char="911" id="token-7-22" morph="none" pos="word" start_char="908">that</TOKEN>
<TOKEN end_char="920" id="token-7-23" morph="none" pos="word" start_char="913">included</TOKEN>
<TOKEN end_char="925" id="token-7-24" morph="none" pos="word" start_char="922">some</TOKEN>
<TOKEN end_char="942" id="token-7-25" morph="none" pos="unknown" start_char="927">gain-of-function</TOKEN>
<TOKEN end_char="947" id="token-7-26" morph="none" pos="word" start_char="944">work</TOKEN>
<TOKEN end_char="948" id="token-7-27" morph="none" pos="punct" start_char="948">.</TOKEN>
</SEG>
<SEG end_char="1110" id="segment-8" start_char="950">
<ORIGINAL_TEXT>The program followed another $3.7 million, 5-year project for collecting and studying bat coronaviruses, which ended in 2019, bringing the total to $7.4 million.</ORIGINAL_TEXT>
<TOKEN end_char="952" id="token-8-0" morph="none" pos="word" start_char="950">The</TOKEN>
<TOKEN end_char="960" id="token-8-1" morph="none" pos="word" start_char="954">program</TOKEN>
<TOKEN end_char="969" id="token-8-2" morph="none" pos="word" start_char="962">followed</TOKEN>
<TOKEN end_char="977" id="token-8-3" morph="none" pos="word" start_char="971">another</TOKEN>
<TOKEN end_char="982" id="token-8-4" morph="none" pos="unknown" start_char="979">$3.7</TOKEN>
<TOKEN end_char="990" id="token-8-5" morph="none" pos="word" start_char="984">million</TOKEN>
<TOKEN end_char="991" id="token-8-6" morph="none" pos="punct" start_char="991">,</TOKEN>
<TOKEN end_char="998" id="token-8-7" morph="none" pos="unknown" start_char="993">5-year</TOKEN>
<TOKEN end_char="1006" id="token-8-8" morph="none" pos="word" start_char="1000">project</TOKEN>
<TOKEN end_char="1010" id="token-8-9" morph="none" pos="word" start_char="1008">for</TOKEN>
<TOKEN end_char="1021" id="token-8-10" morph="none" pos="word" start_char="1012">collecting</TOKEN>
<TOKEN end_char="1025" id="token-8-11" morph="none" pos="word" start_char="1023">and</TOKEN>
<TOKEN end_char="1034" id="token-8-12" morph="none" pos="word" start_char="1027">studying</TOKEN>
<TOKEN end_char="1038" id="token-8-13" morph="none" pos="word" start_char="1036">bat</TOKEN>
<TOKEN end_char="1052" id="token-8-14" morph="none" pos="word" start_char="1040">coronaviruses</TOKEN>
<TOKEN end_char="1053" id="token-8-15" morph="none" pos="punct" start_char="1053">,</TOKEN>
<TOKEN end_char="1059" id="token-8-16" morph="none" pos="word" start_char="1055">which</TOKEN>
<TOKEN end_char="1065" id="token-8-17" morph="none" pos="word" start_char="1061">ended</TOKEN>
<TOKEN end_char="1068" id="token-8-18" morph="none" pos="word" start_char="1067">in</TOKEN>
<TOKEN end_char="1073" id="token-8-19" morph="none" pos="word" start_char="1070">2019</TOKEN>
<TOKEN end_char="1074" id="token-8-20" morph="none" pos="punct" start_char="1074">,</TOKEN>
<TOKEN end_char="1083" id="token-8-21" morph="none" pos="word" start_char="1076">bringing</TOKEN>
<TOKEN end_char="1087" id="token-8-22" morph="none" pos="word" start_char="1085">the</TOKEN>
<TOKEN end_char="1093" id="token-8-23" morph="none" pos="word" start_char="1089">total</TOKEN>
<TOKEN end_char="1096" id="token-8-24" morph="none" pos="word" start_char="1095">to</TOKEN>
<TOKEN end_char="1101" id="token-8-25" morph="none" pos="unknown" start_char="1098">$7.4</TOKEN>
<TOKEN end_char="1109" id="token-8-26" morph="none" pos="word" start_char="1103">million</TOKEN>
<TOKEN end_char="1110" id="token-8-27" morph="none" pos="punct" start_char="1110">.</TOKEN>
</SEG>
<SEG end_char="1340" id="segment-9" start_char="1113">
<ORIGINAL_TEXT>Many scientists have criticized gain of function research, which involves manipulating viruses in the lab to explore their potential for infecting humans, because it creates a risk of starting a pandemic from accidental release.</ORIGINAL_TEXT>
<TOKEN end_char="1116" id="token-9-0" morph="none" pos="word" start_char="1113">Many</TOKEN>
<TOKEN end_char="1127" id="token-9-1" morph="none" pos="word" start_char="1118">scientists</TOKEN>
<TOKEN end_char="1132" id="token-9-2" morph="none" pos="word" start_char="1129">have</TOKEN>
<TOKEN end_char="1143" id="token-9-3" morph="none" pos="word" start_char="1134">criticized</TOKEN>
<TOKEN end_char="1148" id="token-9-4" morph="none" pos="word" start_char="1145">gain</TOKEN>
<TOKEN end_char="1151" id="token-9-5" morph="none" pos="word" start_char="1150">of</TOKEN>
<TOKEN end_char="1160" id="token-9-6" morph="none" pos="word" start_char="1153">function</TOKEN>
<TOKEN end_char="1169" id="token-9-7" morph="none" pos="word" start_char="1162">research</TOKEN>
<TOKEN end_char="1170" id="token-9-8" morph="none" pos="punct" start_char="1170">,</TOKEN>
<TOKEN end_char="1176" id="token-9-9" morph="none" pos="word" start_char="1172">which</TOKEN>
<TOKEN end_char="1185" id="token-9-10" morph="none" pos="word" start_char="1178">involves</TOKEN>
<TOKEN end_char="1198" id="token-9-11" morph="none" pos="word" start_char="1187">manipulating</TOKEN>
<TOKEN end_char="1206" id="token-9-12" morph="none" pos="word" start_char="1200">viruses</TOKEN>
<TOKEN end_char="1209" id="token-9-13" morph="none" pos="word" start_char="1208">in</TOKEN>
<TOKEN end_char="1213" id="token-9-14" morph="none" pos="word" start_char="1211">the</TOKEN>
<TOKEN end_char="1217" id="token-9-15" morph="none" pos="word" start_char="1215">lab</TOKEN>
<TOKEN end_char="1220" id="token-9-16" morph="none" pos="word" start_char="1219">to</TOKEN>
<TOKEN end_char="1228" id="token-9-17" morph="none" pos="word" start_char="1222">explore</TOKEN>
<TOKEN end_char="1234" id="token-9-18" morph="none" pos="word" start_char="1230">their</TOKEN>
<TOKEN end_char="1244" id="token-9-19" morph="none" pos="word" start_char="1236">potential</TOKEN>
<TOKEN end_char="1248" id="token-9-20" morph="none" pos="word" start_char="1246">for</TOKEN>
<TOKEN end_char="1258" id="token-9-21" morph="none" pos="word" start_char="1250">infecting</TOKEN>
<TOKEN end_char="1265" id="token-9-22" morph="none" pos="word" start_char="1260">humans</TOKEN>
<TOKEN end_char="1266" id="token-9-23" morph="none" pos="punct" start_char="1266">,</TOKEN>
<TOKEN end_char="1274" id="token-9-24" morph="none" pos="word" start_char="1268">because</TOKEN>
<TOKEN end_char="1277" id="token-9-25" morph="none" pos="word" start_char="1276">it</TOKEN>
<TOKEN end_char="1285" id="token-9-26" morph="none" pos="word" start_char="1279">creates</TOKEN>
<TOKEN end_char="1287" id="token-9-27" morph="none" pos="word" start_char="1287">a</TOKEN>
<TOKEN end_char="1292" id="token-9-28" morph="none" pos="word" start_char="1289">risk</TOKEN>
<TOKEN end_char="1295" id="token-9-29" morph="none" pos="word" start_char="1294">of</TOKEN>
<TOKEN end_char="1304" id="token-9-30" morph="none" pos="word" start_char="1297">starting</TOKEN>
<TOKEN end_char="1306" id="token-9-31" morph="none" pos="word" start_char="1306">a</TOKEN>
<TOKEN end_char="1315" id="token-9-32" morph="none" pos="word" start_char="1308">pandemic</TOKEN>
<TOKEN end_char="1320" id="token-9-33" morph="none" pos="word" start_char="1317">from</TOKEN>
<TOKEN end_char="1331" id="token-9-34" morph="none" pos="word" start_char="1322">accidental</TOKEN>
<TOKEN end_char="1339" id="token-9-35" morph="none" pos="word" start_char="1333">release</TOKEN>
<TOKEN end_char="1340" id="token-9-36" morph="none" pos="punct" start_char="1340">.</TOKEN>
</SEG>
<SEG end_char="1435" id="segment-10" start_char="1343">
<ORIGINAL_TEXT>SARS-CoV-2 , the virus now causing a global pandemic, is believed to have originated in bats.</ORIGINAL_TEXT>
<TOKEN end_char="1352" id="token-10-0" morph="none" pos="unknown" start_char="1343">SARS-CoV-2</TOKEN>
<TOKEN end_char="1354" id="token-10-1" morph="none" pos="punct" start_char="1354">,</TOKEN>
<TOKEN end_char="1358" id="token-10-2" morph="none" pos="word" start_char="1356">the</TOKEN>
<TOKEN end_char="1364" id="token-10-3" morph="none" pos="word" start_char="1360">virus</TOKEN>
<TOKEN end_char="1368" id="token-10-4" morph="none" pos="word" start_char="1366">now</TOKEN>
<TOKEN end_char="1376" id="token-10-5" morph="none" pos="word" start_char="1370">causing</TOKEN>
<TOKEN end_char="1378" id="token-10-6" morph="none" pos="word" start_char="1378">a</TOKEN>
<TOKEN end_char="1385" id="token-10-7" morph="none" pos="word" start_char="1380">global</TOKEN>
<TOKEN end_char="1394" id="token-10-8" morph="none" pos="word" start_char="1387">pandemic</TOKEN>
<TOKEN end_char="1395" id="token-10-9" morph="none" pos="punct" start_char="1395">,</TOKEN>
<TOKEN end_char="1398" id="token-10-10" morph="none" pos="word" start_char="1397">is</TOKEN>
<TOKEN end_char="1407" id="token-10-11" morph="none" pos="word" start_char="1400">believed</TOKEN>
<TOKEN end_char="1410" id="token-10-12" morph="none" pos="word" start_char="1409">to</TOKEN>
<TOKEN end_char="1415" id="token-10-13" morph="none" pos="word" start_char="1412">have</TOKEN>
<TOKEN end_char="1426" id="token-10-14" morph="none" pos="word" start_char="1417">originated</TOKEN>
<TOKEN end_char="1429" id="token-10-15" morph="none" pos="word" start_char="1428">in</TOKEN>
<TOKEN end_char="1434" id="token-10-16" morph="none" pos="word" start_char="1431">bats</TOKEN>
<TOKEN end_char="1435" id="token-10-17" morph="none" pos="punct" start_char="1435">.</TOKEN>
</SEG>
<SEG end_char="1614" id="segment-11" start_char="1437">
<ORIGINAL_TEXT>U.S. intelligence, after originally asserting that the coronavirus had occurred naturally, conceded last month that the pandemic may have originated in a leak from the Wuhan lab.</ORIGINAL_TEXT>
<TOKEN end_char="1439" id="token-11-0" morph="none" pos="unknown" start_char="1437">U.S</TOKEN>
<TOKEN end_char="1440" id="token-11-1" morph="none" pos="punct" start_char="1440">.</TOKEN>
<TOKEN end_char="1453" id="token-11-2" morph="none" pos="word" start_char="1442">intelligence</TOKEN>
<TOKEN end_char="1454" id="token-11-3" morph="none" pos="punct" start_char="1454">,</TOKEN>
<TOKEN end_char="1460" id="token-11-4" morph="none" pos="word" start_char="1456">after</TOKEN>
<TOKEN end_char="1471" id="token-11-5" morph="none" pos="word" start_char="1462">originally</TOKEN>
<TOKEN end_char="1481" id="token-11-6" morph="none" pos="word" start_char="1473">asserting</TOKEN>
<TOKEN end_char="1486" id="token-11-7" morph="none" pos="word" start_char="1483">that</TOKEN>
<TOKEN end_char="1490" id="token-11-8" morph="none" pos="word" start_char="1488">the</TOKEN>
<TOKEN end_char="1502" id="token-11-9" morph="none" pos="word" start_char="1492">coronavirus</TOKEN>
<TOKEN end_char="1506" id="token-11-10" morph="none" pos="word" start_char="1504">had</TOKEN>
<TOKEN end_char="1515" id="token-11-11" morph="none" pos="word" start_char="1508">occurred</TOKEN>
<TOKEN end_char="1525" id="token-11-12" morph="none" pos="word" start_char="1517">naturally</TOKEN>
<TOKEN end_char="1526" id="token-11-13" morph="none" pos="punct" start_char="1526">,</TOKEN>
<TOKEN end_char="1535" id="token-11-14" morph="none" pos="word" start_char="1528">conceded</TOKEN>
<TOKEN end_char="1540" id="token-11-15" morph="none" pos="word" start_char="1537">last</TOKEN>
<TOKEN end_char="1546" id="token-11-16" morph="none" pos="word" start_char="1542">month</TOKEN>
<TOKEN end_char="1551" id="token-11-17" morph="none" pos="word" start_char="1548">that</TOKEN>
<TOKEN end_char="1555" id="token-11-18" morph="none" pos="word" start_char="1553">the</TOKEN>
<TOKEN end_char="1564" id="token-11-19" morph="none" pos="word" start_char="1557">pandemic</TOKEN>
<TOKEN end_char="1568" id="token-11-20" morph="none" pos="word" start_char="1566">may</TOKEN>
<TOKEN end_char="1573" id="token-11-21" morph="none" pos="word" start_char="1570">have</TOKEN>
<TOKEN end_char="1584" id="token-11-22" morph="none" pos="word" start_char="1575">originated</TOKEN>
<TOKEN end_char="1587" id="token-11-23" morph="none" pos="word" start_char="1586">in</TOKEN>
<TOKEN end_char="1589" id="token-11-24" morph="none" pos="word" start_char="1589">a</TOKEN>
<TOKEN end_char="1594" id="token-11-25" morph="none" pos="word" start_char="1591">leak</TOKEN>
<TOKEN end_char="1599" id="token-11-26" morph="none" pos="word" start_char="1596">from</TOKEN>
<TOKEN end_char="1603" id="token-11-27" morph="none" pos="word" start_char="1601">the</TOKEN>
<TOKEN end_char="1609" id="token-11-28" morph="none" pos="word" start_char="1605">Wuhan</TOKEN>
<TOKEN end_char="1613" id="token-11-29" morph="none" pos="word" start_char="1611">lab</TOKEN>
<TOKEN end_char="1614" id="token-11-30" morph="none" pos="punct" start_char="1614">.</TOKEN>
</SEG>
<SEG end_char="1734" id="segment-12" start_char="1616">
<ORIGINAL_TEXT>(At this point most scientists say it's possible—but not likely—that the pandemic virus was engineered or manipulated.)</ORIGINAL_TEXT>
<TOKEN end_char="1616" id="token-12-0" morph="none" pos="punct" start_char="1616">(</TOKEN>
<TOKEN end_char="1618" id="token-12-1" morph="none" pos="word" start_char="1617">At</TOKEN>
<TOKEN end_char="1623" id="token-12-2" morph="none" pos="word" start_char="1620">this</TOKEN>
<TOKEN end_char="1629" id="token-12-3" morph="none" pos="word" start_char="1625">point</TOKEN>
<TOKEN end_char="1634" id="token-12-4" morph="none" pos="word" start_char="1631">most</TOKEN>
<TOKEN end_char="1645" id="token-12-5" morph="none" pos="word" start_char="1636">scientists</TOKEN>
<TOKEN end_char="1649" id="token-12-6" morph="none" pos="word" start_char="1647">say</TOKEN>
<TOKEN end_char="1654" id="token-12-7" morph="none" pos="word" start_char="1651">it's</TOKEN>
<TOKEN end_char="1667" id="token-12-8" morph="none" pos="unknown" start_char="1656">possible—but</TOKEN>
<TOKEN end_char="1671" id="token-12-9" morph="none" pos="word" start_char="1669">not</TOKEN>
<TOKEN end_char="1683" id="token-12-10" morph="none" pos="unknown" start_char="1673">likely—that</TOKEN>
<TOKEN end_char="1687" id="token-12-11" morph="none" pos="word" start_char="1685">the</TOKEN>
<TOKEN end_char="1696" id="token-12-12" morph="none" pos="word" start_char="1689">pandemic</TOKEN>
<TOKEN end_char="1702" id="token-12-13" morph="none" pos="word" start_char="1698">virus</TOKEN>
<TOKEN end_char="1706" id="token-12-14" morph="none" pos="word" start_char="1704">was</TOKEN>
<TOKEN end_char="1717" id="token-12-15" morph="none" pos="word" start_char="1708">engineered</TOKEN>
<TOKEN end_char="1720" id="token-12-16" morph="none" pos="word" start_char="1719">or</TOKEN>
<TOKEN end_char="1732" id="token-12-17" morph="none" pos="word" start_char="1722">manipulated</TOKEN>
<TOKEN end_char="1734" id="token-12-18" morph="none" pos="punct" start_char="1733">.)</TOKEN>
</SEG>
<SEG end_char="1764" id="segment-13" start_char="1737">
<ORIGINAL_TEXT>Dr. Fauci did not respond to</ORIGINAL_TEXT>
<TOKEN end_char="1738" id="token-13-0" morph="none" pos="word" start_char="1737">Dr</TOKEN>
<TOKEN end_char="1739" id="token-13-1" morph="none" pos="punct" start_char="1739">.</TOKEN>
<TOKEN end_char="1745" id="token-13-2" morph="none" pos="word" start_char="1741">Fauci</TOKEN>
<TOKEN end_char="1749" id="token-13-3" morph="none" pos="word" start_char="1747">did</TOKEN>
<TOKEN end_char="1753" id="token-13-4" morph="none" pos="word" start_char="1751">not</TOKEN>
<TOKEN end_char="1761" id="token-13-5" morph="none" pos="word" start_char="1755">respond</TOKEN>
<TOKEN end_char="1764" id="token-13-6" morph="none" pos="word" start_char="1763">to</TOKEN>
</SEG>
<SEG end_char="1776" id="segment-14" start_char="1767">
<ORIGINAL_TEXT>Newsweek's</ORIGINAL_TEXT>
<TOKEN end_char="1776" id="token-14-0" morph="none" pos="word" start_char="1767">Newsweek's</TOKEN>
<TRANSLATED_TEXT>Newsweek</TRANSLATED_TEXT><DETECTED_LANGUAGE>af</DETECTED_LANGUAGE></SEG>
<SEG end_char="1799" id="segment-15" start_char="1779">
<ORIGINAL_TEXT>requests for comment.</ORIGINAL_TEXT>
<TOKEN end_char="1786" id="token-15-0" morph="none" pos="word" start_char="1779">requests</TOKEN>
<TOKEN end_char="1790" id="token-15-1" morph="none" pos="word" start_char="1788">for</TOKEN>
<TOKEN end_char="1798" id="token-15-2" morph="none" pos="word" start_char="1792">comment</TOKEN>
<TOKEN end_char="1799" id="token-15-3" morph="none" pos="punct" start_char="1799">.</TOKEN>
<TRANSLATED_TEXT>anmodninger om kommentarer.</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="2193" id="segment-16" start_char="1801">
<ORIGINAL_TEXT>NIH responded with a statement that said in part: "Most emerging human viruses come from wildlife, and these represent a significant threat to public health and biosecurity in the US and globally, as demonstrated by the SARS epidemic of 2002-03, and the current COVID-19 pandemic.... scientific research indicates that there is no evidence that suggests the virus was created in a laboratory."</ORIGINAL_TEXT>
<TOKEN end_char="1803" id="token-16-0" morph="none" pos="word" start_char="1801">NIH</TOKEN>
<TOKEN end_char="1813" id="token-16-1" morph="none" pos="word" start_char="1805">responded</TOKEN>
<TOKEN end_char="1818" id="token-16-2" morph="none" pos="word" start_char="1815">with</TOKEN>
<TOKEN end_char="1820" id="token-16-3" morph="none" pos="word" start_char="1820">a</TOKEN>
<TOKEN end_char="1830" id="token-16-4" morph="none" pos="word" start_char="1822">statement</TOKEN>
<TOKEN end_char="1835" id="token-16-5" morph="none" pos="word" start_char="1832">that</TOKEN>
<TOKEN end_char="1840" id="token-16-6" morph="none" pos="word" start_char="1837">said</TOKEN>
<TOKEN end_char="1843" id="token-16-7" morph="none" pos="word" start_char="1842">in</TOKEN>
<TOKEN end_char="1848" id="token-16-8" morph="none" pos="word" start_char="1845">part</TOKEN>
<TOKEN end_char="1849" id="token-16-9" morph="none" pos="punct" start_char="1849">:</TOKEN>
<TOKEN end_char="1851" id="token-16-10" morph="none" pos="punct" start_char="1851">"</TOKEN>
<TOKEN end_char="1855" id="token-16-11" morph="none" pos="word" start_char="1852">Most</TOKEN>
<TOKEN end_char="1864" id="token-16-12" morph="none" pos="word" start_char="1857">emerging</TOKEN>
<TOKEN end_char="1870" id="token-16-13" morph="none" pos="word" start_char="1866">human</TOKEN>
<TOKEN end_char="1878" id="token-16-14" morph="none" pos="word" start_char="1872">viruses</TOKEN>
<TOKEN end_char="1883" id="token-16-15" morph="none" pos="word" start_char="1880">come</TOKEN>
<TOKEN end_char="1888" id="token-16-16" morph="none" pos="word" start_char="1885">from</TOKEN>
<TOKEN end_char="1897" id="token-16-17" morph="none" pos="word" start_char="1890">wildlife</TOKEN>
<TOKEN end_char="1898" id="token-16-18" morph="none" pos="punct" start_char="1898">,</TOKEN>
<TOKEN end_char="1902" id="token-16-19" morph="none" pos="word" start_char="1900">and</TOKEN>
<TOKEN end_char="1908" id="token-16-20" morph="none" pos="word" start_char="1904">these</TOKEN>
<TOKEN end_char="1918" id="token-16-21" morph="none" pos="word" start_char="1910">represent</TOKEN>
<TOKEN end_char="1920" id="token-16-22" morph="none" pos="word" start_char="1920">a</TOKEN>
<TOKEN end_char="1932" id="token-16-23" morph="none" pos="word" start_char="1922">significant</TOKEN>
<TOKEN end_char="1939" id="token-16-24" morph="none" pos="word" start_char="1934">threat</TOKEN>
<TOKEN end_char="1942" id="token-16-25" morph="none" pos="word" start_char="1941">to</TOKEN>
<TOKEN end_char="1949" id="token-16-26" morph="none" pos="word" start_char="1944">public</TOKEN>
<TOKEN end_char="1956" id="token-16-27" morph="none" pos="word" start_char="1951">health</TOKEN>
<TOKEN end_char="1960" id="token-16-28" morph="none" pos="word" start_char="1958">and</TOKEN>
<TOKEN end_char="1972" id="token-16-29" morph="none" pos="word" start_char="1962">biosecurity</TOKEN>
<TOKEN end_char="1975" id="token-16-30" morph="none" pos="word" start_char="1974">in</TOKEN>
<TOKEN end_char="1979" id="token-16-31" morph="none" pos="word" start_char="1977">the</TOKEN>
<TOKEN end_char="1982" id="token-16-32" morph="none" pos="word" start_char="1981">US</TOKEN>
<TOKEN end_char="1986" id="token-16-33" morph="none" pos="word" start_char="1984">and</TOKEN>
<TOKEN end_char="1995" id="token-16-34" morph="none" pos="word" start_char="1988">globally</TOKEN>
<TOKEN end_char="1996" id="token-16-35" morph="none" pos="punct" start_char="1996">,</TOKEN>
<TOKEN end_char="1999" id="token-16-36" morph="none" pos="word" start_char="1998">as</TOKEN>
<TOKEN end_char="2012" id="token-16-37" morph="none" pos="word" start_char="2001">demonstrated</TOKEN>
<TOKEN end_char="2015" id="token-16-38" morph="none" pos="word" start_char="2014">by</TOKEN>
<TOKEN end_char="2019" id="token-16-39" morph="none" pos="word" start_char="2017">the</TOKEN>
<TOKEN end_char="2024" id="token-16-40" morph="none" pos="word" start_char="2021">SARS</TOKEN>
<TOKEN end_char="2033" id="token-16-41" morph="none" pos="word" start_char="2026">epidemic</TOKEN>
<TOKEN end_char="2036" id="token-16-42" morph="none" pos="word" start_char="2035">of</TOKEN>
<TOKEN end_char="2044" id="token-16-43" morph="none" pos="unknown" start_char="2038">2002-03</TOKEN>
<TOKEN end_char="2045" id="token-16-44" morph="none" pos="punct" start_char="2045">,</TOKEN>
<TOKEN end_char="2049" id="token-16-45" morph="none" pos="word" start_char="2047">and</TOKEN>
<TOKEN end_char="2053" id="token-16-46" morph="none" pos="word" start_char="2051">the</TOKEN>
<TOKEN end_char="2061" id="token-16-47" morph="none" pos="word" start_char="2055">current</TOKEN>
<TOKEN end_char="2070" id="token-16-48" morph="none" pos="unknown" start_char="2063">COVID-19</TOKEN>
<TOKEN end_char="2079" id="token-16-49" morph="none" pos="word" start_char="2072">pandemic</TOKEN>
<TOKEN end_char="2083" id="token-16-50" morph="none" pos="punct" start_char="2080">....</TOKEN>
<TOKEN end_char="2094" id="token-16-51" morph="none" pos="word" start_char="2085">scientific</TOKEN>
<TOKEN end_char="2103" id="token-16-52" morph="none" pos="word" start_char="2096">research</TOKEN>
<TOKEN end_char="2113" id="token-16-53" morph="none" pos="word" start_char="2105">indicates</TOKEN>
<TOKEN end_char="2118" id="token-16-54" morph="none" pos="word" start_char="2115">that</TOKEN>
<TOKEN end_char="2124" id="token-16-55" morph="none" pos="word" start_char="2120">there</TOKEN>
<TOKEN end_char="2127" id="token-16-56" morph="none" pos="word" start_char="2126">is</TOKEN>
<TOKEN end_char="2130" id="token-16-57" morph="none" pos="word" start_char="2129">no</TOKEN>
<TOKEN end_char="2139" id="token-16-58" morph="none" pos="word" start_char="2132">evidence</TOKEN>
<TOKEN end_char="2144" id="token-16-59" morph="none" pos="word" start_char="2141">that</TOKEN>
<TOKEN end_char="2153" id="token-16-60" morph="none" pos="word" start_char="2146">suggests</TOKEN>
<TOKEN end_char="2157" id="token-16-61" morph="none" pos="word" start_char="2155">the</TOKEN>
<TOKEN end_char="2163" id="token-16-62" morph="none" pos="word" start_char="2159">virus</TOKEN>
<TOKEN end_char="2167" id="token-16-63" morph="none" pos="word" start_char="2165">was</TOKEN>
<TOKEN end_char="2175" id="token-16-64" morph="none" pos="word" start_char="2169">created</TOKEN>
<TOKEN end_char="2178" id="token-16-65" morph="none" pos="word" start_char="2177">in</TOKEN>
<TOKEN end_char="2180" id="token-16-66" morph="none" pos="word" start_char="2180">a</TOKEN>
<TOKEN end_char="2191" id="token-16-67" morph="none" pos="word" start_char="2182">laboratory</TOKEN>
<TOKEN end_char="2193" id="token-16-68" morph="none" pos="punct" start_char="2192">."</TOKEN>
</SEG>
<SEG end_char="2235" id="segment-17" start_char="2196">
<ORIGINAL_TEXT>The NIH research consisted of two parts.</ORIGINAL_TEXT>
<TOKEN end_char="2198" id="token-17-0" morph="none" pos="word" start_char="2196">The</TOKEN>
<TOKEN end_char="2202" id="token-17-1" morph="none" pos="word" start_char="2200">NIH</TOKEN>
<TOKEN end_char="2211" id="token-17-2" morph="none" pos="word" start_char="2204">research</TOKEN>
<TOKEN end_char="2221" id="token-17-3" morph="none" pos="word" start_char="2213">consisted</TOKEN>
<TOKEN end_char="2224" id="token-17-4" morph="none" pos="word" start_char="2223">of</TOKEN>
<TOKEN end_char="2228" id="token-17-5" morph="none" pos="word" start_char="2226">two</TOKEN>
<TOKEN end_char="2234" id="token-17-6" morph="none" pos="word" start_char="2230">parts</TOKEN>
<TOKEN end_char="2235" id="token-17-7" morph="none" pos="punct" start_char="2235">.</TOKEN>
</SEG>
<SEG end_char="2346" id="segment-18" start_char="2237">
<ORIGINAL_TEXT>The first part began in 2014 and involved surveillance of bat coronaviruses, and had a budget of $3.7 million.</ORIGINAL_TEXT>
<TOKEN end_char="2239" id="token-18-0" morph="none" pos="word" start_char="2237">The</TOKEN>
<TOKEN end_char="2245" id="token-18-1" morph="none" pos="word" start_char="2241">first</TOKEN>
<TOKEN end_char="2250" id="token-18-2" morph="none" pos="word" start_char="2247">part</TOKEN>
<TOKEN end_char="2256" id="token-18-3" morph="none" pos="word" start_char="2252">began</TOKEN>
<TOKEN end_char="2259" id="token-18-4" morph="none" pos="word" start_char="2258">in</TOKEN>
<TOKEN end_char="2264" id="token-18-5" morph="none" pos="word" start_char="2261">2014</TOKEN>
<TOKEN end_char="2268" id="token-18-6" morph="none" pos="word" start_char="2266">and</TOKEN>
<TOKEN end_char="2277" id="token-18-7" morph="none" pos="word" start_char="2270">involved</TOKEN>
<TOKEN end_char="2290" id="token-18-8" morph="none" pos="word" start_char="2279">surveillance</TOKEN>
<TOKEN end_char="2293" id="token-18-9" morph="none" pos="word" start_char="2292">of</TOKEN>
<TOKEN end_char="2297" id="token-18-10" morph="none" pos="word" start_char="2295">bat</TOKEN>
<TOKEN end_char="2311" id="token-18-11" morph="none" pos="word" start_char="2299">coronaviruses</TOKEN>
<TOKEN end_char="2312" id="token-18-12" morph="none" pos="punct" start_char="2312">,</TOKEN>
<TOKEN end_char="2316" id="token-18-13" morph="none" pos="word" start_char="2314">and</TOKEN>
<TOKEN end_char="2320" id="token-18-14" morph="none" pos="word" start_char="2318">had</TOKEN>
<TOKEN end_char="2322" id="token-18-15" morph="none" pos="word" start_char="2322">a</TOKEN>
<TOKEN end_char="2329" id="token-18-16" morph="none" pos="word" start_char="2324">budget</TOKEN>
<TOKEN end_char="2332" id="token-18-17" morph="none" pos="word" start_char="2331">of</TOKEN>
<TOKEN end_char="2337" id="token-18-18" morph="none" pos="unknown" start_char="2334">$3.7</TOKEN>
<TOKEN end_char="2345" id="token-18-19" morph="none" pos="word" start_char="2339">million</TOKEN>
<TOKEN end_char="2346" id="token-18-20" morph="none" pos="punct" start_char="2346">.</TOKEN>
</SEG>
<SEG end_char="2492" id="segment-19" start_char="2348">
<ORIGINAL_TEXT>The program funded Shi Zheng-Li, a virologist at the Wuhan lab, and other researchers to investigate and catalogue bat coronaviruses in the wild.</ORIGINAL_TEXT>
<TOKEN end_char="2350" id="token-19-0" morph="none" pos="word" start_char="2348">The</TOKEN>
<TOKEN end_char="2358" id="token-19-1" morph="none" pos="word" start_char="2352">program</TOKEN>
<TOKEN end_char="2365" id="token-19-2" morph="none" pos="word" start_char="2360">funded</TOKEN>
<TOKEN end_char="2369" id="token-19-3" morph="none" pos="word" start_char="2367">Shi</TOKEN>
<TOKEN end_char="2378" id="token-19-4" morph="none" pos="unknown" start_char="2371">Zheng-Li</TOKEN>
<TOKEN end_char="2379" id="token-19-5" morph="none" pos="punct" start_char="2379">,</TOKEN>
<TOKEN end_char="2381" id="token-19-6" morph="none" pos="word" start_char="2381">a</TOKEN>
<TOKEN end_char="2392" id="token-19-7" morph="none" pos="word" start_char="2383">virologist</TOKEN>
<TOKEN end_char="2395" id="token-19-8" morph="none" pos="word" start_char="2394">at</TOKEN>
<TOKEN end_char="2399" id="token-19-9" morph="none" pos="word" start_char="2397">the</TOKEN>
<TOKEN end_char="2405" id="token-19-10" morph="none" pos="word" start_char="2401">Wuhan</TOKEN>
<TOKEN end_char="2409" id="token-19-11" morph="none" pos="word" start_char="2407">lab</TOKEN>
<TOKEN end_char="2410" id="token-19-12" morph="none" pos="punct" start_char="2410">,</TOKEN>
<TOKEN end_char="2414" id="token-19-13" morph="none" pos="word" start_char="2412">and</TOKEN>
<TOKEN end_char="2420" id="token-19-14" morph="none" pos="word" start_char="2416">other</TOKEN>
<TOKEN end_char="2432" id="token-19-15" morph="none" pos="word" start_char="2422">researchers</TOKEN>
<TOKEN end_char="2435" id="token-19-16" morph="none" pos="word" start_char="2434">to</TOKEN>
<TOKEN end_char="2447" id="token-19-17" morph="none" pos="word" start_char="2437">investigate</TOKEN>
<TOKEN end_char="2451" id="token-19-18" morph="none" pos="word" start_char="2449">and</TOKEN>
<TOKEN end_char="2461" id="token-19-19" morph="none" pos="word" start_char="2453">catalogue</TOKEN>
<TOKEN end_char="2465" id="token-19-20" morph="none" pos="word" start_char="2463">bat</TOKEN>
<TOKEN end_char="2479" id="token-19-21" morph="none" pos="word" start_char="2467">coronaviruses</TOKEN>
<TOKEN end_char="2482" id="token-19-22" morph="none" pos="word" start_char="2481">in</TOKEN>
<TOKEN end_char="2486" id="token-19-23" morph="none" pos="word" start_char="2484">the</TOKEN>
<TOKEN end_char="2491" id="token-19-24" morph="none" pos="word" start_char="2488">wild</TOKEN>
<TOKEN end_char="2492" id="token-19-25" morph="none" pos="punct" start_char="2492">.</TOKEN>
</SEG>
<SEG end_char="2540" id="segment-20" start_char="2494">
<ORIGINAL_TEXT>This part of the project was completed in 2019.</ORIGINAL_TEXT>
<TOKEN end_char="2497" id="token-20-0" morph="none" pos="word" start_char="2494">This</TOKEN>
<TOKEN end_char="2502" id="token-20-1" morph="none" pos="word" start_char="2499">part</TOKEN>
<TOKEN end_char="2505" id="token-20-2" morph="none" pos="word" start_char="2504">of</TOKEN>
<TOKEN end_char="2509" id="token-20-3" morph="none" pos="word" start_char="2507">the</TOKEN>
<TOKEN end_char="2517" id="token-20-4" morph="none" pos="word" start_char="2511">project</TOKEN>
<TOKEN end_char="2521" id="token-20-5" morph="none" pos="word" start_char="2519">was</TOKEN>
<TOKEN end_char="2531" id="token-20-6" morph="none" pos="word" start_char="2523">completed</TOKEN>
<TOKEN end_char="2534" id="token-20-7" morph="none" pos="word" start_char="2533">in</TOKEN>
<TOKEN end_char="2539" id="token-20-8" morph="none" pos="word" start_char="2536">2019</TOKEN>
<TOKEN end_char="2540" id="token-20-9" morph="none" pos="punct" start_char="2540">.</TOKEN>
</SEG>
<SEG end_char="2752" id="segment-21" start_char="2543">
<ORIGINAL_TEXT>A second phase of the project, beginning that year, included additional surveillance work but also gain-of-function research for the purpose of understanding how bat coronaviruses could mutate to attack humans.</ORIGINAL_TEXT>
<TOKEN end_char="2543" id="token-21-0" morph="none" pos="word" start_char="2543">A</TOKEN>
<TOKEN end_char="2550" id="token-21-1" morph="none" pos="word" start_char="2545">second</TOKEN>
<TOKEN end_char="2556" id="token-21-2" morph="none" pos="word" start_char="2552">phase</TOKEN>
<TOKEN end_char="2559" id="token-21-3" morph="none" pos="word" start_char="2558">of</TOKEN>
<TOKEN end_char="2563" id="token-21-4" morph="none" pos="word" start_char="2561">the</TOKEN>
<TOKEN end_char="2571" id="token-21-5" morph="none" pos="word" start_char="2565">project</TOKEN>
<TOKEN end_char="2572" id="token-21-6" morph="none" pos="punct" start_char="2572">,</TOKEN>
<TOKEN end_char="2582" id="token-21-7" morph="none" pos="word" start_char="2574">beginning</TOKEN>
<TOKEN end_char="2587" id="token-21-8" morph="none" pos="word" start_char="2584">that</TOKEN>
<TOKEN end_char="2592" id="token-21-9" morph="none" pos="word" start_char="2589">year</TOKEN>
<TOKEN end_char="2593" id="token-21-10" morph="none" pos="punct" start_char="2593">,</TOKEN>
<TOKEN end_char="2602" id="token-21-11" morph="none" pos="word" start_char="2595">included</TOKEN>
<TOKEN end_char="2613" id="token-21-12" morph="none" pos="word" start_char="2604">additional</TOKEN>
<TOKEN end_char="2626" id="token-21-13" morph="none" pos="word" start_char="2615">surveillance</TOKEN>
<TOKEN end_char="2631" id="token-21-14" morph="none" pos="word" start_char="2628">work</TOKEN>
<TOKEN end_char="2635" id="token-21-15" morph="none" pos="word" start_char="2633">but</TOKEN>
<TOKEN end_char="2640" id="token-21-16" morph="none" pos="word" start_char="2637">also</TOKEN>
<TOKEN end_char="2657" id="token-21-17" morph="none" pos="unknown" start_char="2642">gain-of-function</TOKEN>
<TOKEN end_char="2666" id="token-21-18" morph="none" pos="word" start_char="2659">research</TOKEN>
<TOKEN end_char="2670" id="token-21-19" morph="none" pos="word" start_char="2668">for</TOKEN>
<TOKEN end_char="2674" id="token-21-20" morph="none" pos="word" start_char="2672">the</TOKEN>
<TOKEN end_char="2682" id="token-21-21" morph="none" pos="word" start_char="2676">purpose</TOKEN>
<TOKEN end_char="2685" id="token-21-22" morph="none" pos="word" start_char="2684">of</TOKEN>
<TOKEN end_char="2699" id="token-21-23" morph="none" pos="word" start_char="2687">understanding</TOKEN>
<TOKEN end_char="2703" id="token-21-24" morph="none" pos="word" start_char="2701">how</TOKEN>
<TOKEN end_char="2707" id="token-21-25" morph="none" pos="word" start_char="2705">bat</TOKEN>
<TOKEN end_char="2721" id="token-21-26" morph="none" pos="word" start_char="2709">coronaviruses</TOKEN>
<TOKEN end_char="2727" id="token-21-27" morph="none" pos="word" start_char="2723">could</TOKEN>
<TOKEN end_char="2734" id="token-21-28" morph="none" pos="word" start_char="2729">mutate</TOKEN>
<TOKEN end_char="2737" id="token-21-29" morph="none" pos="word" start_char="2736">to</TOKEN>
<TOKEN end_char="2744" id="token-21-30" morph="none" pos="word" start_char="2739">attack</TOKEN>
<TOKEN end_char="2751" id="token-21-31" morph="none" pos="word" start_char="2746">humans</TOKEN>
<TOKEN end_char="2752" id="token-21-32" morph="none" pos="punct" start_char="2752">.</TOKEN>
</SEG>
<SEG end_char="2901" id="segment-22" start_char="2754">
<ORIGINAL_TEXT>The project was run by EcoHealth Alliance, a non-profit research group, under the direction of President Peter Daszak, an expert on disease ecology.</ORIGINAL_TEXT>
<TOKEN end_char="2756" id="token-22-0" morph="none" pos="word" start_char="2754">The</TOKEN>
<TOKEN end_char="2764" id="token-22-1" morph="none" pos="word" start_char="2758">project</TOKEN>
<TOKEN end_char="2768" id="token-22-2" morph="none" pos="word" start_char="2766">was</TOKEN>
<TOKEN end_char="2772" id="token-22-3" morph="none" pos="word" start_char="2770">run</TOKEN>
<TOKEN end_char="2775" id="token-22-4" morph="none" pos="word" start_char="2774">by</TOKEN>
<TOKEN end_char="2785" id="token-22-5" morph="none" pos="word" start_char="2777">EcoHealth</TOKEN>
<TOKEN end_char="2794" id="token-22-6" morph="none" pos="word" start_char="2787">Alliance</TOKEN>
<TOKEN end_char="2795" id="token-22-7" morph="none" pos="punct" start_char="2795">,</TOKEN>
<TOKEN end_char="2797" id="token-22-8" morph="none" pos="word" start_char="2797">a</TOKEN>
<TOKEN end_char="2808" id="token-22-9" morph="none" pos="unknown" start_char="2799">non-profit</TOKEN>
<TOKEN end_char="2817" id="token-22-10" morph="none" pos="word" start_char="2810">research</TOKEN>
<TOKEN end_char="2823" id="token-22-11" morph="none" pos="word" start_char="2819">group</TOKEN>
<TOKEN end_char="2824" id="token-22-12" morph="none" pos="punct" start_char="2824">,</TOKEN>
<TOKEN end_char="2830" id="token-22-13" morph="none" pos="word" start_char="2826">under</TOKEN>
<TOKEN end_char="2834" id="token-22-14" morph="none" pos="word" start_char="2832">the</TOKEN>
<TOKEN end_char="2844" id="token-22-15" morph="none" pos="word" start_char="2836">direction</TOKEN>
<TOKEN end_char="2847" id="token-22-16" morph="none" pos="word" start_char="2846">of</TOKEN>
<TOKEN end_char="2857" id="token-22-17" morph="none" pos="word" start_char="2849">President</TOKEN>
<TOKEN end_char="2863" id="token-22-18" morph="none" pos="word" start_char="2859">Peter</TOKEN>
<TOKEN end_char="2870" id="token-22-19" morph="none" pos="word" start_char="2865">Daszak</TOKEN>
<TOKEN end_char="2871" id="token-22-20" morph="none" pos="punct" start_char="2871">,</TOKEN>
<TOKEN end_char="2874" id="token-22-21" morph="none" pos="word" start_char="2873">an</TOKEN>
<TOKEN end_char="2881" id="token-22-22" morph="none" pos="word" start_char="2876">expert</TOKEN>
<TOKEN end_char="2884" id="token-22-23" morph="none" pos="word" start_char="2883">on</TOKEN>
<TOKEN end_char="2892" id="token-22-24" morph="none" pos="word" start_char="2886">disease</TOKEN>
<TOKEN end_char="2900" id="token-22-25" morph="none" pos="word" start_char="2894">ecology</TOKEN>
<TOKEN end_char="2901" id="token-22-26" morph="none" pos="punct" start_char="2901">.</TOKEN>
</SEG>
<SEG end_char="2961" id="segment-23" start_char="2903">
<ORIGINAL_TEXT>NIH canceled the project just this past Friday, April 24th,</ORIGINAL_TEXT>
<TOKEN end_char="2905" id="token-23-0" morph="none" pos="word" start_char="2903">NIH</TOKEN>
<TOKEN end_char="2914" id="token-23-1" morph="none" pos="word" start_char="2907">canceled</TOKEN>
<TOKEN end_char="2918" id="token-23-2" morph="none" pos="word" start_char="2916">the</TOKEN>
<TOKEN end_char="2926" id="token-23-3" morph="none" pos="word" start_char="2920">project</TOKEN>
<TOKEN end_char="2931" id="token-23-4" morph="none" pos="word" start_char="2928">just</TOKEN>
<TOKEN end_char="2936" id="token-23-5" morph="none" pos="word" start_char="2933">this</TOKEN>
<TOKEN end_char="2941" id="token-23-6" morph="none" pos="word" start_char="2938">past</TOKEN>
<TOKEN end_char="2948" id="token-23-7" morph="none" pos="word" start_char="2943">Friday</TOKEN>
<TOKEN end_char="2949" id="token-23-8" morph="none" pos="punct" start_char="2949">,</TOKEN>
<TOKEN end_char="2955" id="token-23-9" morph="none" pos="word" start_char="2951">April</TOKEN>
<TOKEN end_char="2960" id="token-23-10" morph="none" pos="word" start_char="2957">24th</TOKEN>
<TOKEN end_char="2961" id="token-23-11" morph="none" pos="punct" start_char="2961">,</TOKEN>
</SEG>
<SEG end_char="2971" id="segment-24" start_char="2964">
<ORIGINAL_TEXT>Politico</ORIGINAL_TEXT>
<TOKEN end_char="2971" id="token-24-0" morph="none" pos="word" start_char="2964">Politico</TOKEN>
<TRANSLATED_TEXT>Politics</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="2982" id="segment-25" start_char="2974">
<ORIGINAL_TEXT>reported.</ORIGINAL_TEXT>
<TOKEN end_char="2981" id="token-25-0" morph="none" pos="word" start_char="2974">reported</TOKEN>
<TOKEN end_char="2982" id="token-25-1" morph="none" pos="punct" start_char="2982">.</TOKEN>
</SEG>
<SEG end_char="3020" id="segment-26" start_char="2984">
<ORIGINAL_TEXT>Daszak did not immediately respond to</ORIGINAL_TEXT>
<TOKEN end_char="2989" id="token-26-0" morph="none" pos="word" start_char="2984">Daszak</TOKEN>
<TOKEN end_char="2993" id="token-26-1" morph="none" pos="word" start_char="2991">did</TOKEN>
<TOKEN end_char="2997" id="token-26-2" morph="none" pos="word" start_char="2995">not</TOKEN>
<TOKEN end_char="3009" id="token-26-3" morph="none" pos="word" start_char="2999">immediately</TOKEN>
<TOKEN end_char="3017" id="token-26-4" morph="none" pos="word" start_char="3011">respond</TOKEN>
<TOKEN end_char="3020" id="token-26-5" morph="none" pos="word" start_char="3019">to</TOKEN>
</SEG>
<SEG end_char="3030" id="segment-27" start_char="3023">
<ORIGINAL_TEXT>Newsweek</ORIGINAL_TEXT>
<TOKEN end_char="3030" id="token-27-0" morph="none" pos="word" start_char="3023">Newsweek</TOKEN>
</SEG>
<SEG end_char="3053" id="segment-28" start_char="3033">
<ORIGINAL_TEXT>requests for comment.</ORIGINAL_TEXT>
<TOKEN end_char="3040" id="token-28-0" morph="none" pos="word" start_char="3033">requests</TOKEN>
<TOKEN end_char="3044" id="token-28-1" morph="none" pos="word" start_char="3042">for</TOKEN>
<TOKEN end_char="3052" id="token-28-2" morph="none" pos="word" start_char="3046">comment</TOKEN>
<TOKEN end_char="3053" id="token-28-3" morph="none" pos="punct" start_char="3053">.</TOKEN>
<TRANSLATED_TEXT>anmodninger om kommentarer.</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="3331" id="segment-29" start_char="3056">
<ORIGINAL_TEXT>The project proposal states: "We will use S protein sequence data, infectious clone technology, in vitro and in vivo infection experiments and analysis of receptor binding to test the hypothesis that % divergence thresholds in S protein sequences predict spillover potential."</ORIGINAL_TEXT>
<TOKEN end_char="3058" id="token-29-0" morph="none" pos="word" start_char="3056">The</TOKEN>
<TOKEN end_char="3066" id="token-29-1" morph="none" pos="word" start_char="3060">project</TOKEN>
<TOKEN end_char="3075" id="token-29-2" morph="none" pos="word" start_char="3068">proposal</TOKEN>
<TOKEN end_char="3082" id="token-29-3" morph="none" pos="word" start_char="3077">states</TOKEN>
<TOKEN end_char="3083" id="token-29-4" morph="none" pos="punct" start_char="3083">:</TOKEN>
<TOKEN end_char="3085" id="token-29-5" morph="none" pos="punct" start_char="3085">"</TOKEN>
<TOKEN end_char="3087" id="token-29-6" morph="none" pos="word" start_char="3086">We</TOKEN>
<TOKEN end_char="3092" id="token-29-7" morph="none" pos="word" start_char="3089">will</TOKEN>
<TOKEN end_char="3096" id="token-29-8" morph="none" pos="word" start_char="3094">use</TOKEN>
<TOKEN end_char="3098" id="token-29-9" morph="none" pos="word" start_char="3098">S</TOKEN>
<TOKEN end_char="3106" id="token-29-10" morph="none" pos="word" start_char="3100">protein</TOKEN>
<TOKEN end_char="3115" id="token-29-11" morph="none" pos="word" start_char="3108">sequence</TOKEN>
<TOKEN end_char="3120" id="token-29-12" morph="none" pos="word" start_char="3117">data</TOKEN>
<TOKEN end_char="3121" id="token-29-13" morph="none" pos="punct" start_char="3121">,</TOKEN>
<TOKEN end_char="3132" id="token-29-14" morph="none" pos="word" start_char="3123">infectious</TOKEN>
<TOKEN end_char="3138" id="token-29-15" morph="none" pos="word" start_char="3134">clone</TOKEN>
<TOKEN end_char="3149" id="token-29-16" morph="none" pos="word" start_char="3140">technology</TOKEN>
<TOKEN end_char="3150" id="token-29-17" morph="none" pos="punct" start_char="3150">,</TOKEN>
<TOKEN end_char="3153" id="token-29-18" morph="none" pos="word" start_char="3152">in</TOKEN>
<TOKEN end_char="3159" id="token-29-19" morph="none" pos="word" start_char="3155">vitro</TOKEN>
<TOKEN end_char="3163" id="token-29-20" morph="none" pos="word" start_char="3161">and</TOKEN>
<TOKEN end_char="3166" id="token-29-21" morph="none" pos="word" start_char="3165">in</TOKEN>
<TOKEN end_char="3171" id="token-29-22" morph="none" pos="word" start_char="3168">vivo</TOKEN>
<TOKEN end_char="3181" id="token-29-23" morph="none" pos="word" start_char="3173">infection</TOKEN>
<TOKEN end_char="3193" id="token-29-24" morph="none" pos="word" start_char="3183">experiments</TOKEN>
<TOKEN end_char="3197" id="token-29-25" morph="none" pos="word" start_char="3195">and</TOKEN>
<TOKEN end_char="3206" id="token-29-26" morph="none" pos="word" start_char="3199">analysis</TOKEN>
<TOKEN end_char="3209" id="token-29-27" morph="none" pos="word" start_char="3208">of</TOKEN>
<TOKEN end_char="3218" id="token-29-28" morph="none" pos="word" start_char="3211">receptor</TOKEN>
<TOKEN end_char="3226" id="token-29-29" morph="none" pos="word" start_char="3220">binding</TOKEN>
<TOKEN end_char="3229" id="token-29-30" morph="none" pos="word" start_char="3228">to</TOKEN>
<TOKEN end_char="3234" id="token-29-31" morph="none" pos="word" start_char="3231">test</TOKEN>
<TOKEN end_char="3238" id="token-29-32" morph="none" pos="word" start_char="3236">the</TOKEN>
<TOKEN end_char="3249" id="token-29-33" morph="none" pos="word" start_char="3240">hypothesis</TOKEN>
<TOKEN end_char="3254" id="token-29-34" morph="none" pos="word" start_char="3251">that</TOKEN>
<TOKEN end_char="3256" id="token-29-35" morph="none" pos="punct" start_char="3256">%</TOKEN>
<TOKEN end_char="3267" id="token-29-36" morph="none" pos="word" start_char="3258">divergence</TOKEN>
<TOKEN end_char="3278" id="token-29-37" morph="none" pos="word" start_char="3269">thresholds</TOKEN>
<TOKEN end_char="3281" id="token-29-38" morph="none" pos="word" start_char="3280">in</TOKEN>
<TOKEN end_char="3283" id="token-29-39" morph="none" pos="word" start_char="3283">S</TOKEN>
<TOKEN end_char="3291" id="token-29-40" morph="none" pos="word" start_char="3285">protein</TOKEN>
<TOKEN end_char="3301" id="token-29-41" morph="none" pos="word" start_char="3293">sequences</TOKEN>
<TOKEN end_char="3309" id="token-29-42" morph="none" pos="word" start_char="3303">predict</TOKEN>
<TOKEN end_char="3319" id="token-29-43" morph="none" pos="word" start_char="3311">spillover</TOKEN>
<TOKEN end_char="3329" id="token-29-44" morph="none" pos="word" start_char="3321">potential</TOKEN>
<TOKEN end_char="3331" id="token-29-45" morph="none" pos="punct" start_char="3330">."</TOKEN>
</SEG>
<SEG end_char="3523" id="segment-30" start_char="3334">
<ORIGINAL_TEXT>In layman's terms, "spillover potential" refers to the ability of a virus to jump from animals to humans, which requires that the virus be able to attach to receptors in the cells of humans.</ORIGINAL_TEXT>
<TOKEN end_char="3335" id="token-30-0" morph="none" pos="word" start_char="3334">In</TOKEN>
<TOKEN end_char="3344" id="token-30-1" morph="none" pos="word" start_char="3337">layman's</TOKEN>
<TOKEN end_char="3350" id="token-30-2" morph="none" pos="word" start_char="3346">terms</TOKEN>
<TOKEN end_char="3351" id="token-30-3" morph="none" pos="punct" start_char="3351">,</TOKEN>
<TOKEN end_char="3353" id="token-30-4" morph="none" pos="punct" start_char="3353">"</TOKEN>
<TOKEN end_char="3362" id="token-30-5" morph="none" pos="word" start_char="3354">spillover</TOKEN>
<TOKEN end_char="3372" id="token-30-6" morph="none" pos="word" start_char="3364">potential</TOKEN>
<TOKEN end_char="3373" id="token-30-7" morph="none" pos="punct" start_char="3373">"</TOKEN>
<TOKEN end_char="3380" id="token-30-8" morph="none" pos="word" start_char="3375">refers</TOKEN>
<TOKEN end_char="3383" id="token-30-9" morph="none" pos="word" start_char="3382">to</TOKEN>
<TOKEN end_char="3387" id="token-30-10" morph="none" pos="word" start_char="3385">the</TOKEN>
<TOKEN end_char="3395" id="token-30-11" morph="none" pos="word" start_char="3389">ability</TOKEN>
<TOKEN end_char="3398" id="token-30-12" morph="none" pos="word" start_char="3397">of</TOKEN>
<TOKEN end_char="3400" id="token-30-13" morph="none" pos="word" start_char="3400">a</TOKEN>
<TOKEN end_char="3406" id="token-30-14" morph="none" pos="word" start_char="3402">virus</TOKEN>
<TOKEN end_char="3409" id="token-30-15" morph="none" pos="word" start_char="3408">to</TOKEN>
<TOKEN end_char="3414" id="token-30-16" morph="none" pos="word" start_char="3411">jump</TOKEN>
<TOKEN end_char="3419" id="token-30-17" morph="none" pos="word" start_char="3416">from</TOKEN>
<TOKEN end_char="3427" id="token-30-18" morph="none" pos="word" start_char="3421">animals</TOKEN>
<TOKEN end_char="3430" id="token-30-19" morph="none" pos="word" start_char="3429">to</TOKEN>
<TOKEN end_char="3437" id="token-30-20" morph="none" pos="word" start_char="3432">humans</TOKEN>
<TOKEN end_char="3438" id="token-30-21" morph="none" pos="punct" start_char="3438">,</TOKEN>
<TOKEN end_char="3444" id="token-30-22" morph="none" pos="word" start_char="3440">which</TOKEN>
<TOKEN end_char="3453" id="token-30-23" morph="none" pos="word" start_char="3446">requires</TOKEN>
<TOKEN end_char="3458" id="token-30-24" morph="none" pos="word" start_char="3455">that</TOKEN>
<TOKEN end_char="3462" id="token-30-25" morph="none" pos="word" start_char="3460">the</TOKEN>
<TOKEN end_char="3468" id="token-30-26" morph="none" pos="word" start_char="3464">virus</TOKEN>
<TOKEN end_char="3471" id="token-30-27" morph="none" pos="word" start_char="3470">be</TOKEN>
<TOKEN end_char="3476" id="token-30-28" morph="none" pos="word" start_char="3473">able</TOKEN>
<TOKEN end_char="3479" id="token-30-29" morph="none" pos="word" start_char="3478">to</TOKEN>
<TOKEN end_char="3486" id="token-30-30" morph="none" pos="word" start_char="3481">attach</TOKEN>
<TOKEN end_char="3489" id="token-30-31" morph="none" pos="word" start_char="3488">to</TOKEN>
<TOKEN end_char="3499" id="token-30-32" morph="none" pos="word" start_char="3491">receptors</TOKEN>
<TOKEN end_char="3502" id="token-30-33" morph="none" pos="word" start_char="3501">in</TOKEN>
<TOKEN end_char="3506" id="token-30-34" morph="none" pos="word" start_char="3504">the</TOKEN>
<TOKEN end_char="3512" id="token-30-35" morph="none" pos="word" start_char="3508">cells</TOKEN>
<TOKEN end_char="3515" id="token-30-36" morph="none" pos="word" start_char="3514">of</TOKEN>
<TOKEN end_char="3522" id="token-30-37" morph="none" pos="word" start_char="3517">humans</TOKEN>
<TOKEN end_char="3523" id="token-30-38" morph="none" pos="punct" start_char="3523">.</TOKEN>
</SEG>
<SEG end_char="3623" id="segment-31" start_char="3525">
<ORIGINAL_TEXT>SARS-CoV-2, for instance, is adept at binding to the ACE2 receptor in human lungs and other organs.</ORIGINAL_TEXT>
<TOKEN end_char="3534" id="token-31-0" morph="none" pos="unknown" start_char="3525">SARS-CoV-2</TOKEN>
<TOKEN end_char="3535" id="token-31-1" morph="none" pos="punct" start_char="3535">,</TOKEN>
<TOKEN end_char="3539" id="token-31-2" morph="none" pos="word" start_char="3537">for</TOKEN>
<TOKEN end_char="3548" id="token-31-3" morph="none" pos="word" start_char="3541">instance</TOKEN>
<TOKEN end_char="3549" id="token-31-4" morph="none" pos="punct" start_char="3549">,</TOKEN>
<TOKEN end_char="3552" id="token-31-5" morph="none" pos="word" start_char="3551">is</TOKEN>
<TOKEN end_char="3558" id="token-31-6" morph="none" pos="word" start_char="3554">adept</TOKEN>
<TOKEN end_char="3561" id="token-31-7" morph="none" pos="word" start_char="3560">at</TOKEN>
<TOKEN end_char="3569" id="token-31-8" morph="none" pos="word" start_char="3563">binding</TOKEN>
<TOKEN end_char="3572" id="token-31-9" morph="none" pos="word" start_char="3571">to</TOKEN>
<TOKEN end_char="3576" id="token-31-10" morph="none" pos="word" start_char="3574">the</TOKEN>
<TOKEN end_char="3581" id="token-31-11" morph="none" pos="word" start_char="3578">ACE2</TOKEN>
<TOKEN end_char="3590" id="token-31-12" morph="none" pos="word" start_char="3583">receptor</TOKEN>
<TOKEN end_char="3593" id="token-31-13" morph="none" pos="word" start_char="3592">in</TOKEN>
<TOKEN end_char="3599" id="token-31-14" morph="none" pos="word" start_char="3595">human</TOKEN>
<TOKEN end_char="3605" id="token-31-15" morph="none" pos="word" start_char="3601">lungs</TOKEN>
<TOKEN end_char="3609" id="token-31-16" morph="none" pos="word" start_char="3607">and</TOKEN>
<TOKEN end_char="3615" id="token-31-17" morph="none" pos="word" start_char="3611">other</TOKEN>
<TOKEN end_char="3622" id="token-31-18" morph="none" pos="word" start_char="3617">organs</TOKEN>
<TOKEN end_char="3623" id="token-31-19" morph="none" pos="punct" start_char="3623">.</TOKEN>
</SEG>
<SEG end_char="3888" id="segment-32" start_char="3626">
<ORIGINAL_TEXT>According to Richard Ebright, an infectious disease expert at Rutgers University, the project description refers to experiments that would enhance the ability of bat coronavirus to infect human cells and laboratory animals using techniques of genetic engineering.</ORIGINAL_TEXT>
<TOKEN end_char="3634" id="token-32-0" morph="none" pos="word" start_char="3626">According</TOKEN>
<TOKEN end_char="3637" id="token-32-1" morph="none" pos="word" start_char="3636">to</TOKEN>
<TOKEN end_char="3645" id="token-32-2" morph="none" pos="word" start_char="3639">Richard</TOKEN>
<TOKEN end_char="3653" id="token-32-3" morph="none" pos="word" start_char="3647">Ebright</TOKEN>
<TOKEN end_char="3654" id="token-32-4" morph="none" pos="punct" start_char="3654">,</TOKEN>
<TOKEN end_char="3657" id="token-32-5" morph="none" pos="word" start_char="3656">an</TOKEN>
<TOKEN end_char="3668" id="token-32-6" morph="none" pos="word" start_char="3659">infectious</TOKEN>
<TOKEN end_char="3676" id="token-32-7" morph="none" pos="word" start_char="3670">disease</TOKEN>
<TOKEN end_char="3683" id="token-32-8" morph="none" pos="word" start_char="3678">expert</TOKEN>
<TOKEN end_char="3686" id="token-32-9" morph="none" pos="word" start_char="3685">at</TOKEN>
<TOKEN end_char="3694" id="token-32-10" morph="none" pos="word" start_char="3688">Rutgers</TOKEN>
<TOKEN end_char="3705" id="token-32-11" morph="none" pos="word" start_char="3696">University</TOKEN>
<TOKEN end_char="3706" id="token-32-12" morph="none" pos="punct" start_char="3706">,</TOKEN>
<TOKEN end_char="3710" id="token-32-13" morph="none" pos="word" start_char="3708">the</TOKEN>
<TOKEN end_char="3718" id="token-32-14" morph="none" pos="word" start_char="3712">project</TOKEN>
<TOKEN end_char="3730" id="token-32-15" morph="none" pos="word" start_char="3720">description</TOKEN>
<TOKEN end_char="3737" id="token-32-16" morph="none" pos="word" start_char="3732">refers</TOKEN>
<TOKEN end_char="3740" id="token-32-17" morph="none" pos="word" start_char="3739">to</TOKEN>
<TOKEN end_char="3752" id="token-32-18" morph="none" pos="word" start_char="3742">experiments</TOKEN>
<TOKEN end_char="3757" id="token-32-19" morph="none" pos="word" start_char="3754">that</TOKEN>
<TOKEN end_char="3763" id="token-32-20" morph="none" pos="word" start_char="3759">would</TOKEN>
<TOKEN end_char="3771" id="token-32-21" morph="none" pos="word" start_char="3765">enhance</TOKEN>
<TOKEN end_char="3775" id="token-32-22" morph="none" pos="word" start_char="3773">the</TOKEN>
<TOKEN end_char="3783" id="token-32-23" morph="none" pos="word" start_char="3777">ability</TOKEN>
<TOKEN end_char="3786" id="token-32-24" morph="none" pos="word" start_char="3785">of</TOKEN>
<TOKEN end_char="3790" id="token-32-25" morph="none" pos="word" start_char="3788">bat</TOKEN>
<TOKEN end_char="3802" id="token-32-26" morph="none" pos="word" start_char="3792">coronavirus</TOKEN>
<TOKEN end_char="3805" id="token-32-27" morph="none" pos="word" start_char="3804">to</TOKEN>
<TOKEN end_char="3812" id="token-32-28" morph="none" pos="word" start_char="3807">infect</TOKEN>
<TOKEN end_char="3818" id="token-32-29" morph="none" pos="word" start_char="3814">human</TOKEN>
<TOKEN end_char="3824" id="token-32-30" morph="none" pos="word" start_char="3820">cells</TOKEN>
<TOKEN end_char="3828" id="token-32-31" morph="none" pos="word" start_char="3826">and</TOKEN>
<TOKEN end_char="3839" id="token-32-32" morph="none" pos="word" start_char="3830">laboratory</TOKEN>
<TOKEN end_char="3847" id="token-32-33" morph="none" pos="word" start_char="3841">animals</TOKEN>
<TOKEN end_char="3853" id="token-32-34" morph="none" pos="word" start_char="3849">using</TOKEN>
<TOKEN end_char="3864" id="token-32-35" morph="none" pos="word" start_char="3855">techniques</TOKEN>
<TOKEN end_char="3867" id="token-32-36" morph="none" pos="word" start_char="3866">of</TOKEN>
<TOKEN end_char="3875" id="token-32-37" morph="none" pos="word" start_char="3869">genetic</TOKEN>
<TOKEN end_char="3887" id="token-32-38" morph="none" pos="word" start_char="3877">engineering</TOKEN>
<TOKEN end_char="3888" id="token-32-39" morph="none" pos="punct" start_char="3888">.</TOKEN>
</SEG>
<SEG end_char="3946" id="segment-33" start_char="3890">
<ORIGINAL_TEXT>In the wake of the pandemic, that is a noteworthy detail.</ORIGINAL_TEXT>
<TOKEN end_char="3891" id="token-33-0" morph="none" pos="word" start_char="3890">In</TOKEN>
<TOKEN end_char="3895" id="token-33-1" morph="none" pos="word" start_char="3893">the</TOKEN>
<TOKEN end_char="3900" id="token-33-2" morph="none" pos="word" start_char="3897">wake</TOKEN>
<TOKEN end_char="3903" id="token-33-3" morph="none" pos="word" start_char="3902">of</TOKEN>
<TOKEN end_char="3907" id="token-33-4" morph="none" pos="word" start_char="3905">the</TOKEN>
<TOKEN end_char="3916" id="token-33-5" morph="none" pos="word" start_char="3909">pandemic</TOKEN>
<TOKEN end_char="3917" id="token-33-6" morph="none" pos="punct" start_char="3917">,</TOKEN>
<TOKEN end_char="3922" id="token-33-7" morph="none" pos="word" start_char="3919">that</TOKEN>
<TOKEN end_char="3925" id="token-33-8" morph="none" pos="word" start_char="3924">is</TOKEN>
<TOKEN end_char="3927" id="token-33-9" morph="none" pos="word" start_char="3927">a</TOKEN>
<TOKEN end_char="3938" id="token-33-10" morph="none" pos="word" start_char="3929">noteworthy</TOKEN>
<TOKEN end_char="3945" id="token-33-11" morph="none" pos="word" start_char="3940">detail</TOKEN>
<TOKEN end_char="3946" id="token-33-12" morph="none" pos="punct" start_char="3946">.</TOKEN>
</SEG>
<SEG end_char="4139" id="segment-34" start_char="3949">
<ORIGINAL_TEXT>Ebright, along with many other scientists, has been a vocal opponent of gain-of-function research because of the risk it presents of creating a pandemic through accidental release from a lab.</ORIGINAL_TEXT>
<TOKEN end_char="3955" id="token-34-0" morph="none" pos="word" start_char="3949">Ebright</TOKEN>
<TOKEN end_char="3956" id="token-34-1" morph="none" pos="punct" start_char="3956">,</TOKEN>
<TOKEN end_char="3962" id="token-34-2" morph="none" pos="word" start_char="3958">along</TOKEN>
<TOKEN end_char="3967" id="token-34-3" morph="none" pos="word" start_char="3964">with</TOKEN>
<TOKEN end_char="3972" id="token-34-4" morph="none" pos="word" start_char="3969">many</TOKEN>
<TOKEN end_char="3978" id="token-34-5" morph="none" pos="word" start_char="3974">other</TOKEN>
<TOKEN end_char="3989" id="token-34-6" morph="none" pos="word" start_char="3980">scientists</TOKEN>
<TOKEN end_char="3990" id="token-34-7" morph="none" pos="punct" start_char="3990">,</TOKEN>
<TOKEN end_char="3994" id="token-34-8" morph="none" pos="word" start_char="3992">has</TOKEN>
<TOKEN end_char="3999" id="token-34-9" morph="none" pos="word" start_char="3996">been</TOKEN>
<TOKEN end_char="4001" id="token-34-10" morph="none" pos="word" start_char="4001">a</TOKEN>
<TOKEN end_char="4007" id="token-34-11" morph="none" pos="word" start_char="4003">vocal</TOKEN>
<TOKEN end_char="4016" id="token-34-12" morph="none" pos="word" start_char="4009">opponent</TOKEN>
<TOKEN end_char="4019" id="token-34-13" morph="none" pos="word" start_char="4018">of</TOKEN>
<TOKEN end_char="4036" id="token-34-14" morph="none" pos="unknown" start_char="4021">gain-of-function</TOKEN>
<TOKEN end_char="4045" id="token-34-15" morph="none" pos="word" start_char="4038">research</TOKEN>
<TOKEN end_char="4053" id="token-34-16" morph="none" pos="word" start_char="4047">because</TOKEN>
<TOKEN end_char="4056" id="token-34-17" morph="none" pos="word" start_char="4055">of</TOKEN>
<TOKEN end_char="4060" id="token-34-18" morph="none" pos="word" start_char="4058">the</TOKEN>
<TOKEN end_char="4065" id="token-34-19" morph="none" pos="word" start_char="4062">risk</TOKEN>
<TOKEN end_char="4068" id="token-34-20" morph="none" pos="word" start_char="4067">it</TOKEN>
<TOKEN end_char="4077" id="token-34-21" morph="none" pos="word" start_char="4070">presents</TOKEN>
<TOKEN end_char="4080" id="token-34-22" morph="none" pos="word" start_char="4079">of</TOKEN>
<TOKEN end_char="4089" id="token-34-23" morph="none" pos="word" start_char="4082">creating</TOKEN>
<TOKEN end_char="4091" id="token-34-24" morph="none" pos="word" start_char="4091">a</TOKEN>
<TOKEN end_char="4100" id="token-34-25" morph="none" pos="word" start_char="4093">pandemic</TOKEN>
<TOKEN end_char="4108" id="token-34-26" morph="none" pos="word" start_char="4102">through</TOKEN>
<TOKEN end_char="4119" id="token-34-27" morph="none" pos="word" start_char="4110">accidental</TOKEN>
<TOKEN end_char="4127" id="token-34-28" morph="none" pos="word" start_char="4121">release</TOKEN>
<TOKEN end_char="4132" id="token-34-29" morph="none" pos="word" start_char="4129">from</TOKEN>
<TOKEN end_char="4134" id="token-34-30" morph="none" pos="word" start_char="4134">a</TOKEN>
<TOKEN end_char="4138" id="token-34-31" morph="none" pos="word" start_char="4136">lab</TOKEN>
<TOKEN end_char="4139" id="token-34-32" morph="none" pos="punct" start_char="4139">.</TOKEN>
</SEG>
<SEG end_char="4212" id="segment-35" start_char="4142">
<ORIGINAL_TEXT>Dr. Fauci is renowned for his work on the HIV/AIDS crisis in the 1990s.</ORIGINAL_TEXT>
<TOKEN end_char="4143" id="token-35-0" morph="none" pos="word" start_char="4142">Dr</TOKEN>
<TOKEN end_char="4144" id="token-35-1" morph="none" pos="punct" start_char="4144">.</TOKEN>
<TOKEN end_char="4150" id="token-35-2" morph="none" pos="word" start_char="4146">Fauci</TOKEN>
<TOKEN end_char="4153" id="token-35-3" morph="none" pos="word" start_char="4152">is</TOKEN>
<TOKEN end_char="4162" id="token-35-4" morph="none" pos="word" start_char="4155">renowned</TOKEN>
<TOKEN end_char="4166" id="token-35-5" morph="none" pos="word" start_char="4164">for</TOKEN>
<TOKEN end_char="4170" id="token-35-6" morph="none" pos="word" start_char="4168">his</TOKEN>
<TOKEN end_char="4175" id="token-35-7" morph="none" pos="word" start_char="4172">work</TOKEN>
<TOKEN end_char="4178" id="token-35-8" morph="none" pos="word" start_char="4177">on</TOKEN>
<TOKEN end_char="4182" id="token-35-9" morph="none" pos="word" start_char="4180">the</TOKEN>
<TOKEN end_char="4191" id="token-35-10" morph="none" pos="unknown" start_char="4184">HIV/AIDS</TOKEN>
<TOKEN end_char="4198" id="token-35-11" morph="none" pos="word" start_char="4193">crisis</TOKEN>
<TOKEN end_char="4201" id="token-35-12" morph="none" pos="word" start_char="4200">in</TOKEN>
<TOKEN end_char="4205" id="token-35-13" morph="none" pos="word" start_char="4203">the</TOKEN>
<TOKEN end_char="4211" id="token-35-14" morph="none" pos="word" start_char="4207">1990s</TOKEN>
<TOKEN end_char="4212" id="token-35-15" morph="none" pos="punct" start_char="4212">.</TOKEN>
</SEG>
<SEG end_char="4311" id="segment-36" start_char="4214">
<ORIGINAL_TEXT>Born in Brooklyn, he graduated first in his class from Cornell University Medical College in 1966.</ORIGINAL_TEXT>
<TOKEN end_char="4217" id="token-36-0" morph="none" pos="word" start_char="4214">Born</TOKEN>
<TOKEN end_char="4220" id="token-36-1" morph="none" pos="word" start_char="4219">in</TOKEN>
<TOKEN end_char="4229" id="token-36-2" morph="none" pos="word" start_char="4222">Brooklyn</TOKEN>
<TOKEN end_char="4230" id="token-36-3" morph="none" pos="punct" start_char="4230">,</TOKEN>
<TOKEN end_char="4233" id="token-36-4" morph="none" pos="word" start_char="4232">he</TOKEN>
<TOKEN end_char="4243" id="token-36-5" morph="none" pos="word" start_char="4235">graduated</TOKEN>
<TOKEN end_char="4249" id="token-36-6" morph="none" pos="word" start_char="4245">first</TOKEN>
<TOKEN end_char="4252" id="token-36-7" morph="none" pos="word" start_char="4251">in</TOKEN>
<TOKEN end_char="4256" id="token-36-8" morph="none" pos="word" start_char="4254">his</TOKEN>
<TOKEN end_char="4262" id="token-36-9" morph="none" pos="word" start_char="4258">class</TOKEN>
<TOKEN end_char="4267" id="token-36-10" morph="none" pos="word" start_char="4264">from</TOKEN>
<TOKEN end_char="4275" id="token-36-11" morph="none" pos="word" start_char="4269">Cornell</TOKEN>
<TOKEN end_char="4286" id="token-36-12" morph="none" pos="word" start_char="4277">University</TOKEN>
<TOKEN end_char="4294" id="token-36-13" morph="none" pos="word" start_char="4288">Medical</TOKEN>
<TOKEN end_char="4302" id="token-36-14" morph="none" pos="word" start_char="4296">College</TOKEN>
<TOKEN end_char="4305" id="token-36-15" morph="none" pos="word" start_char="4304">in</TOKEN>
<TOKEN end_char="4310" id="token-36-16" morph="none" pos="word" start_char="4307">1966</TOKEN>
<TOKEN end_char="4311" id="token-36-17" morph="none" pos="punct" start_char="4311">.</TOKEN>
</SEG>
<SEG end_char="4413" id="segment-37" start_char="4313">
<ORIGINAL_TEXT>As head of NIAID since 1984, he has served as an adviser to every U.S. president since Ronald Reagan.</ORIGINAL_TEXT>
<TOKEN end_char="4314" id="token-37-0" morph="none" pos="word" start_char="4313">As</TOKEN>
<TOKEN end_char="4319" id="token-37-1" morph="none" pos="word" start_char="4316">head</TOKEN>
<TOKEN end_char="4322" id="token-37-2" morph="none" pos="word" start_char="4321">of</TOKEN>
<TOKEN end_char="4328" id="token-37-3" morph="none" pos="word" start_char="4324">NIAID</TOKEN>
<TOKEN end_char="4334" id="token-37-4" morph="none" pos="word" start_char="4330">since</TOKEN>
<TOKEN end_char="4339" id="token-37-5" morph="none" pos="word" start_char="4336">1984</TOKEN>
<TOKEN end_char="4340" id="token-37-6" morph="none" pos="punct" start_char="4340">,</TOKEN>
<TOKEN end_char="4343" id="token-37-7" morph="none" pos="word" start_char="4342">he</TOKEN>
<TOKEN end_char="4347" id="token-37-8" morph="none" pos="word" start_char="4345">has</TOKEN>
<TOKEN end_char="4354" id="token-37-9" morph="none" pos="word" start_char="4349">served</TOKEN>
<TOKEN end_char="4357" id="token-37-10" morph="none" pos="word" start_char="4356">as</TOKEN>
<TOKEN end_char="4360" id="token-37-11" morph="none" pos="word" start_char="4359">an</TOKEN>
<TOKEN end_char="4368" id="token-37-12" morph="none" pos="word" start_char="4362">adviser</TOKEN>
<TOKEN end_char="4371" id="token-37-13" morph="none" pos="word" start_char="4370">to</TOKEN>
<TOKEN end_char="4377" id="token-37-14" morph="none" pos="word" start_char="4373">every</TOKEN>
<TOKEN end_char="4381" id="token-37-15" morph="none" pos="unknown" start_char="4379">U.S</TOKEN>
<TOKEN end_char="4382" id="token-37-16" morph="none" pos="punct" start_char="4382">.</TOKEN>
<TOKEN end_char="4392" id="token-37-17" morph="none" pos="word" start_char="4384">president</TOKEN>
<TOKEN end_char="4398" id="token-37-18" morph="none" pos="word" start_char="4394">since</TOKEN>
<TOKEN end_char="4405" id="token-37-19" morph="none" pos="word" start_char="4400">Ronald</TOKEN>
<TOKEN end_char="4412" id="token-37-20" morph="none" pos="word" start_char="4407">Reagan</TOKEN>
<TOKEN end_char="4413" id="token-37-21" morph="none" pos="punct" start_char="4413">.</TOKEN>
</SEG>
<SEG end_char="4559" id="segment-38" start_char="4416">
<ORIGINAL_TEXT>A decade ago, during a controversy over gain-of-function research on bird-flu viruses, Dr. Fauci played an important role in promoting the work.</ORIGINAL_TEXT>
<TOKEN end_char="4416" id="token-38-0" morph="none" pos="word" start_char="4416">A</TOKEN>
<TOKEN end_char="4423" id="token-38-1" morph="none" pos="word" start_char="4418">decade</TOKEN>
<TOKEN end_char="4427" id="token-38-2" morph="none" pos="word" start_char="4425">ago</TOKEN>
<TOKEN end_char="4428" id="token-38-3" morph="none" pos="punct" start_char="4428">,</TOKEN>
<TOKEN end_char="4435" id="token-38-4" morph="none" pos="word" start_char="4430">during</TOKEN>
<TOKEN end_char="4437" id="token-38-5" morph="none" pos="word" start_char="4437">a</TOKEN>
<TOKEN end_char="4449" id="token-38-6" morph="none" pos="word" start_char="4439">controversy</TOKEN>
<TOKEN end_char="4454" id="token-38-7" morph="none" pos="word" start_char="4451">over</TOKEN>
<TOKEN end_char="4471" id="token-38-8" morph="none" pos="unknown" start_char="4456">gain-of-function</TOKEN>
<TOKEN end_char="4480" id="token-38-9" morph="none" pos="word" start_char="4473">research</TOKEN>
<TOKEN end_char="4483" id="token-38-10" morph="none" pos="word" start_char="4482">on</TOKEN>
<TOKEN end_char="4492" id="token-38-11" morph="none" pos="unknown" start_char="4485">bird-flu</TOKEN>
<TOKEN end_char="4500" id="token-38-12" morph="none" pos="word" start_char="4494">viruses</TOKEN>
<TOKEN end_char="4501" id="token-38-13" morph="none" pos="punct" start_char="4501">,</TOKEN>
<TOKEN end_char="4504" id="token-38-14" morph="none" pos="word" start_char="4503">Dr</TOKEN>
<TOKEN end_char="4505" id="token-38-15" morph="none" pos="punct" start_char="4505">.</TOKEN>
<TOKEN end_char="4511" id="token-38-16" morph="none" pos="word" start_char="4507">Fauci</TOKEN>
<TOKEN end_char="4518" id="token-38-17" morph="none" pos="word" start_char="4513">played</TOKEN>
<TOKEN end_char="4521" id="token-38-18" morph="none" pos="word" start_char="4520">an</TOKEN>
<TOKEN end_char="4531" id="token-38-19" morph="none" pos="word" start_char="4523">important</TOKEN>
<TOKEN end_char="4536" id="token-38-20" morph="none" pos="word" start_char="4533">role</TOKEN>
<TOKEN end_char="4539" id="token-38-21" morph="none" pos="word" start_char="4538">in</TOKEN>
<TOKEN end_char="4549" id="token-38-22" morph="none" pos="word" start_char="4541">promoting</TOKEN>
<TOKEN end_char="4553" id="token-38-23" morph="none" pos="word" start_char="4551">the</TOKEN>
<TOKEN end_char="4558" id="token-38-24" morph="none" pos="word" start_char="4555">work</TOKEN>
<TOKEN end_char="4559" id="token-38-25" morph="none" pos="punct" start_char="4559">.</TOKEN>
</SEG>
<SEG end_char="4779" id="segment-39" start_char="4561">
<ORIGINAL_TEXT>He argued that the research was worth the risk it entailed because it enables scientists to make preparations, such as investigating possible anti-viral medications, that could be useful if and when a pandemic occurred.</ORIGINAL_TEXT>
<TOKEN end_char="4562" id="token-39-0" morph="none" pos="word" start_char="4561">He</TOKEN>
<TOKEN end_char="4569" id="token-39-1" morph="none" pos="word" start_char="4564">argued</TOKEN>
<TOKEN end_char="4574" id="token-39-2" morph="none" pos="word" start_char="4571">that</TOKEN>
<TOKEN end_char="4578" id="token-39-3" morph="none" pos="word" start_char="4576">the</TOKEN>
<TOKEN end_char="4587" id="token-39-4" morph="none" pos="word" start_char="4580">research</TOKEN>
<TOKEN end_char="4591" id="token-39-5" morph="none" pos="word" start_char="4589">was</TOKEN>
<TOKEN end_char="4597" id="token-39-6" morph="none" pos="word" start_char="4593">worth</TOKEN>
<TOKEN end_char="4601" id="token-39-7" morph="none" pos="word" start_char="4599">the</TOKEN>
<TOKEN end_char="4606" id="token-39-8" morph="none" pos="word" start_char="4603">risk</TOKEN>
<TOKEN end_char="4609" id="token-39-9" morph="none" pos="word" start_char="4608">it</TOKEN>
<TOKEN end_char="4618" id="token-39-10" morph="none" pos="word" start_char="4611">entailed</TOKEN>
<TOKEN end_char="4626" id="token-39-11" morph="none" pos="word" start_char="4620">because</TOKEN>
<TOKEN end_char="4629" id="token-39-12" morph="none" pos="word" start_char="4628">it</TOKEN>
<TOKEN end_char="4637" id="token-39-13" morph="none" pos="word" start_char="4631">enables</TOKEN>
<TOKEN end_char="4648" id="token-39-14" morph="none" pos="word" start_char="4639">scientists</TOKEN>
<TOKEN end_char="4651" id="token-39-15" morph="none" pos="word" start_char="4650">to</TOKEN>
<TOKEN end_char="4656" id="token-39-16" morph="none" pos="word" start_char="4653">make</TOKEN>
<TOKEN end_char="4669" id="token-39-17" morph="none" pos="word" start_char="4658">preparations</TOKEN>
<TOKEN end_char="4670" id="token-39-18" morph="none" pos="punct" start_char="4670">,</TOKEN>
<TOKEN end_char="4675" id="token-39-19" morph="none" pos="word" start_char="4672">such</TOKEN>
<TOKEN end_char="4678" id="token-39-20" morph="none" pos="word" start_char="4677">as</TOKEN>
<TOKEN end_char="4692" id="token-39-21" morph="none" pos="word" start_char="4680">investigating</TOKEN>
<TOKEN end_char="4701" id="token-39-22" morph="none" pos="word" start_char="4694">possible</TOKEN>
<TOKEN end_char="4712" id="token-39-23" morph="none" pos="unknown" start_char="4703">anti-viral</TOKEN>
<TOKEN end_char="4724" id="token-39-24" morph="none" pos="word" start_char="4714">medications</TOKEN>
<TOKEN end_char="4725" id="token-39-25" morph="none" pos="punct" start_char="4725">,</TOKEN>
<TOKEN end_char="4730" id="token-39-26" morph="none" pos="word" start_char="4727">that</TOKEN>
<TOKEN end_char="4736" id="token-39-27" morph="none" pos="word" start_char="4732">could</TOKEN>
<TOKEN end_char="4739" id="token-39-28" morph="none" pos="word" start_char="4738">be</TOKEN>
<TOKEN end_char="4746" id="token-39-29" morph="none" pos="word" start_char="4741">useful</TOKEN>
<TOKEN end_char="4749" id="token-39-30" morph="none" pos="word" start_char="4748">if</TOKEN>
<TOKEN end_char="4753" id="token-39-31" morph="none" pos="word" start_char="4751">and</TOKEN>
<TOKEN end_char="4758" id="token-39-32" morph="none" pos="word" start_char="4755">when</TOKEN>
<TOKEN end_char="4760" id="token-39-33" morph="none" pos="word" start_char="4760">a</TOKEN>
<TOKEN end_char="4769" id="token-39-34" morph="none" pos="word" start_char="4762">pandemic</TOKEN>
<TOKEN end_char="4778" id="token-39-35" morph="none" pos="word" start_char="4771">occurred</TOKEN>
<TOKEN end_char="4779" id="token-39-36" morph="none" pos="punct" start_char="4779">.</TOKEN>
</SEG>
<SEG end_char="4978" id="segment-40" start_char="4782">
<ORIGINAL_TEXT>The work in question was a type of gain-of-function research that involved taking wild viruses and passing them through live animals until they mutate into a form that could pose a pandemic threat.</ORIGINAL_TEXT>
<TOKEN end_char="4784" id="token-40-0" morph="none" pos="word" start_char="4782">The</TOKEN>
<TOKEN end_char="4789" id="token-40-1" morph="none" pos="word" start_char="4786">work</TOKEN>
<TOKEN end_char="4792" id="token-40-2" morph="none" pos="word" start_char="4791">in</TOKEN>
<TOKEN end_char="4801" id="token-40-3" morph="none" pos="word" start_char="4794">question</TOKEN>
<TOKEN end_char="4805" id="token-40-4" morph="none" pos="word" start_char="4803">was</TOKEN>
<TOKEN end_char="4807" id="token-40-5" morph="none" pos="word" start_char="4807">a</TOKEN>
<TOKEN end_char="4812" id="token-40-6" morph="none" pos="word" start_char="4809">type</TOKEN>
<TOKEN end_char="4815" id="token-40-7" morph="none" pos="word" start_char="4814">of</TOKEN>
<TOKEN end_char="4832" id="token-40-8" morph="none" pos="unknown" start_char="4817">gain-of-function</TOKEN>
<TOKEN end_char="4841" id="token-40-9" morph="none" pos="word" start_char="4834">research</TOKEN>
<TOKEN end_char="4846" id="token-40-10" morph="none" pos="word" start_char="4843">that</TOKEN>
<TOKEN end_char="4855" id="token-40-11" morph="none" pos="word" start_char="4848">involved</TOKEN>
<TOKEN end_char="4862" id="token-40-12" morph="none" pos="word" start_char="4857">taking</TOKEN>
<TOKEN end_char="4867" id="token-40-13" morph="none" pos="word" start_char="4864">wild</TOKEN>
<TOKEN end_char="4875" id="token-40-14" morph="none" pos="word" start_char="4869">viruses</TOKEN>
<TOKEN end_char="4879" id="token-40-15" morph="none" pos="word" start_char="4877">and</TOKEN>
<TOKEN end_char="4887" id="token-40-16" morph="none" pos="word" start_char="4881">passing</TOKEN>
<TOKEN end_char="4892" id="token-40-17" morph="none" pos="word" start_char="4889">them</TOKEN>
<TOKEN end_char="4900" id="token-40-18" morph="none" pos="word" start_char="4894">through</TOKEN>
<TOKEN end_char="4905" id="token-40-19" morph="none" pos="word" start_char="4902">live</TOKEN>
<TOKEN end_char="4913" id="token-40-20" morph="none" pos="word" start_char="4907">animals</TOKEN>
<TOKEN end_char="4919" id="token-40-21" morph="none" pos="word" start_char="4915">until</TOKEN>
<TOKEN end_char="4924" id="token-40-22" morph="none" pos="word" start_char="4921">they</TOKEN>
<TOKEN end_char="4931" id="token-40-23" morph="none" pos="word" start_char="4926">mutate</TOKEN>
<TOKEN end_char="4936" id="token-40-24" morph="none" pos="word" start_char="4933">into</TOKEN>
<TOKEN end_char="4938" id="token-40-25" morph="none" pos="word" start_char="4938">a</TOKEN>
<TOKEN end_char="4943" id="token-40-26" morph="none" pos="word" start_char="4940">form</TOKEN>
<TOKEN end_char="4948" id="token-40-27" morph="none" pos="word" start_char="4945">that</TOKEN>
<TOKEN end_char="4954" id="token-40-28" morph="none" pos="word" start_char="4950">could</TOKEN>
<TOKEN end_char="4959" id="token-40-29" morph="none" pos="word" start_char="4956">pose</TOKEN>
<TOKEN end_char="4961" id="token-40-30" morph="none" pos="word" start_char="4961">a</TOKEN>
<TOKEN end_char="4970" id="token-40-31" morph="none" pos="word" start_char="4963">pandemic</TOKEN>
<TOKEN end_char="4977" id="token-40-32" morph="none" pos="word" start_char="4972">threat</TOKEN>
<TOKEN end_char="4978" id="token-40-33" morph="none" pos="punct" start_char="4978">.</TOKEN>
</SEG>
<SEG end_char="5137" id="segment-41" start_char="4980">
<ORIGINAL_TEXT>Scientists used it to take a virus that was poorly transmitted among humans and make it into one that was highly transmissible—a hallmark of a pandemic virus.</ORIGINAL_TEXT>
<TOKEN end_char="4989" id="token-41-0" morph="none" pos="word" start_char="4980">Scientists</TOKEN>
<TOKEN end_char="4994" id="token-41-1" morph="none" pos="word" start_char="4991">used</TOKEN>
<TOKEN end_char="4997" id="token-41-2" morph="none" pos="word" start_char="4996">it</TOKEN>
<TOKEN end_char="5000" id="token-41-3" morph="none" pos="word" start_char="4999">to</TOKEN>
<TOKEN end_char="5005" id="token-41-4" morph="none" pos="word" start_char="5002">take</TOKEN>
<TOKEN end_char="5007" id="token-41-5" morph="none" pos="word" start_char="5007">a</TOKEN>
<TOKEN end_char="5013" id="token-41-6" morph="none" pos="word" start_char="5009">virus</TOKEN>
<TOKEN end_char="5018" id="token-41-7" morph="none" pos="word" start_char="5015">that</TOKEN>
<TOKEN end_char="5022" id="token-41-8" morph="none" pos="word" start_char="5020">was</TOKEN>
<TOKEN end_char="5029" id="token-41-9" morph="none" pos="word" start_char="5024">poorly</TOKEN>
<TOKEN end_char="5041" id="token-41-10" morph="none" pos="word" start_char="5031">transmitted</TOKEN>
<TOKEN end_char="5047" id="token-41-11" morph="none" pos="word" start_char="5043">among</TOKEN>
<TOKEN end_char="5054" id="token-41-12" morph="none" pos="word" start_char="5049">humans</TOKEN>
<TOKEN end_char="5058" id="token-41-13" morph="none" pos="word" start_char="5056">and</TOKEN>
<TOKEN end_char="5063" id="token-41-14" morph="none" pos="word" start_char="5060">make</TOKEN>
<TOKEN end_char="5066" id="token-41-15" morph="none" pos="word" start_char="5065">it</TOKEN>
<TOKEN end_char="5071" id="token-41-16" morph="none" pos="word" start_char="5068">into</TOKEN>
<TOKEN end_char="5075" id="token-41-17" morph="none" pos="word" start_char="5073">one</TOKEN>
<TOKEN end_char="5080" id="token-41-18" morph="none" pos="word" start_char="5077">that</TOKEN>
<TOKEN end_char="5084" id="token-41-19" morph="none" pos="word" start_char="5082">was</TOKEN>
<TOKEN end_char="5091" id="token-41-20" morph="none" pos="word" start_char="5086">highly</TOKEN>
<TOKEN end_char="5107" id="token-41-21" morph="none" pos="unknown" start_char="5093">transmissible—a</TOKEN>
<TOKEN end_char="5116" id="token-41-22" morph="none" pos="word" start_char="5109">hallmark</TOKEN>
<TOKEN end_char="5119" id="token-41-23" morph="none" pos="word" start_char="5118">of</TOKEN>
<TOKEN end_char="5121" id="token-41-24" morph="none" pos="word" start_char="5121">a</TOKEN>
<TOKEN end_char="5130" id="token-41-25" morph="none" pos="word" start_char="5123">pandemic</TOKEN>
<TOKEN end_char="5136" id="token-41-26" morph="none" pos="word" start_char="5132">virus</TOKEN>
<TOKEN end_char="5137" id="token-41-27" morph="none" pos="punct" start_char="5137">.</TOKEN>
</SEG>
<SEG end_char="5297" id="segment-42" start_char="5139">
<ORIGINAL_TEXT>This work was done by infecting a series of ferrets, allowing the virus to mutate until a ferret that hadn't been deliberately infected contracted the disease.</ORIGINAL_TEXT>
<TOKEN end_char="5142" id="token-42-0" morph="none" pos="word" start_char="5139">This</TOKEN>
<TOKEN end_char="5147" id="token-42-1" morph="none" pos="word" start_char="5144">work</TOKEN>
<TOKEN end_char="5151" id="token-42-2" morph="none" pos="word" start_char="5149">was</TOKEN>
<TOKEN end_char="5156" id="token-42-3" morph="none" pos="word" start_char="5153">done</TOKEN>
<TOKEN end_char="5159" id="token-42-4" morph="none" pos="word" start_char="5158">by</TOKEN>
<TOKEN end_char="5169" id="token-42-5" morph="none" pos="word" start_char="5161">infecting</TOKEN>
<TOKEN end_char="5171" id="token-42-6" morph="none" pos="word" start_char="5171">a</TOKEN>
<TOKEN end_char="5178" id="token-42-7" morph="none" pos="word" start_char="5173">series</TOKEN>
<TOKEN end_char="5181" id="token-42-8" morph="none" pos="word" start_char="5180">of</TOKEN>
<TOKEN end_char="5189" id="token-42-9" morph="none" pos="word" start_char="5183">ferrets</TOKEN>
<TOKEN end_char="5190" id="token-42-10" morph="none" pos="punct" start_char="5190">,</TOKEN>
<TOKEN end_char="5199" id="token-42-11" morph="none" pos="word" start_char="5192">allowing</TOKEN>
<TOKEN end_char="5203" id="token-42-12" morph="none" pos="word" start_char="5201">the</TOKEN>
<TOKEN end_char="5209" id="token-42-13" morph="none" pos="word" start_char="5205">virus</TOKEN>
<TOKEN end_char="5212" id="token-42-14" morph="none" pos="word" start_char="5211">to</TOKEN>
<TOKEN end_char="5219" id="token-42-15" morph="none" pos="word" start_char="5214">mutate</TOKEN>
<TOKEN end_char="5225" id="token-42-16" morph="none" pos="word" start_char="5221">until</TOKEN>
<TOKEN end_char="5227" id="token-42-17" morph="none" pos="word" start_char="5227">a</TOKEN>
<TOKEN end_char="5234" id="token-42-18" morph="none" pos="word" start_char="5229">ferret</TOKEN>
<TOKEN end_char="5239" id="token-42-19" morph="none" pos="word" start_char="5236">that</TOKEN>
<TOKEN end_char="5246" id="token-42-20" morph="none" pos="word" start_char="5241">hadn't</TOKEN>
<TOKEN end_char="5251" id="token-42-21" morph="none" pos="word" start_char="5248">been</TOKEN>
<TOKEN end_char="5264" id="token-42-22" morph="none" pos="word" start_char="5253">deliberately</TOKEN>
<TOKEN end_char="5273" id="token-42-23" morph="none" pos="word" start_char="5266">infected</TOKEN>
<TOKEN end_char="5284" id="token-42-24" morph="none" pos="word" start_char="5275">contracted</TOKEN>
<TOKEN end_char="5288" id="token-42-25" morph="none" pos="word" start_char="5286">the</TOKEN>
<TOKEN end_char="5296" id="token-42-26" morph="none" pos="word" start_char="5290">disease</TOKEN>
<TOKEN end_char="5297" id="token-42-27" morph="none" pos="punct" start_char="5297">.</TOKEN>
</SEG>
<SEG end_char="5362" id="segment-43" start_char="5300">
<ORIGINAL_TEXT>The work entailed risks that worried even seasoned researchers.</ORIGINAL_TEXT>
<TOKEN end_char="5302" id="token-43-0" morph="none" pos="word" start_char="5300">The</TOKEN>
<TOKEN end_char="5307" id="token-43-1" morph="none" pos="word" start_char="5304">work</TOKEN>
<TOKEN end_char="5316" id="token-43-2" morph="none" pos="word" start_char="5309">entailed</TOKEN>
<TOKEN end_char="5322" id="token-43-3" morph="none" pos="word" start_char="5318">risks</TOKEN>
<TOKEN end_char="5327" id="token-43-4" morph="none" pos="word" start_char="5324">that</TOKEN>
<TOKEN end_char="5335" id="token-43-5" morph="none" pos="word" start_char="5329">worried</TOKEN>
<TOKEN end_char="5340" id="token-43-6" morph="none" pos="word" start_char="5337">even</TOKEN>
<TOKEN end_char="5349" id="token-43-7" morph="none" pos="word" start_char="5342">seasoned</TOKEN>
<TOKEN end_char="5361" id="token-43-8" morph="none" pos="word" start_char="5351">researchers</TOKEN>
<TOKEN end_char="5362" id="token-43-9" morph="none" pos="punct" start_char="5362">.</TOKEN>
</SEG>
<SEG end_char="5421" id="segment-44" start_char="5364">
<ORIGINAL_TEXT>More than 200 scientists called for the work to be halted.</ORIGINAL_TEXT>
<TOKEN end_char="5367" id="token-44-0" morph="none" pos="word" start_char="5364">More</TOKEN>
<TOKEN end_char="5372" id="token-44-1" morph="none" pos="word" start_char="5369">than</TOKEN>
<TOKEN end_char="5376" id="token-44-2" morph="none" pos="word" start_char="5374">200</TOKEN>
<TOKEN end_char="5387" id="token-44-3" morph="none" pos="word" start_char="5378">scientists</TOKEN>
<TOKEN end_char="5394" id="token-44-4" morph="none" pos="word" start_char="5389">called</TOKEN>
<TOKEN end_char="5398" id="token-44-5" morph="none" pos="word" start_char="5396">for</TOKEN>
<TOKEN end_char="5402" id="token-44-6" morph="none" pos="word" start_char="5400">the</TOKEN>
<TOKEN end_char="5407" id="token-44-7" morph="none" pos="word" start_char="5404">work</TOKEN>
<TOKEN end_char="5410" id="token-44-8" morph="none" pos="word" start_char="5409">to</TOKEN>
<TOKEN end_char="5413" id="token-44-9" morph="none" pos="word" start_char="5412">be</TOKEN>
<TOKEN end_char="5420" id="token-44-10" morph="none" pos="word" start_char="5415">halted</TOKEN>
<TOKEN end_char="5421" id="token-44-11" morph="none" pos="punct" start_char="5421">.</TOKEN>
</SEG>
<SEG end_char="5540" id="segment-45" start_char="5423">
<ORIGINAL_TEXT>The problem, they said, is that it increased the likelihood that a pandemic would occur through a laboratory accident.</ORIGINAL_TEXT>
<TOKEN end_char="5425" id="token-45-0" morph="none" pos="word" start_char="5423">The</TOKEN>
<TOKEN end_char="5433" id="token-45-1" morph="none" pos="word" start_char="5427">problem</TOKEN>
<TOKEN end_char="5434" id="token-45-2" morph="none" pos="punct" start_char="5434">,</TOKEN>
<TOKEN end_char="5439" id="token-45-3" morph="none" pos="word" start_char="5436">they</TOKEN>
<TOKEN end_char="5444" id="token-45-4" morph="none" pos="word" start_char="5441">said</TOKEN>
<TOKEN end_char="5445" id="token-45-5" morph="none" pos="punct" start_char="5445">,</TOKEN>
<TOKEN end_char="5448" id="token-45-6" morph="none" pos="word" start_char="5447">is</TOKEN>
<TOKEN end_char="5453" id="token-45-7" morph="none" pos="word" start_char="5450">that</TOKEN>
<TOKEN end_char="5456" id="token-45-8" morph="none" pos="word" start_char="5455">it</TOKEN>
<TOKEN end_char="5466" id="token-45-9" morph="none" pos="word" start_char="5458">increased</TOKEN>
<TOKEN end_char="5470" id="token-45-10" morph="none" pos="word" start_char="5468">the</TOKEN>
<TOKEN end_char="5481" id="token-45-11" morph="none" pos="word" start_char="5472">likelihood</TOKEN>
<TOKEN end_char="5486" id="token-45-12" morph="none" pos="word" start_char="5483">that</TOKEN>
<TOKEN end_char="5488" id="token-45-13" morph="none" pos="word" start_char="5488">a</TOKEN>
<TOKEN end_char="5497" id="token-45-14" morph="none" pos="word" start_char="5490">pandemic</TOKEN>
<TOKEN end_char="5503" id="token-45-15" morph="none" pos="word" start_char="5499">would</TOKEN>
<TOKEN end_char="5509" id="token-45-16" morph="none" pos="word" start_char="5505">occur</TOKEN>
<TOKEN end_char="5517" id="token-45-17" morph="none" pos="word" start_char="5511">through</TOKEN>
<TOKEN end_char="5519" id="token-45-18" morph="none" pos="word" start_char="5519">a</TOKEN>
<TOKEN end_char="5530" id="token-45-19" morph="none" pos="word" start_char="5521">laboratory</TOKEN>
<TOKEN end_char="5539" id="token-45-20" morph="none" pos="word" start_char="5532">accident</TOKEN>
<TOKEN end_char="5540" id="token-45-21" morph="none" pos="punct" start_char="5540">.</TOKEN>
</SEG>
<SEG end_char="5570" id="segment-46" start_char="5543">
<ORIGINAL_TEXT>Dr. Fauci defended the work.</ORIGINAL_TEXT>
<TOKEN end_char="5544" id="token-46-0" morph="none" pos="word" start_char="5543">Dr</TOKEN>
<TOKEN end_char="5545" id="token-46-1" morph="none" pos="punct" start_char="5545">.</TOKEN>
<TOKEN end_char="5551" id="token-46-2" morph="none" pos="word" start_char="5547">Fauci</TOKEN>
<TOKEN end_char="5560" id="token-46-3" morph="none" pos="word" start_char="5553">defended</TOKEN>
<TOKEN end_char="5564" id="token-46-4" morph="none" pos="word" start_char="5562">the</TOKEN>
<TOKEN end_char="5569" id="token-46-5" morph="none" pos="word" start_char="5566">work</TOKEN>
<TOKEN end_char="5570" id="token-46-6" morph="none" pos="punct" start_char="5570">.</TOKEN>
</SEG>
<SEG end_char="5835" id="segment-47" start_char="5572">
<ORIGINAL_TEXT>"[D]etermining the molecular Achilles' heel of these viruses can allow scientists to identify novel antiviral drug targets that could be used to prevent infection in those at risk or to better treat those who become infected," wrote Fauci and two co-authors in the</ORIGINAL_TEXT>
<TOKEN end_char="5573" id="token-47-0" morph="none" pos="punct" start_char="5572">"[</TOKEN>
<TOKEN end_char="5585" id="token-47-1" morph="none" pos="unknown" start_char="5574">D]etermining</TOKEN>
<TOKEN end_char="5589" id="token-47-2" morph="none" pos="word" start_char="5587">the</TOKEN>
<TOKEN end_char="5599" id="token-47-3" morph="none" pos="word" start_char="5591">molecular</TOKEN>
<TOKEN end_char="5608" id="token-47-4" morph="none" pos="word" start_char="5601">Achilles</TOKEN>
<TOKEN end_char="5609" id="token-47-5" morph="none" pos="punct" start_char="5609">'</TOKEN>
<TOKEN end_char="5614" id="token-47-6" morph="none" pos="word" start_char="5611">heel</TOKEN>
<TOKEN end_char="5617" id="token-47-7" morph="none" pos="word" start_char="5616">of</TOKEN>
<TOKEN end_char="5623" id="token-47-8" morph="none" pos="word" start_char="5619">these</TOKEN>
<TOKEN end_char="5631" id="token-47-9" morph="none" pos="word" start_char="5625">viruses</TOKEN>
<TOKEN end_char="5635" id="token-47-10" morph="none" pos="word" start_char="5633">can</TOKEN>
<TOKEN end_char="5641" id="token-47-11" morph="none" pos="word" start_char="5637">allow</TOKEN>
<TOKEN end_char="5652" id="token-47-12" morph="none" pos="word" start_char="5643">scientists</TOKEN>
<TOKEN end_char="5655" id="token-47-13" morph="none" pos="word" start_char="5654">to</TOKEN>
<TOKEN end_char="5664" id="token-47-14" morph="none" pos="word" start_char="5657">identify</TOKEN>
<TOKEN end_char="5670" id="token-47-15" morph="none" pos="word" start_char="5666">novel</TOKEN>
<TOKEN end_char="5680" id="token-47-16" morph="none" pos="word" start_char="5672">antiviral</TOKEN>
<TOKEN end_char="5685" id="token-47-17" morph="none" pos="word" start_char="5682">drug</TOKEN>
<TOKEN end_char="5693" id="token-47-18" morph="none" pos="word" start_char="5687">targets</TOKEN>
<TOKEN end_char="5698" id="token-47-19" morph="none" pos="word" start_char="5695">that</TOKEN>
<TOKEN end_char="5704" id="token-47-20" morph="none" pos="word" start_char="5700">could</TOKEN>
<TOKEN end_char="5707" id="token-47-21" morph="none" pos="word" start_char="5706">be</TOKEN>
<TOKEN end_char="5712" id="token-47-22" morph="none" pos="word" start_char="5709">used</TOKEN>
<TOKEN end_char="5715" id="token-47-23" morph="none" pos="word" start_char="5714">to</TOKEN>
<TOKEN end_char="5723" id="token-47-24" morph="none" pos="word" start_char="5717">prevent</TOKEN>
<TOKEN end_char="5733" id="token-47-25" morph="none" pos="word" start_char="5725">infection</TOKEN>
<TOKEN end_char="5736" id="token-47-26" morph="none" pos="word" start_char="5735">in</TOKEN>
<TOKEN end_char="5742" id="token-47-27" morph="none" pos="word" start_char="5738">those</TOKEN>
<TOKEN end_char="5745" id="token-47-28" morph="none" pos="word" start_char="5744">at</TOKEN>
<TOKEN end_char="5750" id="token-47-29" morph="none" pos="word" start_char="5747">risk</TOKEN>
<TOKEN end_char="5753" id="token-47-30" morph="none" pos="word" start_char="5752">or</TOKEN>
<TOKEN end_char="5756" id="token-47-31" morph="none" pos="word" start_char="5755">to</TOKEN>
<TOKEN end_char="5763" id="token-47-32" morph="none" pos="word" start_char="5758">better</TOKEN>
<TOKEN end_char="5769" id="token-47-33" morph="none" pos="word" start_char="5765">treat</TOKEN>
<TOKEN end_char="5775" id="token-47-34" morph="none" pos="word" start_char="5771">those</TOKEN>
<TOKEN end_char="5779" id="token-47-35" morph="none" pos="word" start_char="5777">who</TOKEN>
<TOKEN end_char="5786" id="token-47-36" morph="none" pos="word" start_char="5781">become</TOKEN>
<TOKEN end_char="5795" id="token-47-37" morph="none" pos="word" start_char="5788">infected</TOKEN>
<TOKEN end_char="5797" id="token-47-38" morph="none" pos="punct" start_char="5796">,"</TOKEN>
<TOKEN end_char="5803" id="token-47-39" morph="none" pos="word" start_char="5799">wrote</TOKEN>
<TOKEN end_char="5809" id="token-47-40" morph="none" pos="word" start_char="5805">Fauci</TOKEN>
<TOKEN end_char="5813" id="token-47-41" morph="none" pos="word" start_char="5811">and</TOKEN>
<TOKEN end_char="5817" id="token-47-42" morph="none" pos="word" start_char="5815">two</TOKEN>
<TOKEN end_char="5828" id="token-47-43" morph="none" pos="unknown" start_char="5819">co-authors</TOKEN>
<TOKEN end_char="5831" id="token-47-44" morph="none" pos="word" start_char="5830">in</TOKEN>
<TOKEN end_char="5835" id="token-47-45" morph="none" pos="word" start_char="5833">the</TOKEN>
</SEG>
<SEG end_char="5852" id="segment-48" start_char="5838">
<ORIGINAL_TEXT>Washington Post</ORIGINAL_TEXT>
<TOKEN end_char="5847" id="token-48-0" morph="none" pos="word" start_char="5838">Washington</TOKEN>
<TOKEN end_char="5852" id="token-48-1" morph="none" pos="word" start_char="5849">Post</TOKEN>
</SEG>
<SEG end_char="5875" id="segment-49" start_char="5855">
<ORIGINAL_TEXT>on December 30, 2011.</ORIGINAL_TEXT>
<TOKEN end_char="5856" id="token-49-0" morph="none" pos="word" start_char="5855">on</TOKEN>
<TOKEN end_char="5865" id="token-49-1" morph="none" pos="word" start_char="5858">December</TOKEN>
<TOKEN end_char="5868" id="token-49-2" morph="none" pos="word" start_char="5867">30</TOKEN>
<TOKEN end_char="5869" id="token-49-3" morph="none" pos="punct" start_char="5869">,</TOKEN>
<TOKEN end_char="5874" id="token-49-4" morph="none" pos="word" start_char="5871">2011</TOKEN>
<TOKEN end_char="5875" id="token-49-5" morph="none" pos="punct" start_char="5875">.</TOKEN>
</SEG>
<SEG end_char="6141" id="segment-50" start_char="5877">
<ORIGINAL_TEXT>"Decades of experience tells us that disseminating information gained through biomedical research to legitimate scientists and health officials provides a critical foundation for generating appropriate countermeasures and, ultimately, protecting the public health."</ORIGINAL_TEXT>
<TOKEN end_char="5877" id="token-50-0" morph="none" pos="punct" start_char="5877">"</TOKEN>
<TOKEN end_char="5884" id="token-50-1" morph="none" pos="word" start_char="5878">Decades</TOKEN>
<TOKEN end_char="5887" id="token-50-2" morph="none" pos="word" start_char="5886">of</TOKEN>
<TOKEN end_char="5898" id="token-50-3" morph="none" pos="word" start_char="5889">experience</TOKEN>
<TOKEN end_char="5904" id="token-50-4" morph="none" pos="word" start_char="5900">tells</TOKEN>
<TOKEN end_char="5907" id="token-50-5" morph="none" pos="word" start_char="5906">us</TOKEN>
<TOKEN end_char="5912" id="token-50-6" morph="none" pos="word" start_char="5909">that</TOKEN>
<TOKEN end_char="5926" id="token-50-7" morph="none" pos="word" start_char="5914">disseminating</TOKEN>
<TOKEN end_char="5938" id="token-50-8" morph="none" pos="word" start_char="5928">information</TOKEN>
<TOKEN end_char="5945" id="token-50-9" morph="none" pos="word" start_char="5940">gained</TOKEN>
<TOKEN end_char="5953" id="token-50-10" morph="none" pos="word" start_char="5947">through</TOKEN>
<TOKEN end_char="5964" id="token-50-11" morph="none" pos="word" start_char="5955">biomedical</TOKEN>
<TOKEN end_char="5973" id="token-50-12" morph="none" pos="word" start_char="5966">research</TOKEN>
<TOKEN end_char="5976" id="token-50-13" morph="none" pos="word" start_char="5975">to</TOKEN>
<TOKEN end_char="5987" id="token-50-14" morph="none" pos="word" start_char="5978">legitimate</TOKEN>
<TOKEN end_char="5998" id="token-50-15" morph="none" pos="word" start_char="5989">scientists</TOKEN>
<TOKEN end_char="6002" id="token-50-16" morph="none" pos="word" start_char="6000">and</TOKEN>
<TOKEN end_char="6009" id="token-50-17" morph="none" pos="word" start_char="6004">health</TOKEN>
<TOKEN end_char="6019" id="token-50-18" morph="none" pos="word" start_char="6011">officials</TOKEN>
<TOKEN end_char="6028" id="token-50-19" morph="none" pos="word" start_char="6021">provides</TOKEN>
<TOKEN end_char="6030" id="token-50-20" morph="none" pos="word" start_char="6030">a</TOKEN>
<TOKEN end_char="6039" id="token-50-21" morph="none" pos="word" start_char="6032">critical</TOKEN>
<TOKEN end_char="6050" id="token-50-22" morph="none" pos="word" start_char="6041">foundation</TOKEN>
<TOKEN end_char="6054" id="token-50-23" morph="none" pos="word" start_char="6052">for</TOKEN>
<TOKEN end_char="6065" id="token-50-24" morph="none" pos="word" start_char="6056">generating</TOKEN>
<TOKEN end_char="6077" id="token-50-25" morph="none" pos="word" start_char="6067">appropriate</TOKEN>
<TOKEN end_char="6093" id="token-50-26" morph="none" pos="word" start_char="6079">countermeasures</TOKEN>
<TOKEN end_char="6097" id="token-50-27" morph="none" pos="word" start_char="6095">and</TOKEN>
<TOKEN end_char="6098" id="token-50-28" morph="none" pos="punct" start_char="6098">,</TOKEN>
<TOKEN end_char="6109" id="token-50-29" morph="none" pos="word" start_char="6100">ultimately</TOKEN>
<TOKEN end_char="6110" id="token-50-30" morph="none" pos="punct" start_char="6110">,</TOKEN>
<TOKEN end_char="6121" id="token-50-31" morph="none" pos="word" start_char="6112">protecting</TOKEN>
<TOKEN end_char="6125" id="token-50-32" morph="none" pos="word" start_char="6123">the</TOKEN>
<TOKEN end_char="6132" id="token-50-33" morph="none" pos="word" start_char="6127">public</TOKEN>
<TOKEN end_char="6139" id="token-50-34" morph="none" pos="word" start_char="6134">health</TOKEN>
<TOKEN end_char="6141" id="token-50-35" morph="none" pos="punct" start_char="6140">."</TOKEN>
</SEG>
<SEG end_char="6308" id="segment-51" start_char="6144">
<ORIGINAL_TEXT>Nevertheless, in 2014, under pressure from the Obama administration, the National of Institutes of Health instituted a moratorium on the work, suspending 21 studies.</ORIGINAL_TEXT>
<TOKEN end_char="6155" id="token-51-0" morph="none" pos="word" start_char="6144">Nevertheless</TOKEN>
<TOKEN end_char="6156" id="token-51-1" morph="none" pos="punct" start_char="6156">,</TOKEN>
<TOKEN end_char="6159" id="token-51-2" morph="none" pos="word" start_char="6158">in</TOKEN>
<TOKEN end_char="6164" id="token-51-3" morph="none" pos="word" start_char="6161">2014</TOKEN>
<TOKEN end_char="6165" id="token-51-4" morph="none" pos="punct" start_char="6165">,</TOKEN>
<TOKEN end_char="6171" id="token-51-5" morph="none" pos="word" start_char="6167">under</TOKEN>
<TOKEN end_char="6180" id="token-51-6" morph="none" pos="word" start_char="6173">pressure</TOKEN>
<TOKEN end_char="6185" id="token-51-7" morph="none" pos="word" start_char="6182">from</TOKEN>
<TOKEN end_char="6189" id="token-51-8" morph="none" pos="word" start_char="6187">the</TOKEN>
<TOKEN end_char="6195" id="token-51-9" morph="none" pos="word" start_char="6191">Obama</TOKEN>
<TOKEN end_char="6210" id="token-51-10" morph="none" pos="word" start_char="6197">administration</TOKEN>
<TOKEN end_char="6211" id="token-51-11" morph="none" pos="punct" start_char="6211">,</TOKEN>
<TOKEN end_char="6215" id="token-51-12" morph="none" pos="word" start_char="6213">the</TOKEN>
<TOKEN end_char="6224" id="token-51-13" morph="none" pos="word" start_char="6217">National</TOKEN>
<TOKEN end_char="6227" id="token-51-14" morph="none" pos="word" start_char="6226">of</TOKEN>
<TOKEN end_char="6238" id="token-51-15" morph="none" pos="word" start_char="6229">Institutes</TOKEN>
<TOKEN end_char="6241" id="token-51-16" morph="none" pos="word" start_char="6240">of</TOKEN>
<TOKEN end_char="6248" id="token-51-17" morph="none" pos="word" start_char="6243">Health</TOKEN>
<TOKEN end_char="6259" id="token-51-18" morph="none" pos="word" start_char="6250">instituted</TOKEN>
<TOKEN end_char="6261" id="token-51-19" morph="none" pos="word" start_char="6261">a</TOKEN>
<TOKEN end_char="6272" id="token-51-20" morph="none" pos="word" start_char="6263">moratorium</TOKEN>
<TOKEN end_char="6275" id="token-51-21" morph="none" pos="word" start_char="6274">on</TOKEN>
<TOKEN end_char="6279" id="token-51-22" morph="none" pos="word" start_char="6277">the</TOKEN>
<TOKEN end_char="6284" id="token-51-23" morph="none" pos="word" start_char="6281">work</TOKEN>
<TOKEN end_char="6285" id="token-51-24" morph="none" pos="punct" start_char="6285">,</TOKEN>
<TOKEN end_char="6296" id="token-51-25" morph="none" pos="word" start_char="6287">suspending</TOKEN>
<TOKEN end_char="6299" id="token-51-26" morph="none" pos="word" start_char="6298">21</TOKEN>
<TOKEN end_char="6307" id="token-51-27" morph="none" pos="word" start_char="6301">studies</TOKEN>
<TOKEN end_char="6308" id="token-51-28" morph="none" pos="punct" start_char="6308">.</TOKEN>
</SEG>
<SEG end_char="6477" id="segment-52" start_char="6311">
<ORIGINAL_TEXT>Three years later, though—in December 2017—the NIH ended the moratorium and the second phase of the NIAID project, which included the gain-of-function research, began.</ORIGINAL_TEXT>
<TOKEN end_char="6315" id="token-52-0" morph="none" pos="word" start_char="6311">Three</TOKEN>
<TOKEN end_char="6321" id="token-52-1" morph="none" pos="word" start_char="6317">years</TOKEN>
<TOKEN end_char="6327" id="token-52-2" morph="none" pos="word" start_char="6323">later</TOKEN>
<TOKEN end_char="6328" id="token-52-3" morph="none" pos="punct" start_char="6328">,</TOKEN>
<TOKEN end_char="6338" id="token-52-4" morph="none" pos="unknown" start_char="6330">though—in</TOKEN>
<TOKEN end_char="6347" id="token-52-5" morph="none" pos="word" start_char="6340">December</TOKEN>
<TOKEN end_char="6356" id="token-52-6" morph="none" pos="unknown" start_char="6349">2017—the</TOKEN>
<TOKEN end_char="6360" id="token-52-7" morph="none" pos="word" start_char="6358">NIH</TOKEN>
<TOKEN end_char="6366" id="token-52-8" morph="none" pos="word" start_char="6362">ended</TOKEN>
<TOKEN end_char="6370" id="token-52-9" morph="none" pos="word" start_char="6368">the</TOKEN>
<TOKEN end_char="6381" id="token-52-10" morph="none" pos="word" start_char="6372">moratorium</TOKEN>
<TOKEN end_char="6385" id="token-52-11" morph="none" pos="word" start_char="6383">and</TOKEN>
<TOKEN end_char="6389" id="token-52-12" morph="none" pos="word" start_char="6387">the</TOKEN>
<TOKEN end_char="6396" id="token-52-13" morph="none" pos="word" start_char="6391">second</TOKEN>
<TOKEN end_char="6402" id="token-52-14" morph="none" pos="word" start_char="6398">phase</TOKEN>
<TOKEN end_char="6405" id="token-52-15" morph="none" pos="word" start_char="6404">of</TOKEN>
<TOKEN end_char="6409" id="token-52-16" morph="none" pos="word" start_char="6407">the</TOKEN>
<TOKEN end_char="6415" id="token-52-17" morph="none" pos="word" start_char="6411">NIAID</TOKEN>
<TOKEN end_char="6423" id="token-52-18" morph="none" pos="word" start_char="6417">project</TOKEN>
<TOKEN end_char="6424" id="token-52-19" morph="none" pos="punct" start_char="6424">,</TOKEN>
<TOKEN end_char="6430" id="token-52-20" morph="none" pos="word" start_char="6426">which</TOKEN>
<TOKEN end_char="6439" id="token-52-21" morph="none" pos="word" start_char="6432">included</TOKEN>
<TOKEN end_char="6443" id="token-52-22" morph="none" pos="word" start_char="6441">the</TOKEN>
<TOKEN end_char="6460" id="token-52-23" morph="none" pos="unknown" start_char="6445">gain-of-function</TOKEN>
<TOKEN end_char="6469" id="token-52-24" morph="none" pos="word" start_char="6462">research</TOKEN>
<TOKEN end_char="6470" id="token-52-25" morph="none" pos="punct" start_char="6470">,</TOKEN>
<TOKEN end_char="6476" id="token-52-26" morph="none" pos="word" start_char="6472">began</TOKEN>
<TOKEN end_char="6477" id="token-52-27" morph="none" pos="punct" start_char="6477">.</TOKEN>
</SEG>
<SEG end_char="6668" id="segment-53" start_char="6479">
<ORIGINAL_TEXT>The NIH established a framework for determining how the research would go forward: scientists have to get approval from a panel of experts, who would decide whether the risks were justified.</ORIGINAL_TEXT>
<TOKEN end_char="6481" id="token-53-0" morph="none" pos="word" start_char="6479">The</TOKEN>
<TOKEN end_char="6485" id="token-53-1" morph="none" pos="word" start_char="6483">NIH</TOKEN>
<TOKEN end_char="6497" id="token-53-2" morph="none" pos="word" start_char="6487">established</TOKEN>
<TOKEN end_char="6499" id="token-53-3" morph="none" pos="word" start_char="6499">a</TOKEN>
<TOKEN end_char="6509" id="token-53-4" morph="none" pos="word" start_char="6501">framework</TOKEN>
<TOKEN end_char="6513" id="token-53-5" morph="none" pos="word" start_char="6511">for</TOKEN>
<TOKEN end_char="6525" id="token-53-6" morph="none" pos="word" start_char="6515">determining</TOKEN>
<TOKEN end_char="6529" id="token-53-7" morph="none" pos="word" start_char="6527">how</TOKEN>
<TOKEN end_char="6533" id="token-53-8" morph="none" pos="word" start_char="6531">the</TOKEN>
<TOKEN end_char="6542" id="token-53-9" morph="none" pos="word" start_char="6535">research</TOKEN>
<TOKEN end_char="6548" id="token-53-10" morph="none" pos="word" start_char="6544">would</TOKEN>
<TOKEN end_char="6551" id="token-53-11" morph="none" pos="word" start_char="6550">go</TOKEN>
<TOKEN end_char="6559" id="token-53-12" morph="none" pos="word" start_char="6553">forward</TOKEN>
<TOKEN end_char="6560" id="token-53-13" morph="none" pos="punct" start_char="6560">:</TOKEN>
<TOKEN end_char="6571" id="token-53-14" morph="none" pos="word" start_char="6562">scientists</TOKEN>
<TOKEN end_char="6576" id="token-53-15" morph="none" pos="word" start_char="6573">have</TOKEN>
<TOKEN end_char="6579" id="token-53-16" morph="none" pos="word" start_char="6578">to</TOKEN>
<TOKEN end_char="6583" id="token-53-17" morph="none" pos="word" start_char="6581">get</TOKEN>
<TOKEN end_char="6592" id="token-53-18" morph="none" pos="word" start_char="6585">approval</TOKEN>
<TOKEN end_char="6597" id="token-53-19" morph="none" pos="word" start_char="6594">from</TOKEN>
<TOKEN end_char="6599" id="token-53-20" morph="none" pos="word" start_char="6599">a</TOKEN>
<TOKEN end_char="6605" id="token-53-21" morph="none" pos="word" start_char="6601">panel</TOKEN>
<TOKEN end_char="6608" id="token-53-22" morph="none" pos="word" start_char="6607">of</TOKEN>
<TOKEN end_char="6616" id="token-53-23" morph="none" pos="word" start_char="6610">experts</TOKEN>
<TOKEN end_char="6617" id="token-53-24" morph="none" pos="punct" start_char="6617">,</TOKEN>
<TOKEN end_char="6621" id="token-53-25" morph="none" pos="word" start_char="6619">who</TOKEN>
<TOKEN end_char="6627" id="token-53-26" morph="none" pos="word" start_char="6623">would</TOKEN>
<TOKEN end_char="6634" id="token-53-27" morph="none" pos="word" start_char="6629">decide</TOKEN>
<TOKEN end_char="6642" id="token-53-28" morph="none" pos="word" start_char="6636">whether</TOKEN>
<TOKEN end_char="6646" id="token-53-29" morph="none" pos="word" start_char="6644">the</TOKEN>
<TOKEN end_char="6652" id="token-53-30" morph="none" pos="word" start_char="6648">risks</TOKEN>
<TOKEN end_char="6657" id="token-53-31" morph="none" pos="word" start_char="6654">were</TOKEN>
<TOKEN end_char="6667" id="token-53-32" morph="none" pos="word" start_char="6659">justified</TOKEN>
<TOKEN end_char="6668" id="token-53-33" morph="none" pos="punct" start_char="6668">.</TOKEN>
</SEG>
<SEG end_char="6757" id="segment-54" start_char="6671">
<ORIGINAL_TEXT>The reviews were indeed conducted—but in secret, for which the NIH has drawn criticism.</ORIGINAL_TEXT>
<TOKEN end_char="6673" id="token-54-0" morph="none" pos="word" start_char="6671">The</TOKEN>
<TOKEN end_char="6681" id="token-54-1" morph="none" pos="word" start_char="6675">reviews</TOKEN>
<TOKEN end_char="6686" id="token-54-2" morph="none" pos="word" start_char="6683">were</TOKEN>
<TOKEN end_char="6693" id="token-54-3" morph="none" pos="word" start_char="6688">indeed</TOKEN>
<TOKEN end_char="6707" id="token-54-4" morph="none" pos="unknown" start_char="6695">conducted—but</TOKEN>
<TOKEN end_char="6710" id="token-54-5" morph="none" pos="word" start_char="6709">in</TOKEN>
<TOKEN end_char="6717" id="token-54-6" morph="none" pos="word" start_char="6712">secret</TOKEN>
<TOKEN end_char="6718" id="token-54-7" morph="none" pos="punct" start_char="6718">,</TOKEN>
<TOKEN end_char="6722" id="token-54-8" morph="none" pos="word" start_char="6720">for</TOKEN>
<TOKEN end_char="6728" id="token-54-9" morph="none" pos="word" start_char="6724">which</TOKEN>
<TOKEN end_char="6732" id="token-54-10" morph="none" pos="word" start_char="6730">the</TOKEN>
<TOKEN end_char="6736" id="token-54-11" morph="none" pos="word" start_char="6734">NIH</TOKEN>
<TOKEN end_char="6740" id="token-54-12" morph="none" pos="word" start_char="6738">has</TOKEN>
<TOKEN end_char="6746" id="token-54-13" morph="none" pos="word" start_char="6742">drawn</TOKEN>
<TOKEN end_char="6756" id="token-54-14" morph="none" pos="word" start_char="6748">criticism</TOKEN>
<TOKEN end_char="6757" id="token-54-15" morph="none" pos="punct" start_char="6757">.</TOKEN>
</SEG>
<SEG end_char="6793" id="segment-55" start_char="6759">
<ORIGINAL_TEXT>In early 2019, after a reporter for</ORIGINAL_TEXT>
<TOKEN end_char="6760" id="token-55-0" morph="none" pos="word" start_char="6759">In</TOKEN>
<TOKEN end_char="6766" id="token-55-1" morph="none" pos="word" start_char="6762">early</TOKEN>
<TOKEN end_char="6771" id="token-55-2" morph="none" pos="word" start_char="6768">2019</TOKEN>
<TOKEN end_char="6772" id="token-55-3" morph="none" pos="punct" start_char="6772">,</TOKEN>
<TOKEN end_char="6778" id="token-55-4" morph="none" pos="word" start_char="6774">after</TOKEN>
<TOKEN end_char="6780" id="token-55-5" morph="none" pos="word" start_char="6780">a</TOKEN>
<TOKEN end_char="6789" id="token-55-6" morph="none" pos="word" start_char="6782">reporter</TOKEN>
<TOKEN end_char="6793" id="token-55-7" morph="none" pos="word" start_char="6791">for</TOKEN>
</SEG>
<SEG end_char="6802" id="segment-56" start_char="6796">
<ORIGINAL_TEXT>Science</ORIGINAL_TEXT>
<TOKEN end_char="6802" id="token-56-0" morph="none" pos="word" start_char="6796">Science</TOKEN>
<TRANSLATED_TEXT>Wetenschap</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="7003" id="segment-57" start_char="6805">
<ORIGINAL_TEXT>magazine discovered that the NIH had approved two influenza research projects that used gain of function methods, scientists who oppose this kind of research excoriated the NIH in an editorial in the</ORIGINAL_TEXT>
<TOKEN end_char="6812" id="token-57-0" morph="none" pos="word" start_char="6805">magazine</TOKEN>
<TOKEN end_char="6823" id="token-57-1" morph="none" pos="word" start_char="6814">discovered</TOKEN>
<TOKEN end_char="6828" id="token-57-2" morph="none" pos="word" start_char="6825">that</TOKEN>
<TOKEN end_char="6832" id="token-57-3" morph="none" pos="word" start_char="6830">the</TOKEN>
<TOKEN end_char="6836" id="token-57-4" morph="none" pos="word" start_char="6834">NIH</TOKEN>
<TOKEN end_char="6840" id="token-57-5" morph="none" pos="word" start_char="6838">had</TOKEN>
<TOKEN end_char="6849" id="token-57-6" morph="none" pos="word" start_char="6842">approved</TOKEN>
<TOKEN end_char="6853" id="token-57-7" morph="none" pos="word" start_char="6851">two</TOKEN>
<TOKEN end_char="6863" id="token-57-8" morph="none" pos="word" start_char="6855">influenza</TOKEN>
<TOKEN end_char="6872" id="token-57-9" morph="none" pos="word" start_char="6865">research</TOKEN>
<TOKEN end_char="6881" id="token-57-10" morph="none" pos="word" start_char="6874">projects</TOKEN>
<TOKEN end_char="6886" id="token-57-11" morph="none" pos="word" start_char="6883">that</TOKEN>
<TOKEN end_char="6891" id="token-57-12" morph="none" pos="word" start_char="6888">used</TOKEN>
<TOKEN end_char="6896" id="token-57-13" morph="none" pos="word" start_char="6893">gain</TOKEN>
<TOKEN end_char="6899" id="token-57-14" morph="none" pos="word" start_char="6898">of</TOKEN>
<TOKEN end_char="6908" id="token-57-15" morph="none" pos="word" start_char="6901">function</TOKEN>
<TOKEN end_char="6916" id="token-57-16" morph="none" pos="word" start_char="6910">methods</TOKEN>
<TOKEN end_char="6917" id="token-57-17" morph="none" pos="punct" start_char="6917">,</TOKEN>
<TOKEN end_char="6928" id="token-57-18" morph="none" pos="word" start_char="6919">scientists</TOKEN>
<TOKEN end_char="6932" id="token-57-19" morph="none" pos="word" start_char="6930">who</TOKEN>
<TOKEN end_char="6939" id="token-57-20" morph="none" pos="word" start_char="6934">oppose</TOKEN>
<TOKEN end_char="6944" id="token-57-21" morph="none" pos="word" start_char="6941">this</TOKEN>
<TOKEN end_char="6949" id="token-57-22" morph="none" pos="word" start_char="6946">kind</TOKEN>
<TOKEN end_char="6952" id="token-57-23" morph="none" pos="word" start_char="6951">of</TOKEN>
<TOKEN end_char="6961" id="token-57-24" morph="none" pos="word" start_char="6954">research</TOKEN>
<TOKEN end_char="6972" id="token-57-25" morph="none" pos="word" start_char="6963">excoriated</TOKEN>
<TOKEN end_char="6976" id="token-57-26" morph="none" pos="word" start_char="6974">the</TOKEN>
<TOKEN end_char="6980" id="token-57-27" morph="none" pos="word" start_char="6978">NIH</TOKEN>
<TOKEN end_char="6983" id="token-57-28" morph="none" pos="word" start_char="6982">in</TOKEN>
<TOKEN end_char="6986" id="token-57-29" morph="none" pos="word" start_char="6985">an</TOKEN>
<TOKEN end_char="6996" id="token-57-30" morph="none" pos="word" start_char="6988">editorial</TOKEN>
<TOKEN end_char="6999" id="token-57-31" morph="none" pos="word" start_char="6998">in</TOKEN>
<TOKEN end_char="7003" id="token-57-32" morph="none" pos="word" start_char="7001">the</TOKEN>
</SEG>
<SEG end_char="7020" id="segment-58" start_char="7006">
<ORIGINAL_TEXT>Washington Post</ORIGINAL_TEXT>
<TOKEN end_char="7015" id="token-58-0" morph="none" pos="word" start_char="7006">Washington</TOKEN>
<TOKEN end_char="7020" id="token-58-1" morph="none" pos="word" start_char="7017">Post</TOKEN>
</SEG>
<SEG end_char="7023" id="segment-59" start_char="7023">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN end_char="7023" id="token-59-0" morph="none" pos="punct" start_char="7023">.</TOKEN>
</SEG>
<SEG end_char="7186" id="segment-60" start_char="7026">
<ORIGINAL_TEXT>"We have serious doubts about whether these experiments should be conducted at all," wrote Tom Inglesby of Johns Hopkins University and Marc Lipsitch of Harvard.</ORIGINAL_TEXT>
<TOKEN end_char="7026" id="token-60-0" morph="none" pos="punct" start_char="7026">"</TOKEN>
<TOKEN end_char="7028" id="token-60-1" morph="none" pos="word" start_char="7027">We</TOKEN>
<TOKEN end_char="7033" id="token-60-2" morph="none" pos="word" start_char="7030">have</TOKEN>
<TOKEN end_char="7041" id="token-60-3" morph="none" pos="word" start_char="7035">serious</TOKEN>
<TOKEN end_char="7048" id="token-60-4" morph="none" pos="word" start_char="7043">doubts</TOKEN>
<TOKEN end_char="7054" id="token-60-5" morph="none" pos="word" start_char="7050">about</TOKEN>
<TOKEN end_char="7062" id="token-60-6" morph="none" pos="word" start_char="7056">whether</TOKEN>
<TOKEN end_char="7068" id="token-60-7" morph="none" pos="word" start_char="7064">these</TOKEN>
<TOKEN end_char="7080" id="token-60-8" morph="none" pos="word" start_char="7070">experiments</TOKEN>
<TOKEN end_char="7087" id="token-60-9" morph="none" pos="word" start_char="7082">should</TOKEN>
<TOKEN end_char="7090" id="token-60-10" morph="none" pos="word" start_char="7089">be</TOKEN>
<TOKEN end_char="7100" id="token-60-11" morph="none" pos="word" start_char="7092">conducted</TOKEN>
<TOKEN end_char="7103" id="token-60-12" morph="none" pos="word" start_char="7102">at</TOKEN>
<TOKEN end_char="7107" id="token-60-13" morph="none" pos="word" start_char="7105">all</TOKEN>
<TOKEN end_char="7109" id="token-60-14" morph="none" pos="punct" start_char="7108">,"</TOKEN>
<TOKEN end_char="7115" id="token-60-15" morph="none" pos="word" start_char="7111">wrote</TOKEN>
<TOKEN end_char="7119" id="token-60-16" morph="none" pos="word" start_char="7117">Tom</TOKEN>
<TOKEN end_char="7128" id="token-60-17" morph="none" pos="word" start_char="7121">Inglesby</TOKEN>
<TOKEN end_char="7131" id="token-60-18" morph="none" pos="word" start_char="7130">of</TOKEN>
<TOKEN end_char="7137" id="token-60-19" morph="none" pos="word" start_char="7133">Johns</TOKEN>
<TOKEN end_char="7145" id="token-60-20" morph="none" pos="word" start_char="7139">Hopkins</TOKEN>
<TOKEN end_char="7156" id="token-60-21" morph="none" pos="word" start_char="7147">University</TOKEN>
<TOKEN end_char="7160" id="token-60-22" morph="none" pos="word" start_char="7158">and</TOKEN>
<TOKEN end_char="7165" id="token-60-23" morph="none" pos="word" start_char="7162">Marc</TOKEN>
<TOKEN end_char="7174" id="token-60-24" morph="none" pos="word" start_char="7167">Lipsitch</TOKEN>
<TOKEN end_char="7177" id="token-60-25" morph="none" pos="word" start_char="7176">of</TOKEN>
<TOKEN end_char="7185" id="token-60-26" morph="none" pos="word" start_char="7179">Harvard</TOKEN>
<TOKEN end_char="7186" id="token-60-27" morph="none" pos="punct" start_char="7186">.</TOKEN>
</SEG>
<SEG end_char="7385" id="segment-61" start_char="7188">
<ORIGINAL_TEXT>"[W]ith deliberations kept behind closed doors, none of us will have the opportunity to understand how the government arrived at these decisions or to judge the rigor and integrity of that process."</ORIGINAL_TEXT>
<TOKEN end_char="7189" id="token-61-0" morph="none" pos="punct" start_char="7188">"[</TOKEN>
<TOKEN end_char="7194" id="token-61-1" morph="none" pos="unknown" start_char="7190">W]ith</TOKEN>
<TOKEN end_char="7208" id="token-61-2" morph="none" pos="word" start_char="7196">deliberations</TOKEN>
<TOKEN end_char="7213" id="token-61-3" morph="none" pos="word" start_char="7210">kept</TOKEN>
<TOKEN end_char="7220" id="token-61-4" morph="none" pos="word" start_char="7215">behind</TOKEN>
<TOKEN end_char="7227" id="token-61-5" morph="none" pos="word" start_char="7222">closed</TOKEN>
<TOKEN end_char="7233" id="token-61-6" morph="none" pos="word" start_char="7229">doors</TOKEN>
<TOKEN end_char="7234" id="token-61-7" morph="none" pos="punct" start_char="7234">,</TOKEN>
<TOKEN end_char="7239" id="token-61-8" morph="none" pos="word" start_char="7236">none</TOKEN>
<TOKEN end_char="7242" id="token-61-9" morph="none" pos="word" start_char="7241">of</TOKEN>
<TOKEN end_char="7245" id="token-61-10" morph="none" pos="word" start_char="7244">us</TOKEN>
<TOKEN end_char="7250" id="token-61-11" morph="none" pos="word" start_char="7247">will</TOKEN>
<TOKEN end_char="7255" id="token-61-12" morph="none" pos="word" start_char="7252">have</TOKEN>
<TOKEN end_char="7259" id="token-61-13" morph="none" pos="word" start_char="7257">the</TOKEN>
<TOKEN end_char="7271" id="token-61-14" morph="none" pos="word" start_char="7261">opportunity</TOKEN>
<TOKEN end_char="7274" id="token-61-15" morph="none" pos="word" start_char="7273">to</TOKEN>
<TOKEN end_char="7285" id="token-61-16" morph="none" pos="word" start_char="7276">understand</TOKEN>
<TOKEN end_char="7289" id="token-61-17" morph="none" pos="word" start_char="7287">how</TOKEN>
<TOKEN end_char="7293" id="token-61-18" morph="none" pos="word" start_char="7291">the</TOKEN>
<TOKEN end_char="7304" id="token-61-19" morph="none" pos="word" start_char="7295">government</TOKEN>
<TOKEN end_char="7312" id="token-61-20" morph="none" pos="word" start_char="7306">arrived</TOKEN>
<TOKEN end_char="7315" id="token-61-21" morph="none" pos="word" start_char="7314">at</TOKEN>
<TOKEN end_char="7321" id="token-61-22" morph="none" pos="word" start_char="7317">these</TOKEN>
<TOKEN end_char="7331" id="token-61-23" morph="none" pos="word" start_char="7323">decisions</TOKEN>
<TOKEN end_char="7334" id="token-61-24" morph="none" pos="word" start_char="7333">or</TOKEN>
<TOKEN end_char="7337" id="token-61-25" morph="none" pos="word" start_char="7336">to</TOKEN>
<TOKEN end_char="7343" id="token-61-26" morph="none" pos="word" start_char="7339">judge</TOKEN>
<TOKEN end_char="7347" id="token-61-27" morph="none" pos="word" start_char="7345">the</TOKEN>
<TOKEN end_char="7353" id="token-61-28" morph="none" pos="word" start_char="7349">rigor</TOKEN>
<TOKEN end_char="7357" id="token-61-29" morph="none" pos="word" start_char="7355">and</TOKEN>
<TOKEN end_char="7367" id="token-61-30" morph="none" pos="word" start_char="7359">integrity</TOKEN>
<TOKEN end_char="7370" id="token-61-31" morph="none" pos="word" start_char="7369">of</TOKEN>
<TOKEN end_char="7375" id="token-61-32" morph="none" pos="word" start_char="7372">that</TOKEN>
<TOKEN end_char="7383" id="token-61-33" morph="none" pos="word" start_char="7377">process</TOKEN>
<TOKEN end_char="7385" id="token-61-34" morph="none" pos="punct" start_char="7384">."</TOKEN>
</SEG>
<SEG end_char="7413" id="segment-62" start_char="7388">
<ORIGINAL_TEXT>Correction 5/5, 6:20 p.m.:</ORIGINAL_TEXT>
<TOKEN end_char="7397" id="token-62-0" morph="none" pos="word" start_char="7388">Correction</TOKEN>
<TOKEN end_char="7401" id="token-62-1" morph="none" pos="unknown" start_char="7399">5/5</TOKEN>
<TOKEN end_char="7402" id="token-62-2" morph="none" pos="punct" start_char="7402">,</TOKEN>
<TOKEN end_char="7407" id="token-62-3" morph="none" pos="unknown" start_char="7404">6:20</TOKEN>
<TOKEN end_char="7411" id="token-62-4" morph="none" pos="unknown" start_char="7409">p.m</TOKEN>
<TOKEN end_char="7413" id="token-62-5" morph="none" pos="punct" start_char="7412">.:</TOKEN>
</SEG>
<SEG end_char="7573" id="segment-63" start_char="7416">
<ORIGINAL_TEXT>The headline of this story has been corrected to reflect that the Wuhan lab received only a part of the millions of U.S. dollars allocated for virus research.</ORIGINAL_TEXT>
<TOKEN end_char="7418" id="token-63-0" morph="none" pos="word" start_char="7416">The</TOKEN>
<TOKEN end_char="7427" id="token-63-1" morph="none" pos="word" start_char="7420">headline</TOKEN>
<TOKEN end_char="7430" id="token-63-2" morph="none" pos="word" start_char="7429">of</TOKEN>
<TOKEN end_char="7435" id="token-63-3" morph="none" pos="word" start_char="7432">this</TOKEN>
<TOKEN end_char="7441" id="token-63-4" morph="none" pos="word" start_char="7437">story</TOKEN>
<TOKEN end_char="7445" id="token-63-5" morph="none" pos="word" start_char="7443">has</TOKEN>
<TOKEN end_char="7450" id="token-63-6" morph="none" pos="word" start_char="7447">been</TOKEN>
<TOKEN end_char="7460" id="token-63-7" morph="none" pos="word" start_char="7452">corrected</TOKEN>
<TOKEN end_char="7463" id="token-63-8" morph="none" pos="word" start_char="7462">to</TOKEN>
<TOKEN end_char="7471" id="token-63-9" morph="none" pos="word" start_char="7465">reflect</TOKEN>
<TOKEN end_char="7476" id="token-63-10" morph="none" pos="word" start_char="7473">that</TOKEN>
<TOKEN end_char="7480" id="token-63-11" morph="none" pos="word" start_char="7478">the</TOKEN>
<TOKEN end_char="7486" id="token-63-12" morph="none" pos="word" start_char="7482">Wuhan</TOKEN>
<TOKEN end_char="7490" id="token-63-13" morph="none" pos="word" start_char="7488">lab</TOKEN>
<TOKEN end_char="7499" id="token-63-14" morph="none" pos="word" start_char="7492">received</TOKEN>
<TOKEN end_char="7504" id="token-63-15" morph="none" pos="word" start_char="7501">only</TOKEN>
<TOKEN end_char="7506" id="token-63-16" morph="none" pos="word" start_char="7506">a</TOKEN>
<TOKEN end_char="7511" id="token-63-17" morph="none" pos="word" start_char="7508">part</TOKEN>
<TOKEN end_char="7514" id="token-63-18" morph="none" pos="word" start_char="7513">of</TOKEN>
<TOKEN end_char="7518" id="token-63-19" morph="none" pos="word" start_char="7516">the</TOKEN>
<TOKEN end_char="7527" id="token-63-20" morph="none" pos="word" start_char="7520">millions</TOKEN>
<TOKEN end_char="7530" id="token-63-21" morph="none" pos="word" start_char="7529">of</TOKEN>
<TOKEN end_char="7534" id="token-63-22" morph="none" pos="unknown" start_char="7532">U.S</TOKEN>
<TOKEN end_char="7535" id="token-63-23" morph="none" pos="punct" start_char="7535">.</TOKEN>
<TOKEN end_char="7543" id="token-63-24" morph="none" pos="word" start_char="7537">dollars</TOKEN>
<TOKEN end_char="7553" id="token-63-25" morph="none" pos="word" start_char="7545">allocated</TOKEN>
<TOKEN end_char="7557" id="token-63-26" morph="none" pos="word" start_char="7555">for</TOKEN>
<TOKEN end_char="7563" id="token-63-27" morph="none" pos="word" start_char="7559">virus</TOKEN>
<TOKEN end_char="7572" id="token-63-28" morph="none" pos="word" start_char="7565">research</TOKEN>
<TOKEN end_char="7573" id="token-63-29" morph="none" pos="punct" start_char="7573">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>