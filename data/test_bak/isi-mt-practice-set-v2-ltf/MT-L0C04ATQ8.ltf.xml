<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04ATQ8" lang="spa" raw_text_char_length="1455" raw_text_md5="097a16bc4c05e462dabff7c9131476ec" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="78" id="segment-0" start_char="1">
<ORIGINAL_TEXT>El coronavirus ya circulaba por Europa en noviembre de 2019, revela un estudio</ORIGINAL_TEXT>
<TOKEN end_char="2" id="token-0-0" morph="none" pos="word" start_char="1">El</TOKEN>
<TOKEN end_char="14" id="token-0-1" morph="none" pos="word" start_char="4">coronavirus</TOKEN>
<TOKEN end_char="17" id="token-0-2" morph="none" pos="word" start_char="16">ya</TOKEN>
<TOKEN end_char="27" id="token-0-3" morph="none" pos="word" start_char="19">circulaba</TOKEN>
<TOKEN end_char="31" id="token-0-4" morph="none" pos="word" start_char="29">por</TOKEN>
<TOKEN end_char="38" id="token-0-5" morph="none" pos="word" start_char="33">Europa</TOKEN>
<TOKEN end_char="41" id="token-0-6" morph="none" pos="word" start_char="40">en</TOKEN>
<TOKEN end_char="51" id="token-0-7" morph="none" pos="word" start_char="43">noviembre</TOKEN>
<TOKEN end_char="54" id="token-0-8" morph="none" pos="word" start_char="53">de</TOKEN>
<TOKEN end_char="59" id="token-0-9" morph="none" pos="word" start_char="56">2019</TOKEN>
<TOKEN end_char="60" id="token-0-10" morph="none" pos="punct" start_char="60">,</TOKEN>
<TOKEN end_char="67" id="token-0-11" morph="none" pos="word" start_char="62">revela</TOKEN>
<TOKEN end_char="70" id="token-0-12" morph="none" pos="word" start_char="69">un</TOKEN>
<TOKEN end_char="78" id="token-0-13" morph="none" pos="word" start_char="72">estudio</TOKEN>
<TRANSLATED_TEXT>The coronavirus was already circulating in Europe in November 2019, reveals a study</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="93" id="segment-1" start_char="82">
<ORIGINAL_TEXT>EUROPA PRESS</ORIGINAL_TEXT>
<TOKEN end_char="87" id="token-1-0" morph="none" pos="word" start_char="82">EUROPA</TOKEN>
<TOKEN end_char="93" id="token-1-1" morph="none" pos="word" start_char="89">PRESS</TOKEN>
</SEG>
<SEG end_char="164" id="segment-2" start_char="96">
<ORIGINAL_TEXT>Un hombre con mascarilla frente a la Torre Eiffel en París (Francia).</ORIGINAL_TEXT>
<TOKEN end_char="97" id="token-2-0" morph="none" pos="word" start_char="96">Un</TOKEN>
<TOKEN end_char="104" id="token-2-1" morph="none" pos="word" start_char="99">hombre</TOKEN>
<TOKEN end_char="108" id="token-2-2" morph="none" pos="word" start_char="106">con</TOKEN>
<TOKEN end_char="119" id="token-2-3" morph="none" pos="word" start_char="110">mascarilla</TOKEN>
<TOKEN end_char="126" id="token-2-4" morph="none" pos="word" start_char="121">frente</TOKEN>
<TOKEN end_char="128" id="token-2-5" morph="none" pos="word" start_char="128">a</TOKEN>
<TOKEN end_char="131" id="token-2-6" morph="none" pos="word" start_char="130">la</TOKEN>
<TOKEN end_char="137" id="token-2-7" morph="none" pos="word" start_char="133">Torre</TOKEN>
<TOKEN end_char="144" id="token-2-8" morph="none" pos="word" start_char="139">Eiffel</TOKEN>
<TOKEN end_char="147" id="token-2-9" morph="none" pos="word" start_char="146">en</TOKEN>
<TOKEN end_char="153" id="token-2-10" morph="none" pos="word" start_char="149">París</TOKEN>
<TOKEN end_char="155" id="token-2-11" morph="none" pos="punct" start_char="155">(</TOKEN>
<TOKEN end_char="162" id="token-2-12" morph="none" pos="word" start_char="156">Francia</TOKEN>
<TOKEN end_char="164" id="token-2-13" morph="none" pos="punct" start_char="163">).</TOKEN>
<TRANSLATED_TEXT>A man with a mask in front of the Eiffel Tower in Paris, France.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="318" id="segment-3" start_char="168">
<ORIGINAL_TEXT>La covid-19 ya circulaba por Europa en noviembre de 2019, en concreto en Francia, según ha revelado un estudio publicado a principios de este mes en el</ORIGINAL_TEXT>
<TOKEN end_char="169" id="token-3-0" morph="none" pos="word" start_char="168">La</TOKEN>
<TOKEN end_char="178" id="token-3-1" morph="none" pos="unknown" start_char="171">covid-19</TOKEN>
<TOKEN end_char="181" id="token-3-2" morph="none" pos="word" start_char="180">ya</TOKEN>
<TOKEN end_char="191" id="token-3-3" morph="none" pos="word" start_char="183">circulaba</TOKEN>
<TOKEN end_char="195" id="token-3-4" morph="none" pos="word" start_char="193">por</TOKEN>
<TOKEN end_char="202" id="token-3-5" morph="none" pos="word" start_char="197">Europa</TOKEN>
<TOKEN end_char="205" id="token-3-6" morph="none" pos="word" start_char="204">en</TOKEN>
<TOKEN end_char="215" id="token-3-7" morph="none" pos="word" start_char="207">noviembre</TOKEN>
<TOKEN end_char="218" id="token-3-8" morph="none" pos="word" start_char="217">de</TOKEN>
<TOKEN end_char="223" id="token-3-9" morph="none" pos="word" start_char="220">2019</TOKEN>
<TOKEN end_char="224" id="token-3-10" morph="none" pos="punct" start_char="224">,</TOKEN>
<TOKEN end_char="227" id="token-3-11" morph="none" pos="word" start_char="226">en</TOKEN>
<TOKEN end_char="236" id="token-3-12" morph="none" pos="word" start_char="229">concreto</TOKEN>
<TOKEN end_char="239" id="token-3-13" morph="none" pos="word" start_char="238">en</TOKEN>
<TOKEN end_char="247" id="token-3-14" morph="none" pos="word" start_char="241">Francia</TOKEN>
<TOKEN end_char="248" id="token-3-15" morph="none" pos="punct" start_char="248">,</TOKEN>
<TOKEN end_char="254" id="token-3-16" morph="none" pos="word" start_char="250">según</TOKEN>
<TOKEN end_char="257" id="token-3-17" morph="none" pos="word" start_char="256">ha</TOKEN>
<TOKEN end_char="266" id="token-3-18" morph="none" pos="word" start_char="259">revelado</TOKEN>
<TOKEN end_char="269" id="token-3-19" morph="none" pos="word" start_char="268">un</TOKEN>
<TOKEN end_char="277" id="token-3-20" morph="none" pos="word" start_char="271">estudio</TOKEN>
<TOKEN end_char="287" id="token-3-21" morph="none" pos="word" start_char="279">publicado</TOKEN>
<TOKEN end_char="289" id="token-3-22" morph="none" pos="word" start_char="289">a</TOKEN>
<TOKEN end_char="300" id="token-3-23" morph="none" pos="word" start_char="291">principios</TOKEN>
<TOKEN end_char="303" id="token-3-24" morph="none" pos="word" start_char="302">de</TOKEN>
<TOKEN end_char="308" id="token-3-25" morph="none" pos="word" start_char="305">este</TOKEN>
<TOKEN end_char="312" id="token-3-26" morph="none" pos="word" start_char="310">mes</TOKEN>
<TOKEN end_char="315" id="token-3-27" morph="none" pos="word" start_char="314">en</TOKEN>
<TOKEN end_char="318" id="token-3-28" morph="none" pos="word" start_char="317">el</TOKEN>
<TRANSLATED_TEXT>The covid-19 was already circulating throughout Europe in November 2019, specifically in France, according to a study published earlier this month in the</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="353" id="segment-4" start_char="321">
<ORIGINAL_TEXT>European Journal of Epidemiology.</ORIGINAL_TEXT>
<TOKEN end_char="328" id="token-4-0" morph="none" pos="word" start_char="321">European</TOKEN>
<TOKEN end_char="336" id="token-4-1" morph="none" pos="word" start_char="330">Journal</TOKEN>
<TOKEN end_char="339" id="token-4-2" morph="none" pos="word" start_char="338">of</TOKEN>
<TOKEN end_char="352" id="token-4-3" morph="none" pos="word" start_char="341">Epidemiology</TOKEN>
<TOKEN end_char="353" id="token-4-4" morph="none" pos="punct" start_char="353">.</TOKEN>
</SEG>
<SEG end_char="533" id="segment-5" start_char="357">
<ORIGINAL_TEXT>Este estudio desafía la teoría de que la pandemia se inició a finales del año pasado en la ciudad china de Wuhan, en concreto en los alrededores del mercado húmedo de la ciudad.</ORIGINAL_TEXT>
<TOKEN end_char="360" id="token-5-0" morph="none" pos="word" start_char="357">Este</TOKEN>
<TOKEN end_char="368" id="token-5-1" morph="none" pos="word" start_char="362">estudio</TOKEN>
<TOKEN end_char="376" id="token-5-2" morph="none" pos="word" start_char="370">desafía</TOKEN>
<TOKEN end_char="379" id="token-5-3" morph="none" pos="word" start_char="378">la</TOKEN>
<TOKEN end_char="386" id="token-5-4" morph="none" pos="word" start_char="381">teoría</TOKEN>
<TOKEN end_char="389" id="token-5-5" morph="none" pos="word" start_char="388">de</TOKEN>
<TOKEN end_char="393" id="token-5-6" morph="none" pos="word" start_char="391">que</TOKEN>
<TOKEN end_char="396" id="token-5-7" morph="none" pos="word" start_char="395">la</TOKEN>
<TOKEN end_char="405" id="token-5-8" morph="none" pos="word" start_char="398">pandemia</TOKEN>
<TOKEN end_char="408" id="token-5-9" morph="none" pos="word" start_char="407">se</TOKEN>
<TOKEN end_char="415" id="token-5-10" morph="none" pos="word" start_char="410">inició</TOKEN>
<TOKEN end_char="417" id="token-5-11" morph="none" pos="word" start_char="417">a</TOKEN>
<TOKEN end_char="425" id="token-5-12" morph="none" pos="word" start_char="419">finales</TOKEN>
<TOKEN end_char="429" id="token-5-13" morph="none" pos="word" start_char="427">del</TOKEN>
<TOKEN end_char="433" id="token-5-14" morph="none" pos="word" start_char="431">año</TOKEN>
<TOKEN end_char="440" id="token-5-15" morph="none" pos="word" start_char="435">pasado</TOKEN>
<TOKEN end_char="443" id="token-5-16" morph="none" pos="word" start_char="442">en</TOKEN>
<TOKEN end_char="446" id="token-5-17" morph="none" pos="word" start_char="445">la</TOKEN>
<TOKEN end_char="453" id="token-5-18" morph="none" pos="word" start_char="448">ciudad</TOKEN>
<TOKEN end_char="459" id="token-5-19" morph="none" pos="word" start_char="455">china</TOKEN>
<TOKEN end_char="462" id="token-5-20" morph="none" pos="word" start_char="461">de</TOKEN>
<TOKEN end_char="468" id="token-5-21" morph="none" pos="word" start_char="464">Wuhan</TOKEN>
<TOKEN end_char="469" id="token-5-22" morph="none" pos="punct" start_char="469">,</TOKEN>
<TOKEN end_char="472" id="token-5-23" morph="none" pos="word" start_char="471">en</TOKEN>
<TOKEN end_char="481" id="token-5-24" morph="none" pos="word" start_char="474">concreto</TOKEN>
<TOKEN end_char="484" id="token-5-25" morph="none" pos="word" start_char="483">en</TOKEN>
<TOKEN end_char="488" id="token-5-26" morph="none" pos="word" start_char="486">los</TOKEN>
<TOKEN end_char="500" id="token-5-27" morph="none" pos="word" start_char="490">alrededores</TOKEN>
<TOKEN end_char="504" id="token-5-28" morph="none" pos="word" start_char="502">del</TOKEN>
<TOKEN end_char="512" id="token-5-29" morph="none" pos="word" start_char="506">mercado</TOKEN>
<TOKEN end_char="519" id="token-5-30" morph="none" pos="word" start_char="514">húmedo</TOKEN>
<TOKEN end_char="522" id="token-5-31" morph="none" pos="word" start_char="521">de</TOKEN>
<TOKEN end_char="525" id="token-5-32" morph="none" pos="word" start_char="524">la</TOKEN>
<TOKEN end_char="532" id="token-5-33" morph="none" pos="word" start_char="527">ciudad</TOKEN>
<TOKEN end_char="533" id="token-5-34" morph="none" pos="punct" start_char="533">.</TOKEN>
<TRANSLATED_TEXT>This study challenges the theory that the pandemic began late last year in the Chinese city of Wuhan, specifically around the city's wet market.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="766" id="segment-6" start_char="536">
<ORIGINAL_TEXT>"Este informe sugiere que la infección por SARS-CoV-2 puede haber ocurrido ya en noviembre de 2019 en Francia", dice el documento, firmado por científicos de la Universidad de París, de la Sorbona y de la Universidad Aix-Marseille.</ORIGINAL_TEXT>
<TOKEN end_char="536" id="token-6-0" morph="none" pos="punct" start_char="536">"</TOKEN>
<TOKEN end_char="540" id="token-6-1" morph="none" pos="word" start_char="537">Este</TOKEN>
<TOKEN end_char="548" id="token-6-2" morph="none" pos="word" start_char="542">informe</TOKEN>
<TOKEN end_char="556" id="token-6-3" morph="none" pos="word" start_char="550">sugiere</TOKEN>
<TOKEN end_char="560" id="token-6-4" morph="none" pos="word" start_char="558">que</TOKEN>
<TOKEN end_char="563" id="token-6-5" morph="none" pos="word" start_char="562">la</TOKEN>
<TOKEN end_char="573" id="token-6-6" morph="none" pos="word" start_char="565">infección</TOKEN>
<TOKEN end_char="577" id="token-6-7" morph="none" pos="word" start_char="575">por</TOKEN>
<TOKEN end_char="588" id="token-6-8" morph="none" pos="unknown" start_char="579">SARS-CoV-2</TOKEN>
<TOKEN end_char="594" id="token-6-9" morph="none" pos="word" start_char="590">puede</TOKEN>
<TOKEN end_char="600" id="token-6-10" morph="none" pos="word" start_char="596">haber</TOKEN>
<TOKEN end_char="609" id="token-6-11" morph="none" pos="word" start_char="602">ocurrido</TOKEN>
<TOKEN end_char="612" id="token-6-12" morph="none" pos="word" start_char="611">ya</TOKEN>
<TOKEN end_char="615" id="token-6-13" morph="none" pos="word" start_char="614">en</TOKEN>
<TOKEN end_char="625" id="token-6-14" morph="none" pos="word" start_char="617">noviembre</TOKEN>
<TOKEN end_char="628" id="token-6-15" morph="none" pos="word" start_char="627">de</TOKEN>
<TOKEN end_char="633" id="token-6-16" morph="none" pos="word" start_char="630">2019</TOKEN>
<TOKEN end_char="636" id="token-6-17" morph="none" pos="word" start_char="635">en</TOKEN>
<TOKEN end_char="644" id="token-6-18" morph="none" pos="word" start_char="638">Francia</TOKEN>
<TOKEN end_char="646" id="token-6-19" morph="none" pos="punct" start_char="645">",</TOKEN>
<TOKEN end_char="651" id="token-6-20" morph="none" pos="word" start_char="648">dice</TOKEN>
<TOKEN end_char="654" id="token-6-21" morph="none" pos="word" start_char="653">el</TOKEN>
<TOKEN end_char="664" id="token-6-22" morph="none" pos="word" start_char="656">documento</TOKEN>
<TOKEN end_char="665" id="token-6-23" morph="none" pos="punct" start_char="665">,</TOKEN>
<TOKEN end_char="673" id="token-6-24" morph="none" pos="word" start_char="667">firmado</TOKEN>
<TOKEN end_char="677" id="token-6-25" morph="none" pos="word" start_char="675">por</TOKEN>
<TOKEN end_char="689" id="token-6-26" morph="none" pos="word" start_char="679">científicos</TOKEN>
<TOKEN end_char="692" id="token-6-27" morph="none" pos="word" start_char="691">de</TOKEN>
<TOKEN end_char="695" id="token-6-28" morph="none" pos="word" start_char="694">la</TOKEN>
<TOKEN end_char="707" id="token-6-29" morph="none" pos="word" start_char="697">Universidad</TOKEN>
<TOKEN end_char="710" id="token-6-30" morph="none" pos="word" start_char="709">de</TOKEN>
<TOKEN end_char="716" id="token-6-31" morph="none" pos="word" start_char="712">París</TOKEN>
<TOKEN end_char="717" id="token-6-32" morph="none" pos="punct" start_char="717">,</TOKEN>
<TOKEN end_char="720" id="token-6-33" morph="none" pos="word" start_char="719">de</TOKEN>
<TOKEN end_char="723" id="token-6-34" morph="none" pos="word" start_char="722">la</TOKEN>
<TOKEN end_char="731" id="token-6-35" morph="none" pos="word" start_char="725">Sorbona</TOKEN>
<TOKEN end_char="733" id="token-6-36" morph="none" pos="word" start_char="733">y</TOKEN>
<TOKEN end_char="736" id="token-6-37" morph="none" pos="word" start_char="735">de</TOKEN>
<TOKEN end_char="739" id="token-6-38" morph="none" pos="word" start_char="738">la</TOKEN>
<TOKEN end_char="751" id="token-6-39" morph="none" pos="word" start_char="741">Universidad</TOKEN>
<TOKEN end_char="765" id="token-6-40" morph="none" pos="unknown" start_char="753">Aix-Marseille</TOKEN>
<TOKEN end_char="766" id="token-6-41" morph="none" pos="punct" start_char="766">.</TOKEN>
<TRANSLATED_TEXT>"This report suggests that SARS-CoV-2 infection may have occurred as early as November 2019 in France," says the document, signed by scientists from the University of Paris, the Sorbonne and the University of Aix-Marseille.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="956" id="segment-7" start_char="769">
<ORIGINAL_TEXT>"En varios participantes identificamos síntomas, antecedentes de posibles exposiciones o eventos específicos compatibles con la infección temprana por SARS-CoV-2", revela la investigación.</ORIGINAL_TEXT>
<TOKEN end_char="769" id="token-7-0" morph="none" pos="punct" start_char="769">"</TOKEN>
<TOKEN end_char="771" id="token-7-1" morph="none" pos="word" start_char="770">En</TOKEN>
<TOKEN end_char="778" id="token-7-2" morph="none" pos="word" start_char="773">varios</TOKEN>
<TOKEN end_char="792" id="token-7-3" morph="none" pos="word" start_char="780">participantes</TOKEN>
<TOKEN end_char="806" id="token-7-4" morph="none" pos="word" start_char="794">identificamos</TOKEN>
<TOKEN end_char="815" id="token-7-5" morph="none" pos="word" start_char="808">síntomas</TOKEN>
<TOKEN end_char="816" id="token-7-6" morph="none" pos="punct" start_char="816">,</TOKEN>
<TOKEN end_char="829" id="token-7-7" morph="none" pos="word" start_char="818">antecedentes</TOKEN>
<TOKEN end_char="832" id="token-7-8" morph="none" pos="word" start_char="831">de</TOKEN>
<TOKEN end_char="841" id="token-7-9" morph="none" pos="word" start_char="834">posibles</TOKEN>
<TOKEN end_char="854" id="token-7-10" morph="none" pos="word" start_char="843">exposiciones</TOKEN>
<TOKEN end_char="856" id="token-7-11" morph="none" pos="word" start_char="856">o</TOKEN>
<TOKEN end_char="864" id="token-7-12" morph="none" pos="word" start_char="858">eventos</TOKEN>
<TOKEN end_char="876" id="token-7-13" morph="none" pos="word" start_char="866">específicos</TOKEN>
<TOKEN end_char="888" id="token-7-14" morph="none" pos="word" start_char="878">compatibles</TOKEN>
<TOKEN end_char="892" id="token-7-15" morph="none" pos="word" start_char="890">con</TOKEN>
<TOKEN end_char="895" id="token-7-16" morph="none" pos="word" start_char="894">la</TOKEN>
<TOKEN end_char="905" id="token-7-17" morph="none" pos="word" start_char="897">infección</TOKEN>
<TOKEN end_char="914" id="token-7-18" morph="none" pos="word" start_char="907">temprana</TOKEN>
<TOKEN end_char="918" id="token-7-19" morph="none" pos="word" start_char="916">por</TOKEN>
<TOKEN end_char="929" id="token-7-20" morph="none" pos="unknown" start_char="920">SARS-CoV-2</TOKEN>
<TOKEN end_char="931" id="token-7-21" morph="none" pos="punct" start_char="930">",</TOKEN>
<TOKEN end_char="938" id="token-7-22" morph="none" pos="word" start_char="933">revela</TOKEN>
<TOKEN end_char="941" id="token-7-23" morph="none" pos="word" start_char="940">la</TOKEN>
<TOKEN end_char="955" id="token-7-24" morph="none" pos="word" start_char="943">investigación</TOKEN>
<TOKEN end_char="956" id="token-7-25" morph="none" pos="punct" start_char="956">.</TOKEN>
<TRANSLATED_TEXT>"In several participants we identify symptoms, background to possible exposures or specific events consistent with early SARS-CoV-2 infection," the research reveals.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1266" id="segment-8" start_char="959">
<ORIGINAL_TEXT>El estudio, dirigido por el profesor Fabrice Carrat, director del Instituto Pierre-Louis de Epidemiología y Salud Pública para Inserm en la Universidad de la Sorbona, involucró la recolección de muestras de suero de 9.144 adultos en la población francesa, de los cuales 353 habían dado positivo por covid-19.</ORIGINAL_TEXT>
<TOKEN end_char="960" id="token-8-0" morph="none" pos="word" start_char="959">El</TOKEN>
<TOKEN end_char="968" id="token-8-1" morph="none" pos="word" start_char="962">estudio</TOKEN>
<TOKEN end_char="969" id="token-8-2" morph="none" pos="punct" start_char="969">,</TOKEN>
<TOKEN end_char="978" id="token-8-3" morph="none" pos="word" start_char="971">dirigido</TOKEN>
<TOKEN end_char="982" id="token-8-4" morph="none" pos="word" start_char="980">por</TOKEN>
<TOKEN end_char="985" id="token-8-5" morph="none" pos="word" start_char="984">el</TOKEN>
<TOKEN end_char="994" id="token-8-6" morph="none" pos="word" start_char="987">profesor</TOKEN>
<TOKEN end_char="1002" id="token-8-7" morph="none" pos="word" start_char="996">Fabrice</TOKEN>
<TOKEN end_char="1009" id="token-8-8" morph="none" pos="word" start_char="1004">Carrat</TOKEN>
<TOKEN end_char="1010" id="token-8-9" morph="none" pos="punct" start_char="1010">,</TOKEN>
<TOKEN end_char="1019" id="token-8-10" morph="none" pos="word" start_char="1012">director</TOKEN>
<TOKEN end_char="1023" id="token-8-11" morph="none" pos="word" start_char="1021">del</TOKEN>
<TOKEN end_char="1033" id="token-8-12" morph="none" pos="word" start_char="1025">Instituto</TOKEN>
<TOKEN end_char="1046" id="token-8-13" morph="none" pos="unknown" start_char="1035">Pierre-Louis</TOKEN>
<TOKEN end_char="1049" id="token-8-14" morph="none" pos="word" start_char="1048">de</TOKEN>
<TOKEN end_char="1063" id="token-8-15" morph="none" pos="word" start_char="1051">Epidemiología</TOKEN>
<TOKEN end_char="1065" id="token-8-16" morph="none" pos="word" start_char="1065">y</TOKEN>
<TOKEN end_char="1071" id="token-8-17" morph="none" pos="word" start_char="1067">Salud</TOKEN>
<TOKEN end_char="1079" id="token-8-18" morph="none" pos="word" start_char="1073">Pública</TOKEN>
<TOKEN end_char="1084" id="token-8-19" morph="none" pos="word" start_char="1081">para</TOKEN>
<TOKEN end_char="1091" id="token-8-20" morph="none" pos="word" start_char="1086">Inserm</TOKEN>
<TOKEN end_char="1094" id="token-8-21" morph="none" pos="word" start_char="1093">en</TOKEN>
<TOKEN end_char="1097" id="token-8-22" morph="none" pos="word" start_char="1096">la</TOKEN>
<TOKEN end_char="1109" id="token-8-23" morph="none" pos="word" start_char="1099">Universidad</TOKEN>
<TOKEN end_char="1112" id="token-8-24" morph="none" pos="word" start_char="1111">de</TOKEN>
<TOKEN end_char="1115" id="token-8-25" morph="none" pos="word" start_char="1114">la</TOKEN>
<TOKEN end_char="1123" id="token-8-26" morph="none" pos="word" start_char="1117">Sorbona</TOKEN>
<TOKEN end_char="1124" id="token-8-27" morph="none" pos="punct" start_char="1124">,</TOKEN>
<TOKEN end_char="1134" id="token-8-28" morph="none" pos="word" start_char="1126">involucró</TOKEN>
<TOKEN end_char="1137" id="token-8-29" morph="none" pos="word" start_char="1136">la</TOKEN>
<TOKEN end_char="1149" id="token-8-30" morph="none" pos="word" start_char="1139">recolección</TOKEN>
<TOKEN end_char="1152" id="token-8-31" morph="none" pos="word" start_char="1151">de</TOKEN>
<TOKEN end_char="1161" id="token-8-32" morph="none" pos="word" start_char="1154">muestras</TOKEN>
<TOKEN end_char="1164" id="token-8-33" morph="none" pos="word" start_char="1163">de</TOKEN>
<TOKEN end_char="1170" id="token-8-34" morph="none" pos="word" start_char="1166">suero</TOKEN>
<TOKEN end_char="1173" id="token-8-35" morph="none" pos="word" start_char="1172">de</TOKEN>
<TOKEN end_char="1179" id="token-8-36" morph="none" pos="word" start_char="1175">9.144</TOKEN>
<TOKEN end_char="1187" id="token-8-37" morph="none" pos="word" start_char="1181">adultos</TOKEN>
<TOKEN end_char="1190" id="token-8-38" morph="none" pos="word" start_char="1189">en</TOKEN>
<TOKEN end_char="1193" id="token-8-39" morph="none" pos="word" start_char="1192">la</TOKEN>
<TOKEN end_char="1203" id="token-8-40" morph="none" pos="word" start_char="1195">población</TOKEN>
<TOKEN end_char="1212" id="token-8-41" morph="none" pos="word" start_char="1205">francesa</TOKEN>
<TOKEN end_char="1213" id="token-8-42" morph="none" pos="punct" start_char="1213">,</TOKEN>
<TOKEN end_char="1216" id="token-8-43" morph="none" pos="word" start_char="1215">de</TOKEN>
<TOKEN end_char="1220" id="token-8-44" morph="none" pos="word" start_char="1218">los</TOKEN>
<TOKEN end_char="1227" id="token-8-45" morph="none" pos="word" start_char="1222">cuales</TOKEN>
<TOKEN end_char="1231" id="token-8-46" morph="none" pos="word" start_char="1229">353</TOKEN>
<TOKEN end_char="1238" id="token-8-47" morph="none" pos="word" start_char="1233">habían</TOKEN>
<TOKEN end_char="1243" id="token-8-48" morph="none" pos="word" start_char="1240">dado</TOKEN>
<TOKEN end_char="1252" id="token-8-49" morph="none" pos="word" start_char="1245">positivo</TOKEN>
<TOKEN end_char="1256" id="token-8-50" morph="none" pos="word" start_char="1254">por</TOKEN>
<TOKEN end_char="1265" id="token-8-51" morph="none" pos="unknown" start_char="1258">covid-19</TOKEN>
<TOKEN end_char="1266" id="token-8-52" morph="none" pos="punct" start_char="1266">.</TOKEN>
<TRANSLATED_TEXT>The study, led by Professor Fabrice Carrat, director of the Pierre-Louis Institute of Epidemiology and Public Health for Inserm at the Sorbonne University, involved the collection of serum samples of 9,144 adults in the French population, of whom 353 had tested positive for covid-19.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="1451" id="segment-9" start_char="1269">
<ORIGINAL_TEXT>De esos positivos, 13 fueron muestreados entre noviembre de 2019 y enero de 2020 y fueron confirmados mediante pruebas de anticuerpos neutralizantes, revela la investigación francesa.</ORIGINAL_TEXT>
<TOKEN end_char="1270" id="token-9-0" morph="none" pos="word" start_char="1269">De</TOKEN>
<TOKEN end_char="1275" id="token-9-1" morph="none" pos="word" start_char="1272">esos</TOKEN>
<TOKEN end_char="1285" id="token-9-2" morph="none" pos="word" start_char="1277">positivos</TOKEN>
<TOKEN end_char="1286" id="token-9-3" morph="none" pos="punct" start_char="1286">,</TOKEN>
<TOKEN end_char="1289" id="token-9-4" morph="none" pos="word" start_char="1288">13</TOKEN>
<TOKEN end_char="1296" id="token-9-5" morph="none" pos="word" start_char="1291">fueron</TOKEN>
<TOKEN end_char="1308" id="token-9-6" morph="none" pos="word" start_char="1298">muestreados</TOKEN>
<TOKEN end_char="1314" id="token-9-7" morph="none" pos="word" start_char="1310">entre</TOKEN>
<TOKEN end_char="1324" id="token-9-8" morph="none" pos="word" start_char="1316">noviembre</TOKEN>
<TOKEN end_char="1327" id="token-9-9" morph="none" pos="word" start_char="1326">de</TOKEN>
<TOKEN end_char="1332" id="token-9-10" morph="none" pos="word" start_char="1329">2019</TOKEN>
<TOKEN end_char="1334" id="token-9-11" morph="none" pos="word" start_char="1334">y</TOKEN>
<TOKEN end_char="1340" id="token-9-12" morph="none" pos="word" start_char="1336">enero</TOKEN>
<TOKEN end_char="1343" id="token-9-13" morph="none" pos="word" start_char="1342">de</TOKEN>
<TOKEN end_char="1348" id="token-9-14" morph="none" pos="word" start_char="1345">2020</TOKEN>
<TOKEN end_char="1350" id="token-9-15" morph="none" pos="word" start_char="1350">y</TOKEN>
<TOKEN end_char="1357" id="token-9-16" morph="none" pos="word" start_char="1352">fueron</TOKEN>
<TOKEN end_char="1369" id="token-9-17" morph="none" pos="word" start_char="1359">confirmados</TOKEN>
<TOKEN end_char="1378" id="token-9-18" morph="none" pos="word" start_char="1371">mediante</TOKEN>
<TOKEN end_char="1386" id="token-9-19" morph="none" pos="word" start_char="1380">pruebas</TOKEN>
<TOKEN end_char="1389" id="token-9-20" morph="none" pos="word" start_char="1388">de</TOKEN>
<TOKEN end_char="1401" id="token-9-21" morph="none" pos="word" start_char="1391">anticuerpos</TOKEN>
<TOKEN end_char="1416" id="token-9-22" morph="none" pos="word" start_char="1403">neutralizantes</TOKEN>
<TOKEN end_char="1417" id="token-9-23" morph="none" pos="punct" start_char="1417">,</TOKEN>
<TOKEN end_char="1424" id="token-9-24" morph="none" pos="word" start_char="1419">revela</TOKEN>
<TOKEN end_char="1427" id="token-9-25" morph="none" pos="word" start_char="1426">la</TOKEN>
<TOKEN end_char="1441" id="token-9-26" morph="none" pos="word" start_char="1429">investigación</TOKEN>
<TOKEN end_char="1450" id="token-9-27" morph="none" pos="word" start_char="1443">francesa</TOKEN>
<TOKEN end_char="1451" id="token-9-28" morph="none" pos="punct" start_char="1451">.</TOKEN>
<TRANSLATED_TEXT>Of these positives, 13 were sampled between November 2019 and January 2020 and were confirmed by testing neutralizing antibodies, French research reveals.</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>