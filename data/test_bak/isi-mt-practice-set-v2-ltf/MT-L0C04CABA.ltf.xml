<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CABA" lang="spa" raw_text_char_length="5753" raw_text_md5="4a5a9470930c0ab65f88519a25c475f5" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="120" id="segment-0" start_char="1">
<ORIGINAL_TEXT>COVID-19 May Have Been Circulating From August, 2019 – Study of Wuhan Hospital Traffic &amp;amp;amp; Online Keyword Searches</ORIGINAL_TEXT>
<TOKEN end_char="8" id="token-0-0" morph="none" pos="unknown" start_char="1">COVID-19</TOKEN>
<TOKEN end_char="12" id="token-0-1" morph="none" pos="word" start_char="10">May</TOKEN>
<TOKEN end_char="17" id="token-0-2" morph="none" pos="word" start_char="14">Have</TOKEN>
<TOKEN end_char="22" id="token-0-3" morph="none" pos="word" start_char="19">Been</TOKEN>
<TOKEN end_char="34" id="token-0-4" morph="none" pos="word" start_char="24">Circulating</TOKEN>
<TOKEN end_char="39" id="token-0-5" morph="none" pos="word" start_char="36">From</TOKEN>
<TOKEN end_char="46" id="token-0-6" morph="none" pos="word" start_char="41">August</TOKEN>
<TOKEN end_char="47" id="token-0-7" morph="none" pos="punct" start_char="47">,</TOKEN>
<TOKEN end_char="52" id="token-0-8" morph="none" pos="word" start_char="49">2019</TOKEN>
<TOKEN end_char="54" id="token-0-9" morph="none" pos="punct" start_char="54">–</TOKEN>
<TOKEN end_char="60" id="token-0-10" morph="none" pos="word" start_char="56">Study</TOKEN>
<TOKEN end_char="63" id="token-0-11" morph="none" pos="word" start_char="62">of</TOKEN>
<TOKEN end_char="69" id="token-0-12" morph="none" pos="word" start_char="65">Wuhan</TOKEN>
<TOKEN end_char="78" id="token-0-13" morph="none" pos="word" start_char="71">Hospital</TOKEN>
<TOKEN end_char="86" id="token-0-14" morph="none" pos="word" start_char="80">Traffic</TOKEN>
<TOKEN end_char="88" id="token-0-15" morph="none" pos="punct" start_char="88">&amp;</TOKEN>
<TOKEN end_char="95" id="token-0-16" morph="none" pos="unknown" start_char="89">amp;amp</TOKEN>
<TOKEN end_char="96" id="token-0-17" morph="none" pos="punct" start_char="96">;</TOKEN>
<TOKEN end_char="103" id="token-0-18" morph="none" pos="word" start_char="98">Online</TOKEN>
<TOKEN end_char="111" id="token-0-19" morph="none" pos="word" start_char="105">Keyword</TOKEN>
<TOKEN end_char="120" id="token-0-20" morph="none" pos="word" start_char="113">Searches</TOKEN>
</SEG>
<SEG end_char="171" id="segment-1" start_char="124">
<ORIGINAL_TEXT>Shoppers in Wuhan, China, post-COVID-19 lockdown</ORIGINAL_TEXT>
<TOKEN end_char="131" id="token-1-0" morph="none" pos="word" start_char="124">Shoppers</TOKEN>
<TOKEN end_char="134" id="token-1-1" morph="none" pos="word" start_char="133">in</TOKEN>
<TOKEN end_char="140" id="token-1-2" morph="none" pos="word" start_char="136">Wuhan</TOKEN>
<TOKEN end_char="141" id="token-1-3" morph="none" pos="punct" start_char="141">,</TOKEN>
<TOKEN end_char="147" id="token-1-4" morph="none" pos="word" start_char="143">China</TOKEN>
<TOKEN end_char="148" id="token-1-5" morph="none" pos="punct" start_char="148">,</TOKEN>
<TOKEN end_char="162" id="token-1-6" morph="none" pos="unknown" start_char="150">post-COVID-19</TOKEN>
<TOKEN end_char="171" id="token-1-7" morph="none" pos="word" start_char="164">lockdown</TOKEN>
</SEG>
<SEG end_char="478" id="segment-2" start_char="175">
<ORIGINAL_TEXT>A novel study published in preprint form by Harvard University suggests that cases of the novel SARS-COV-2 virus that has caused a global pandemic may have begun appearing in Wuhan hospitals as early as August 2019 – several months before China officially admitted that a new coronavirus was circulating.</ORIGINAL_TEXT>
<TOKEN end_char="175" id="token-2-0" morph="none" pos="word" start_char="175">A</TOKEN>
<TOKEN end_char="181" id="token-2-1" morph="none" pos="word" start_char="177">novel</TOKEN>
<TOKEN end_char="187" id="token-2-2" morph="none" pos="word" start_char="183">study</TOKEN>
<TOKEN end_char="197" id="token-2-3" morph="none" pos="word" start_char="189">published</TOKEN>
<TOKEN end_char="200" id="token-2-4" morph="none" pos="word" start_char="199">in</TOKEN>
<TOKEN end_char="209" id="token-2-5" morph="none" pos="word" start_char="202">preprint</TOKEN>
<TOKEN end_char="214" id="token-2-6" morph="none" pos="word" start_char="211">form</TOKEN>
<TOKEN end_char="217" id="token-2-7" morph="none" pos="word" start_char="216">by</TOKEN>
<TOKEN end_char="225" id="token-2-8" morph="none" pos="word" start_char="219">Harvard</TOKEN>
<TOKEN end_char="236" id="token-2-9" morph="none" pos="word" start_char="227">University</TOKEN>
<TOKEN end_char="245" id="token-2-10" morph="none" pos="word" start_char="238">suggests</TOKEN>
<TOKEN end_char="250" id="token-2-11" morph="none" pos="word" start_char="247">that</TOKEN>
<TOKEN end_char="256" id="token-2-12" morph="none" pos="word" start_char="252">cases</TOKEN>
<TOKEN end_char="259" id="token-2-13" morph="none" pos="word" start_char="258">of</TOKEN>
<TOKEN end_char="263" id="token-2-14" morph="none" pos="word" start_char="261">the</TOKEN>
<TOKEN end_char="269" id="token-2-15" morph="none" pos="word" start_char="265">novel</TOKEN>
<TOKEN end_char="280" id="token-2-16" morph="none" pos="unknown" start_char="271">SARS-COV-2</TOKEN>
<TOKEN end_char="286" id="token-2-17" morph="none" pos="word" start_char="282">virus</TOKEN>
<TOKEN end_char="291" id="token-2-18" morph="none" pos="word" start_char="288">that</TOKEN>
<TOKEN end_char="295" id="token-2-19" morph="none" pos="word" start_char="293">has</TOKEN>
<TOKEN end_char="302" id="token-2-20" morph="none" pos="word" start_char="297">caused</TOKEN>
<TOKEN end_char="304" id="token-2-21" morph="none" pos="word" start_char="304">a</TOKEN>
<TOKEN end_char="311" id="token-2-22" morph="none" pos="word" start_char="306">global</TOKEN>
<TOKEN end_char="320" id="token-2-23" morph="none" pos="word" start_char="313">pandemic</TOKEN>
<TOKEN end_char="324" id="token-2-24" morph="none" pos="word" start_char="322">may</TOKEN>
<TOKEN end_char="329" id="token-2-25" morph="none" pos="word" start_char="326">have</TOKEN>
<TOKEN end_char="335" id="token-2-26" morph="none" pos="word" start_char="331">begun</TOKEN>
<TOKEN end_char="345" id="token-2-27" morph="none" pos="word" start_char="337">appearing</TOKEN>
<TOKEN end_char="348" id="token-2-28" morph="none" pos="word" start_char="347">in</TOKEN>
<TOKEN end_char="354" id="token-2-29" morph="none" pos="word" start_char="350">Wuhan</TOKEN>
<TOKEN end_char="364" id="token-2-30" morph="none" pos="word" start_char="356">hospitals</TOKEN>
<TOKEN end_char="367" id="token-2-31" morph="none" pos="word" start_char="366">as</TOKEN>
<TOKEN end_char="373" id="token-2-32" morph="none" pos="word" start_char="369">early</TOKEN>
<TOKEN end_char="376" id="token-2-33" morph="none" pos="word" start_char="375">as</TOKEN>
<TOKEN end_char="383" id="token-2-34" morph="none" pos="word" start_char="378">August</TOKEN>
<TOKEN end_char="388" id="token-2-35" morph="none" pos="word" start_char="385">2019</TOKEN>
<TOKEN end_char="390" id="token-2-36" morph="none" pos="punct" start_char="390">–</TOKEN>
<TOKEN end_char="398" id="token-2-37" morph="none" pos="word" start_char="392">several</TOKEN>
<TOKEN end_char="405" id="token-2-38" morph="none" pos="word" start_char="400">months</TOKEN>
<TOKEN end_char="412" id="token-2-39" morph="none" pos="word" start_char="407">before</TOKEN>
<TOKEN end_char="418" id="token-2-40" morph="none" pos="word" start_char="414">China</TOKEN>
<TOKEN end_char="429" id="token-2-41" morph="none" pos="word" start_char="420">officially</TOKEN>
<TOKEN end_char="438" id="token-2-42" morph="none" pos="word" start_char="431">admitted</TOKEN>
<TOKEN end_char="443" id="token-2-43" morph="none" pos="word" start_char="440">that</TOKEN>
<TOKEN end_char="445" id="token-2-44" morph="none" pos="word" start_char="445">a</TOKEN>
<TOKEN end_char="449" id="token-2-45" morph="none" pos="word" start_char="447">new</TOKEN>
<TOKEN end_char="461" id="token-2-46" morph="none" pos="word" start_char="451">coronavirus</TOKEN>
<TOKEN end_char="465" id="token-2-47" morph="none" pos="word" start_char="463">was</TOKEN>
<TOKEN end_char="477" id="token-2-48" morph="none" pos="word" start_char="467">circulating</TOKEN>
<TOKEN end_char="478" id="token-2-49" morph="none" pos="punct" start_char="478">.</TOKEN>
</SEG>
<SEG end_char="816" id="segment-3" start_char="481">
<ORIGINAL_TEXT>The study Tuesday found an unusual uptick in traffic at several Wuhan hospitals in the late summer and early fall 2019, as compared to the year before, as well as a significant increase in online searches in China for diseases related to "cough" as well as "diarrhoea" – the latter symptom a distinctive feature of early COVID-19 onset.</ORIGINAL_TEXT>
<TOKEN end_char="483" id="token-3-0" morph="none" pos="word" start_char="481">The</TOKEN>
<TOKEN end_char="489" id="token-3-1" morph="none" pos="word" start_char="485">study</TOKEN>
<TOKEN end_char="497" id="token-3-2" morph="none" pos="word" start_char="491">Tuesday</TOKEN>
<TOKEN end_char="503" id="token-3-3" morph="none" pos="word" start_char="499">found</TOKEN>
<TOKEN end_char="506" id="token-3-4" morph="none" pos="word" start_char="505">an</TOKEN>
<TOKEN end_char="514" id="token-3-5" morph="none" pos="word" start_char="508">unusual</TOKEN>
<TOKEN end_char="521" id="token-3-6" morph="none" pos="word" start_char="516">uptick</TOKEN>
<TOKEN end_char="524" id="token-3-7" morph="none" pos="word" start_char="523">in</TOKEN>
<TOKEN end_char="532" id="token-3-8" morph="none" pos="word" start_char="526">traffic</TOKEN>
<TOKEN end_char="535" id="token-3-9" morph="none" pos="word" start_char="534">at</TOKEN>
<TOKEN end_char="543" id="token-3-10" morph="none" pos="word" start_char="537">several</TOKEN>
<TOKEN end_char="549" id="token-3-11" morph="none" pos="word" start_char="545">Wuhan</TOKEN>
<TOKEN end_char="559" id="token-3-12" morph="none" pos="word" start_char="551">hospitals</TOKEN>
<TOKEN end_char="562" id="token-3-13" morph="none" pos="word" start_char="561">in</TOKEN>
<TOKEN end_char="566" id="token-3-14" morph="none" pos="word" start_char="564">the</TOKEN>
<TOKEN end_char="571" id="token-3-15" morph="none" pos="word" start_char="568">late</TOKEN>
<TOKEN end_char="578" id="token-3-16" morph="none" pos="word" start_char="573">summer</TOKEN>
<TOKEN end_char="582" id="token-3-17" morph="none" pos="word" start_char="580">and</TOKEN>
<TOKEN end_char="588" id="token-3-18" morph="none" pos="word" start_char="584">early</TOKEN>
<TOKEN end_char="593" id="token-3-19" morph="none" pos="word" start_char="590">fall</TOKEN>
<TOKEN end_char="598" id="token-3-20" morph="none" pos="word" start_char="595">2019</TOKEN>
<TOKEN end_char="599" id="token-3-21" morph="none" pos="punct" start_char="599">,</TOKEN>
<TOKEN end_char="602" id="token-3-22" morph="none" pos="word" start_char="601">as</TOKEN>
<TOKEN end_char="611" id="token-3-23" morph="none" pos="word" start_char="604">compared</TOKEN>
<TOKEN end_char="614" id="token-3-24" morph="none" pos="word" start_char="613">to</TOKEN>
<TOKEN end_char="618" id="token-3-25" morph="none" pos="word" start_char="616">the</TOKEN>
<TOKEN end_char="623" id="token-3-26" morph="none" pos="word" start_char="620">year</TOKEN>
<TOKEN end_char="630" id="token-3-27" morph="none" pos="word" start_char="625">before</TOKEN>
<TOKEN end_char="631" id="token-3-28" morph="none" pos="punct" start_char="631">,</TOKEN>
<TOKEN end_char="634" id="token-3-29" morph="none" pos="word" start_char="633">as</TOKEN>
<TOKEN end_char="639" id="token-3-30" morph="none" pos="word" start_char="636">well</TOKEN>
<TOKEN end_char="642" id="token-3-31" morph="none" pos="word" start_char="641">as</TOKEN>
<TOKEN end_char="644" id="token-3-32" morph="none" pos="word" start_char="644">a</TOKEN>
<TOKEN end_char="656" id="token-3-33" morph="none" pos="word" start_char="646">significant</TOKEN>
<TOKEN end_char="665" id="token-3-34" morph="none" pos="word" start_char="658">increase</TOKEN>
<TOKEN end_char="668" id="token-3-35" morph="none" pos="word" start_char="667">in</TOKEN>
<TOKEN end_char="675" id="token-3-36" morph="none" pos="word" start_char="670">online</TOKEN>
<TOKEN end_char="684" id="token-3-37" morph="none" pos="word" start_char="677">searches</TOKEN>
<TOKEN end_char="687" id="token-3-38" morph="none" pos="word" start_char="686">in</TOKEN>
<TOKEN end_char="693" id="token-3-39" morph="none" pos="word" start_char="689">China</TOKEN>
<TOKEN end_char="697" id="token-3-40" morph="none" pos="word" start_char="695">for</TOKEN>
<TOKEN end_char="706" id="token-3-41" morph="none" pos="word" start_char="699">diseases</TOKEN>
<TOKEN end_char="714" id="token-3-42" morph="none" pos="word" start_char="708">related</TOKEN>
<TOKEN end_char="717" id="token-3-43" morph="none" pos="word" start_char="716">to</TOKEN>
<TOKEN end_char="719" id="token-3-44" morph="none" pos="punct" start_char="719">"</TOKEN>
<TOKEN end_char="724" id="token-3-45" morph="none" pos="word" start_char="720">cough</TOKEN>
<TOKEN end_char="725" id="token-3-46" morph="none" pos="punct" start_char="725">"</TOKEN>
<TOKEN end_char="728" id="token-3-47" morph="none" pos="word" start_char="727">as</TOKEN>
<TOKEN end_char="733" id="token-3-48" morph="none" pos="word" start_char="730">well</TOKEN>
<TOKEN end_char="736" id="token-3-49" morph="none" pos="word" start_char="735">as</TOKEN>
<TOKEN end_char="738" id="token-3-50" morph="none" pos="punct" start_char="738">"</TOKEN>
<TOKEN end_char="747" id="token-3-51" morph="none" pos="word" start_char="739">diarrhoea</TOKEN>
<TOKEN end_char="748" id="token-3-52" morph="none" pos="punct" start_char="748">"</TOKEN>
<TOKEN end_char="750" id="token-3-53" morph="none" pos="punct" start_char="750">–</TOKEN>
<TOKEN end_char="754" id="token-3-54" morph="none" pos="word" start_char="752">the</TOKEN>
<TOKEN end_char="761" id="token-3-55" morph="none" pos="word" start_char="756">latter</TOKEN>
<TOKEN end_char="769" id="token-3-56" morph="none" pos="word" start_char="763">symptom</TOKEN>
<TOKEN end_char="771" id="token-3-57" morph="none" pos="word" start_char="771">a</TOKEN>
<TOKEN end_char="783" id="token-3-58" morph="none" pos="word" start_char="773">distinctive</TOKEN>
<TOKEN end_char="791" id="token-3-59" morph="none" pos="word" start_char="785">feature</TOKEN>
<TOKEN end_char="794" id="token-3-60" morph="none" pos="word" start_char="793">of</TOKEN>
<TOKEN end_char="800" id="token-3-61" morph="none" pos="word" start_char="796">early</TOKEN>
<TOKEN end_char="809" id="token-3-62" morph="none" pos="unknown" start_char="802">COVID-19</TOKEN>
<TOKEN end_char="815" id="token-3-63" morph="none" pos="word" start_char="811">onset</TOKEN>
<TOKEN end_char="816" id="token-3-64" morph="none" pos="punct" start_char="816">.</TOKEN>
</SEG>
<SEG end_char="948" id="segment-4" start_char="819">
<ORIGINAL_TEXT>Analysis of hospital traffic in Wuhan China indicates shows high number of visitors, compared to the same time, in the year before</ORIGINAL_TEXT>
<TOKEN end_char="826" id="token-4-0" morph="none" pos="word" start_char="819">Analysis</TOKEN>
<TOKEN end_char="829" id="token-4-1" morph="none" pos="word" start_char="828">of</TOKEN>
<TOKEN end_char="838" id="token-4-2" morph="none" pos="word" start_char="831">hospital</TOKEN>
<TOKEN end_char="846" id="token-4-3" morph="none" pos="word" start_char="840">traffic</TOKEN>
<TOKEN end_char="849" id="token-4-4" morph="none" pos="word" start_char="848">in</TOKEN>
<TOKEN end_char="855" id="token-4-5" morph="none" pos="word" start_char="851">Wuhan</TOKEN>
<TOKEN end_char="861" id="token-4-6" morph="none" pos="word" start_char="857">China</TOKEN>
<TOKEN end_char="871" id="token-4-7" morph="none" pos="word" start_char="863">indicates</TOKEN>
<TOKEN end_char="877" id="token-4-8" morph="none" pos="word" start_char="873">shows</TOKEN>
<TOKEN end_char="882" id="token-4-9" morph="none" pos="word" start_char="879">high</TOKEN>
<TOKEN end_char="889" id="token-4-10" morph="none" pos="word" start_char="884">number</TOKEN>
<TOKEN end_char="892" id="token-4-11" morph="none" pos="word" start_char="891">of</TOKEN>
<TOKEN end_char="901" id="token-4-12" morph="none" pos="word" start_char="894">visitors</TOKEN>
<TOKEN end_char="902" id="token-4-13" morph="none" pos="punct" start_char="902">,</TOKEN>
<TOKEN end_char="911" id="token-4-14" morph="none" pos="word" start_char="904">compared</TOKEN>
<TOKEN end_char="914" id="token-4-15" morph="none" pos="word" start_char="913">to</TOKEN>
<TOKEN end_char="918" id="token-4-16" morph="none" pos="word" start_char="916">the</TOKEN>
<TOKEN end_char="923" id="token-4-17" morph="none" pos="word" start_char="920">same</TOKEN>
<TOKEN end_char="928" id="token-4-18" morph="none" pos="word" start_char="925">time</TOKEN>
<TOKEN end_char="929" id="token-4-19" morph="none" pos="punct" start_char="929">,</TOKEN>
<TOKEN end_char="932" id="token-4-20" morph="none" pos="word" start_char="931">in</TOKEN>
<TOKEN end_char="936" id="token-4-21" morph="none" pos="word" start_char="934">the</TOKEN>
<TOKEN end_char="941" id="token-4-22" morph="none" pos="word" start_char="938">year</TOKEN>
<TOKEN end_char="948" id="token-4-23" morph="none" pos="word" start_char="943">before</TOKEN>
</SEG>
<SEG end_char="1062" id="segment-5" start_char="952">
<ORIGINAL_TEXT>"We observe an upward trend in hospital traffic and search volume beginning in late Summer and early Fall 2019.</ORIGINAL_TEXT>
<TOKEN end_char="952" id="token-5-0" morph="none" pos="punct" start_char="952">"</TOKEN>
<TOKEN end_char="954" id="token-5-1" morph="none" pos="word" start_char="953">We</TOKEN>
<TOKEN end_char="962" id="token-5-2" morph="none" pos="word" start_char="956">observe</TOKEN>
<TOKEN end_char="965" id="token-5-3" morph="none" pos="word" start_char="964">an</TOKEN>
<TOKEN end_char="972" id="token-5-4" morph="none" pos="word" start_char="967">upward</TOKEN>
<TOKEN end_char="978" id="token-5-5" morph="none" pos="word" start_char="974">trend</TOKEN>
<TOKEN end_char="981" id="token-5-6" morph="none" pos="word" start_char="980">in</TOKEN>
<TOKEN end_char="990" id="token-5-7" morph="none" pos="word" start_char="983">hospital</TOKEN>
<TOKEN end_char="998" id="token-5-8" morph="none" pos="word" start_char="992">traffic</TOKEN>
<TOKEN end_char="1002" id="token-5-9" morph="none" pos="word" start_char="1000">and</TOKEN>
<TOKEN end_char="1009" id="token-5-10" morph="none" pos="word" start_char="1004">search</TOKEN>
<TOKEN end_char="1016" id="token-5-11" morph="none" pos="word" start_char="1011">volume</TOKEN>
<TOKEN end_char="1026" id="token-5-12" morph="none" pos="word" start_char="1018">beginning</TOKEN>
<TOKEN end_char="1029" id="token-5-13" morph="none" pos="word" start_char="1028">in</TOKEN>
<TOKEN end_char="1034" id="token-5-14" morph="none" pos="word" start_char="1031">late</TOKEN>
<TOKEN end_char="1041" id="token-5-15" morph="none" pos="word" start_char="1036">Summer</TOKEN>
<TOKEN end_char="1045" id="token-5-16" morph="none" pos="word" start_char="1043">and</TOKEN>
<TOKEN end_char="1051" id="token-5-17" morph="none" pos="word" start_char="1047">early</TOKEN>
<TOKEN end_char="1056" id="token-5-18" morph="none" pos="word" start_char="1053">Fall</TOKEN>
<TOKEN end_char="1061" id="token-5-19" morph="none" pos="word" start_char="1058">2019</TOKEN>
<TOKEN end_char="1062" id="token-5-20" morph="none" pos="punct" start_char="1062">.</TOKEN>
</SEG>
<SEG end_char="1284" id="segment-6" start_char="1064">
<ORIGINAL_TEXT>While queries of the respiratory symptom "cough" show seasonal fluctuations coinciding with yearly influenza seasons, "diarrhea" is a more COVID-19 specific symptom and only shows an association with the current epidemic.</ORIGINAL_TEXT>
<TOKEN end_char="1068" id="token-6-0" morph="none" pos="word" start_char="1064">While</TOKEN>
<TOKEN end_char="1076" id="token-6-1" morph="none" pos="word" start_char="1070">queries</TOKEN>
<TOKEN end_char="1079" id="token-6-2" morph="none" pos="word" start_char="1078">of</TOKEN>
<TOKEN end_char="1083" id="token-6-3" morph="none" pos="word" start_char="1081">the</TOKEN>
<TOKEN end_char="1095" id="token-6-4" morph="none" pos="word" start_char="1085">respiratory</TOKEN>
<TOKEN end_char="1103" id="token-6-5" morph="none" pos="word" start_char="1097">symptom</TOKEN>
<TOKEN end_char="1105" id="token-6-6" morph="none" pos="punct" start_char="1105">"</TOKEN>
<TOKEN end_char="1110" id="token-6-7" morph="none" pos="word" start_char="1106">cough</TOKEN>
<TOKEN end_char="1111" id="token-6-8" morph="none" pos="punct" start_char="1111">"</TOKEN>
<TOKEN end_char="1116" id="token-6-9" morph="none" pos="word" start_char="1113">show</TOKEN>
<TOKEN end_char="1125" id="token-6-10" morph="none" pos="word" start_char="1118">seasonal</TOKEN>
<TOKEN end_char="1138" id="token-6-11" morph="none" pos="word" start_char="1127">fluctuations</TOKEN>
<TOKEN end_char="1149" id="token-6-12" morph="none" pos="word" start_char="1140">coinciding</TOKEN>
<TOKEN end_char="1154" id="token-6-13" morph="none" pos="word" start_char="1151">with</TOKEN>
<TOKEN end_char="1161" id="token-6-14" morph="none" pos="word" start_char="1156">yearly</TOKEN>
<TOKEN end_char="1171" id="token-6-15" morph="none" pos="word" start_char="1163">influenza</TOKEN>
<TOKEN end_char="1179" id="token-6-16" morph="none" pos="word" start_char="1173">seasons</TOKEN>
<TOKEN end_char="1180" id="token-6-17" morph="none" pos="punct" start_char="1180">,</TOKEN>
<TOKEN end_char="1182" id="token-6-18" morph="none" pos="punct" start_char="1182">"</TOKEN>
<TOKEN end_char="1190" id="token-6-19" morph="none" pos="word" start_char="1183">diarrhea</TOKEN>
<TOKEN end_char="1191" id="token-6-20" morph="none" pos="punct" start_char="1191">"</TOKEN>
<TOKEN end_char="1194" id="token-6-21" morph="none" pos="word" start_char="1193">is</TOKEN>
<TOKEN end_char="1196" id="token-6-22" morph="none" pos="word" start_char="1196">a</TOKEN>
<TOKEN end_char="1201" id="token-6-23" morph="none" pos="word" start_char="1198">more</TOKEN>
<TOKEN end_char="1210" id="token-6-24" morph="none" pos="unknown" start_char="1203">COVID-19</TOKEN>
<TOKEN end_char="1219" id="token-6-25" morph="none" pos="word" start_char="1212">specific</TOKEN>
<TOKEN end_char="1227" id="token-6-26" morph="none" pos="word" start_char="1221">symptom</TOKEN>
<TOKEN end_char="1231" id="token-6-27" morph="none" pos="word" start_char="1229">and</TOKEN>
<TOKEN end_char="1236" id="token-6-28" morph="none" pos="word" start_char="1233">only</TOKEN>
<TOKEN end_char="1242" id="token-6-29" morph="none" pos="word" start_char="1238">shows</TOKEN>
<TOKEN end_char="1245" id="token-6-30" morph="none" pos="word" start_char="1244">an</TOKEN>
<TOKEN end_char="1257" id="token-6-31" morph="none" pos="word" start_char="1247">association</TOKEN>
<TOKEN end_char="1262" id="token-6-32" morph="none" pos="word" start_char="1259">with</TOKEN>
<TOKEN end_char="1266" id="token-6-33" morph="none" pos="word" start_char="1264">the</TOKEN>
<TOKEN end_char="1274" id="token-6-34" morph="none" pos="word" start_char="1268">current</TOKEN>
<TOKEN end_char="1283" id="token-6-35" morph="none" pos="word" start_char="1276">epidemic</TOKEN>
<TOKEN end_char="1284" id="token-6-36" morph="none" pos="punct" start_char="1284">.</TOKEN>
</SEG>
<SEG end_char="1722" id="segment-7" start_char="1287">
<ORIGINAL_TEXT>"The increase of both signals precede the documented start of the COVID-19 pandemic in December, highlighting the value of novel digital sources for surveillance of emerging pathogen," says the preprint study, published on Harvard’s DASH repository, and authored by four experts in computational epidemiology and biomedical informatics at Harvard Medical School, Boston University School of Public Health and Boston Children’s Hospital.</ORIGINAL_TEXT>
<TOKEN end_char="1287" id="token-7-0" morph="none" pos="punct" start_char="1287">"</TOKEN>
<TOKEN end_char="1290" id="token-7-1" morph="none" pos="word" start_char="1288">The</TOKEN>
<TOKEN end_char="1299" id="token-7-2" morph="none" pos="word" start_char="1292">increase</TOKEN>
<TOKEN end_char="1302" id="token-7-3" morph="none" pos="word" start_char="1301">of</TOKEN>
<TOKEN end_char="1307" id="token-7-4" morph="none" pos="word" start_char="1304">both</TOKEN>
<TOKEN end_char="1315" id="token-7-5" morph="none" pos="word" start_char="1309">signals</TOKEN>
<TOKEN end_char="1323" id="token-7-6" morph="none" pos="word" start_char="1317">precede</TOKEN>
<TOKEN end_char="1327" id="token-7-7" morph="none" pos="word" start_char="1325">the</TOKEN>
<TOKEN end_char="1338" id="token-7-8" morph="none" pos="word" start_char="1329">documented</TOKEN>
<TOKEN end_char="1344" id="token-7-9" morph="none" pos="word" start_char="1340">start</TOKEN>
<TOKEN end_char="1347" id="token-7-10" morph="none" pos="word" start_char="1346">of</TOKEN>
<TOKEN end_char="1351" id="token-7-11" morph="none" pos="word" start_char="1349">the</TOKEN>
<TOKEN end_char="1360" id="token-7-12" morph="none" pos="unknown" start_char="1353">COVID-19</TOKEN>
<TOKEN end_char="1369" id="token-7-13" morph="none" pos="word" start_char="1362">pandemic</TOKEN>
<TOKEN end_char="1372" id="token-7-14" morph="none" pos="word" start_char="1371">in</TOKEN>
<TOKEN end_char="1381" id="token-7-15" morph="none" pos="word" start_char="1374">December</TOKEN>
<TOKEN end_char="1382" id="token-7-16" morph="none" pos="punct" start_char="1382">,</TOKEN>
<TOKEN end_char="1395" id="token-7-17" morph="none" pos="word" start_char="1384">highlighting</TOKEN>
<TOKEN end_char="1399" id="token-7-18" morph="none" pos="word" start_char="1397">the</TOKEN>
<TOKEN end_char="1405" id="token-7-19" morph="none" pos="word" start_char="1401">value</TOKEN>
<TOKEN end_char="1408" id="token-7-20" morph="none" pos="word" start_char="1407">of</TOKEN>
<TOKEN end_char="1414" id="token-7-21" morph="none" pos="word" start_char="1410">novel</TOKEN>
<TOKEN end_char="1422" id="token-7-22" morph="none" pos="word" start_char="1416">digital</TOKEN>
<TOKEN end_char="1430" id="token-7-23" morph="none" pos="word" start_char="1424">sources</TOKEN>
<TOKEN end_char="1434" id="token-7-24" morph="none" pos="word" start_char="1432">for</TOKEN>
<TOKEN end_char="1447" id="token-7-25" morph="none" pos="word" start_char="1436">surveillance</TOKEN>
<TOKEN end_char="1450" id="token-7-26" morph="none" pos="word" start_char="1449">of</TOKEN>
<TOKEN end_char="1459" id="token-7-27" morph="none" pos="word" start_char="1452">emerging</TOKEN>
<TOKEN end_char="1468" id="token-7-28" morph="none" pos="word" start_char="1461">pathogen</TOKEN>
<TOKEN end_char="1470" id="token-7-29" morph="none" pos="punct" start_char="1469">,"</TOKEN>
<TOKEN end_char="1475" id="token-7-30" morph="none" pos="word" start_char="1472">says</TOKEN>
<TOKEN end_char="1479" id="token-7-31" morph="none" pos="word" start_char="1477">the</TOKEN>
<TOKEN end_char="1488" id="token-7-32" morph="none" pos="word" start_char="1481">preprint</TOKEN>
<TOKEN end_char="1494" id="token-7-33" morph="none" pos="word" start_char="1490">study</TOKEN>
<TOKEN end_char="1495" id="token-7-34" morph="none" pos="punct" start_char="1495">,</TOKEN>
<TOKEN end_char="1505" id="token-7-35" morph="none" pos="word" start_char="1497">published</TOKEN>
<TOKEN end_char="1508" id="token-7-36" morph="none" pos="word" start_char="1507">on</TOKEN>
<TOKEN end_char="1518" id="token-7-37" morph="none" pos="word" start_char="1510">Harvard’s</TOKEN>
<TOKEN end_char="1523" id="token-7-38" morph="none" pos="word" start_char="1520">DASH</TOKEN>
<TOKEN end_char="1534" id="token-7-39" morph="none" pos="word" start_char="1525">repository</TOKEN>
<TOKEN end_char="1535" id="token-7-40" morph="none" pos="punct" start_char="1535">,</TOKEN>
<TOKEN end_char="1539" id="token-7-41" morph="none" pos="word" start_char="1537">and</TOKEN>
<TOKEN end_char="1548" id="token-7-42" morph="none" pos="word" start_char="1541">authored</TOKEN>
<TOKEN end_char="1551" id="token-7-43" morph="none" pos="word" start_char="1550">by</TOKEN>
<TOKEN end_char="1556" id="token-7-44" morph="none" pos="word" start_char="1553">four</TOKEN>
<TOKEN end_char="1564" id="token-7-45" morph="none" pos="word" start_char="1558">experts</TOKEN>
<TOKEN end_char="1567" id="token-7-46" morph="none" pos="word" start_char="1566">in</TOKEN>
<TOKEN end_char="1581" id="token-7-47" morph="none" pos="word" start_char="1569">computational</TOKEN>
<TOKEN end_char="1594" id="token-7-48" morph="none" pos="word" start_char="1583">epidemiology</TOKEN>
<TOKEN end_char="1598" id="token-7-49" morph="none" pos="word" start_char="1596">and</TOKEN>
<TOKEN end_char="1609" id="token-7-50" morph="none" pos="word" start_char="1600">biomedical</TOKEN>
<TOKEN end_char="1621" id="token-7-51" morph="none" pos="word" start_char="1611">informatics</TOKEN>
<TOKEN end_char="1624" id="token-7-52" morph="none" pos="word" start_char="1623">at</TOKEN>
<TOKEN end_char="1632" id="token-7-53" morph="none" pos="word" start_char="1626">Harvard</TOKEN>
<TOKEN end_char="1640" id="token-7-54" morph="none" pos="word" start_char="1634">Medical</TOKEN>
<TOKEN end_char="1647" id="token-7-55" morph="none" pos="word" start_char="1642">School</TOKEN>
<TOKEN end_char="1648" id="token-7-56" morph="none" pos="punct" start_char="1648">,</TOKEN>
<TOKEN end_char="1655" id="token-7-57" morph="none" pos="word" start_char="1650">Boston</TOKEN>
<TOKEN end_char="1666" id="token-7-58" morph="none" pos="word" start_char="1657">University</TOKEN>
<TOKEN end_char="1673" id="token-7-59" morph="none" pos="word" start_char="1668">School</TOKEN>
<TOKEN end_char="1676" id="token-7-60" morph="none" pos="word" start_char="1675">of</TOKEN>
<TOKEN end_char="1683" id="token-7-61" morph="none" pos="word" start_char="1678">Public</TOKEN>
<TOKEN end_char="1690" id="token-7-62" morph="none" pos="word" start_char="1685">Health</TOKEN>
<TOKEN end_char="1694" id="token-7-63" morph="none" pos="word" start_char="1692">and</TOKEN>
<TOKEN end_char="1701" id="token-7-64" morph="none" pos="word" start_char="1696">Boston</TOKEN>
<TOKEN end_char="1712" id="token-7-65" morph="none" pos="word" start_char="1703">Children’s</TOKEN>
<TOKEN end_char="1721" id="token-7-66" morph="none" pos="word" start_char="1714">Hospital</TOKEN>
<TOKEN end_char="1722" id="token-7-67" morph="none" pos="punct" start_char="1722">.</TOKEN>
</SEG>
<SEG end_char="1808" id="segment-8" start_char="1725">
<ORIGINAL_TEXT>Study Strengthens Theory That Virus Emerged From Source Outside Wuhan Seafood Market</ORIGINAL_TEXT>
<TOKEN end_char="1729" id="token-8-0" morph="none" pos="word" start_char="1725">Study</TOKEN>
<TOKEN end_char="1741" id="token-8-1" morph="none" pos="word" start_char="1731">Strengthens</TOKEN>
<TOKEN end_char="1748" id="token-8-2" morph="none" pos="word" start_char="1743">Theory</TOKEN>
<TOKEN end_char="1753" id="token-8-3" morph="none" pos="word" start_char="1750">That</TOKEN>
<TOKEN end_char="1759" id="token-8-4" morph="none" pos="word" start_char="1755">Virus</TOKEN>
<TOKEN end_char="1767" id="token-8-5" morph="none" pos="word" start_char="1761">Emerged</TOKEN>
<TOKEN end_char="1772" id="token-8-6" morph="none" pos="word" start_char="1769">From</TOKEN>
<TOKEN end_char="1779" id="token-8-7" morph="none" pos="word" start_char="1774">Source</TOKEN>
<TOKEN end_char="1787" id="token-8-8" morph="none" pos="word" start_char="1781">Outside</TOKEN>
<TOKEN end_char="1793" id="token-8-9" morph="none" pos="word" start_char="1789">Wuhan</TOKEN>
<TOKEN end_char="1801" id="token-8-10" morph="none" pos="word" start_char="1795">Seafood</TOKEN>
<TOKEN end_char="1808" id="token-8-11" morph="none" pos="word" start_char="1803">Market</TOKEN>
</SEG>
<SEG end_char="2077" id="segment-9" start_char="1812">
<ORIGINAL_TEXT>The authors said that their findings strengthen the theory that the novel coronavirus may have already been circulating in Wuhan, a city of 10 million people, prior to the identification of a large cluster of infected people associated with the Hunan Seafood Market.</ORIGINAL_TEXT>
<TOKEN end_char="1814" id="token-9-0" morph="none" pos="word" start_char="1812">The</TOKEN>
<TOKEN end_char="1822" id="token-9-1" morph="none" pos="word" start_char="1816">authors</TOKEN>
<TOKEN end_char="1827" id="token-9-2" morph="none" pos="word" start_char="1824">said</TOKEN>
<TOKEN end_char="1832" id="token-9-3" morph="none" pos="word" start_char="1829">that</TOKEN>
<TOKEN end_char="1838" id="token-9-4" morph="none" pos="word" start_char="1834">their</TOKEN>
<TOKEN end_char="1847" id="token-9-5" morph="none" pos="word" start_char="1840">findings</TOKEN>
<TOKEN end_char="1858" id="token-9-6" morph="none" pos="word" start_char="1849">strengthen</TOKEN>
<TOKEN end_char="1862" id="token-9-7" morph="none" pos="word" start_char="1860">the</TOKEN>
<TOKEN end_char="1869" id="token-9-8" morph="none" pos="word" start_char="1864">theory</TOKEN>
<TOKEN end_char="1874" id="token-9-9" morph="none" pos="word" start_char="1871">that</TOKEN>
<TOKEN end_char="1878" id="token-9-10" morph="none" pos="word" start_char="1876">the</TOKEN>
<TOKEN end_char="1884" id="token-9-11" morph="none" pos="word" start_char="1880">novel</TOKEN>
<TOKEN end_char="1896" id="token-9-12" morph="none" pos="word" start_char="1886">coronavirus</TOKEN>
<TOKEN end_char="1900" id="token-9-13" morph="none" pos="word" start_char="1898">may</TOKEN>
<TOKEN end_char="1905" id="token-9-14" morph="none" pos="word" start_char="1902">have</TOKEN>
<TOKEN end_char="1913" id="token-9-15" morph="none" pos="word" start_char="1907">already</TOKEN>
<TOKEN end_char="1918" id="token-9-16" morph="none" pos="word" start_char="1915">been</TOKEN>
<TOKEN end_char="1930" id="token-9-17" morph="none" pos="word" start_char="1920">circulating</TOKEN>
<TOKEN end_char="1933" id="token-9-18" morph="none" pos="word" start_char="1932">in</TOKEN>
<TOKEN end_char="1939" id="token-9-19" morph="none" pos="word" start_char="1935">Wuhan</TOKEN>
<TOKEN end_char="1940" id="token-9-20" morph="none" pos="punct" start_char="1940">,</TOKEN>
<TOKEN end_char="1942" id="token-9-21" morph="none" pos="word" start_char="1942">a</TOKEN>
<TOKEN end_char="1947" id="token-9-22" morph="none" pos="word" start_char="1944">city</TOKEN>
<TOKEN end_char="1950" id="token-9-23" morph="none" pos="word" start_char="1949">of</TOKEN>
<TOKEN end_char="1953" id="token-9-24" morph="none" pos="word" start_char="1952">10</TOKEN>
<TOKEN end_char="1961" id="token-9-25" morph="none" pos="word" start_char="1955">million</TOKEN>
<TOKEN end_char="1968" id="token-9-26" morph="none" pos="word" start_char="1963">people</TOKEN>
<TOKEN end_char="1969" id="token-9-27" morph="none" pos="punct" start_char="1969">,</TOKEN>
<TOKEN end_char="1975" id="token-9-28" morph="none" pos="word" start_char="1971">prior</TOKEN>
<TOKEN end_char="1978" id="token-9-29" morph="none" pos="word" start_char="1977">to</TOKEN>
<TOKEN end_char="1982" id="token-9-30" morph="none" pos="word" start_char="1980">the</TOKEN>
<TOKEN end_char="1997" id="token-9-31" morph="none" pos="word" start_char="1984">identification</TOKEN>
<TOKEN end_char="2000" id="token-9-32" morph="none" pos="word" start_char="1999">of</TOKEN>
<TOKEN end_char="2002" id="token-9-33" morph="none" pos="word" start_char="2002">a</TOKEN>
<TOKEN end_char="2008" id="token-9-34" morph="none" pos="word" start_char="2004">large</TOKEN>
<TOKEN end_char="2016" id="token-9-35" morph="none" pos="word" start_char="2010">cluster</TOKEN>
<TOKEN end_char="2019" id="token-9-36" morph="none" pos="word" start_char="2018">of</TOKEN>
<TOKEN end_char="2028" id="token-9-37" morph="none" pos="word" start_char="2021">infected</TOKEN>
<TOKEN end_char="2035" id="token-9-38" morph="none" pos="word" start_char="2030">people</TOKEN>
<TOKEN end_char="2046" id="token-9-39" morph="none" pos="word" start_char="2037">associated</TOKEN>
<TOKEN end_char="2051" id="token-9-40" morph="none" pos="word" start_char="2048">with</TOKEN>
<TOKEN end_char="2055" id="token-9-41" morph="none" pos="word" start_char="2053">the</TOKEN>
<TOKEN end_char="2061" id="token-9-42" morph="none" pos="word" start_char="2057">Hunan</TOKEN>
<TOKEN end_char="2069" id="token-9-43" morph="none" pos="word" start_char="2063">Seafood</TOKEN>
<TOKEN end_char="2076" id="token-9-44" morph="none" pos="word" start_char="2071">Market</TOKEN>
<TOKEN end_char="2077" id="token-9-45" morph="none" pos="punct" start_char="2077">.</TOKEN>
</SEG>
<SEG end_char="2424" id="segment-10" start_char="2080">
<ORIGINAL_TEXT>While it has been suggested that the original source of the virus was a wild animal in the market, where mammals, reptiles and other animals were kept in crowded, contained spaces in close proximity to market workers and shoppers, no direct connection to the market has been found for the first 14 individuals who became infected with the virus.</ORIGINAL_TEXT>
<TOKEN end_char="2084" id="token-10-0" morph="none" pos="word" start_char="2080">While</TOKEN>
<TOKEN end_char="2087" id="token-10-1" morph="none" pos="word" start_char="2086">it</TOKEN>
<TOKEN end_char="2091" id="token-10-2" morph="none" pos="word" start_char="2089">has</TOKEN>
<TOKEN end_char="2096" id="token-10-3" morph="none" pos="word" start_char="2093">been</TOKEN>
<TOKEN end_char="2106" id="token-10-4" morph="none" pos="word" start_char="2098">suggested</TOKEN>
<TOKEN end_char="2111" id="token-10-5" morph="none" pos="word" start_char="2108">that</TOKEN>
<TOKEN end_char="2115" id="token-10-6" morph="none" pos="word" start_char="2113">the</TOKEN>
<TOKEN end_char="2124" id="token-10-7" morph="none" pos="word" start_char="2117">original</TOKEN>
<TOKEN end_char="2131" id="token-10-8" morph="none" pos="word" start_char="2126">source</TOKEN>
<TOKEN end_char="2134" id="token-10-9" morph="none" pos="word" start_char="2133">of</TOKEN>
<TOKEN end_char="2138" id="token-10-10" morph="none" pos="word" start_char="2136">the</TOKEN>
<TOKEN end_char="2144" id="token-10-11" morph="none" pos="word" start_char="2140">virus</TOKEN>
<TOKEN end_char="2148" id="token-10-12" morph="none" pos="word" start_char="2146">was</TOKEN>
<TOKEN end_char="2150" id="token-10-13" morph="none" pos="word" start_char="2150">a</TOKEN>
<TOKEN end_char="2155" id="token-10-14" morph="none" pos="word" start_char="2152">wild</TOKEN>
<TOKEN end_char="2162" id="token-10-15" morph="none" pos="word" start_char="2157">animal</TOKEN>
<TOKEN end_char="2165" id="token-10-16" morph="none" pos="word" start_char="2164">in</TOKEN>
<TOKEN end_char="2169" id="token-10-17" morph="none" pos="word" start_char="2167">the</TOKEN>
<TOKEN end_char="2176" id="token-10-18" morph="none" pos="word" start_char="2171">market</TOKEN>
<TOKEN end_char="2177" id="token-10-19" morph="none" pos="punct" start_char="2177">,</TOKEN>
<TOKEN end_char="2183" id="token-10-20" morph="none" pos="word" start_char="2179">where</TOKEN>
<TOKEN end_char="2191" id="token-10-21" morph="none" pos="word" start_char="2185">mammals</TOKEN>
<TOKEN end_char="2192" id="token-10-22" morph="none" pos="punct" start_char="2192">,</TOKEN>
<TOKEN end_char="2201" id="token-10-23" morph="none" pos="word" start_char="2194">reptiles</TOKEN>
<TOKEN end_char="2205" id="token-10-24" morph="none" pos="word" start_char="2203">and</TOKEN>
<TOKEN end_char="2211" id="token-10-25" morph="none" pos="word" start_char="2207">other</TOKEN>
<TOKEN end_char="2219" id="token-10-26" morph="none" pos="word" start_char="2213">animals</TOKEN>
<TOKEN end_char="2224" id="token-10-27" morph="none" pos="word" start_char="2221">were</TOKEN>
<TOKEN end_char="2229" id="token-10-28" morph="none" pos="word" start_char="2226">kept</TOKEN>
<TOKEN end_char="2232" id="token-10-29" morph="none" pos="word" start_char="2231">in</TOKEN>
<TOKEN end_char="2240" id="token-10-30" morph="none" pos="word" start_char="2234">crowded</TOKEN>
<TOKEN end_char="2241" id="token-10-31" morph="none" pos="punct" start_char="2241">,</TOKEN>
<TOKEN end_char="2251" id="token-10-32" morph="none" pos="word" start_char="2243">contained</TOKEN>
<TOKEN end_char="2258" id="token-10-33" morph="none" pos="word" start_char="2253">spaces</TOKEN>
<TOKEN end_char="2261" id="token-10-34" morph="none" pos="word" start_char="2260">in</TOKEN>
<TOKEN end_char="2267" id="token-10-35" morph="none" pos="word" start_char="2263">close</TOKEN>
<TOKEN end_char="2277" id="token-10-36" morph="none" pos="word" start_char="2269">proximity</TOKEN>
<TOKEN end_char="2280" id="token-10-37" morph="none" pos="word" start_char="2279">to</TOKEN>
<TOKEN end_char="2287" id="token-10-38" morph="none" pos="word" start_char="2282">market</TOKEN>
<TOKEN end_char="2295" id="token-10-39" morph="none" pos="word" start_char="2289">workers</TOKEN>
<TOKEN end_char="2299" id="token-10-40" morph="none" pos="word" start_char="2297">and</TOKEN>
<TOKEN end_char="2308" id="token-10-41" morph="none" pos="word" start_char="2301">shoppers</TOKEN>
<TOKEN end_char="2309" id="token-10-42" morph="none" pos="punct" start_char="2309">,</TOKEN>
<TOKEN end_char="2312" id="token-10-43" morph="none" pos="word" start_char="2311">no</TOKEN>
<TOKEN end_char="2319" id="token-10-44" morph="none" pos="word" start_char="2314">direct</TOKEN>
<TOKEN end_char="2330" id="token-10-45" morph="none" pos="word" start_char="2321">connection</TOKEN>
<TOKEN end_char="2333" id="token-10-46" morph="none" pos="word" start_char="2332">to</TOKEN>
<TOKEN end_char="2337" id="token-10-47" morph="none" pos="word" start_char="2335">the</TOKEN>
<TOKEN end_char="2344" id="token-10-48" morph="none" pos="word" start_char="2339">market</TOKEN>
<TOKEN end_char="2348" id="token-10-49" morph="none" pos="word" start_char="2346">has</TOKEN>
<TOKEN end_char="2353" id="token-10-50" morph="none" pos="word" start_char="2350">been</TOKEN>
<TOKEN end_char="2359" id="token-10-51" morph="none" pos="word" start_char="2355">found</TOKEN>
<TOKEN end_char="2363" id="token-10-52" morph="none" pos="word" start_char="2361">for</TOKEN>
<TOKEN end_char="2367" id="token-10-53" morph="none" pos="word" start_char="2365">the</TOKEN>
<TOKEN end_char="2373" id="token-10-54" morph="none" pos="word" start_char="2369">first</TOKEN>
<TOKEN end_char="2376" id="token-10-55" morph="none" pos="word" start_char="2375">14</TOKEN>
<TOKEN end_char="2388" id="token-10-56" morph="none" pos="word" start_char="2378">individuals</TOKEN>
<TOKEN end_char="2392" id="token-10-57" morph="none" pos="word" start_char="2390">who</TOKEN>
<TOKEN end_char="2399" id="token-10-58" morph="none" pos="word" start_char="2394">became</TOKEN>
<TOKEN end_char="2408" id="token-10-59" morph="none" pos="word" start_char="2401">infected</TOKEN>
<TOKEN end_char="2413" id="token-10-60" morph="none" pos="word" start_char="2410">with</TOKEN>
<TOKEN end_char="2417" id="token-10-61" morph="none" pos="word" start_char="2415">the</TOKEN>
<TOKEN end_char="2423" id="token-10-62" morph="none" pos="word" start_char="2419">virus</TOKEN>
<TOKEN end_char="2424" id="token-10-63" morph="none" pos="punct" start_char="2424">.</TOKEN>
</SEG>
<SEG end_char="2603" id="segment-11" start_char="2426">
<ORIGINAL_TEXT>Nor have virology samples taken from wildlife at the market been linked to SARS-COV2, the study notes, "leaving open the possibility of alternate points of origin and infection."</ORIGINAL_TEXT>
<TOKEN end_char="2428" id="token-11-0" morph="none" pos="word" start_char="2426">Nor</TOKEN>
<TOKEN end_char="2433" id="token-11-1" morph="none" pos="word" start_char="2430">have</TOKEN>
<TOKEN end_char="2442" id="token-11-2" morph="none" pos="word" start_char="2435">virology</TOKEN>
<TOKEN end_char="2450" id="token-11-3" morph="none" pos="word" start_char="2444">samples</TOKEN>
<TOKEN end_char="2456" id="token-11-4" morph="none" pos="word" start_char="2452">taken</TOKEN>
<TOKEN end_char="2461" id="token-11-5" morph="none" pos="word" start_char="2458">from</TOKEN>
<TOKEN end_char="2470" id="token-11-6" morph="none" pos="word" start_char="2463">wildlife</TOKEN>
<TOKEN end_char="2473" id="token-11-7" morph="none" pos="word" start_char="2472">at</TOKEN>
<TOKEN end_char="2477" id="token-11-8" morph="none" pos="word" start_char="2475">the</TOKEN>
<TOKEN end_char="2484" id="token-11-9" morph="none" pos="word" start_char="2479">market</TOKEN>
<TOKEN end_char="2489" id="token-11-10" morph="none" pos="word" start_char="2486">been</TOKEN>
<TOKEN end_char="2496" id="token-11-11" morph="none" pos="word" start_char="2491">linked</TOKEN>
<TOKEN end_char="2499" id="token-11-12" morph="none" pos="word" start_char="2498">to</TOKEN>
<TOKEN end_char="2509" id="token-11-13" morph="none" pos="unknown" start_char="2501">SARS-COV2</TOKEN>
<TOKEN end_char="2510" id="token-11-14" morph="none" pos="punct" start_char="2510">,</TOKEN>
<TOKEN end_char="2514" id="token-11-15" morph="none" pos="word" start_char="2512">the</TOKEN>
<TOKEN end_char="2520" id="token-11-16" morph="none" pos="word" start_char="2516">study</TOKEN>
<TOKEN end_char="2526" id="token-11-17" morph="none" pos="word" start_char="2522">notes</TOKEN>
<TOKEN end_char="2527" id="token-11-18" morph="none" pos="punct" start_char="2527">,</TOKEN>
<TOKEN end_char="2529" id="token-11-19" morph="none" pos="punct" start_char="2529">"</TOKEN>
<TOKEN end_char="2536" id="token-11-20" morph="none" pos="word" start_char="2530">leaving</TOKEN>
<TOKEN end_char="2541" id="token-11-21" morph="none" pos="word" start_char="2538">open</TOKEN>
<TOKEN end_char="2545" id="token-11-22" morph="none" pos="word" start_char="2543">the</TOKEN>
<TOKEN end_char="2557" id="token-11-23" morph="none" pos="word" start_char="2547">possibility</TOKEN>
<TOKEN end_char="2560" id="token-11-24" morph="none" pos="word" start_char="2559">of</TOKEN>
<TOKEN end_char="2570" id="token-11-25" morph="none" pos="word" start_char="2562">alternate</TOKEN>
<TOKEN end_char="2577" id="token-11-26" morph="none" pos="word" start_char="2572">points</TOKEN>
<TOKEN end_char="2580" id="token-11-27" morph="none" pos="word" start_char="2579">of</TOKEN>
<TOKEN end_char="2587" id="token-11-28" morph="none" pos="word" start_char="2582">origin</TOKEN>
<TOKEN end_char="2591" id="token-11-29" morph="none" pos="word" start_char="2589">and</TOKEN>
<TOKEN end_char="2601" id="token-11-30" morph="none" pos="word" start_char="2593">infection</TOKEN>
<TOKEN end_char="2603" id="token-11-31" morph="none" pos="punct" start_char="2602">."</TOKEN>
</SEG>
<SEG end_char="2747" id="segment-12" start_char="2606">
<ORIGINAL_TEXT>"Here we consider that SARS-CoV-2 may have already been circulating in the community prior to the identification of the Huanan Market cluster.</ORIGINAL_TEXT>
<TOKEN end_char="2606" id="token-12-0" morph="none" pos="punct" start_char="2606">"</TOKEN>
<TOKEN end_char="2610" id="token-12-1" morph="none" pos="word" start_char="2607">Here</TOKEN>
<TOKEN end_char="2613" id="token-12-2" morph="none" pos="word" start_char="2612">we</TOKEN>
<TOKEN end_char="2622" id="token-12-3" morph="none" pos="word" start_char="2615">consider</TOKEN>
<TOKEN end_char="2627" id="token-12-4" morph="none" pos="word" start_char="2624">that</TOKEN>
<TOKEN end_char="2638" id="token-12-5" morph="none" pos="unknown" start_char="2629">SARS-CoV-2</TOKEN>
<TOKEN end_char="2642" id="token-12-6" morph="none" pos="word" start_char="2640">may</TOKEN>
<TOKEN end_char="2647" id="token-12-7" morph="none" pos="word" start_char="2644">have</TOKEN>
<TOKEN end_char="2655" id="token-12-8" morph="none" pos="word" start_char="2649">already</TOKEN>
<TOKEN end_char="2660" id="token-12-9" morph="none" pos="word" start_char="2657">been</TOKEN>
<TOKEN end_char="2672" id="token-12-10" morph="none" pos="word" start_char="2662">circulating</TOKEN>
<TOKEN end_char="2675" id="token-12-11" morph="none" pos="word" start_char="2674">in</TOKEN>
<TOKEN end_char="2679" id="token-12-12" morph="none" pos="word" start_char="2677">the</TOKEN>
<TOKEN end_char="2689" id="token-12-13" morph="none" pos="word" start_char="2681">community</TOKEN>
<TOKEN end_char="2695" id="token-12-14" morph="none" pos="word" start_char="2691">prior</TOKEN>
<TOKEN end_char="2698" id="token-12-15" morph="none" pos="word" start_char="2697">to</TOKEN>
<TOKEN end_char="2702" id="token-12-16" morph="none" pos="word" start_char="2700">the</TOKEN>
<TOKEN end_char="2717" id="token-12-17" morph="none" pos="word" start_char="2704">identification</TOKEN>
<TOKEN end_char="2720" id="token-12-18" morph="none" pos="word" start_char="2719">of</TOKEN>
<TOKEN end_char="2724" id="token-12-19" morph="none" pos="word" start_char="2722">the</TOKEN>
<TOKEN end_char="2731" id="token-12-20" morph="none" pos="word" start_char="2726">Huanan</TOKEN>
<TOKEN end_char="2738" id="token-12-21" morph="none" pos="word" start_char="2733">Market</TOKEN>
<TOKEN end_char="2746" id="token-12-22" morph="none" pos="word" start_char="2740">cluster</TOKEN>
<TOKEN end_char="2747" id="token-12-23" morph="none" pos="punct" start_char="2747">.</TOKEN>
</SEG>
<SEG end_char="3018" id="segment-13" start_char="2749">
<ORIGINAL_TEXT>This hypothesis is supported by emerging epidemiologic and phylogenetic evidence indicating that the virus emerged in southern China, and may have already spread internationally, and adapted for efficient human transmission by the time it was detected in late December."</ORIGINAL_TEXT>
<TOKEN end_char="2752" id="token-13-0" morph="none" pos="word" start_char="2749">This</TOKEN>
<TOKEN end_char="2763" id="token-13-1" morph="none" pos="word" start_char="2754">hypothesis</TOKEN>
<TOKEN end_char="2766" id="token-13-2" morph="none" pos="word" start_char="2765">is</TOKEN>
<TOKEN end_char="2776" id="token-13-3" morph="none" pos="word" start_char="2768">supported</TOKEN>
<TOKEN end_char="2779" id="token-13-4" morph="none" pos="word" start_char="2778">by</TOKEN>
<TOKEN end_char="2788" id="token-13-5" morph="none" pos="word" start_char="2781">emerging</TOKEN>
<TOKEN end_char="2802" id="token-13-6" morph="none" pos="word" start_char="2790">epidemiologic</TOKEN>
<TOKEN end_char="2806" id="token-13-7" morph="none" pos="word" start_char="2804">and</TOKEN>
<TOKEN end_char="2819" id="token-13-8" morph="none" pos="word" start_char="2808">phylogenetic</TOKEN>
<TOKEN end_char="2828" id="token-13-9" morph="none" pos="word" start_char="2821">evidence</TOKEN>
<TOKEN end_char="2839" id="token-13-10" morph="none" pos="word" start_char="2830">indicating</TOKEN>
<TOKEN end_char="2844" id="token-13-11" morph="none" pos="word" start_char="2841">that</TOKEN>
<TOKEN end_char="2848" id="token-13-12" morph="none" pos="word" start_char="2846">the</TOKEN>
<TOKEN end_char="2854" id="token-13-13" morph="none" pos="word" start_char="2850">virus</TOKEN>
<TOKEN end_char="2862" id="token-13-14" morph="none" pos="word" start_char="2856">emerged</TOKEN>
<TOKEN end_char="2865" id="token-13-15" morph="none" pos="word" start_char="2864">in</TOKEN>
<TOKEN end_char="2874" id="token-13-16" morph="none" pos="word" start_char="2867">southern</TOKEN>
<TOKEN end_char="2880" id="token-13-17" morph="none" pos="word" start_char="2876">China</TOKEN>
<TOKEN end_char="2881" id="token-13-18" morph="none" pos="punct" start_char="2881">,</TOKEN>
<TOKEN end_char="2885" id="token-13-19" morph="none" pos="word" start_char="2883">and</TOKEN>
<TOKEN end_char="2889" id="token-13-20" morph="none" pos="word" start_char="2887">may</TOKEN>
<TOKEN end_char="2894" id="token-13-21" morph="none" pos="word" start_char="2891">have</TOKEN>
<TOKEN end_char="2902" id="token-13-22" morph="none" pos="word" start_char="2896">already</TOKEN>
<TOKEN end_char="2909" id="token-13-23" morph="none" pos="word" start_char="2904">spread</TOKEN>
<TOKEN end_char="2925" id="token-13-24" morph="none" pos="word" start_char="2911">internationally</TOKEN>
<TOKEN end_char="2926" id="token-13-25" morph="none" pos="punct" start_char="2926">,</TOKEN>
<TOKEN end_char="2930" id="token-13-26" morph="none" pos="word" start_char="2928">and</TOKEN>
<TOKEN end_char="2938" id="token-13-27" morph="none" pos="word" start_char="2932">adapted</TOKEN>
<TOKEN end_char="2942" id="token-13-28" morph="none" pos="word" start_char="2940">for</TOKEN>
<TOKEN end_char="2952" id="token-13-29" morph="none" pos="word" start_char="2944">efficient</TOKEN>
<TOKEN end_char="2958" id="token-13-30" morph="none" pos="word" start_char="2954">human</TOKEN>
<TOKEN end_char="2971" id="token-13-31" morph="none" pos="word" start_char="2960">transmission</TOKEN>
<TOKEN end_char="2974" id="token-13-32" morph="none" pos="word" start_char="2973">by</TOKEN>
<TOKEN end_char="2978" id="token-13-33" morph="none" pos="word" start_char="2976">the</TOKEN>
<TOKEN end_char="2983" id="token-13-34" morph="none" pos="word" start_char="2980">time</TOKEN>
<TOKEN end_char="2986" id="token-13-35" morph="none" pos="word" start_char="2985">it</TOKEN>
<TOKEN end_char="2990" id="token-13-36" morph="none" pos="word" start_char="2988">was</TOKEN>
<TOKEN end_char="2999" id="token-13-37" morph="none" pos="word" start_char="2992">detected</TOKEN>
<TOKEN end_char="3002" id="token-13-38" morph="none" pos="word" start_char="3001">in</TOKEN>
<TOKEN end_char="3007" id="token-13-39" morph="none" pos="word" start_char="3004">late</TOKEN>
<TOKEN end_char="3016" id="token-13-40" morph="none" pos="word" start_char="3009">December</TOKEN>
<TOKEN end_char="3018" id="token-13-41" morph="none" pos="punct" start_char="3017">."</TOKEN>
</SEG>
<SEG end_char="3107" id="segment-14" start_char="3021">
<ORIGINAL_TEXT>Hospital traffic and Covid-19 symptoms search queries both rise sharply in autumn, 2019</ORIGINAL_TEXT>
<TOKEN end_char="3028" id="token-14-0" morph="none" pos="word" start_char="3021">Hospital</TOKEN>
<TOKEN end_char="3036" id="token-14-1" morph="none" pos="word" start_char="3030">traffic</TOKEN>
<TOKEN end_char="3040" id="token-14-2" morph="none" pos="word" start_char="3038">and</TOKEN>
<TOKEN end_char="3049" id="token-14-3" morph="none" pos="unknown" start_char="3042">Covid-19</TOKEN>
<TOKEN end_char="3058" id="token-14-4" morph="none" pos="word" start_char="3051">symptoms</TOKEN>
<TOKEN end_char="3065" id="token-14-5" morph="none" pos="word" start_char="3060">search</TOKEN>
<TOKEN end_char="3073" id="token-14-6" morph="none" pos="word" start_char="3067">queries</TOKEN>
<TOKEN end_char="3078" id="token-14-7" morph="none" pos="word" start_char="3075">both</TOKEN>
<TOKEN end_char="3083" id="token-14-8" morph="none" pos="word" start_char="3080">rise</TOKEN>
<TOKEN end_char="3091" id="token-14-9" morph="none" pos="word" start_char="3085">sharply</TOKEN>
<TOKEN end_char="3094" id="token-14-10" morph="none" pos="word" start_char="3093">in</TOKEN>
<TOKEN end_char="3101" id="token-14-11" morph="none" pos="word" start_char="3096">autumn</TOKEN>
<TOKEN end_char="3102" id="token-14-12" morph="none" pos="punct" start_char="3102">,</TOKEN>
<TOKEN end_char="3107" id="token-14-13" morph="none" pos="word" start_char="3104">2019</TOKEN>
</SEG>
<SEG end_char="3253" id="segment-15" start_char="3111">
<ORIGINAL_TEXT>The study examined satellite images of traffic patterns at six Wuhan hospitals as well as several other control sites, to draw its conclusions.</ORIGINAL_TEXT>
<TOKEN end_char="3113" id="token-15-0" morph="none" pos="word" start_char="3111">The</TOKEN>
<TOKEN end_char="3119" id="token-15-1" morph="none" pos="word" start_char="3115">study</TOKEN>
<TOKEN end_char="3128" id="token-15-2" morph="none" pos="word" start_char="3121">examined</TOKEN>
<TOKEN end_char="3138" id="token-15-3" morph="none" pos="word" start_char="3130">satellite</TOKEN>
<TOKEN end_char="3145" id="token-15-4" morph="none" pos="word" start_char="3140">images</TOKEN>
<TOKEN end_char="3148" id="token-15-5" morph="none" pos="word" start_char="3147">of</TOKEN>
<TOKEN end_char="3156" id="token-15-6" morph="none" pos="word" start_char="3150">traffic</TOKEN>
<TOKEN end_char="3165" id="token-15-7" morph="none" pos="word" start_char="3158">patterns</TOKEN>
<TOKEN end_char="3168" id="token-15-8" morph="none" pos="word" start_char="3167">at</TOKEN>
<TOKEN end_char="3172" id="token-15-9" morph="none" pos="word" start_char="3170">six</TOKEN>
<TOKEN end_char="3178" id="token-15-10" morph="none" pos="word" start_char="3174">Wuhan</TOKEN>
<TOKEN end_char="3188" id="token-15-11" morph="none" pos="word" start_char="3180">hospitals</TOKEN>
<TOKEN end_char="3191" id="token-15-12" morph="none" pos="word" start_char="3190">as</TOKEN>
<TOKEN end_char="3196" id="token-15-13" morph="none" pos="word" start_char="3193">well</TOKEN>
<TOKEN end_char="3199" id="token-15-14" morph="none" pos="word" start_char="3198">as</TOKEN>
<TOKEN end_char="3207" id="token-15-15" morph="none" pos="word" start_char="3201">several</TOKEN>
<TOKEN end_char="3213" id="token-15-16" morph="none" pos="word" start_char="3209">other</TOKEN>
<TOKEN end_char="3221" id="token-15-17" morph="none" pos="word" start_char="3215">control</TOKEN>
<TOKEN end_char="3227" id="token-15-18" morph="none" pos="word" start_char="3223">sites</TOKEN>
<TOKEN end_char="3228" id="token-15-19" morph="none" pos="punct" start_char="3228">,</TOKEN>
<TOKEN end_char="3231" id="token-15-20" morph="none" pos="word" start_char="3230">to</TOKEN>
<TOKEN end_char="3236" id="token-15-21" morph="none" pos="word" start_char="3233">draw</TOKEN>
<TOKEN end_char="3240" id="token-15-22" morph="none" pos="word" start_char="3238">its</TOKEN>
<TOKEN end_char="3252" id="token-15-23" morph="none" pos="word" start_char="3242">conclusions</TOKEN>
<TOKEN end_char="3253" id="token-15-24" morph="none" pos="punct" start_char="3253">.</TOKEN>
</SEG>
<SEG end_char="3406" id="segment-16" start_char="3255">
<ORIGINAL_TEXT>Along with that it searched terms in the Chinese "Baidu" search engine, noting that the same method has been used to estimate influenza trends in China.</ORIGINAL_TEXT>
<TOKEN end_char="3259" id="token-16-0" morph="none" pos="word" start_char="3255">Along</TOKEN>
<TOKEN end_char="3264" id="token-16-1" morph="none" pos="word" start_char="3261">with</TOKEN>
<TOKEN end_char="3269" id="token-16-2" morph="none" pos="word" start_char="3266">that</TOKEN>
<TOKEN end_char="3272" id="token-16-3" morph="none" pos="word" start_char="3271">it</TOKEN>
<TOKEN end_char="3281" id="token-16-4" morph="none" pos="word" start_char="3274">searched</TOKEN>
<TOKEN end_char="3287" id="token-16-5" morph="none" pos="word" start_char="3283">terms</TOKEN>
<TOKEN end_char="3290" id="token-16-6" morph="none" pos="word" start_char="3289">in</TOKEN>
<TOKEN end_char="3294" id="token-16-7" morph="none" pos="word" start_char="3292">the</TOKEN>
<TOKEN end_char="3302" id="token-16-8" morph="none" pos="word" start_char="3296">Chinese</TOKEN>
<TOKEN end_char="3304" id="token-16-9" morph="none" pos="punct" start_char="3304">"</TOKEN>
<TOKEN end_char="3309" id="token-16-10" morph="none" pos="word" start_char="3305">Baidu</TOKEN>
<TOKEN end_char="3310" id="token-16-11" morph="none" pos="punct" start_char="3310">"</TOKEN>
<TOKEN end_char="3317" id="token-16-12" morph="none" pos="word" start_char="3312">search</TOKEN>
<TOKEN end_char="3324" id="token-16-13" morph="none" pos="word" start_char="3319">engine</TOKEN>
<TOKEN end_char="3325" id="token-16-14" morph="none" pos="punct" start_char="3325">,</TOKEN>
<TOKEN end_char="3332" id="token-16-15" morph="none" pos="word" start_char="3327">noting</TOKEN>
<TOKEN end_char="3337" id="token-16-16" morph="none" pos="word" start_char="3334">that</TOKEN>
<TOKEN end_char="3341" id="token-16-17" morph="none" pos="word" start_char="3339">the</TOKEN>
<TOKEN end_char="3346" id="token-16-18" morph="none" pos="word" start_char="3343">same</TOKEN>
<TOKEN end_char="3353" id="token-16-19" morph="none" pos="word" start_char="3348">method</TOKEN>
<TOKEN end_char="3357" id="token-16-20" morph="none" pos="word" start_char="3355">has</TOKEN>
<TOKEN end_char="3362" id="token-16-21" morph="none" pos="word" start_char="3359">been</TOKEN>
<TOKEN end_char="3367" id="token-16-22" morph="none" pos="word" start_char="3364">used</TOKEN>
<TOKEN end_char="3370" id="token-16-23" morph="none" pos="word" start_char="3369">to</TOKEN>
<TOKEN end_char="3379" id="token-16-24" morph="none" pos="word" start_char="3372">estimate</TOKEN>
<TOKEN end_char="3389" id="token-16-25" morph="none" pos="word" start_char="3381">influenza</TOKEN>
<TOKEN end_char="3396" id="token-16-26" morph="none" pos="word" start_char="3391">trends</TOKEN>
<TOKEN end_char="3399" id="token-16-27" morph="none" pos="word" start_char="3398">in</TOKEN>
<TOKEN end_char="3405" id="token-16-28" morph="none" pos="word" start_char="3401">China</TOKEN>
<TOKEN end_char="3406" id="token-16-29" morph="none" pos="punct" start_char="3406">.</TOKEN>
</SEG>
<SEG end_char="3585" id="segment-17" start_char="3409">
<ORIGINAL_TEXT>Between September and October 2019, 5 of the 6 hospitals studied show their highest relative daily volume of traffic in the series of images that were analysed, the study found.</ORIGINAL_TEXT>
<TOKEN end_char="3415" id="token-17-0" morph="none" pos="word" start_char="3409">Between</TOKEN>
<TOKEN end_char="3425" id="token-17-1" morph="none" pos="word" start_char="3417">September</TOKEN>
<TOKEN end_char="3429" id="token-17-2" morph="none" pos="word" start_char="3427">and</TOKEN>
<TOKEN end_char="3437" id="token-17-3" morph="none" pos="word" start_char="3431">October</TOKEN>
<TOKEN end_char="3442" id="token-17-4" morph="none" pos="word" start_char="3439">2019</TOKEN>
<TOKEN end_char="3443" id="token-17-5" morph="none" pos="punct" start_char="3443">,</TOKEN>
<TOKEN end_char="3445" id="token-17-6" morph="none" pos="word" start_char="3445">5</TOKEN>
<TOKEN end_char="3448" id="token-17-7" morph="none" pos="word" start_char="3447">of</TOKEN>
<TOKEN end_char="3452" id="token-17-8" morph="none" pos="word" start_char="3450">the</TOKEN>
<TOKEN end_char="3454" id="token-17-9" morph="none" pos="word" start_char="3454">6</TOKEN>
<TOKEN end_char="3464" id="token-17-10" morph="none" pos="word" start_char="3456">hospitals</TOKEN>
<TOKEN end_char="3472" id="token-17-11" morph="none" pos="word" start_char="3466">studied</TOKEN>
<TOKEN end_char="3477" id="token-17-12" morph="none" pos="word" start_char="3474">show</TOKEN>
<TOKEN end_char="3483" id="token-17-13" morph="none" pos="word" start_char="3479">their</TOKEN>
<TOKEN end_char="3491" id="token-17-14" morph="none" pos="word" start_char="3485">highest</TOKEN>
<TOKEN end_char="3500" id="token-17-15" morph="none" pos="word" start_char="3493">relative</TOKEN>
<TOKEN end_char="3506" id="token-17-16" morph="none" pos="word" start_char="3502">daily</TOKEN>
<TOKEN end_char="3513" id="token-17-17" morph="none" pos="word" start_char="3508">volume</TOKEN>
<TOKEN end_char="3516" id="token-17-18" morph="none" pos="word" start_char="3515">of</TOKEN>
<TOKEN end_char="3524" id="token-17-19" morph="none" pos="word" start_char="3518">traffic</TOKEN>
<TOKEN end_char="3527" id="token-17-20" morph="none" pos="word" start_char="3526">in</TOKEN>
<TOKEN end_char="3531" id="token-17-21" morph="none" pos="word" start_char="3529">the</TOKEN>
<TOKEN end_char="3538" id="token-17-22" morph="none" pos="word" start_char="3533">series</TOKEN>
<TOKEN end_char="3541" id="token-17-23" morph="none" pos="word" start_char="3540">of</TOKEN>
<TOKEN end_char="3548" id="token-17-24" morph="none" pos="word" start_char="3543">images</TOKEN>
<TOKEN end_char="3553" id="token-17-25" morph="none" pos="word" start_char="3550">that</TOKEN>
<TOKEN end_char="3558" id="token-17-26" morph="none" pos="word" start_char="3555">were</TOKEN>
<TOKEN end_char="3567" id="token-17-27" morph="none" pos="word" start_char="3560">analysed</TOKEN>
<TOKEN end_char="3568" id="token-17-28" morph="none" pos="punct" start_char="3568">,</TOKEN>
<TOKEN end_char="3572" id="token-17-29" morph="none" pos="word" start_char="3570">the</TOKEN>
<TOKEN end_char="3578" id="token-17-30" morph="none" pos="word" start_char="3574">study</TOKEN>
<TOKEN end_char="3584" id="token-17-31" morph="none" pos="word" start_char="3580">found</TOKEN>
<TOKEN end_char="3585" id="token-17-32" morph="none" pos="punct" start_char="3585">.</TOKEN>
</SEG>
<SEG end_char="3678" id="segment-18" start_char="3587">
<ORIGINAL_TEXT>"coinciding with elevated levels of Baidu search queries for the terms "diarrhea and cough".</ORIGINAL_TEXT>
<TOKEN end_char="3587" id="token-18-0" morph="none" pos="punct" start_char="3587">"</TOKEN>
<TOKEN end_char="3597" id="token-18-1" morph="none" pos="word" start_char="3588">coinciding</TOKEN>
<TOKEN end_char="3602" id="token-18-2" morph="none" pos="word" start_char="3599">with</TOKEN>
<TOKEN end_char="3611" id="token-18-3" morph="none" pos="word" start_char="3604">elevated</TOKEN>
<TOKEN end_char="3618" id="token-18-4" morph="none" pos="word" start_char="3613">levels</TOKEN>
<TOKEN end_char="3621" id="token-18-5" morph="none" pos="word" start_char="3620">of</TOKEN>
<TOKEN end_char="3627" id="token-18-6" morph="none" pos="word" start_char="3623">Baidu</TOKEN>
<TOKEN end_char="3634" id="token-18-7" morph="none" pos="word" start_char="3629">search</TOKEN>
<TOKEN end_char="3642" id="token-18-8" morph="none" pos="word" start_char="3636">queries</TOKEN>
<TOKEN end_char="3646" id="token-18-9" morph="none" pos="word" start_char="3644">for</TOKEN>
<TOKEN end_char="3650" id="token-18-10" morph="none" pos="word" start_char="3648">the</TOKEN>
<TOKEN end_char="3656" id="token-18-11" morph="none" pos="word" start_char="3652">terms</TOKEN>
<TOKEN end_char="3658" id="token-18-12" morph="none" pos="punct" start_char="3658">"</TOKEN>
<TOKEN end_char="3666" id="token-18-13" morph="none" pos="word" start_char="3659">diarrhea</TOKEN>
<TOKEN end_char="3670" id="token-18-14" morph="none" pos="word" start_char="3668">and</TOKEN>
<TOKEN end_char="3676" id="token-18-15" morph="none" pos="word" start_char="3672">cough</TOKEN>
<TOKEN end_char="3678" id="token-18-16" morph="none" pos="punct" start_char="3677">".</TOKEN>
</SEG>
<SEG end_char="3828" id="segment-19" start_char="3680">
<ORIGINAL_TEXT>While searches for cough alone are typical of the influenza season, diarrhea has been one of the more distinctive symptoms marking onset of Covid-19.</ORIGINAL_TEXT>
<TOKEN end_char="3684" id="token-19-0" morph="none" pos="word" start_char="3680">While</TOKEN>
<TOKEN end_char="3693" id="token-19-1" morph="none" pos="word" start_char="3686">searches</TOKEN>
<TOKEN end_char="3697" id="token-19-2" morph="none" pos="word" start_char="3695">for</TOKEN>
<TOKEN end_char="3703" id="token-19-3" morph="none" pos="word" start_char="3699">cough</TOKEN>
<TOKEN end_char="3709" id="token-19-4" morph="none" pos="word" start_char="3705">alone</TOKEN>
<TOKEN end_char="3713" id="token-19-5" morph="none" pos="word" start_char="3711">are</TOKEN>
<TOKEN end_char="3721" id="token-19-6" morph="none" pos="word" start_char="3715">typical</TOKEN>
<TOKEN end_char="3724" id="token-19-7" morph="none" pos="word" start_char="3723">of</TOKEN>
<TOKEN end_char="3728" id="token-19-8" morph="none" pos="word" start_char="3726">the</TOKEN>
<TOKEN end_char="3738" id="token-19-9" morph="none" pos="word" start_char="3730">influenza</TOKEN>
<TOKEN end_char="3745" id="token-19-10" morph="none" pos="word" start_char="3740">season</TOKEN>
<TOKEN end_char="3746" id="token-19-11" morph="none" pos="punct" start_char="3746">,</TOKEN>
<TOKEN end_char="3755" id="token-19-12" morph="none" pos="word" start_char="3748">diarrhea</TOKEN>
<TOKEN end_char="3759" id="token-19-13" morph="none" pos="word" start_char="3757">has</TOKEN>
<TOKEN end_char="3764" id="token-19-14" morph="none" pos="word" start_char="3761">been</TOKEN>
<TOKEN end_char="3768" id="token-19-15" morph="none" pos="word" start_char="3766">one</TOKEN>
<TOKEN end_char="3771" id="token-19-16" morph="none" pos="word" start_char="3770">of</TOKEN>
<TOKEN end_char="3775" id="token-19-17" morph="none" pos="word" start_char="3773">the</TOKEN>
<TOKEN end_char="3780" id="token-19-18" morph="none" pos="word" start_char="3777">more</TOKEN>
<TOKEN end_char="3792" id="token-19-19" morph="none" pos="word" start_char="3782">distinctive</TOKEN>
<TOKEN end_char="3801" id="token-19-20" morph="none" pos="word" start_char="3794">symptoms</TOKEN>
<TOKEN end_char="3809" id="token-19-21" morph="none" pos="word" start_char="3803">marking</TOKEN>
<TOKEN end_char="3815" id="token-19-22" morph="none" pos="word" start_char="3811">onset</TOKEN>
<TOKEN end_char="3818" id="token-19-23" morph="none" pos="word" start_char="3817">of</TOKEN>
<TOKEN end_char="3827" id="token-19-24" morph="none" pos="unknown" start_char="3820">Covid-19</TOKEN>
<TOKEN end_char="3828" id="token-19-25" morph="none" pos="punct" start_char="3828">.</TOKEN>
</SEG>
<SEG end_char="3892" id="segment-20" start_char="3831">
<ORIGINAL_TEXT>Chinese Officials Reject Findings of Study – Still in Preprint</ORIGINAL_TEXT>
<TOKEN end_char="3837" id="token-20-0" morph="none" pos="word" start_char="3831">Chinese</TOKEN>
<TOKEN end_char="3847" id="token-20-1" morph="none" pos="word" start_char="3839">Officials</TOKEN>
<TOKEN end_char="3854" id="token-20-2" morph="none" pos="word" start_char="3849">Reject</TOKEN>
<TOKEN end_char="3863" id="token-20-3" morph="none" pos="word" start_char="3856">Findings</TOKEN>
<TOKEN end_char="3866" id="token-20-4" morph="none" pos="word" start_char="3865">of</TOKEN>
<TOKEN end_char="3872" id="token-20-5" morph="none" pos="word" start_char="3868">Study</TOKEN>
<TOKEN end_char="3874" id="token-20-6" morph="none" pos="punct" start_char="3874">–</TOKEN>
<TOKEN end_char="3880" id="token-20-7" morph="none" pos="word" start_char="3876">Still</TOKEN>
<TOKEN end_char="3883" id="token-20-8" morph="none" pos="word" start_char="3882">in</TOKEN>
<TOKEN end_char="3892" id="token-20-9" morph="none" pos="word" start_char="3885">Preprint</TOKEN>
</SEG>
<SEG end_char="3943" id="segment-21" start_char="3896">
<ORIGINAL_TEXT>Hua Chunying, China Foreign Ministry Spokeswoman</ORIGINAL_TEXT>
<TOKEN end_char="3898" id="token-21-0" morph="none" pos="word" start_char="3896">Hua</TOKEN>
<TOKEN end_char="3907" id="token-21-1" morph="none" pos="word" start_char="3900">Chunying</TOKEN>
<TOKEN end_char="3908" id="token-21-2" morph="none" pos="punct" start_char="3908">,</TOKEN>
<TOKEN end_char="3914" id="token-21-3" morph="none" pos="word" start_char="3910">China</TOKEN>
<TOKEN end_char="3922" id="token-21-4" morph="none" pos="word" start_char="3916">Foreign</TOKEN>
<TOKEN end_char="3931" id="token-21-5" morph="none" pos="word" start_char="3924">Ministry</TOKEN>
<TOKEN end_char="3943" id="token-21-6" morph="none" pos="word" start_char="3933">Spokeswoman</TOKEN>
</SEG>
<SEG end_char="4216" id="segment-22" start_char="3947">
<ORIGINAL_TEXT>Speaking at a press briefing on Tuesday, China’s foreign ministry spokeswoman Hua Chunying rejected the study’s findings, saying, "I think it is absurd, actually extremely absurd, to draw this kind of conclusion based on superficial observations such as traffic volume."</ORIGINAL_TEXT>
<TOKEN end_char="3954" id="token-22-0" morph="none" pos="word" start_char="3947">Speaking</TOKEN>
<TOKEN end_char="3957" id="token-22-1" morph="none" pos="word" start_char="3956">at</TOKEN>
<TOKEN end_char="3959" id="token-22-2" morph="none" pos="word" start_char="3959">a</TOKEN>
<TOKEN end_char="3965" id="token-22-3" morph="none" pos="word" start_char="3961">press</TOKEN>
<TOKEN end_char="3974" id="token-22-4" morph="none" pos="word" start_char="3967">briefing</TOKEN>
<TOKEN end_char="3977" id="token-22-5" morph="none" pos="word" start_char="3976">on</TOKEN>
<TOKEN end_char="3985" id="token-22-6" morph="none" pos="word" start_char="3979">Tuesday</TOKEN>
<TOKEN end_char="3986" id="token-22-7" morph="none" pos="punct" start_char="3986">,</TOKEN>
<TOKEN end_char="3994" id="token-22-8" morph="none" pos="word" start_char="3988">China’s</TOKEN>
<TOKEN end_char="4002" id="token-22-9" morph="none" pos="word" start_char="3996">foreign</TOKEN>
<TOKEN end_char="4011" id="token-22-10" morph="none" pos="word" start_char="4004">ministry</TOKEN>
<TOKEN end_char="4023" id="token-22-11" morph="none" pos="word" start_char="4013">spokeswoman</TOKEN>
<TOKEN end_char="4027" id="token-22-12" morph="none" pos="word" start_char="4025">Hua</TOKEN>
<TOKEN end_char="4036" id="token-22-13" morph="none" pos="word" start_char="4029">Chunying</TOKEN>
<TOKEN end_char="4045" id="token-22-14" morph="none" pos="word" start_char="4038">rejected</TOKEN>
<TOKEN end_char="4049" id="token-22-15" morph="none" pos="word" start_char="4047">the</TOKEN>
<TOKEN end_char="4057" id="token-22-16" morph="none" pos="word" start_char="4051">study’s</TOKEN>
<TOKEN end_char="4066" id="token-22-17" morph="none" pos="word" start_char="4059">findings</TOKEN>
<TOKEN end_char="4067" id="token-22-18" morph="none" pos="punct" start_char="4067">,</TOKEN>
<TOKEN end_char="4074" id="token-22-19" morph="none" pos="word" start_char="4069">saying</TOKEN>
<TOKEN end_char="4075" id="token-22-20" morph="none" pos="punct" start_char="4075">,</TOKEN>
<TOKEN end_char="4077" id="token-22-21" morph="none" pos="punct" start_char="4077">"</TOKEN>
<TOKEN end_char="4078" id="token-22-22" morph="none" pos="word" start_char="4078">I</TOKEN>
<TOKEN end_char="4084" id="token-22-23" morph="none" pos="word" start_char="4080">think</TOKEN>
<TOKEN end_char="4087" id="token-22-24" morph="none" pos="word" start_char="4086">it</TOKEN>
<TOKEN end_char="4090" id="token-22-25" morph="none" pos="word" start_char="4089">is</TOKEN>
<TOKEN end_char="4097" id="token-22-26" morph="none" pos="word" start_char="4092">absurd</TOKEN>
<TOKEN end_char="4098" id="token-22-27" morph="none" pos="punct" start_char="4098">,</TOKEN>
<TOKEN end_char="4107" id="token-22-28" morph="none" pos="word" start_char="4100">actually</TOKEN>
<TOKEN end_char="4117" id="token-22-29" morph="none" pos="word" start_char="4109">extremely</TOKEN>
<TOKEN end_char="4124" id="token-22-30" morph="none" pos="word" start_char="4119">absurd</TOKEN>
<TOKEN end_char="4125" id="token-22-31" morph="none" pos="punct" start_char="4125">,</TOKEN>
<TOKEN end_char="4128" id="token-22-32" morph="none" pos="word" start_char="4127">to</TOKEN>
<TOKEN end_char="4133" id="token-22-33" morph="none" pos="word" start_char="4130">draw</TOKEN>
<TOKEN end_char="4138" id="token-22-34" morph="none" pos="word" start_char="4135">this</TOKEN>
<TOKEN end_char="4143" id="token-22-35" morph="none" pos="word" start_char="4140">kind</TOKEN>
<TOKEN end_char="4146" id="token-22-36" morph="none" pos="word" start_char="4145">of</TOKEN>
<TOKEN end_char="4157" id="token-22-37" morph="none" pos="word" start_char="4148">conclusion</TOKEN>
<TOKEN end_char="4163" id="token-22-38" morph="none" pos="word" start_char="4159">based</TOKEN>
<TOKEN end_char="4166" id="token-22-39" morph="none" pos="word" start_char="4165">on</TOKEN>
<TOKEN end_char="4178" id="token-22-40" morph="none" pos="word" start_char="4168">superficial</TOKEN>
<TOKEN end_char="4191" id="token-22-41" morph="none" pos="word" start_char="4180">observations</TOKEN>
<TOKEN end_char="4196" id="token-22-42" morph="none" pos="word" start_char="4193">such</TOKEN>
<TOKEN end_char="4199" id="token-22-43" morph="none" pos="word" start_char="4198">as</TOKEN>
<TOKEN end_char="4207" id="token-22-44" morph="none" pos="word" start_char="4201">traffic</TOKEN>
<TOKEN end_char="4214" id="token-22-45" morph="none" pos="word" start_char="4209">volume</TOKEN>
<TOKEN end_char="4216" id="token-22-46" morph="none" pos="punct" start_char="4215">."</TOKEN>
</SEG>
<SEG end_char="4492" id="segment-23" start_char="4219">
<ORIGINAL_TEXT>Other experts also urged caution in interpreting the study’s results: "It’s important to remember that the data are only correlative and (as the authors admit) cannot identify the cause of the uptick," said Paul Digard, a virologist professor at the University of Edinburgh.</ORIGINAL_TEXT>
<TOKEN end_char="4223" id="token-23-0" morph="none" pos="word" start_char="4219">Other</TOKEN>
<TOKEN end_char="4231" id="token-23-1" morph="none" pos="word" start_char="4225">experts</TOKEN>
<TOKEN end_char="4236" id="token-23-2" morph="none" pos="word" start_char="4233">also</TOKEN>
<TOKEN end_char="4242" id="token-23-3" morph="none" pos="word" start_char="4238">urged</TOKEN>
<TOKEN end_char="4250" id="token-23-4" morph="none" pos="word" start_char="4244">caution</TOKEN>
<TOKEN end_char="4253" id="token-23-5" morph="none" pos="word" start_char="4252">in</TOKEN>
<TOKEN end_char="4266" id="token-23-6" morph="none" pos="word" start_char="4255">interpreting</TOKEN>
<TOKEN end_char="4270" id="token-23-7" morph="none" pos="word" start_char="4268">the</TOKEN>
<TOKEN end_char="4278" id="token-23-8" morph="none" pos="word" start_char="4272">study’s</TOKEN>
<TOKEN end_char="4286" id="token-23-9" morph="none" pos="word" start_char="4280">results</TOKEN>
<TOKEN end_char="4287" id="token-23-10" morph="none" pos="punct" start_char="4287">:</TOKEN>
<TOKEN end_char="4289" id="token-23-11" morph="none" pos="punct" start_char="4289">"</TOKEN>
<TOKEN end_char="4293" id="token-23-12" morph="none" pos="word" start_char="4290">It’s</TOKEN>
<TOKEN end_char="4303" id="token-23-13" morph="none" pos="word" start_char="4295">important</TOKEN>
<TOKEN end_char="4306" id="token-23-14" morph="none" pos="word" start_char="4305">to</TOKEN>
<TOKEN end_char="4315" id="token-23-15" morph="none" pos="word" start_char="4308">remember</TOKEN>
<TOKEN end_char="4320" id="token-23-16" morph="none" pos="word" start_char="4317">that</TOKEN>
<TOKEN end_char="4324" id="token-23-17" morph="none" pos="word" start_char="4322">the</TOKEN>
<TOKEN end_char="4329" id="token-23-18" morph="none" pos="word" start_char="4326">data</TOKEN>
<TOKEN end_char="4333" id="token-23-19" morph="none" pos="word" start_char="4331">are</TOKEN>
<TOKEN end_char="4338" id="token-23-20" morph="none" pos="word" start_char="4335">only</TOKEN>
<TOKEN end_char="4350" id="token-23-21" morph="none" pos="word" start_char="4340">correlative</TOKEN>
<TOKEN end_char="4354" id="token-23-22" morph="none" pos="word" start_char="4352">and</TOKEN>
<TOKEN end_char="4356" id="token-23-23" morph="none" pos="punct" start_char="4356">(</TOKEN>
<TOKEN end_char="4358" id="token-23-24" morph="none" pos="word" start_char="4357">as</TOKEN>
<TOKEN end_char="4362" id="token-23-25" morph="none" pos="word" start_char="4360">the</TOKEN>
<TOKEN end_char="4370" id="token-23-26" morph="none" pos="word" start_char="4364">authors</TOKEN>
<TOKEN end_char="4376" id="token-23-27" morph="none" pos="word" start_char="4372">admit</TOKEN>
<TOKEN end_char="4377" id="token-23-28" morph="none" pos="punct" start_char="4377">)</TOKEN>
<TOKEN end_char="4384" id="token-23-29" morph="none" pos="word" start_char="4379">cannot</TOKEN>
<TOKEN end_char="4393" id="token-23-30" morph="none" pos="word" start_char="4386">identify</TOKEN>
<TOKEN end_char="4397" id="token-23-31" morph="none" pos="word" start_char="4395">the</TOKEN>
<TOKEN end_char="4403" id="token-23-32" morph="none" pos="word" start_char="4399">cause</TOKEN>
<TOKEN end_char="4406" id="token-23-33" morph="none" pos="word" start_char="4405">of</TOKEN>
<TOKEN end_char="4410" id="token-23-34" morph="none" pos="word" start_char="4408">the</TOKEN>
<TOKEN end_char="4417" id="token-23-35" morph="none" pos="word" start_char="4412">uptick</TOKEN>
<TOKEN end_char="4419" id="token-23-36" morph="none" pos="punct" start_char="4418">,"</TOKEN>
<TOKEN end_char="4424" id="token-23-37" morph="none" pos="word" start_char="4421">said</TOKEN>
<TOKEN end_char="4429" id="token-23-38" morph="none" pos="word" start_char="4426">Paul</TOKEN>
<TOKEN end_char="4436" id="token-23-39" morph="none" pos="word" start_char="4431">Digard</TOKEN>
<TOKEN end_char="4437" id="token-23-40" morph="none" pos="punct" start_char="4437">,</TOKEN>
<TOKEN end_char="4439" id="token-23-41" morph="none" pos="word" start_char="4439">a</TOKEN>
<TOKEN end_char="4450" id="token-23-42" morph="none" pos="word" start_char="4441">virologist</TOKEN>
<TOKEN end_char="4460" id="token-23-43" morph="none" pos="word" start_char="4452">professor</TOKEN>
<TOKEN end_char="4463" id="token-23-44" morph="none" pos="word" start_char="4462">at</TOKEN>
<TOKEN end_char="4467" id="token-23-45" morph="none" pos="word" start_char="4465">the</TOKEN>
<TOKEN end_char="4478" id="token-23-46" morph="none" pos="word" start_char="4469">University</TOKEN>
<TOKEN end_char="4481" id="token-23-47" morph="none" pos="word" start_char="4480">of</TOKEN>
<TOKEN end_char="4491" id="token-23-48" morph="none" pos="word" start_char="4483">Edinburgh</TOKEN>
<TOKEN end_char="4492" id="token-23-49" morph="none" pos="punct" start_char="4492">.</TOKEN>
</SEG>
<SEG end_char="4606" id="segment-24" start_char="4494">
<ORIGINAL_TEXT>"By focusing on hospitals in Wuhan, the acknowledged epicentre of the outbreak, the study forces the correlation.</ORIGINAL_TEXT>
<TOKEN end_char="4494" id="token-24-0" morph="none" pos="punct" start_char="4494">"</TOKEN>
<TOKEN end_char="4496" id="token-24-1" morph="none" pos="word" start_char="4495">By</TOKEN>
<TOKEN end_char="4505" id="token-24-2" morph="none" pos="word" start_char="4498">focusing</TOKEN>
<TOKEN end_char="4508" id="token-24-3" morph="none" pos="word" start_char="4507">on</TOKEN>
<TOKEN end_char="4518" id="token-24-4" morph="none" pos="word" start_char="4510">hospitals</TOKEN>
<TOKEN end_char="4521" id="token-24-5" morph="none" pos="word" start_char="4520">in</TOKEN>
<TOKEN end_char="4527" id="token-24-6" morph="none" pos="word" start_char="4523">Wuhan</TOKEN>
<TOKEN end_char="4528" id="token-24-7" morph="none" pos="punct" start_char="4528">,</TOKEN>
<TOKEN end_char="4532" id="token-24-8" morph="none" pos="word" start_char="4530">the</TOKEN>
<TOKEN end_char="4545" id="token-24-9" morph="none" pos="word" start_char="4534">acknowledged</TOKEN>
<TOKEN end_char="4555" id="token-24-10" morph="none" pos="word" start_char="4547">epicentre</TOKEN>
<TOKEN end_char="4558" id="token-24-11" morph="none" pos="word" start_char="4557">of</TOKEN>
<TOKEN end_char="4562" id="token-24-12" morph="none" pos="word" start_char="4560">the</TOKEN>
<TOKEN end_char="4571" id="token-24-13" morph="none" pos="word" start_char="4564">outbreak</TOKEN>
<TOKEN end_char="4572" id="token-24-14" morph="none" pos="punct" start_char="4572">,</TOKEN>
<TOKEN end_char="4576" id="token-24-15" morph="none" pos="word" start_char="4574">the</TOKEN>
<TOKEN end_char="4582" id="token-24-16" morph="none" pos="word" start_char="4578">study</TOKEN>
<TOKEN end_char="4589" id="token-24-17" morph="none" pos="word" start_char="4584">forces</TOKEN>
<TOKEN end_char="4593" id="token-24-18" morph="none" pos="word" start_char="4591">the</TOKEN>
<TOKEN end_char="4605" id="token-24-19" morph="none" pos="word" start_char="4595">correlation</TOKEN>
<TOKEN end_char="4606" id="token-24-20" morph="none" pos="punct" start_char="4606">.</TOKEN>
</SEG>
<SEG end_char="4766" id="segment-25" start_char="4608">
<ORIGINAL_TEXT>It would have been interesting (and possibly much more convincing) to have seen control analyses of other Chinese cities outside of the Hubei region," he said.</ORIGINAL_TEXT>
<TOKEN end_char="4609" id="token-25-0" morph="none" pos="word" start_char="4608">It</TOKEN>
<TOKEN end_char="4615" id="token-25-1" morph="none" pos="word" start_char="4611">would</TOKEN>
<TOKEN end_char="4620" id="token-25-2" morph="none" pos="word" start_char="4617">have</TOKEN>
<TOKEN end_char="4625" id="token-25-3" morph="none" pos="word" start_char="4622">been</TOKEN>
<TOKEN end_char="4637" id="token-25-4" morph="none" pos="word" start_char="4627">interesting</TOKEN>
<TOKEN end_char="4639" id="token-25-5" morph="none" pos="punct" start_char="4639">(</TOKEN>
<TOKEN end_char="4642" id="token-25-6" morph="none" pos="word" start_char="4640">and</TOKEN>
<TOKEN end_char="4651" id="token-25-7" morph="none" pos="word" start_char="4644">possibly</TOKEN>
<TOKEN end_char="4656" id="token-25-8" morph="none" pos="word" start_char="4653">much</TOKEN>
<TOKEN end_char="4661" id="token-25-9" morph="none" pos="word" start_char="4658">more</TOKEN>
<TOKEN end_char="4672" id="token-25-10" morph="none" pos="word" start_char="4663">convincing</TOKEN>
<TOKEN end_char="4673" id="token-25-11" morph="none" pos="punct" start_char="4673">)</TOKEN>
<TOKEN end_char="4676" id="token-25-12" morph="none" pos="word" start_char="4675">to</TOKEN>
<TOKEN end_char="4681" id="token-25-13" morph="none" pos="word" start_char="4678">have</TOKEN>
<TOKEN end_char="4686" id="token-25-14" morph="none" pos="word" start_char="4683">seen</TOKEN>
<TOKEN end_char="4694" id="token-25-15" morph="none" pos="word" start_char="4688">control</TOKEN>
<TOKEN end_char="4703" id="token-25-16" morph="none" pos="word" start_char="4696">analyses</TOKEN>
<TOKEN end_char="4706" id="token-25-17" morph="none" pos="word" start_char="4705">of</TOKEN>
<TOKEN end_char="4712" id="token-25-18" morph="none" pos="word" start_char="4708">other</TOKEN>
<TOKEN end_char="4720" id="token-25-19" morph="none" pos="word" start_char="4714">Chinese</TOKEN>
<TOKEN end_char="4727" id="token-25-20" morph="none" pos="word" start_char="4722">cities</TOKEN>
<TOKEN end_char="4735" id="token-25-21" morph="none" pos="word" start_char="4729">outside</TOKEN>
<TOKEN end_char="4738" id="token-25-22" morph="none" pos="word" start_char="4737">of</TOKEN>
<TOKEN end_char="4742" id="token-25-23" morph="none" pos="word" start_char="4740">the</TOKEN>
<TOKEN end_char="4748" id="token-25-24" morph="none" pos="word" start_char="4744">Hubei</TOKEN>
<TOKEN end_char="4755" id="token-25-25" morph="none" pos="word" start_char="4750">region</TOKEN>
<TOKEN end_char="4757" id="token-25-26" morph="none" pos="punct" start_char="4756">,"</TOKEN>
<TOKEN end_char="4760" id="token-25-27" morph="none" pos="word" start_char="4759">he</TOKEN>
<TOKEN end_char="4765" id="token-25-28" morph="none" pos="word" start_char="4762">said</TOKEN>
<TOKEN end_char="4766" id="token-25-29" morph="none" pos="punct" start_char="4766">.</TOKEN>
</SEG>
<SEG end_char="4860" id="segment-26" start_char="4769">
<ORIGINAL_TEXT>The study is still in its preprint form, and had not undergone peer review, they also noted.</ORIGINAL_TEXT>
<TOKEN end_char="4771" id="token-26-0" morph="none" pos="word" start_char="4769">The</TOKEN>
<TOKEN end_char="4777" id="token-26-1" morph="none" pos="word" start_char="4773">study</TOKEN>
<TOKEN end_char="4780" id="token-26-2" morph="none" pos="word" start_char="4779">is</TOKEN>
<TOKEN end_char="4786" id="token-26-3" morph="none" pos="word" start_char="4782">still</TOKEN>
<TOKEN end_char="4789" id="token-26-4" morph="none" pos="word" start_char="4788">in</TOKEN>
<TOKEN end_char="4793" id="token-26-5" morph="none" pos="word" start_char="4791">its</TOKEN>
<TOKEN end_char="4802" id="token-26-6" morph="none" pos="word" start_char="4795">preprint</TOKEN>
<TOKEN end_char="4807" id="token-26-7" morph="none" pos="word" start_char="4804">form</TOKEN>
<TOKEN end_char="4808" id="token-26-8" morph="none" pos="punct" start_char="4808">,</TOKEN>
<TOKEN end_char="4812" id="token-26-9" morph="none" pos="word" start_char="4810">and</TOKEN>
<TOKEN end_char="4816" id="token-26-10" morph="none" pos="word" start_char="4814">had</TOKEN>
<TOKEN end_char="4820" id="token-26-11" morph="none" pos="word" start_char="4818">not</TOKEN>
<TOKEN end_char="4830" id="token-26-12" morph="none" pos="word" start_char="4822">undergone</TOKEN>
<TOKEN end_char="4835" id="token-26-13" morph="none" pos="word" start_char="4832">peer</TOKEN>
<TOKEN end_char="4842" id="token-26-14" morph="none" pos="word" start_char="4837">review</TOKEN>
<TOKEN end_char="4843" id="token-26-15" morph="none" pos="punct" start_char="4843">,</TOKEN>
<TOKEN end_char="4848" id="token-26-16" morph="none" pos="word" start_char="4845">they</TOKEN>
<TOKEN end_char="4853" id="token-26-17" morph="none" pos="word" start_char="4850">also</TOKEN>
<TOKEN end_char="4859" id="token-26-18" morph="none" pos="word" start_char="4855">noted</TOKEN>
<TOKEN end_char="4860" id="token-26-19" morph="none" pos="punct" start_char="4860">.</TOKEN>
</SEG>
<SEG end_char="5151" id="segment-27" start_char="4862">
<ORIGINAL_TEXT>This seemed apparent from one obvious error in the preprint PDF, which refers to a: "large decrease in hospital [traffic] voume and search query data, following the public health lockdown of Wuhan on January 23, 2019" – an apparently erroneous reference to the lockdown of January 23, 2020.</ORIGINAL_TEXT>
<TOKEN end_char="4865" id="token-27-0" morph="none" pos="word" start_char="4862">This</TOKEN>
<TOKEN end_char="4872" id="token-27-1" morph="none" pos="word" start_char="4867">seemed</TOKEN>
<TOKEN end_char="4881" id="token-27-2" morph="none" pos="word" start_char="4874">apparent</TOKEN>
<TOKEN end_char="4886" id="token-27-3" morph="none" pos="word" start_char="4883">from</TOKEN>
<TOKEN end_char="4890" id="token-27-4" morph="none" pos="word" start_char="4888">one</TOKEN>
<TOKEN end_char="4898" id="token-27-5" morph="none" pos="word" start_char="4892">obvious</TOKEN>
<TOKEN end_char="4904" id="token-27-6" morph="none" pos="word" start_char="4900">error</TOKEN>
<TOKEN end_char="4907" id="token-27-7" morph="none" pos="word" start_char="4906">in</TOKEN>
<TOKEN end_char="4911" id="token-27-8" morph="none" pos="word" start_char="4909">the</TOKEN>
<TOKEN end_char="4920" id="token-27-9" morph="none" pos="word" start_char="4913">preprint</TOKEN>
<TOKEN end_char="4924" id="token-27-10" morph="none" pos="word" start_char="4922">PDF</TOKEN>
<TOKEN end_char="4925" id="token-27-11" morph="none" pos="punct" start_char="4925">,</TOKEN>
<TOKEN end_char="4931" id="token-27-12" morph="none" pos="word" start_char="4927">which</TOKEN>
<TOKEN end_char="4938" id="token-27-13" morph="none" pos="word" start_char="4933">refers</TOKEN>
<TOKEN end_char="4941" id="token-27-14" morph="none" pos="word" start_char="4940">to</TOKEN>
<TOKEN end_char="4943" id="token-27-15" morph="none" pos="word" start_char="4943">a</TOKEN>
<TOKEN end_char="4944" id="token-27-16" morph="none" pos="punct" start_char="4944">:</TOKEN>
<TOKEN end_char="4946" id="token-27-17" morph="none" pos="punct" start_char="4946">"</TOKEN>
<TOKEN end_char="4951" id="token-27-18" morph="none" pos="word" start_char="4947">large</TOKEN>
<TOKEN end_char="4960" id="token-27-19" morph="none" pos="word" start_char="4953">decrease</TOKEN>
<TOKEN end_char="4963" id="token-27-20" morph="none" pos="word" start_char="4962">in</TOKEN>
<TOKEN end_char="4972" id="token-27-21" morph="none" pos="word" start_char="4965">hospital</TOKEN>
<TOKEN end_char="4974" id="token-27-22" morph="none" pos="punct" start_char="4974">[</TOKEN>
<TOKEN end_char="4981" id="token-27-23" morph="none" pos="word" start_char="4975">traffic</TOKEN>
<TOKEN end_char="4982" id="token-27-24" morph="none" pos="punct" start_char="4982">]</TOKEN>
<TOKEN end_char="4988" id="token-27-25" morph="none" pos="word" start_char="4984">voume</TOKEN>
<TOKEN end_char="4992" id="token-27-26" morph="none" pos="word" start_char="4990">and</TOKEN>
<TOKEN end_char="4999" id="token-27-27" morph="none" pos="word" start_char="4994">search</TOKEN>
<TOKEN end_char="5005" id="token-27-28" morph="none" pos="word" start_char="5001">query</TOKEN>
<TOKEN end_char="5010" id="token-27-29" morph="none" pos="word" start_char="5007">data</TOKEN>
<TOKEN end_char="5011" id="token-27-30" morph="none" pos="punct" start_char="5011">,</TOKEN>
<TOKEN end_char="5021" id="token-27-31" morph="none" pos="word" start_char="5013">following</TOKEN>
<TOKEN end_char="5025" id="token-27-32" morph="none" pos="word" start_char="5023">the</TOKEN>
<TOKEN end_char="5032" id="token-27-33" morph="none" pos="word" start_char="5027">public</TOKEN>
<TOKEN end_char="5039" id="token-27-34" morph="none" pos="word" start_char="5034">health</TOKEN>
<TOKEN end_char="5048" id="token-27-35" morph="none" pos="word" start_char="5041">lockdown</TOKEN>
<TOKEN end_char="5051" id="token-27-36" morph="none" pos="word" start_char="5050">of</TOKEN>
<TOKEN end_char="5057" id="token-27-37" morph="none" pos="word" start_char="5053">Wuhan</TOKEN>
<TOKEN end_char="5060" id="token-27-38" morph="none" pos="word" start_char="5059">on</TOKEN>
<TOKEN end_char="5068" id="token-27-39" morph="none" pos="word" start_char="5062">January</TOKEN>
<TOKEN end_char="5071" id="token-27-40" morph="none" pos="word" start_char="5070">23</TOKEN>
<TOKEN end_char="5072" id="token-27-41" morph="none" pos="punct" start_char="5072">,</TOKEN>
<TOKEN end_char="5077" id="token-27-42" morph="none" pos="word" start_char="5074">2019</TOKEN>
<TOKEN end_char="5078" id="token-27-43" morph="none" pos="punct" start_char="5078">"</TOKEN>
<TOKEN end_char="5080" id="token-27-44" morph="none" pos="punct" start_char="5080">–</TOKEN>
<TOKEN end_char="5083" id="token-27-45" morph="none" pos="word" start_char="5082">an</TOKEN>
<TOKEN end_char="5094" id="token-27-46" morph="none" pos="word" start_char="5085">apparently</TOKEN>
<TOKEN end_char="5104" id="token-27-47" morph="none" pos="word" start_char="5096">erroneous</TOKEN>
<TOKEN end_char="5114" id="token-27-48" morph="none" pos="word" start_char="5106">reference</TOKEN>
<TOKEN end_char="5117" id="token-27-49" morph="none" pos="word" start_char="5116">to</TOKEN>
<TOKEN end_char="5121" id="token-27-50" morph="none" pos="word" start_char="5119">the</TOKEN>
<TOKEN end_char="5130" id="token-27-51" morph="none" pos="word" start_char="5123">lockdown</TOKEN>
<TOKEN end_char="5133" id="token-27-52" morph="none" pos="word" start_char="5132">of</TOKEN>
<TOKEN end_char="5141" id="token-27-53" morph="none" pos="word" start_char="5135">January</TOKEN>
<TOKEN end_char="5144" id="token-27-54" morph="none" pos="word" start_char="5143">23</TOKEN>
<TOKEN end_char="5145" id="token-27-55" morph="none" pos="punct" start_char="5145">,</TOKEN>
<TOKEN end_char="5150" id="token-27-56" morph="none" pos="word" start_char="5147">2020</TOKEN>
<TOKEN end_char="5151" id="token-27-57" morph="none" pos="punct" start_char="5151">.</TOKEN>
</SEG>
<SEG end_char="5214" id="segment-28" start_char="5154">
<ORIGINAL_TEXT>Image Credits: José Mauquer , Nsoesie, Elaine Okanyene et al.</ORIGINAL_TEXT>
<TOKEN end_char="5158" id="token-28-0" morph="none" pos="word" start_char="5154">Image</TOKEN>
<TOKEN end_char="5166" id="token-28-1" morph="none" pos="word" start_char="5160">Credits</TOKEN>
<TOKEN end_char="5167" id="token-28-2" morph="none" pos="punct" start_char="5167">:</TOKEN>
<TOKEN end_char="5172" id="token-28-3" morph="none" pos="word" start_char="5169">José</TOKEN>
<TOKEN end_char="5180" id="token-28-4" morph="none" pos="word" start_char="5174">Mauquer</TOKEN>
<TOKEN end_char="5182" id="token-28-5" morph="none" pos="punct" start_char="5182">,</TOKEN>
<TOKEN end_char="5190" id="token-28-6" morph="none" pos="word" start_char="5184">Nsoesie</TOKEN>
<TOKEN end_char="5191" id="token-28-7" morph="none" pos="punct" start_char="5191">,</TOKEN>
<TOKEN end_char="5198" id="token-28-8" morph="none" pos="word" start_char="5193">Elaine</TOKEN>
<TOKEN end_char="5207" id="token-28-9" morph="none" pos="word" start_char="5200">Okanyene</TOKEN>
<TOKEN end_char="5210" id="token-28-10" morph="none" pos="word" start_char="5209">et</TOKEN>
<TOKEN end_char="5213" id="token-28-11" morph="none" pos="word" start_char="5212">al</TOKEN>
<TOKEN end_char="5214" id="token-28-12" morph="none" pos="punct" start_char="5214">.</TOKEN>
<TRANSLATED_TEXT>Image Credits: José Mauquer, Nsoesie, Elaine Okanyene et al.</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="5378" id="segment-29" start_char="5216">
<ORIGINAL_TEXT>Harvard University Pre-print Repository, , Analysis of hospital traffic and search engine data in Wuhan China indicates early disease activity in the Fall of 2019.</ORIGINAL_TEXT>
<TOKEN end_char="5222" id="token-29-0" morph="none" pos="word" start_char="5216">Harvard</TOKEN>
<TOKEN end_char="5233" id="token-29-1" morph="none" pos="word" start_char="5224">University</TOKEN>
<TOKEN end_char="5243" id="token-29-2" morph="none" pos="unknown" start_char="5235">Pre-print</TOKEN>
<TOKEN end_char="5254" id="token-29-3" morph="none" pos="word" start_char="5245">Repository</TOKEN>
<TOKEN end_char="5255" id="token-29-4" morph="none" pos="punct" start_char="5255">,</TOKEN>
<TOKEN end_char="5257" id="token-29-5" morph="none" pos="punct" start_char="5257">,</TOKEN>
<TOKEN end_char="5266" id="token-29-6" morph="none" pos="word" start_char="5259">Analysis</TOKEN>
<TOKEN end_char="5269" id="token-29-7" morph="none" pos="word" start_char="5268">of</TOKEN>
<TOKEN end_char="5278" id="token-29-8" morph="none" pos="word" start_char="5271">hospital</TOKEN>
<TOKEN end_char="5286" id="token-29-9" morph="none" pos="word" start_char="5280">traffic</TOKEN>
<TOKEN end_char="5290" id="token-29-10" morph="none" pos="word" start_char="5288">and</TOKEN>
<TOKEN end_char="5297" id="token-29-11" morph="none" pos="word" start_char="5292">search</TOKEN>
<TOKEN end_char="5304" id="token-29-12" morph="none" pos="word" start_char="5299">engine</TOKEN>
<TOKEN end_char="5309" id="token-29-13" morph="none" pos="word" start_char="5306">data</TOKEN>
<TOKEN end_char="5312" id="token-29-14" morph="none" pos="word" start_char="5311">in</TOKEN>
<TOKEN end_char="5318" id="token-29-15" morph="none" pos="word" start_char="5314">Wuhan</TOKEN>
<TOKEN end_char="5324" id="token-29-16" morph="none" pos="word" start_char="5320">China</TOKEN>
<TOKEN end_char="5334" id="token-29-17" morph="none" pos="word" start_char="5326">indicates</TOKEN>
<TOKEN end_char="5340" id="token-29-18" morph="none" pos="word" start_char="5336">early</TOKEN>
<TOKEN end_char="5348" id="token-29-19" morph="none" pos="word" start_char="5342">disease</TOKEN>
<TOKEN end_char="5357" id="token-29-20" morph="none" pos="word" start_char="5350">activity</TOKEN>
<TOKEN end_char="5360" id="token-29-21" morph="none" pos="word" start_char="5359">in</TOKEN>
<TOKEN end_char="5364" id="token-29-22" morph="none" pos="word" start_char="5362">the</TOKEN>
<TOKEN end_char="5369" id="token-29-23" morph="none" pos="word" start_char="5366">Fall</TOKEN>
<TOKEN end_char="5372" id="token-29-24" morph="none" pos="word" start_char="5371">of</TOKEN>
<TOKEN end_char="5377" id="token-29-25" morph="none" pos="word" start_char="5374">2019</TOKEN>
<TOKEN end_char="5378" id="token-29-26" morph="none" pos="punct" start_char="5378">.</TOKEN>
</SEG>
<SEG end_char="5481" id="segment-30" start_char="5381">
<ORIGINAL_TEXT>Combat the infodemic in health information and support health policy reporting from the global South.</ORIGINAL_TEXT>
<TOKEN end_char="5386" id="token-30-0" morph="none" pos="word" start_char="5381">Combat</TOKEN>
<TOKEN end_char="5390" id="token-30-1" morph="none" pos="word" start_char="5388">the</TOKEN>
<TOKEN end_char="5400" id="token-30-2" morph="none" pos="word" start_char="5392">infodemic</TOKEN>
<TOKEN end_char="5403" id="token-30-3" morph="none" pos="word" start_char="5402">in</TOKEN>
<TOKEN end_char="5410" id="token-30-4" morph="none" pos="word" start_char="5405">health</TOKEN>
<TOKEN end_char="5422" id="token-30-5" morph="none" pos="word" start_char="5412">information</TOKEN>
<TOKEN end_char="5426" id="token-30-6" morph="none" pos="word" start_char="5424">and</TOKEN>
<TOKEN end_char="5434" id="token-30-7" morph="none" pos="word" start_char="5428">support</TOKEN>
<TOKEN end_char="5441" id="token-30-8" morph="none" pos="word" start_char="5436">health</TOKEN>
<TOKEN end_char="5448" id="token-30-9" morph="none" pos="word" start_char="5443">policy</TOKEN>
<TOKEN end_char="5458" id="token-30-10" morph="none" pos="word" start_char="5450">reporting</TOKEN>
<TOKEN end_char="5463" id="token-30-11" morph="none" pos="word" start_char="5460">from</TOKEN>
<TOKEN end_char="5467" id="token-30-12" morph="none" pos="word" start_char="5465">the</TOKEN>
<TOKEN end_char="5474" id="token-30-13" morph="none" pos="word" start_char="5469">global</TOKEN>
<TOKEN end_char="5480" id="token-30-14" morph="none" pos="word" start_char="5476">South</TOKEN>
<TOKEN end_char="5481" id="token-30-15" morph="none" pos="punct" start_char="5481">.</TOKEN>
</SEG>
<SEG end_char="5677" id="segment-31" start_char="5483">
<ORIGINAL_TEXT>Our growing network of journalists in Africa, Asia, Geneva and New York connect the dots between regional realities and the big global debates, with evidence-based, open access news and analysis.</ORIGINAL_TEXT>
<TOKEN end_char="5485" id="token-31-0" morph="none" pos="word" start_char="5483">Our</TOKEN>
<TOKEN end_char="5493" id="token-31-1" morph="none" pos="word" start_char="5487">growing</TOKEN>
<TOKEN end_char="5501" id="token-31-2" morph="none" pos="word" start_char="5495">network</TOKEN>
<TOKEN end_char="5504" id="token-31-3" morph="none" pos="word" start_char="5503">of</TOKEN>
<TOKEN end_char="5516" id="token-31-4" morph="none" pos="word" start_char="5506">journalists</TOKEN>
<TOKEN end_char="5519" id="token-31-5" morph="none" pos="word" start_char="5518">in</TOKEN>
<TOKEN end_char="5526" id="token-31-6" morph="none" pos="word" start_char="5521">Africa</TOKEN>
<TOKEN end_char="5527" id="token-31-7" morph="none" pos="punct" start_char="5527">,</TOKEN>
<TOKEN end_char="5532" id="token-31-8" morph="none" pos="word" start_char="5529">Asia</TOKEN>
<TOKEN end_char="5533" id="token-31-9" morph="none" pos="punct" start_char="5533">,</TOKEN>
<TOKEN end_char="5540" id="token-31-10" morph="none" pos="word" start_char="5535">Geneva</TOKEN>
<TOKEN end_char="5544" id="token-31-11" morph="none" pos="word" start_char="5542">and</TOKEN>
<TOKEN end_char="5548" id="token-31-12" morph="none" pos="word" start_char="5546">New</TOKEN>
<TOKEN end_char="5553" id="token-31-13" morph="none" pos="word" start_char="5550">York</TOKEN>
<TOKEN end_char="5561" id="token-31-14" morph="none" pos="word" start_char="5555">connect</TOKEN>
<TOKEN end_char="5565" id="token-31-15" morph="none" pos="word" start_char="5563">the</TOKEN>
<TOKEN end_char="5570" id="token-31-16" morph="none" pos="word" start_char="5567">dots</TOKEN>
<TOKEN end_char="5578" id="token-31-17" morph="none" pos="word" start_char="5572">between</TOKEN>
<TOKEN end_char="5587" id="token-31-18" morph="none" pos="word" start_char="5580">regional</TOKEN>
<TOKEN end_char="5597" id="token-31-19" morph="none" pos="word" start_char="5589">realities</TOKEN>
<TOKEN end_char="5601" id="token-31-20" morph="none" pos="word" start_char="5599">and</TOKEN>
<TOKEN end_char="5605" id="token-31-21" morph="none" pos="word" start_char="5603">the</TOKEN>
<TOKEN end_char="5609" id="token-31-22" morph="none" pos="word" start_char="5607">big</TOKEN>
<TOKEN end_char="5616" id="token-31-23" morph="none" pos="word" start_char="5611">global</TOKEN>
<TOKEN end_char="5624" id="token-31-24" morph="none" pos="word" start_char="5618">debates</TOKEN>
<TOKEN end_char="5625" id="token-31-25" morph="none" pos="punct" start_char="5625">,</TOKEN>
<TOKEN end_char="5630" id="token-31-26" morph="none" pos="word" start_char="5627">with</TOKEN>
<TOKEN end_char="5645" id="token-31-27" morph="none" pos="unknown" start_char="5632">evidence-based</TOKEN>
<TOKEN end_char="5646" id="token-31-28" morph="none" pos="punct" start_char="5646">,</TOKEN>
<TOKEN end_char="5651" id="token-31-29" morph="none" pos="word" start_char="5648">open</TOKEN>
<TOKEN end_char="5658" id="token-31-30" morph="none" pos="word" start_char="5653">access</TOKEN>
<TOKEN end_char="5663" id="token-31-31" morph="none" pos="word" start_char="5660">news</TOKEN>
<TOKEN end_char="5667" id="token-31-32" morph="none" pos="word" start_char="5665">and</TOKEN>
<TOKEN end_char="5676" id="token-31-33" morph="none" pos="word" start_char="5669">analysis</TOKEN>
<TOKEN end_char="5677" id="token-31-34" morph="none" pos="punct" start_char="5677">.</TOKEN>
</SEG>
<SEG end_char="5749" id="segment-32" start_char="5679">
<ORIGINAL_TEXT>To make a personal or organisational contribution click here on PayPal.</ORIGINAL_TEXT>
<TOKEN end_char="5680" id="token-32-0" morph="none" pos="word" start_char="5679">To</TOKEN>
<TOKEN end_char="5685" id="token-32-1" morph="none" pos="word" start_char="5682">make</TOKEN>
<TOKEN end_char="5687" id="token-32-2" morph="none" pos="word" start_char="5687">a</TOKEN>
<TOKEN end_char="5696" id="token-32-3" morph="none" pos="word" start_char="5689">personal</TOKEN>
<TOKEN end_char="5699" id="token-32-4" morph="none" pos="word" start_char="5698">or</TOKEN>
<TOKEN end_char="5714" id="token-32-5" morph="none" pos="word" start_char="5701">organisational</TOKEN>
<TOKEN end_char="5727" id="token-32-6" morph="none" pos="word" start_char="5716">contribution</TOKEN>
<TOKEN end_char="5733" id="token-32-7" morph="none" pos="word" start_char="5729">click</TOKEN>
<TOKEN end_char="5738" id="token-32-8" morph="none" pos="word" start_char="5735">here</TOKEN>
<TOKEN end_char="5741" id="token-32-9" morph="none" pos="word" start_char="5740">on</TOKEN>
<TOKEN end_char="5748" id="token-32-10" morph="none" pos="word" start_char="5743">PayPal</TOKEN>
<TOKEN end_char="5749" id="token-32-11" morph="none" pos="punct" start_char="5749">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>