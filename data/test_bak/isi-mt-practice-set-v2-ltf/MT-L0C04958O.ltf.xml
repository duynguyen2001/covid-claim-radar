<LCTL_TEXT lang="rus">
<DOC grammar="none" id="L0C04958O" lang="rus" raw_text_char_length="12126" raw_text_md5="cef7beb8eec5915cedaea3679cd5eae8" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="52" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Experts know the new coronavirus is not a bioweapon.</ORIGINAL_TEXT>
<TOKEN end_char="7" id="token-0-0" morph="none" pos="word" start_char="1">Experts</TOKEN>
<TOKEN end_char="12" id="token-0-1" morph="none" pos="word" start_char="9">know</TOKEN>
<TOKEN end_char="16" id="token-0-2" morph="none" pos="word" start_char="14">the</TOKEN>
<TOKEN end_char="20" id="token-0-3" morph="none" pos="word" start_char="18">new</TOKEN>
<TOKEN end_char="32" id="token-0-4" morph="none" pos="word" start_char="22">coronavirus</TOKEN>
<TOKEN end_char="35" id="token-0-5" morph="none" pos="word" start_char="34">is</TOKEN>
<TOKEN end_char="39" id="token-0-6" morph="none" pos="word" start_char="37">not</TOKEN>
<TOKEN end_char="41" id="token-0-7" morph="none" pos="word" start_char="41">a</TOKEN>
<TOKEN end_char="51" id="token-0-8" morph="none" pos="word" start_char="43">bioweapon</TOKEN>
<TOKEN end_char="52" id="token-0-9" morph="none" pos="punct" start_char="52">.</TOKEN>
</SEG>
<SEG end_char="118" id="segment-1" start_char="54">
<ORIGINAL_TEXT>They disagree on whether it could have leaked from a research lab</ORIGINAL_TEXT>
<TOKEN end_char="57" id="token-1-0" morph="none" pos="word" start_char="54">They</TOKEN>
<TOKEN end_char="66" id="token-1-1" morph="none" pos="word" start_char="59">disagree</TOKEN>
<TOKEN end_char="69" id="token-1-2" morph="none" pos="word" start_char="68">on</TOKEN>
<TOKEN end_char="77" id="token-1-3" morph="none" pos="word" start_char="71">whether</TOKEN>
<TOKEN end_char="80" id="token-1-4" morph="none" pos="word" start_char="79">it</TOKEN>
<TOKEN end_char="86" id="token-1-5" morph="none" pos="word" start_char="82">could</TOKEN>
<TOKEN end_char="91" id="token-1-6" morph="none" pos="word" start_char="88">have</TOKEN>
<TOKEN end_char="98" id="token-1-7" morph="none" pos="word" start_char="93">leaked</TOKEN>
<TOKEN end_char="103" id="token-1-8" morph="none" pos="word" start_char="100">from</TOKEN>
<TOKEN end_char="105" id="token-1-9" morph="none" pos="word" start_char="105">a</TOKEN>
<TOKEN end_char="114" id="token-1-10" morph="none" pos="word" start_char="107">research</TOKEN>
<TOKEN end_char="118" id="token-1-11" morph="none" pos="word" start_char="116">lab</TOKEN>
</SEG>
<SEG end_char="206" id="segment-2" start_char="122">
<ORIGINAL_TEXT>I agree with Ebright that one shouldn’t rule out a laboratory accident as the source.</ORIGINAL_TEXT>
<TOKEN end_char="122" id="token-2-0" morph="none" pos="word" start_char="122">I</TOKEN>
<TOKEN end_char="128" id="token-2-1" morph="none" pos="word" start_char="124">agree</TOKEN>
<TOKEN end_char="133" id="token-2-2" morph="none" pos="word" start_char="130">with</TOKEN>
<TOKEN end_char="141" id="token-2-3" morph="none" pos="word" start_char="135">Ebright</TOKEN>
<TOKEN end_char="146" id="token-2-4" morph="none" pos="word" start_char="143">that</TOKEN>
<TOKEN end_char="150" id="token-2-5" morph="none" pos="word" start_char="148">one</TOKEN>
<TOKEN end_char="160" id="token-2-6" morph="none" pos="word" start_char="152">shouldn’t</TOKEN>
<TOKEN end_char="165" id="token-2-7" morph="none" pos="word" start_char="162">rule</TOKEN>
<TOKEN end_char="169" id="token-2-8" morph="none" pos="word" start_char="167">out</TOKEN>
<TOKEN end_char="171" id="token-2-9" morph="none" pos="word" start_char="171">a</TOKEN>
<TOKEN end_char="182" id="token-2-10" morph="none" pos="word" start_char="173">laboratory</TOKEN>
<TOKEN end_char="191" id="token-2-11" morph="none" pos="word" start_char="184">accident</TOKEN>
<TOKEN end_char="194" id="token-2-12" morph="none" pos="word" start_char="193">as</TOKEN>
<TOKEN end_char="198" id="token-2-13" morph="none" pos="word" start_char="196">the</TOKEN>
<TOKEN end_char="205" id="token-2-14" morph="none" pos="word" start_char="200">source</TOKEN>
<TOKEN end_char="206" id="token-2-15" morph="none" pos="punct" start_char="206">.</TOKEN>
</SEG>
<SEG end_char="416" id="segment-3" start_char="208">
<ORIGINAL_TEXT>The Nature opinion piece authors are right that the unusual molecular structure of the virus and the randomness in the mutations from the HKU3-1 bat virus suggest no purposeful manipulation for bioweapons use.</ORIGINAL_TEXT>
<TOKEN end_char="210" id="token-3-0" morph="none" pos="word" start_char="208">The</TOKEN>
<TOKEN end_char="217" id="token-3-1" morph="none" pos="word" start_char="212">Nature</TOKEN>
<TOKEN end_char="225" id="token-3-2" morph="none" pos="word" start_char="219">opinion</TOKEN>
<TOKEN end_char="231" id="token-3-3" morph="none" pos="word" start_char="227">piece</TOKEN>
<TOKEN end_char="239" id="token-3-4" morph="none" pos="word" start_char="233">authors</TOKEN>
<TOKEN end_char="243" id="token-3-5" morph="none" pos="word" start_char="241">are</TOKEN>
<TOKEN end_char="249" id="token-3-6" morph="none" pos="word" start_char="245">right</TOKEN>
<TOKEN end_char="254" id="token-3-7" morph="none" pos="word" start_char="251">that</TOKEN>
<TOKEN end_char="258" id="token-3-8" morph="none" pos="word" start_char="256">the</TOKEN>
<TOKEN end_char="266" id="token-3-9" morph="none" pos="word" start_char="260">unusual</TOKEN>
<TOKEN end_char="276" id="token-3-10" morph="none" pos="word" start_char="268">molecular</TOKEN>
<TOKEN end_char="286" id="token-3-11" morph="none" pos="word" start_char="278">structure</TOKEN>
<TOKEN end_char="289" id="token-3-12" morph="none" pos="word" start_char="288">of</TOKEN>
<TOKEN end_char="293" id="token-3-13" morph="none" pos="word" start_char="291">the</TOKEN>
<TOKEN end_char="299" id="token-3-14" morph="none" pos="word" start_char="295">virus</TOKEN>
<TOKEN end_char="303" id="token-3-15" morph="none" pos="word" start_char="301">and</TOKEN>
<TOKEN end_char="307" id="token-3-16" morph="none" pos="word" start_char="305">the</TOKEN>
<TOKEN end_char="318" id="token-3-17" morph="none" pos="word" start_char="309">randomness</TOKEN>
<TOKEN end_char="321" id="token-3-18" morph="none" pos="word" start_char="320">in</TOKEN>
<TOKEN end_char="325" id="token-3-19" morph="none" pos="word" start_char="323">the</TOKEN>
<TOKEN end_char="335" id="token-3-20" morph="none" pos="word" start_char="327">mutations</TOKEN>
<TOKEN end_char="340" id="token-3-21" morph="none" pos="word" start_char="337">from</TOKEN>
<TOKEN end_char="344" id="token-3-22" morph="none" pos="word" start_char="342">the</TOKEN>
<TOKEN end_char="351" id="token-3-23" morph="none" pos="unknown" start_char="346">HKU3-1</TOKEN>
<TOKEN end_char="355" id="token-3-24" morph="none" pos="word" start_char="353">bat</TOKEN>
<TOKEN end_char="361" id="token-3-25" morph="none" pos="word" start_char="357">virus</TOKEN>
<TOKEN end_char="369" id="token-3-26" morph="none" pos="word" start_char="363">suggest</TOKEN>
<TOKEN end_char="372" id="token-3-27" morph="none" pos="word" start_char="371">no</TOKEN>
<TOKEN end_char="383" id="token-3-28" morph="none" pos="word" start_char="374">purposeful</TOKEN>
<TOKEN end_char="396" id="token-3-29" morph="none" pos="word" start_char="385">manipulation</TOKEN>
<TOKEN end_char="400" id="token-3-30" morph="none" pos="word" start_char="398">for</TOKEN>
<TOKEN end_char="411" id="token-3-31" morph="none" pos="word" start_char="402">bioweapons</TOKEN>
<TOKEN end_char="415" id="token-3-32" morph="none" pos="word" start_char="413">use</TOKEN>
<TOKEN end_char="416" id="token-3-33" morph="none" pos="punct" start_char="416">.</TOKEN>
</SEG>
<SEG end_char="498" id="segment-4" start_char="418">
<ORIGINAL_TEXT>However, an accident in a disease transmission experiment shouldn’t be dismissed.</ORIGINAL_TEXT>
<TOKEN end_char="424" id="token-4-0" morph="none" pos="word" start_char="418">However</TOKEN>
<TOKEN end_char="425" id="token-4-1" morph="none" pos="punct" start_char="425">,</TOKEN>
<TOKEN end_char="428" id="token-4-2" morph="none" pos="word" start_char="427">an</TOKEN>
<TOKEN end_char="437" id="token-4-3" morph="none" pos="word" start_char="430">accident</TOKEN>
<TOKEN end_char="440" id="token-4-4" morph="none" pos="word" start_char="439">in</TOKEN>
<TOKEN end_char="442" id="token-4-5" morph="none" pos="word" start_char="442">a</TOKEN>
<TOKEN end_char="450" id="token-4-6" morph="none" pos="word" start_char="444">disease</TOKEN>
<TOKEN end_char="463" id="token-4-7" morph="none" pos="word" start_char="452">transmission</TOKEN>
<TOKEN end_char="474" id="token-4-8" morph="none" pos="word" start_char="465">experiment</TOKEN>
<TOKEN end_char="484" id="token-4-9" morph="none" pos="word" start_char="476">shouldn’t</TOKEN>
<TOKEN end_char="487" id="token-4-10" morph="none" pos="word" start_char="486">be</TOKEN>
<TOKEN end_char="497" id="token-4-11" morph="none" pos="word" start_char="489">dismissed</TOKEN>
<TOKEN end_char="498" id="token-4-12" morph="none" pos="punct" start_char="498">.</TOKEN>
</SEG>
<SEG end_char="670" id="segment-5" start_char="500">
<ORIGINAL_TEXT>The Wuhan lab has been conducting experiments for almost 2 decades to understand how the 2003 SARS-CoV jumped to humans and to identify the natural reservoir of the virus.</ORIGINAL_TEXT>
<TOKEN end_char="502" id="token-5-0" morph="none" pos="word" start_char="500">The</TOKEN>
<TOKEN end_char="508" id="token-5-1" morph="none" pos="word" start_char="504">Wuhan</TOKEN>
<TOKEN end_char="512" id="token-5-2" morph="none" pos="word" start_char="510">lab</TOKEN>
<TOKEN end_char="516" id="token-5-3" morph="none" pos="word" start_char="514">has</TOKEN>
<TOKEN end_char="521" id="token-5-4" morph="none" pos="word" start_char="518">been</TOKEN>
<TOKEN end_char="532" id="token-5-5" morph="none" pos="word" start_char="523">conducting</TOKEN>
<TOKEN end_char="544" id="token-5-6" morph="none" pos="word" start_char="534">experiments</TOKEN>
<TOKEN end_char="548" id="token-5-7" morph="none" pos="word" start_char="546">for</TOKEN>
<TOKEN end_char="555" id="token-5-8" morph="none" pos="word" start_char="550">almost</TOKEN>
<TOKEN end_char="557" id="token-5-9" morph="none" pos="word" start_char="557">2</TOKEN>
<TOKEN end_char="565" id="token-5-10" morph="none" pos="word" start_char="559">decades</TOKEN>
<TOKEN end_char="568" id="token-5-11" morph="none" pos="word" start_char="567">to</TOKEN>
<TOKEN end_char="579" id="token-5-12" morph="none" pos="word" start_char="570">understand</TOKEN>
<TOKEN end_char="583" id="token-5-13" morph="none" pos="word" start_char="581">how</TOKEN>
<TOKEN end_char="587" id="token-5-14" morph="none" pos="word" start_char="585">the</TOKEN>
<TOKEN end_char="592" id="token-5-15" morph="none" pos="word" start_char="589">2003</TOKEN>
<TOKEN end_char="601" id="token-5-16" morph="none" pos="unknown" start_char="594">SARS-CoV</TOKEN>
<TOKEN end_char="608" id="token-5-17" morph="none" pos="word" start_char="603">jumped</TOKEN>
<TOKEN end_char="611" id="token-5-18" morph="none" pos="word" start_char="610">to</TOKEN>
<TOKEN end_char="618" id="token-5-19" morph="none" pos="word" start_char="613">humans</TOKEN>
<TOKEN end_char="622" id="token-5-20" morph="none" pos="word" start_char="620">and</TOKEN>
<TOKEN end_char="625" id="token-5-21" morph="none" pos="word" start_char="624">to</TOKEN>
<TOKEN end_char="634" id="token-5-22" morph="none" pos="word" start_char="627">identify</TOKEN>
<TOKEN end_char="638" id="token-5-23" morph="none" pos="word" start_char="636">the</TOKEN>
<TOKEN end_char="646" id="token-5-24" morph="none" pos="word" start_char="640">natural</TOKEN>
<TOKEN end_char="656" id="token-5-25" morph="none" pos="word" start_char="648">reservoir</TOKEN>
<TOKEN end_char="659" id="token-5-26" morph="none" pos="word" start_char="658">of</TOKEN>
<TOKEN end_char="663" id="token-5-27" morph="none" pos="word" start_char="661">the</TOKEN>
<TOKEN end_char="669" id="token-5-28" morph="none" pos="word" start_char="665">virus</TOKEN>
<TOKEN end_char="670" id="token-5-29" morph="none" pos="punct" start_char="670">.</TOKEN>
</SEG>
<SEG end_char="741" id="segment-6" start_char="672">
<ORIGINAL_TEXT>Bat coronaviruses typically bond to bat ACE2 and rarely to human ACE2.</ORIGINAL_TEXT>
<TOKEN end_char="674" id="token-6-0" morph="none" pos="word" start_char="672">Bat</TOKEN>
<TOKEN end_char="688" id="token-6-1" morph="none" pos="word" start_char="676">coronaviruses</TOKEN>
<TOKEN end_char="698" id="token-6-2" morph="none" pos="word" start_char="690">typically</TOKEN>
<TOKEN end_char="703" id="token-6-3" morph="none" pos="word" start_char="700">bond</TOKEN>
<TOKEN end_char="706" id="token-6-4" morph="none" pos="word" start_char="705">to</TOKEN>
<TOKEN end_char="710" id="token-6-5" morph="none" pos="word" start_char="708">bat</TOKEN>
<TOKEN end_char="715" id="token-6-6" morph="none" pos="word" start_char="712">ACE2</TOKEN>
<TOKEN end_char="719" id="token-6-7" morph="none" pos="word" start_char="717">and</TOKEN>
<TOKEN end_char="726" id="token-6-8" morph="none" pos="word" start_char="721">rarely</TOKEN>
<TOKEN end_char="729" id="token-6-9" morph="none" pos="word" start_char="728">to</TOKEN>
<TOKEN end_char="735" id="token-6-10" morph="none" pos="word" start_char="731">human</TOKEN>
<TOKEN end_char="740" id="token-6-11" morph="none" pos="word" start_char="737">ACE2</TOKEN>
<TOKEN end_char="741" id="token-6-12" morph="none" pos="punct" start_char="741">.</TOKEN>
</SEG>
<SEG end_char="755" id="segment-7" start_char="743">
<ORIGINAL_TEXT>… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="743" id="token-7-0" morph="none" pos="punct" start_char="743">…</TOKEN>
<TOKEN end_char="748" id="token-7-1" morph="none" pos="word" start_char="745">Read</TOKEN>
<TOKEN end_char="753" id="token-7-2" morph="none" pos="word" start_char="750">more</TOKEN>
<TOKEN end_char="755" id="token-7-3" morph="none" pos="punct" start_char="755">»</TOKEN>
</SEG>
<SEG end_char="796" id="segment-8" start_char="759">
<ORIGINAL_TEXT>Correction: I meant RaTG13 not HKU3-1.</ORIGINAL_TEXT>
<TOKEN end_char="768" id="token-8-0" morph="none" pos="word" start_char="759">Correction</TOKEN>
<TOKEN end_char="769" id="token-8-1" morph="none" pos="punct" start_char="769">:</TOKEN>
<TOKEN end_char="771" id="token-8-2" morph="none" pos="word" start_char="771">I</TOKEN>
<TOKEN end_char="777" id="token-8-3" morph="none" pos="word" start_char="773">meant</TOKEN>
<TOKEN end_char="784" id="token-8-4" morph="none" pos="word" start_char="779">RaTG13</TOKEN>
<TOKEN end_char="788" id="token-8-5" morph="none" pos="word" start_char="786">not</TOKEN>
<TOKEN end_char="795" id="token-8-6" morph="none" pos="unknown" start_char="790">HKU3-1</TOKEN>
<TOKEN end_char="796" id="token-8-7" morph="none" pos="punct" start_char="796">.</TOKEN>
</SEG>
<SEG end_char="832" id="segment-9" start_char="798">
<ORIGINAL_TEXT>I misread the legend on the figure.</ORIGINAL_TEXT>
<TOKEN end_char="798" id="token-9-0" morph="none" pos="word" start_char="798">I</TOKEN>
<TOKEN end_char="806" id="token-9-1" morph="none" pos="word" start_char="800">misread</TOKEN>
<TOKEN end_char="810" id="token-9-2" morph="none" pos="word" start_char="808">the</TOKEN>
<TOKEN end_char="817" id="token-9-3" morph="none" pos="word" start_char="812">legend</TOKEN>
<TOKEN end_char="820" id="token-9-4" morph="none" pos="word" start_char="819">on</TOKEN>
<TOKEN end_char="824" id="token-9-5" morph="none" pos="word" start_char="822">the</TOKEN>
<TOKEN end_char="831" id="token-9-6" morph="none" pos="word" start_char="826">figure</TOKEN>
<TOKEN end_char="832" id="token-9-7" morph="none" pos="punct" start_char="832">.</TOKEN>
</SEG>
<SEG end_char="936" id="segment-10" start_char="836">
<ORIGINAL_TEXT>What advantage might a deliberate leaker have anticipated –therefore why would any sane person do it?</ORIGINAL_TEXT>
<TOKEN end_char="839" id="token-10-0" morph="none" pos="word" start_char="836">What</TOKEN>
<TOKEN end_char="849" id="token-10-1" morph="none" pos="word" start_char="841">advantage</TOKEN>
<TOKEN end_char="855" id="token-10-2" morph="none" pos="word" start_char="851">might</TOKEN>
<TOKEN end_char="857" id="token-10-3" morph="none" pos="word" start_char="857">a</TOKEN>
<TOKEN end_char="868" id="token-10-4" morph="none" pos="word" start_char="859">deliberate</TOKEN>
<TOKEN end_char="875" id="token-10-5" morph="none" pos="word" start_char="870">leaker</TOKEN>
<TOKEN end_char="880" id="token-10-6" morph="none" pos="word" start_char="877">have</TOKEN>
<TOKEN end_char="892" id="token-10-7" morph="none" pos="word" start_char="882">anticipated</TOKEN>
<TOKEN end_char="894" id="token-10-8" morph="none" pos="punct" start_char="894">–</TOKEN>
<TOKEN end_char="903" id="token-10-9" morph="none" pos="word" start_char="895">therefore</TOKEN>
<TOKEN end_char="907" id="token-10-10" morph="none" pos="word" start_char="905">why</TOKEN>
<TOKEN end_char="913" id="token-10-11" morph="none" pos="word" start_char="909">would</TOKEN>
<TOKEN end_char="917" id="token-10-12" morph="none" pos="word" start_char="915">any</TOKEN>
<TOKEN end_char="922" id="token-10-13" morph="none" pos="word" start_char="919">sane</TOKEN>
<TOKEN end_char="929" id="token-10-14" morph="none" pos="word" start_char="924">person</TOKEN>
<TOKEN end_char="932" id="token-10-15" morph="none" pos="word" start_char="931">do</TOKEN>
<TOKEN end_char="935" id="token-10-16" morph="none" pos="word" start_char="934">it</TOKEN>
<TOKEN end_char="936" id="token-10-17" morph="none" pos="punct" start_char="936">?</TOKEN>
</SEG>
<SEG end_char="1045" id="segment-11" start_char="940">
<ORIGINAL_TEXT>We know the old dictum: Don’t attribute to malevolence that which can be explained by simple incompetence.</ORIGINAL_TEXT>
<TOKEN end_char="941" id="token-11-0" morph="none" pos="word" start_char="940">We</TOKEN>
<TOKEN end_char="946" id="token-11-1" morph="none" pos="word" start_char="943">know</TOKEN>
<TOKEN end_char="950" id="token-11-2" morph="none" pos="word" start_char="948">the</TOKEN>
<TOKEN end_char="954" id="token-11-3" morph="none" pos="word" start_char="952">old</TOKEN>
<TOKEN end_char="961" id="token-11-4" morph="none" pos="word" start_char="956">dictum</TOKEN>
<TOKEN end_char="962" id="token-11-5" morph="none" pos="punct" start_char="962">:</TOKEN>
<TOKEN end_char="968" id="token-11-6" morph="none" pos="word" start_char="964">Don’t</TOKEN>
<TOKEN end_char="978" id="token-11-7" morph="none" pos="word" start_char="970">attribute</TOKEN>
<TOKEN end_char="981" id="token-11-8" morph="none" pos="word" start_char="980">to</TOKEN>
<TOKEN end_char="993" id="token-11-9" morph="none" pos="word" start_char="983">malevolence</TOKEN>
<TOKEN end_char="998" id="token-11-10" morph="none" pos="word" start_char="995">that</TOKEN>
<TOKEN end_char="1004" id="token-11-11" morph="none" pos="word" start_char="1000">which</TOKEN>
<TOKEN end_char="1008" id="token-11-12" morph="none" pos="word" start_char="1006">can</TOKEN>
<TOKEN end_char="1011" id="token-11-13" morph="none" pos="word" start_char="1010">be</TOKEN>
<TOKEN end_char="1021" id="token-11-14" morph="none" pos="word" start_char="1013">explained</TOKEN>
<TOKEN end_char="1024" id="token-11-15" morph="none" pos="word" start_char="1023">by</TOKEN>
<TOKEN end_char="1031" id="token-11-16" morph="none" pos="word" start_char="1026">simple</TOKEN>
<TOKEN end_char="1044" id="token-11-17" morph="none" pos="word" start_char="1033">incompetence</TOKEN>
<TOKEN end_char="1045" id="token-11-18" morph="none" pos="punct" start_char="1045">.</TOKEN>
</SEG>
<SEG end_char="1171" id="segment-12" start_char="1047">
<ORIGINAL_TEXT>All it takes is an accidental spill, a torn shoe cover, a configuration error in the HVAC system, and we’re off to the races.</ORIGINAL_TEXT>
<TOKEN end_char="1049" id="token-12-0" morph="none" pos="word" start_char="1047">All</TOKEN>
<TOKEN end_char="1052" id="token-12-1" morph="none" pos="word" start_char="1051">it</TOKEN>
<TOKEN end_char="1058" id="token-12-2" morph="none" pos="word" start_char="1054">takes</TOKEN>
<TOKEN end_char="1061" id="token-12-3" morph="none" pos="word" start_char="1060">is</TOKEN>
<TOKEN end_char="1064" id="token-12-4" morph="none" pos="word" start_char="1063">an</TOKEN>
<TOKEN end_char="1075" id="token-12-5" morph="none" pos="word" start_char="1066">accidental</TOKEN>
<TOKEN end_char="1081" id="token-12-6" morph="none" pos="word" start_char="1077">spill</TOKEN>
<TOKEN end_char="1082" id="token-12-7" morph="none" pos="punct" start_char="1082">,</TOKEN>
<TOKEN end_char="1084" id="token-12-8" morph="none" pos="word" start_char="1084">a</TOKEN>
<TOKEN end_char="1089" id="token-12-9" morph="none" pos="word" start_char="1086">torn</TOKEN>
<TOKEN end_char="1094" id="token-12-10" morph="none" pos="word" start_char="1091">shoe</TOKEN>
<TOKEN end_char="1100" id="token-12-11" morph="none" pos="word" start_char="1096">cover</TOKEN>
<TOKEN end_char="1101" id="token-12-12" morph="none" pos="punct" start_char="1101">,</TOKEN>
<TOKEN end_char="1103" id="token-12-13" morph="none" pos="word" start_char="1103">a</TOKEN>
<TOKEN end_char="1117" id="token-12-14" morph="none" pos="word" start_char="1105">configuration</TOKEN>
<TOKEN end_char="1123" id="token-12-15" morph="none" pos="word" start_char="1119">error</TOKEN>
<TOKEN end_char="1126" id="token-12-16" morph="none" pos="word" start_char="1125">in</TOKEN>
<TOKEN end_char="1130" id="token-12-17" morph="none" pos="word" start_char="1128">the</TOKEN>
<TOKEN end_char="1135" id="token-12-18" morph="none" pos="word" start_char="1132">HVAC</TOKEN>
<TOKEN end_char="1142" id="token-12-19" morph="none" pos="word" start_char="1137">system</TOKEN>
<TOKEN end_char="1143" id="token-12-20" morph="none" pos="punct" start_char="1143">,</TOKEN>
<TOKEN end_char="1147" id="token-12-21" morph="none" pos="word" start_char="1145">and</TOKEN>
<TOKEN end_char="1153" id="token-12-22" morph="none" pos="word" start_char="1149">we’re</TOKEN>
<TOKEN end_char="1157" id="token-12-23" morph="none" pos="word" start_char="1155">off</TOKEN>
<TOKEN end_char="1160" id="token-12-24" morph="none" pos="word" start_char="1159">to</TOKEN>
<TOKEN end_char="1164" id="token-12-25" morph="none" pos="word" start_char="1162">the</TOKEN>
<TOKEN end_char="1170" id="token-12-26" morph="none" pos="word" start_char="1166">races</TOKEN>
<TOKEN end_char="1171" id="token-12-27" morph="none" pos="punct" start_char="1171">.</TOKEN>
</SEG>
<SEG end_char="1236" id="segment-13" start_char="1173">
<ORIGINAL_TEXT>Containment, location, and monitoring need a lot more attention.</ORIGINAL_TEXT>
<TOKEN end_char="1183" id="token-13-0" morph="none" pos="word" start_char="1173">Containment</TOKEN>
<TOKEN end_char="1184" id="token-13-1" morph="none" pos="punct" start_char="1184">,</TOKEN>
<TOKEN end_char="1193" id="token-13-2" morph="none" pos="word" start_char="1186">location</TOKEN>
<TOKEN end_char="1194" id="token-13-3" morph="none" pos="punct" start_char="1194">,</TOKEN>
<TOKEN end_char="1198" id="token-13-4" morph="none" pos="word" start_char="1196">and</TOKEN>
<TOKEN end_char="1209" id="token-13-5" morph="none" pos="word" start_char="1200">monitoring</TOKEN>
<TOKEN end_char="1214" id="token-13-6" morph="none" pos="word" start_char="1211">need</TOKEN>
<TOKEN end_char="1216" id="token-13-7" morph="none" pos="word" start_char="1216">a</TOKEN>
<TOKEN end_char="1220" id="token-13-8" morph="none" pos="word" start_char="1218">lot</TOKEN>
<TOKEN end_char="1225" id="token-13-9" morph="none" pos="word" start_char="1222">more</TOKEN>
<TOKEN end_char="1235" id="token-13-10" morph="none" pos="word" start_char="1227">attention</TOKEN>
<TOKEN end_char="1236" id="token-13-11" morph="none" pos="punct" start_char="1236">.</TOKEN>
</SEG>
<SEG end_char="1289" id="segment-14" start_char="1240">
<ORIGINAL_TEXT>No one said it would be a sane person or group.. .</ORIGINAL_TEXT>
<TOKEN end_char="1241" id="token-14-0" morph="none" pos="word" start_char="1240">No</TOKEN>
<TOKEN end_char="1245" id="token-14-1" morph="none" pos="word" start_char="1243">one</TOKEN>
<TOKEN end_char="1250" id="token-14-2" morph="none" pos="word" start_char="1247">said</TOKEN>
<TOKEN end_char="1253" id="token-14-3" morph="none" pos="word" start_char="1252">it</TOKEN>
<TOKEN end_char="1259" id="token-14-4" morph="none" pos="word" start_char="1255">would</TOKEN>
<TOKEN end_char="1262" id="token-14-5" morph="none" pos="word" start_char="1261">be</TOKEN>
<TOKEN end_char="1264" id="token-14-6" morph="none" pos="word" start_char="1264">a</TOKEN>
<TOKEN end_char="1269" id="token-14-7" morph="none" pos="word" start_char="1266">sane</TOKEN>
<TOKEN end_char="1276" id="token-14-8" morph="none" pos="word" start_char="1271">person</TOKEN>
<TOKEN end_char="1279" id="token-14-9" morph="none" pos="word" start_char="1278">or</TOKEN>
<TOKEN end_char="1285" id="token-14-10" morph="none" pos="word" start_char="1281">group</TOKEN>
<TOKEN end_char="1287" id="token-14-11" morph="none" pos="punct" start_char="1286">..</TOKEN>
<TOKEN end_char="1289" id="token-14-12" morph="none" pos="punct" start_char="1289">.</TOKEN>
</SEG>
<SEG end_char="1820" id="segment-15" start_char="1291">
<ORIGINAL_TEXT>but some think person/group/whatever would want to kill off a huge population of global humans and initiate economic disaster, globally to take over the world, over something that for U.S. figures is causing at this point 30x less at minimum projections to 60X less at high end projections of the yearly Influenza cases, which this year is projected at 39,000,000 to 56,000,000 in U.S. compared to 532,879 for Covid 19 and at possible 1/3 of the deaths 24,000 to 62,000 of the regular influenza this year, compared to… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="1293" id="token-15-0" morph="none" pos="word" start_char="1291">but</TOKEN>
<TOKEN end_char="1298" id="token-15-1" morph="none" pos="word" start_char="1295">some</TOKEN>
<TOKEN end_char="1304" id="token-15-2" morph="none" pos="word" start_char="1300">think</TOKEN>
<TOKEN end_char="1326" id="token-15-3" morph="none" pos="unknown" start_char="1306">person/group/whatever</TOKEN>
<TOKEN end_char="1332" id="token-15-4" morph="none" pos="word" start_char="1328">would</TOKEN>
<TOKEN end_char="1337" id="token-15-5" morph="none" pos="word" start_char="1334">want</TOKEN>
<TOKEN end_char="1340" id="token-15-6" morph="none" pos="word" start_char="1339">to</TOKEN>
<TOKEN end_char="1345" id="token-15-7" morph="none" pos="word" start_char="1342">kill</TOKEN>
<TOKEN end_char="1349" id="token-15-8" morph="none" pos="word" start_char="1347">off</TOKEN>
<TOKEN end_char="1351" id="token-15-9" morph="none" pos="word" start_char="1351">a</TOKEN>
<TOKEN end_char="1356" id="token-15-10" morph="none" pos="word" start_char="1353">huge</TOKEN>
<TOKEN end_char="1367" id="token-15-11" morph="none" pos="word" start_char="1358">population</TOKEN>
<TOKEN end_char="1370" id="token-15-12" morph="none" pos="word" start_char="1369">of</TOKEN>
<TOKEN end_char="1377" id="token-15-13" morph="none" pos="word" start_char="1372">global</TOKEN>
<TOKEN end_char="1384" id="token-15-14" morph="none" pos="word" start_char="1379">humans</TOKEN>
<TOKEN end_char="1388" id="token-15-15" morph="none" pos="word" start_char="1386">and</TOKEN>
<TOKEN end_char="1397" id="token-15-16" morph="none" pos="word" start_char="1390">initiate</TOKEN>
<TOKEN end_char="1406" id="token-15-17" morph="none" pos="word" start_char="1399">economic</TOKEN>
<TOKEN end_char="1415" id="token-15-18" morph="none" pos="word" start_char="1408">disaster</TOKEN>
<TOKEN end_char="1416" id="token-15-19" morph="none" pos="punct" start_char="1416">,</TOKEN>
<TOKEN end_char="1425" id="token-15-20" morph="none" pos="word" start_char="1418">globally</TOKEN>
<TOKEN end_char="1428" id="token-15-21" morph="none" pos="word" start_char="1427">to</TOKEN>
<TOKEN end_char="1433" id="token-15-22" morph="none" pos="word" start_char="1430">take</TOKEN>
<TOKEN end_char="1438" id="token-15-23" morph="none" pos="word" start_char="1435">over</TOKEN>
<TOKEN end_char="1442" id="token-15-24" morph="none" pos="word" start_char="1440">the</TOKEN>
<TOKEN end_char="1448" id="token-15-25" morph="none" pos="word" start_char="1444">world</TOKEN>
<TOKEN end_char="1449" id="token-15-26" morph="none" pos="punct" start_char="1449">,</TOKEN>
<TOKEN end_char="1454" id="token-15-27" morph="none" pos="word" start_char="1451">over</TOKEN>
<TOKEN end_char="1464" id="token-15-28" morph="none" pos="word" start_char="1456">something</TOKEN>
<TOKEN end_char="1469" id="token-15-29" morph="none" pos="word" start_char="1466">that</TOKEN>
<TOKEN end_char="1473" id="token-15-30" morph="none" pos="word" start_char="1471">for</TOKEN>
<TOKEN end_char="1477" id="token-15-31" morph="none" pos="unknown" start_char="1475">U.S</TOKEN>
<TOKEN end_char="1478" id="token-15-32" morph="none" pos="punct" start_char="1478">.</TOKEN>
<TOKEN end_char="1486" id="token-15-33" morph="none" pos="word" start_char="1480">figures</TOKEN>
<TOKEN end_char="1489" id="token-15-34" morph="none" pos="word" start_char="1488">is</TOKEN>
<TOKEN end_char="1497" id="token-15-35" morph="none" pos="word" start_char="1491">causing</TOKEN>
<TOKEN end_char="1500" id="token-15-36" morph="none" pos="word" start_char="1499">at</TOKEN>
<TOKEN end_char="1505" id="token-15-37" morph="none" pos="word" start_char="1502">this</TOKEN>
<TOKEN end_char="1511" id="token-15-38" morph="none" pos="word" start_char="1507">point</TOKEN>
<TOKEN end_char="1515" id="token-15-39" morph="none" pos="word" start_char="1513">30x</TOKEN>
<TOKEN end_char="1520" id="token-15-40" morph="none" pos="word" start_char="1517">less</TOKEN>
<TOKEN end_char="1523" id="token-15-41" morph="none" pos="word" start_char="1522">at</TOKEN>
<TOKEN end_char="1531" id="token-15-42" morph="none" pos="word" start_char="1525">minimum</TOKEN>
<TOKEN end_char="1543" id="token-15-43" morph="none" pos="word" start_char="1533">projections</TOKEN>
<TOKEN end_char="1546" id="token-15-44" morph="none" pos="word" start_char="1545">to</TOKEN>
<TOKEN end_char="1550" id="token-15-45" morph="none" pos="word" start_char="1548">60X</TOKEN>
<TOKEN end_char="1555" id="token-15-46" morph="none" pos="word" start_char="1552">less</TOKEN>
<TOKEN end_char="1558" id="token-15-47" morph="none" pos="word" start_char="1557">at</TOKEN>
<TOKEN end_char="1563" id="token-15-48" morph="none" pos="word" start_char="1560">high</TOKEN>
<TOKEN end_char="1567" id="token-15-49" morph="none" pos="word" start_char="1565">end</TOKEN>
<TOKEN end_char="1579" id="token-15-50" morph="none" pos="word" start_char="1569">projections</TOKEN>
<TOKEN end_char="1582" id="token-15-51" morph="none" pos="word" start_char="1581">of</TOKEN>
<TOKEN end_char="1586" id="token-15-52" morph="none" pos="word" start_char="1584">the</TOKEN>
<TOKEN end_char="1593" id="token-15-53" morph="none" pos="word" start_char="1588">yearly</TOKEN>
<TOKEN end_char="1603" id="token-15-54" morph="none" pos="word" start_char="1595">Influenza</TOKEN>
<TOKEN end_char="1609" id="token-15-55" morph="none" pos="word" start_char="1605">cases</TOKEN>
<TOKEN end_char="1610" id="token-15-56" morph="none" pos="punct" start_char="1610">,</TOKEN>
<TOKEN end_char="1616" id="token-15-57" morph="none" pos="word" start_char="1612">which</TOKEN>
<TOKEN end_char="1621" id="token-15-58" morph="none" pos="word" start_char="1618">this</TOKEN>
<TOKEN end_char="1626" id="token-15-59" morph="none" pos="word" start_char="1623">year</TOKEN>
<TOKEN end_char="1629" id="token-15-60" morph="none" pos="word" start_char="1628">is</TOKEN>
<TOKEN end_char="1639" id="token-15-61" morph="none" pos="word" start_char="1631">projected</TOKEN>
<TOKEN end_char="1642" id="token-15-62" morph="none" pos="word" start_char="1641">at</TOKEN>
<TOKEN end_char="1653" id="token-15-63" morph="none" pos="unknown" start_char="1644">39,000,000</TOKEN>
<TOKEN end_char="1656" id="token-15-64" morph="none" pos="word" start_char="1655">to</TOKEN>
<TOKEN end_char="1667" id="token-15-65" morph="none" pos="unknown" start_char="1658">56,000,000</TOKEN>
<TOKEN end_char="1670" id="token-15-66" morph="none" pos="word" start_char="1669">in</TOKEN>
<TOKEN end_char="1674" id="token-15-67" morph="none" pos="unknown" start_char="1672">U.S</TOKEN>
<TOKEN end_char="1675" id="token-15-68" morph="none" pos="punct" start_char="1675">.</TOKEN>
<TOKEN end_char="1684" id="token-15-69" morph="none" pos="word" start_char="1677">compared</TOKEN>
<TOKEN end_char="1687" id="token-15-70" morph="none" pos="word" start_char="1686">to</TOKEN>
<TOKEN end_char="1695" id="token-15-71" morph="none" pos="unknown" start_char="1689">532,879</TOKEN>
<TOKEN end_char="1699" id="token-15-72" morph="none" pos="word" start_char="1697">for</TOKEN>
<TOKEN end_char="1705" id="token-15-73" morph="none" pos="word" start_char="1701">Covid</TOKEN>
<TOKEN end_char="1708" id="token-15-74" morph="none" pos="word" start_char="1707">19</TOKEN>
<TOKEN end_char="1712" id="token-15-75" morph="none" pos="word" start_char="1710">and</TOKEN>
<TOKEN end_char="1715" id="token-15-76" morph="none" pos="word" start_char="1714">at</TOKEN>
<TOKEN end_char="1724" id="token-15-77" morph="none" pos="word" start_char="1717">possible</TOKEN>
<TOKEN end_char="1728" id="token-15-78" morph="none" pos="unknown" start_char="1726">1/3</TOKEN>
<TOKEN end_char="1731" id="token-15-79" morph="none" pos="word" start_char="1730">of</TOKEN>
<TOKEN end_char="1735" id="token-15-80" morph="none" pos="word" start_char="1733">the</TOKEN>
<TOKEN end_char="1742" id="token-15-81" morph="none" pos="word" start_char="1737">deaths</TOKEN>
<TOKEN end_char="1749" id="token-15-82" morph="none" pos="unknown" start_char="1744">24,000</TOKEN>
<TOKEN end_char="1752" id="token-15-83" morph="none" pos="word" start_char="1751">to</TOKEN>
<TOKEN end_char="1759" id="token-15-84" morph="none" pos="unknown" start_char="1754">62,000</TOKEN>
<TOKEN end_char="1762" id="token-15-85" morph="none" pos="word" start_char="1761">of</TOKEN>
<TOKEN end_char="1766" id="token-15-86" morph="none" pos="word" start_char="1764">the</TOKEN>
<TOKEN end_char="1774" id="token-15-87" morph="none" pos="word" start_char="1768">regular</TOKEN>
<TOKEN end_char="1784" id="token-15-88" morph="none" pos="word" start_char="1776">influenza</TOKEN>
<TOKEN end_char="1789" id="token-15-89" morph="none" pos="word" start_char="1786">this</TOKEN>
<TOKEN end_char="1794" id="token-15-90" morph="none" pos="word" start_char="1791">year</TOKEN>
<TOKEN end_char="1795" id="token-15-91" morph="none" pos="punct" start_char="1795">,</TOKEN>
<TOKEN end_char="1804" id="token-15-92" morph="none" pos="word" start_char="1797">compared</TOKEN>
<TOKEN end_char="1807" id="token-15-93" morph="none" pos="word" start_char="1806">to</TOKEN>
<TOKEN end_char="1808" id="token-15-94" morph="none" pos="punct" start_char="1808">…</TOKEN>
<TOKEN end_char="1813" id="token-15-95" morph="none" pos="word" start_char="1810">Read</TOKEN>
<TOKEN end_char="1818" id="token-15-96" morph="none" pos="word" start_char="1815">more</TOKEN>
<TOKEN end_char="1820" id="token-15-97" morph="none" pos="punct" start_char="1820">»</TOKEN>
</SEG>
<SEG end_char="1854" id="segment-16" start_char="1824">
<ORIGINAL_TEXT>Because Covid19 IS NOT THE FLU.</ORIGINAL_TEXT>
<TOKEN end_char="1830" id="token-16-0" morph="none" pos="word" start_char="1824">Because</TOKEN>
<TOKEN end_char="1838" id="token-16-1" morph="none" pos="word" start_char="1832">Covid19</TOKEN>
<TOKEN end_char="1841" id="token-16-2" morph="none" pos="word" start_char="1840">IS</TOKEN>
<TOKEN end_char="1845" id="token-16-3" morph="none" pos="word" start_char="1843">NOT</TOKEN>
<TOKEN end_char="1849" id="token-16-4" morph="none" pos="word" start_char="1847">THE</TOKEN>
<TOKEN end_char="1853" id="token-16-5" morph="none" pos="word" start_char="1851">FLU</TOKEN>
<TOKEN end_char="1854" id="token-16-6" morph="none" pos="punct" start_char="1854">.</TOKEN>
</SEG>
<SEG end_char="1976" id="segment-17" start_char="1856">
<ORIGINAL_TEXT>It is more dangerous and more contagious, therefore, if we didn’t do a lockdown, the numbers would be worse than the flu.</ORIGINAL_TEXT>
<TOKEN end_char="1857" id="token-17-0" morph="none" pos="word" start_char="1856">It</TOKEN>
<TOKEN end_char="1860" id="token-17-1" morph="none" pos="word" start_char="1859">is</TOKEN>
<TOKEN end_char="1865" id="token-17-2" morph="none" pos="word" start_char="1862">more</TOKEN>
<TOKEN end_char="1875" id="token-17-3" morph="none" pos="word" start_char="1867">dangerous</TOKEN>
<TOKEN end_char="1879" id="token-17-4" morph="none" pos="word" start_char="1877">and</TOKEN>
<TOKEN end_char="1884" id="token-17-5" morph="none" pos="word" start_char="1881">more</TOKEN>
<TOKEN end_char="1895" id="token-17-6" morph="none" pos="word" start_char="1886">contagious</TOKEN>
<TOKEN end_char="1896" id="token-17-7" morph="none" pos="punct" start_char="1896">,</TOKEN>
<TOKEN end_char="1906" id="token-17-8" morph="none" pos="word" start_char="1898">therefore</TOKEN>
<TOKEN end_char="1907" id="token-17-9" morph="none" pos="punct" start_char="1907">,</TOKEN>
<TOKEN end_char="1910" id="token-17-10" morph="none" pos="word" start_char="1909">if</TOKEN>
<TOKEN end_char="1913" id="token-17-11" morph="none" pos="word" start_char="1912">we</TOKEN>
<TOKEN end_char="1920" id="token-17-12" morph="none" pos="word" start_char="1915">didn’t</TOKEN>
<TOKEN end_char="1923" id="token-17-13" morph="none" pos="word" start_char="1922">do</TOKEN>
<TOKEN end_char="1925" id="token-17-14" morph="none" pos="word" start_char="1925">a</TOKEN>
<TOKEN end_char="1934" id="token-17-15" morph="none" pos="word" start_char="1927">lockdown</TOKEN>
<TOKEN end_char="1935" id="token-17-16" morph="none" pos="punct" start_char="1935">,</TOKEN>
<TOKEN end_char="1939" id="token-17-17" morph="none" pos="word" start_char="1937">the</TOKEN>
<TOKEN end_char="1947" id="token-17-18" morph="none" pos="word" start_char="1941">numbers</TOKEN>
<TOKEN end_char="1953" id="token-17-19" morph="none" pos="word" start_char="1949">would</TOKEN>
<TOKEN end_char="1956" id="token-17-20" morph="none" pos="word" start_char="1955">be</TOKEN>
<TOKEN end_char="1962" id="token-17-21" morph="none" pos="word" start_char="1958">worse</TOKEN>
<TOKEN end_char="1967" id="token-17-22" morph="none" pos="word" start_char="1964">than</TOKEN>
<TOKEN end_char="1971" id="token-17-23" morph="none" pos="word" start_char="1969">the</TOKEN>
<TOKEN end_char="1975" id="token-17-24" morph="none" pos="word" start_char="1973">flu</TOKEN>
<TOKEN end_char="1976" id="token-17-25" morph="none" pos="punct" start_char="1976">.</TOKEN>
</SEG>
<SEG end_char="2043" id="segment-18" start_char="1978">
<ORIGINAL_TEXT>The numbers are low because of the lockdown and social distancing.</ORIGINAL_TEXT>
<TOKEN end_char="1980" id="token-18-0" morph="none" pos="word" start_char="1978">The</TOKEN>
<TOKEN end_char="1988" id="token-18-1" morph="none" pos="word" start_char="1982">numbers</TOKEN>
<TOKEN end_char="1992" id="token-18-2" morph="none" pos="word" start_char="1990">are</TOKEN>
<TOKEN end_char="1996" id="token-18-3" morph="none" pos="word" start_char="1994">low</TOKEN>
<TOKEN end_char="2004" id="token-18-4" morph="none" pos="word" start_char="1998">because</TOKEN>
<TOKEN end_char="2007" id="token-18-5" morph="none" pos="word" start_char="2006">of</TOKEN>
<TOKEN end_char="2011" id="token-18-6" morph="none" pos="word" start_char="2009">the</TOKEN>
<TOKEN end_char="2020" id="token-18-7" morph="none" pos="word" start_char="2013">lockdown</TOKEN>
<TOKEN end_char="2024" id="token-18-8" morph="none" pos="word" start_char="2022">and</TOKEN>
<TOKEN end_char="2031" id="token-18-9" morph="none" pos="word" start_char="2026">social</TOKEN>
<TOKEN end_char="2042" id="token-18-10" morph="none" pos="word" start_char="2033">distancing</TOKEN>
<TOKEN end_char="2043" id="token-18-11" morph="none" pos="punct" start_char="2043">.</TOKEN>
</SEG>
<SEG end_char="2120" id="segment-19" start_char="2047">
<ORIGINAL_TEXT>It seems the theory lab leakage , accidental or deliberate need more study</ORIGINAL_TEXT>
<TOKEN end_char="2048" id="token-19-0" morph="none" pos="word" start_char="2047">It</TOKEN>
<TOKEN end_char="2054" id="token-19-1" morph="none" pos="word" start_char="2050">seems</TOKEN>
<TOKEN end_char="2058" id="token-19-2" morph="none" pos="word" start_char="2056">the</TOKEN>
<TOKEN end_char="2065" id="token-19-3" morph="none" pos="word" start_char="2060">theory</TOKEN>
<TOKEN end_char="2069" id="token-19-4" morph="none" pos="word" start_char="2067">lab</TOKEN>
<TOKEN end_char="2077" id="token-19-5" morph="none" pos="word" start_char="2071">leakage</TOKEN>
<TOKEN end_char="2079" id="token-19-6" morph="none" pos="punct" start_char="2079">,</TOKEN>
<TOKEN end_char="2090" id="token-19-7" morph="none" pos="word" start_char="2081">accidental</TOKEN>
<TOKEN end_char="2093" id="token-19-8" morph="none" pos="word" start_char="2092">or</TOKEN>
<TOKEN end_char="2104" id="token-19-9" morph="none" pos="word" start_char="2095">deliberate</TOKEN>
<TOKEN end_char="2109" id="token-19-10" morph="none" pos="word" start_char="2106">need</TOKEN>
<TOKEN end_char="2114" id="token-19-11" morph="none" pos="word" start_char="2111">more</TOKEN>
<TOKEN end_char="2120" id="token-19-12" morph="none" pos="word" start_char="2116">study</TOKEN>
</SEG>
<SEG end_char="2209" id="segment-20" start_char="2124">
<ORIGINAL_TEXT>I don’t think they are implying it was an intentional leak but moresso a lab accident.</ORIGINAL_TEXT>
<TOKEN end_char="2124" id="token-20-0" morph="none" pos="word" start_char="2124">I</TOKEN>
<TOKEN end_char="2130" id="token-20-1" morph="none" pos="word" start_char="2126">don’t</TOKEN>
<TOKEN end_char="2136" id="token-20-2" morph="none" pos="word" start_char="2132">think</TOKEN>
<TOKEN end_char="2141" id="token-20-3" morph="none" pos="word" start_char="2138">they</TOKEN>
<TOKEN end_char="2145" id="token-20-4" morph="none" pos="word" start_char="2143">are</TOKEN>
<TOKEN end_char="2154" id="token-20-5" morph="none" pos="word" start_char="2147">implying</TOKEN>
<TOKEN end_char="2157" id="token-20-6" morph="none" pos="word" start_char="2156">it</TOKEN>
<TOKEN end_char="2161" id="token-20-7" morph="none" pos="word" start_char="2159">was</TOKEN>
<TOKEN end_char="2164" id="token-20-8" morph="none" pos="word" start_char="2163">an</TOKEN>
<TOKEN end_char="2176" id="token-20-9" morph="none" pos="word" start_char="2166">intentional</TOKEN>
<TOKEN end_char="2181" id="token-20-10" morph="none" pos="word" start_char="2178">leak</TOKEN>
<TOKEN end_char="2185" id="token-20-11" morph="none" pos="word" start_char="2183">but</TOKEN>
<TOKEN end_char="2193" id="token-20-12" morph="none" pos="word" start_char="2187">moresso</TOKEN>
<TOKEN end_char="2195" id="token-20-13" morph="none" pos="word" start_char="2195">a</TOKEN>
<TOKEN end_char="2199" id="token-20-14" morph="none" pos="word" start_char="2197">lab</TOKEN>
<TOKEN end_char="2208" id="token-20-15" morph="none" pos="word" start_char="2201">accident</TOKEN>
<TOKEN end_char="2209" id="token-20-16" morph="none" pos="punct" start_char="2209">.</TOKEN>
</SEG>
<SEG end_char="2332" id="segment-21" start_char="2211">
<ORIGINAL_TEXT>The most intriguing part to me is the fact that a lab worker/doctor was found guilty selling test animals to meat markets.</ORIGINAL_TEXT>
<TOKEN end_char="2213" id="token-21-0" morph="none" pos="word" start_char="2211">The</TOKEN>
<TOKEN end_char="2218" id="token-21-1" morph="none" pos="word" start_char="2215">most</TOKEN>
<TOKEN end_char="2229" id="token-21-2" morph="none" pos="word" start_char="2220">intriguing</TOKEN>
<TOKEN end_char="2234" id="token-21-3" morph="none" pos="word" start_char="2231">part</TOKEN>
<TOKEN end_char="2237" id="token-21-4" morph="none" pos="word" start_char="2236">to</TOKEN>
<TOKEN end_char="2240" id="token-21-5" morph="none" pos="word" start_char="2239">me</TOKEN>
<TOKEN end_char="2243" id="token-21-6" morph="none" pos="word" start_char="2242">is</TOKEN>
<TOKEN end_char="2247" id="token-21-7" morph="none" pos="word" start_char="2245">the</TOKEN>
<TOKEN end_char="2252" id="token-21-8" morph="none" pos="word" start_char="2249">fact</TOKEN>
<TOKEN end_char="2257" id="token-21-9" morph="none" pos="word" start_char="2254">that</TOKEN>
<TOKEN end_char="2259" id="token-21-10" morph="none" pos="word" start_char="2259">a</TOKEN>
<TOKEN end_char="2263" id="token-21-11" morph="none" pos="word" start_char="2261">lab</TOKEN>
<TOKEN end_char="2277" id="token-21-12" morph="none" pos="unknown" start_char="2265">worker/doctor</TOKEN>
<TOKEN end_char="2281" id="token-21-13" morph="none" pos="word" start_char="2279">was</TOKEN>
<TOKEN end_char="2287" id="token-21-14" morph="none" pos="word" start_char="2283">found</TOKEN>
<TOKEN end_char="2294" id="token-21-15" morph="none" pos="word" start_char="2289">guilty</TOKEN>
<TOKEN end_char="2302" id="token-21-16" morph="none" pos="word" start_char="2296">selling</TOKEN>
<TOKEN end_char="2307" id="token-21-17" morph="none" pos="word" start_char="2304">test</TOKEN>
<TOKEN end_char="2315" id="token-21-18" morph="none" pos="word" start_char="2309">animals</TOKEN>
<TOKEN end_char="2318" id="token-21-19" morph="none" pos="word" start_char="2317">to</TOKEN>
<TOKEN end_char="2323" id="token-21-20" morph="none" pos="word" start_char="2320">meat</TOKEN>
<TOKEN end_char="2331" id="token-21-21" morph="none" pos="word" start_char="2325">markets</TOKEN>
<TOKEN end_char="2332" id="token-21-22" morph="none" pos="punct" start_char="2332">.</TOKEN>
</SEG>
<SEG end_char="2390" id="segment-22" start_char="2334">
<ORIGINAL_TEXT>That could have been a way it became prevalent in humans.</ORIGINAL_TEXT>
<TOKEN end_char="2337" id="token-22-0" morph="none" pos="word" start_char="2334">That</TOKEN>
<TOKEN end_char="2343" id="token-22-1" morph="none" pos="word" start_char="2339">could</TOKEN>
<TOKEN end_char="2348" id="token-22-2" morph="none" pos="word" start_char="2345">have</TOKEN>
<TOKEN end_char="2353" id="token-22-3" morph="none" pos="word" start_char="2350">been</TOKEN>
<TOKEN end_char="2355" id="token-22-4" morph="none" pos="word" start_char="2355">a</TOKEN>
<TOKEN end_char="2359" id="token-22-5" morph="none" pos="word" start_char="2357">way</TOKEN>
<TOKEN end_char="2362" id="token-22-6" morph="none" pos="word" start_char="2361">it</TOKEN>
<TOKEN end_char="2369" id="token-22-7" morph="none" pos="word" start_char="2364">became</TOKEN>
<TOKEN end_char="2379" id="token-22-8" morph="none" pos="word" start_char="2371">prevalent</TOKEN>
<TOKEN end_char="2382" id="token-22-9" morph="none" pos="word" start_char="2381">in</TOKEN>
<TOKEN end_char="2389" id="token-22-10" morph="none" pos="word" start_char="2384">humans</TOKEN>
<TOKEN end_char="2390" id="token-22-11" morph="none" pos="punct" start_char="2390">.</TOKEN>
</SEG>
<SEG end_char="2450" id="segment-23" start_char="2394">
<ORIGINAL_TEXT>Can I have a source for that report of them being guilty?</ORIGINAL_TEXT>
<TOKEN end_char="2396" id="token-23-0" morph="none" pos="word" start_char="2394">Can</TOKEN>
<TOKEN end_char="2398" id="token-23-1" morph="none" pos="word" start_char="2398">I</TOKEN>
<TOKEN end_char="2403" id="token-23-2" morph="none" pos="word" start_char="2400">have</TOKEN>
<TOKEN end_char="2405" id="token-23-3" morph="none" pos="word" start_char="2405">a</TOKEN>
<TOKEN end_char="2412" id="token-23-4" morph="none" pos="word" start_char="2407">source</TOKEN>
<TOKEN end_char="2416" id="token-23-5" morph="none" pos="word" start_char="2414">for</TOKEN>
<TOKEN end_char="2421" id="token-23-6" morph="none" pos="word" start_char="2418">that</TOKEN>
<TOKEN end_char="2428" id="token-23-7" morph="none" pos="word" start_char="2423">report</TOKEN>
<TOKEN end_char="2431" id="token-23-8" morph="none" pos="word" start_char="2430">of</TOKEN>
<TOKEN end_char="2436" id="token-23-9" morph="none" pos="word" start_char="2433">them</TOKEN>
<TOKEN end_char="2442" id="token-23-10" morph="none" pos="word" start_char="2438">being</TOKEN>
<TOKEN end_char="2449" id="token-23-11" morph="none" pos="word" start_char="2444">guilty</TOKEN>
<TOKEN end_char="2450" id="token-23-12" morph="none" pos="punct" start_char="2450">?</TOKEN>
</SEG>
<SEG end_char="2841" id="segment-24" start_char="2454">
<ORIGINAL_TEXT>Dr Li Ning, an expert in cloning and former director of the State Key Laboratory of Agrobiotechnology, was found guilty of illegally transferring the funds in the form of "investments" to several companies he controlled, though there was no evidence he spent any of the money on himself, the Intermediate People’s Court of Songyuan in northeast China’s Jilin province said in its verdict.</ORIGINAL_TEXT>
<TOKEN end_char="2455" id="token-24-0" morph="none" pos="word" start_char="2454">Dr</TOKEN>
<TOKEN end_char="2458" id="token-24-1" morph="none" pos="word" start_char="2457">Li</TOKEN>
<TOKEN end_char="2463" id="token-24-2" morph="none" pos="word" start_char="2460">Ning</TOKEN>
<TOKEN end_char="2464" id="token-24-3" morph="none" pos="punct" start_char="2464">,</TOKEN>
<TOKEN end_char="2467" id="token-24-4" morph="none" pos="word" start_char="2466">an</TOKEN>
<TOKEN end_char="2474" id="token-24-5" morph="none" pos="word" start_char="2469">expert</TOKEN>
<TOKEN end_char="2477" id="token-24-6" morph="none" pos="word" start_char="2476">in</TOKEN>
<TOKEN end_char="2485" id="token-24-7" morph="none" pos="word" start_char="2479">cloning</TOKEN>
<TOKEN end_char="2489" id="token-24-8" morph="none" pos="word" start_char="2487">and</TOKEN>
<TOKEN end_char="2496" id="token-24-9" morph="none" pos="word" start_char="2491">former</TOKEN>
<TOKEN end_char="2505" id="token-24-10" morph="none" pos="word" start_char="2498">director</TOKEN>
<TOKEN end_char="2508" id="token-24-11" morph="none" pos="word" start_char="2507">of</TOKEN>
<TOKEN end_char="2512" id="token-24-12" morph="none" pos="word" start_char="2510">the</TOKEN>
<TOKEN end_char="2518" id="token-24-13" morph="none" pos="word" start_char="2514">State</TOKEN>
<TOKEN end_char="2522" id="token-24-14" morph="none" pos="word" start_char="2520">Key</TOKEN>
<TOKEN end_char="2533" id="token-24-15" morph="none" pos="word" start_char="2524">Laboratory</TOKEN>
<TOKEN end_char="2536" id="token-24-16" morph="none" pos="word" start_char="2535">of</TOKEN>
<TOKEN end_char="2554" id="token-24-17" morph="none" pos="word" start_char="2538">Agrobiotechnology</TOKEN>
<TOKEN end_char="2555" id="token-24-18" morph="none" pos="punct" start_char="2555">,</TOKEN>
<TOKEN end_char="2559" id="token-24-19" morph="none" pos="word" start_char="2557">was</TOKEN>
<TOKEN end_char="2565" id="token-24-20" morph="none" pos="word" start_char="2561">found</TOKEN>
<TOKEN end_char="2572" id="token-24-21" morph="none" pos="word" start_char="2567">guilty</TOKEN>
<TOKEN end_char="2575" id="token-24-22" morph="none" pos="word" start_char="2574">of</TOKEN>
<TOKEN end_char="2585" id="token-24-23" morph="none" pos="word" start_char="2577">illegally</TOKEN>
<TOKEN end_char="2598" id="token-24-24" morph="none" pos="word" start_char="2587">transferring</TOKEN>
<TOKEN end_char="2602" id="token-24-25" morph="none" pos="word" start_char="2600">the</TOKEN>
<TOKEN end_char="2608" id="token-24-26" morph="none" pos="word" start_char="2604">funds</TOKEN>
<TOKEN end_char="2611" id="token-24-27" morph="none" pos="word" start_char="2610">in</TOKEN>
<TOKEN end_char="2615" id="token-24-28" morph="none" pos="word" start_char="2613">the</TOKEN>
<TOKEN end_char="2620" id="token-24-29" morph="none" pos="word" start_char="2617">form</TOKEN>
<TOKEN end_char="2623" id="token-24-30" morph="none" pos="word" start_char="2622">of</TOKEN>
<TOKEN end_char="2625" id="token-24-31" morph="none" pos="punct" start_char="2625">"</TOKEN>
<TOKEN end_char="2636" id="token-24-32" morph="none" pos="word" start_char="2626">investments</TOKEN>
<TOKEN end_char="2637" id="token-24-33" morph="none" pos="punct" start_char="2637">"</TOKEN>
<TOKEN end_char="2640" id="token-24-34" morph="none" pos="word" start_char="2639">to</TOKEN>
<TOKEN end_char="2648" id="token-24-35" morph="none" pos="word" start_char="2642">several</TOKEN>
<TOKEN end_char="2658" id="token-24-36" morph="none" pos="word" start_char="2650">companies</TOKEN>
<TOKEN end_char="2661" id="token-24-37" morph="none" pos="word" start_char="2660">he</TOKEN>
<TOKEN end_char="2672" id="token-24-38" morph="none" pos="word" start_char="2663">controlled</TOKEN>
<TOKEN end_char="2673" id="token-24-39" morph="none" pos="punct" start_char="2673">,</TOKEN>
<TOKEN end_char="2680" id="token-24-40" morph="none" pos="word" start_char="2675">though</TOKEN>
<TOKEN end_char="2686" id="token-24-41" morph="none" pos="word" start_char="2682">there</TOKEN>
<TOKEN end_char="2690" id="token-24-42" morph="none" pos="word" start_char="2688">was</TOKEN>
<TOKEN end_char="2693" id="token-24-43" morph="none" pos="word" start_char="2692">no</TOKEN>
<TOKEN end_char="2702" id="token-24-44" morph="none" pos="word" start_char="2695">evidence</TOKEN>
<TOKEN end_char="2705" id="token-24-45" morph="none" pos="word" start_char="2704">he</TOKEN>
<TOKEN end_char="2711" id="token-24-46" morph="none" pos="word" start_char="2707">spent</TOKEN>
<TOKEN end_char="2715" id="token-24-47" morph="none" pos="word" start_char="2713">any</TOKEN>
<TOKEN end_char="2718" id="token-24-48" morph="none" pos="word" start_char="2717">of</TOKEN>
<TOKEN end_char="2722" id="token-24-49" morph="none" pos="word" start_char="2720">the</TOKEN>
<TOKEN end_char="2728" id="token-24-50" morph="none" pos="word" start_char="2724">money</TOKEN>
<TOKEN end_char="2731" id="token-24-51" morph="none" pos="word" start_char="2730">on</TOKEN>
<TOKEN end_char="2739" id="token-24-52" morph="none" pos="word" start_char="2733">himself</TOKEN>
<TOKEN end_char="2740" id="token-24-53" morph="none" pos="punct" start_char="2740">,</TOKEN>
<TOKEN end_char="2744" id="token-24-54" morph="none" pos="word" start_char="2742">the</TOKEN>
<TOKEN end_char="2757" id="token-24-55" morph="none" pos="word" start_char="2746">Intermediate</TOKEN>
<TOKEN end_char="2766" id="token-24-56" morph="none" pos="word" start_char="2759">People’s</TOKEN>
<TOKEN end_char="2772" id="token-24-57" morph="none" pos="word" start_char="2768">Court</TOKEN>
<TOKEN end_char="2775" id="token-24-58" morph="none" pos="word" start_char="2774">of</TOKEN>
<TOKEN end_char="2784" id="token-24-59" morph="none" pos="word" start_char="2777">Songyuan</TOKEN>
<TOKEN end_char="2787" id="token-24-60" morph="none" pos="word" start_char="2786">in</TOKEN>
<TOKEN end_char="2797" id="token-24-61" morph="none" pos="word" start_char="2789">northeast</TOKEN>
<TOKEN end_char="2805" id="token-24-62" morph="none" pos="word" start_char="2799">China’s</TOKEN>
<TOKEN end_char="2811" id="token-24-63" morph="none" pos="word" start_char="2807">Jilin</TOKEN>
<TOKEN end_char="2820" id="token-24-64" morph="none" pos="word" start_char="2813">province</TOKEN>
<TOKEN end_char="2825" id="token-24-65" morph="none" pos="word" start_char="2822">said</TOKEN>
<TOKEN end_char="2828" id="token-24-66" morph="none" pos="word" start_char="2827">in</TOKEN>
<TOKEN end_char="2832" id="token-24-67" morph="none" pos="word" start_char="2830">its</TOKEN>
<TOKEN end_char="2840" id="token-24-68" morph="none" pos="word" start_char="2834">verdict</TOKEN>
<TOKEN end_char="2841" id="token-24-69" morph="none" pos="punct" start_char="2841">.</TOKEN>
</SEG>
<SEG end_char="3009" id="segment-25" start_char="2844">
<ORIGINAL_TEXT>Li was also fined 3 million yuan, while his assistant, Dr Zhang Lei, was sentenced to five years and eight months in prison and fined 200,000 yuan on the same charge.</ORIGINAL_TEXT>
<TOKEN end_char="2845" id="token-25-0" morph="none" pos="word" start_char="2844">Li</TOKEN>
<TOKEN end_char="2849" id="token-25-1" morph="none" pos="word" start_char="2847">was</TOKEN>
<TOKEN end_char="2854" id="token-25-2" morph="none" pos="word" start_char="2851">also</TOKEN>
<TOKEN end_char="2860" id="token-25-3" morph="none" pos="word" start_char="2856">fined</TOKEN>
<TOKEN end_char="2862" id="token-25-4" morph="none" pos="word" start_char="2862">3</TOKEN>
<TOKEN end_char="2870" id="token-25-5" morph="none" pos="word" start_char="2864">million</TOKEN>
<TOKEN end_char="2875" id="token-25-6" morph="none" pos="word" start_char="2872">yuan</TOKEN>
<TOKEN end_char="2876" id="token-25-7" morph="none" pos="punct" start_char="2876">,</TOKEN>
<TOKEN end_char="2882" id="token-25-8" morph="none" pos="word" start_char="2878">while</TOKEN>
<TOKEN end_char="2886" id="token-25-9" morph="none" pos="word" start_char="2884">his</TOKEN>
<TOKEN end_char="2896" id="token-25-10" morph="none" pos="word" start_char="2888">assistant</TOKEN>
<TOKEN end_char="2897" id="token-25-11" morph="none" pos="punct" start_char="2897">,</TOKEN>
<TOKEN end_char="2900" id="token-25-12" morph="none" pos="word" start_char="2899">Dr</TOKEN>
<TOKEN end_char="2906" id="token-25-13" morph="none" pos="word" start_char="2902">Zhang</TOKEN>
<TOKEN end_char="2910" id="token-25-14" morph="none" pos="word" start_char="2908">Lei</TOKEN>
<TOKEN end_char="2911" id="token-25-15" morph="none" pos="punct" start_char="2911">,</TOKEN>
<TOKEN end_char="2915" id="token-25-16" morph="none" pos="word" start_char="2913">was</TOKEN>
<TOKEN end_char="2925" id="token-25-17" morph="none" pos="word" start_char="2917">sentenced</TOKEN>
<TOKEN end_char="2928" id="token-25-18" morph="none" pos="word" start_char="2927">to</TOKEN>
<TOKEN end_char="2933" id="token-25-19" morph="none" pos="word" start_char="2930">five</TOKEN>
<TOKEN end_char="2939" id="token-25-20" morph="none" pos="word" start_char="2935">years</TOKEN>
<TOKEN end_char="2943" id="token-25-21" morph="none" pos="word" start_char="2941">and</TOKEN>
<TOKEN end_char="2949" id="token-25-22" morph="none" pos="word" start_char="2945">eight</TOKEN>
<TOKEN end_char="2956" id="token-25-23" morph="none" pos="word" start_char="2951">months</TOKEN>
<TOKEN end_char="2959" id="token-25-24" morph="none" pos="word" start_char="2958">in</TOKEN>
<TOKEN end_char="2966" id="token-25-25" morph="none" pos="word" start_char="2961">prison</TOKEN>
<TOKEN end_char="2970" id="token-25-26" morph="none" pos="word" start_char="2968">and</TOKEN>
<TOKEN end_char="2976" id="token-25-27" morph="none" pos="word" start_char="2972">fined</TOKEN>
<TOKEN end_char="2984" id="token-25-28" morph="none" pos="unknown" start_char="2978">200,000</TOKEN>
<TOKEN end_char="2989" id="token-25-29" morph="none" pos="word" start_char="2986">yuan</TOKEN>
<TOKEN end_char="2992" id="token-25-30" morph="none" pos="word" start_char="2991">on</TOKEN>
<TOKEN end_char="2996" id="token-25-31" morph="none" pos="word" start_char="2994">the</TOKEN>
<TOKEN end_char="3001" id="token-25-32" morph="none" pos="word" start_char="2998">same</TOKEN>
<TOKEN end_char="3008" id="token-25-33" morph="none" pos="word" start_char="3003">charge</TOKEN>
<TOKEN end_char="3009" id="token-25-34" morph="none" pos="punct" start_char="3009">.</TOKEN>
</SEG>
<SEG end_char="3150" id="segment-26" start_char="3012">
<ORIGINAL_TEXT>https://www.google.com/amp/s/amp.scmp.com/news/china/society/article/3044556/chinese-scientist-li-ning-gets-12-years-prison-embezzling-us43</ORIGINAL_TEXT>
<TOKEN end_char="3150" id="token-26-0" morph="none" pos="url" start_char="3012">https://www.google.com/amp/s/amp.scmp.com/news/china/society/article/3044556/chinese-scientist-li-ning-gets-12-years-prison-embezzling-us43</TOKEN>
<TRANSLATED_TEXT>https: / / www.google.com / amp / s / amp.scmp.com / news / china / society / article / 3044556 / chinese-scientist-li-ning-gets-12-years-prison-embezzling-us43</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="3205" id="segment-27" start_char="3154">
<ORIGINAL_TEXT>Good points but i disagree with the final statement.</ORIGINAL_TEXT>
<TOKEN end_char="3157" id="token-27-0" morph="none" pos="word" start_char="3154">Good</TOKEN>
<TOKEN end_char="3164" id="token-27-1" morph="none" pos="word" start_char="3159">points</TOKEN>
<TOKEN end_char="3168" id="token-27-2" morph="none" pos="word" start_char="3166">but</TOKEN>
<TOKEN end_char="3170" id="token-27-3" morph="none" pos="word" start_char="3170">i</TOKEN>
<TOKEN end_char="3179" id="token-27-4" morph="none" pos="word" start_char="3172">disagree</TOKEN>
<TOKEN end_char="3184" id="token-27-5" morph="none" pos="word" start_char="3181">with</TOKEN>
<TOKEN end_char="3188" id="token-27-6" morph="none" pos="word" start_char="3186">the</TOKEN>
<TOKEN end_char="3194" id="token-27-7" morph="none" pos="word" start_char="3190">final</TOKEN>
<TOKEN end_char="3204" id="token-27-8" morph="none" pos="word" start_char="3196">statement</TOKEN>
<TOKEN end_char="3205" id="token-27-9" morph="none" pos="punct" start_char="3205">.</TOKEN>
</SEG>
<SEG end_char="3413" id="segment-28" start_char="3207">
<ORIGINAL_TEXT>We all know that high containment labs need to be better managed as our own side has several more documented instances of accidents/violations than mentioned in this article, human nature being human nature.</ORIGINAL_TEXT>
<TOKEN end_char="3208" id="token-28-0" morph="none" pos="word" start_char="3207">We</TOKEN>
<TOKEN end_char="3212" id="token-28-1" morph="none" pos="word" start_char="3210">all</TOKEN>
<TOKEN end_char="3217" id="token-28-2" morph="none" pos="word" start_char="3214">know</TOKEN>
<TOKEN end_char="3222" id="token-28-3" morph="none" pos="word" start_char="3219">that</TOKEN>
<TOKEN end_char="3227" id="token-28-4" morph="none" pos="word" start_char="3224">high</TOKEN>
<TOKEN end_char="3239" id="token-28-5" morph="none" pos="word" start_char="3229">containment</TOKEN>
<TOKEN end_char="3244" id="token-28-6" morph="none" pos="word" start_char="3241">labs</TOKEN>
<TOKEN end_char="3249" id="token-28-7" morph="none" pos="word" start_char="3246">need</TOKEN>
<TOKEN end_char="3252" id="token-28-8" morph="none" pos="word" start_char="3251">to</TOKEN>
<TOKEN end_char="3255" id="token-28-9" morph="none" pos="word" start_char="3254">be</TOKEN>
<TOKEN end_char="3262" id="token-28-10" morph="none" pos="word" start_char="3257">better</TOKEN>
<TOKEN end_char="3270" id="token-28-11" morph="none" pos="word" start_char="3264">managed</TOKEN>
<TOKEN end_char="3273" id="token-28-12" morph="none" pos="word" start_char="3272">as</TOKEN>
<TOKEN end_char="3277" id="token-28-13" morph="none" pos="word" start_char="3275">our</TOKEN>
<TOKEN end_char="3281" id="token-28-14" morph="none" pos="word" start_char="3279">own</TOKEN>
<TOKEN end_char="3286" id="token-28-15" morph="none" pos="word" start_char="3283">side</TOKEN>
<TOKEN end_char="3290" id="token-28-16" morph="none" pos="word" start_char="3288">has</TOKEN>
<TOKEN end_char="3298" id="token-28-17" morph="none" pos="word" start_char="3292">several</TOKEN>
<TOKEN end_char="3303" id="token-28-18" morph="none" pos="word" start_char="3300">more</TOKEN>
<TOKEN end_char="3314" id="token-28-19" morph="none" pos="word" start_char="3305">documented</TOKEN>
<TOKEN end_char="3324" id="token-28-20" morph="none" pos="word" start_char="3316">instances</TOKEN>
<TOKEN end_char="3327" id="token-28-21" morph="none" pos="word" start_char="3326">of</TOKEN>
<TOKEN end_char="3348" id="token-28-22" morph="none" pos="unknown" start_char="3329">accidents/violations</TOKEN>
<TOKEN end_char="3353" id="token-28-23" morph="none" pos="word" start_char="3350">than</TOKEN>
<TOKEN end_char="3363" id="token-28-24" morph="none" pos="word" start_char="3355">mentioned</TOKEN>
<TOKEN end_char="3366" id="token-28-25" morph="none" pos="word" start_char="3365">in</TOKEN>
<TOKEN end_char="3371" id="token-28-26" morph="none" pos="word" start_char="3368">this</TOKEN>
<TOKEN end_char="3379" id="token-28-27" morph="none" pos="word" start_char="3373">article</TOKEN>
<TOKEN end_char="3380" id="token-28-28" morph="none" pos="punct" start_char="3380">,</TOKEN>
<TOKEN end_char="3386" id="token-28-29" morph="none" pos="word" start_char="3382">human</TOKEN>
<TOKEN end_char="3393" id="token-28-30" morph="none" pos="word" start_char="3388">nature</TOKEN>
<TOKEN end_char="3399" id="token-28-31" morph="none" pos="word" start_char="3395">being</TOKEN>
<TOKEN end_char="3405" id="token-28-32" morph="none" pos="word" start_char="3401">human</TOKEN>
<TOKEN end_char="3412" id="token-28-33" morph="none" pos="word" start_char="3407">nature</TOKEN>
<TOKEN end_char="3413" id="token-28-34" morph="none" pos="punct" start_char="3413">.</TOKEN>
</SEG>
<SEG end_char="3455" id="segment-29" start_char="3415">
<ORIGINAL_TEXT>AI will no doubt help with those matters.</ORIGINAL_TEXT>
<TOKEN end_char="3416" id="token-29-0" morph="none" pos="word" start_char="3415">AI</TOKEN>
<TOKEN end_char="3421" id="token-29-1" morph="none" pos="word" start_char="3418">will</TOKEN>
<TOKEN end_char="3424" id="token-29-2" morph="none" pos="word" start_char="3423">no</TOKEN>
<TOKEN end_char="3430" id="token-29-3" morph="none" pos="word" start_char="3426">doubt</TOKEN>
<TOKEN end_char="3435" id="token-29-4" morph="none" pos="word" start_char="3432">help</TOKEN>
<TOKEN end_char="3440" id="token-29-5" morph="none" pos="word" start_char="3437">with</TOKEN>
<TOKEN end_char="3446" id="token-29-6" morph="none" pos="word" start_char="3442">those</TOKEN>
<TOKEN end_char="3454" id="token-29-7" morph="none" pos="word" start_char="3448">matters</TOKEN>
<TOKEN end_char="3455" id="token-29-8" morph="none" pos="punct" start_char="3455">.</TOKEN>
</SEG>
<SEG end_char="3569" id="segment-30" start_char="3457">
<ORIGINAL_TEXT>But determining the source, assigning blame, matters less than realizing that this will not be the last pandemic.</ORIGINAL_TEXT>
<TOKEN end_char="3459" id="token-30-0" morph="none" pos="word" start_char="3457">But</TOKEN>
<TOKEN end_char="3471" id="token-30-1" morph="none" pos="word" start_char="3461">determining</TOKEN>
<TOKEN end_char="3475" id="token-30-2" morph="none" pos="word" start_char="3473">the</TOKEN>
<TOKEN end_char="3482" id="token-30-3" morph="none" pos="word" start_char="3477">source</TOKEN>
<TOKEN end_char="3483" id="token-30-4" morph="none" pos="punct" start_char="3483">,</TOKEN>
<TOKEN end_char="3493" id="token-30-5" morph="none" pos="word" start_char="3485">assigning</TOKEN>
<TOKEN end_char="3499" id="token-30-6" morph="none" pos="word" start_char="3495">blame</TOKEN>
<TOKEN end_char="3500" id="token-30-7" morph="none" pos="punct" start_char="3500">,</TOKEN>
<TOKEN end_char="3508" id="token-30-8" morph="none" pos="word" start_char="3502">matters</TOKEN>
<TOKEN end_char="3513" id="token-30-9" morph="none" pos="word" start_char="3510">less</TOKEN>
<TOKEN end_char="3518" id="token-30-10" morph="none" pos="word" start_char="3515">than</TOKEN>
<TOKEN end_char="3528" id="token-30-11" morph="none" pos="word" start_char="3520">realizing</TOKEN>
<TOKEN end_char="3533" id="token-30-12" morph="none" pos="word" start_char="3530">that</TOKEN>
<TOKEN end_char="3538" id="token-30-13" morph="none" pos="word" start_char="3535">this</TOKEN>
<TOKEN end_char="3543" id="token-30-14" morph="none" pos="word" start_char="3540">will</TOKEN>
<TOKEN end_char="3547" id="token-30-15" morph="none" pos="word" start_char="3545">not</TOKEN>
<TOKEN end_char="3550" id="token-30-16" morph="none" pos="word" start_char="3549">be</TOKEN>
<TOKEN end_char="3554" id="token-30-17" morph="none" pos="word" start_char="3552">the</TOKEN>
<TOKEN end_char="3559" id="token-30-18" morph="none" pos="word" start_char="3556">last</TOKEN>
<TOKEN end_char="3568" id="token-30-19" morph="none" pos="word" start_char="3561">pandemic</TOKEN>
<TOKEN end_char="3569" id="token-30-20" morph="none" pos="punct" start_char="3569">.</TOKEN>
</SEG>
<SEG end_char="3756" id="segment-31" start_char="3571">
<ORIGINAL_TEXT>We need to devote our resources into strengthening defenses against pandemics and understanding the thousands of potential human pathogenic viruses, how they work and how to defeat them.</ORIGINAL_TEXT>
<TOKEN end_char="3572" id="token-31-0" morph="none" pos="word" start_char="3571">We</TOKEN>
<TOKEN end_char="3577" id="token-31-1" morph="none" pos="word" start_char="3574">need</TOKEN>
<TOKEN end_char="3580" id="token-31-2" morph="none" pos="word" start_char="3579">to</TOKEN>
<TOKEN end_char="3587" id="token-31-3" morph="none" pos="word" start_char="3582">devote</TOKEN>
<TOKEN end_char="3591" id="token-31-4" morph="none" pos="word" start_char="3589">our</TOKEN>
<TOKEN end_char="3601" id="token-31-5" morph="none" pos="word" start_char="3593">resources</TOKEN>
<TOKEN end_char="3606" id="token-31-6" morph="none" pos="word" start_char="3603">into</TOKEN>
<TOKEN end_char="3620" id="token-31-7" morph="none" pos="word" start_char="3608">strengthening</TOKEN>
<TOKEN end_char="3629" id="token-31-8" morph="none" pos="word" start_char="3622">defenses</TOKEN>
<TOKEN end_char="3637" id="token-31-9" morph="none" pos="word" start_char="3631">against</TOKEN>
<TOKEN end_char="3647" id="token-31-10" morph="none" pos="word" start_char="3639">pandemics</TOKEN>
<TOKEN end_char="3651" id="token-31-11" morph="none" pos="word" start_char="3649">and</TOKEN>
<TOKEN end_char="3665" id="token-31-12" morph="none" pos="word" start_char="3653">understanding</TOKEN>
<TOKEN end_char="3669" id="token-31-13" morph="none" pos="word" start_char="3667">the</TOKEN>
<TOKEN end_char="3679" id="token-31-14" morph="none" pos="word" start_char="3671">thousands</TOKEN>
<TOKEN end_char="3682" id="token-31-15" morph="none" pos="word" start_char="3681">of</TOKEN>
<TOKEN end_char="3692" id="token-31-16" morph="none" pos="word" start_char="3684">potential</TOKEN>
<TOKEN end_char="3698" id="token-31-17" morph="none" pos="word" start_char="3694">human</TOKEN>
<TOKEN end_char="3709" id="token-31-18" morph="none" pos="word" start_char="3700">pathogenic</TOKEN>
<TOKEN end_char="3717" id="token-31-19" morph="none" pos="word" start_char="3711">viruses</TOKEN>
<TOKEN end_char="3718" id="token-31-20" morph="none" pos="punct" start_char="3718">,</TOKEN>
<TOKEN end_char="3722" id="token-31-21" morph="none" pos="word" start_char="3720">how</TOKEN>
<TOKEN end_char="3727" id="token-31-22" morph="none" pos="word" start_char="3724">they</TOKEN>
<TOKEN end_char="3732" id="token-31-23" morph="none" pos="word" start_char="3729">work</TOKEN>
<TOKEN end_char="3736" id="token-31-24" morph="none" pos="word" start_char="3734">and</TOKEN>
<TOKEN end_char="3740" id="token-31-25" morph="none" pos="word" start_char="3738">how</TOKEN>
<TOKEN end_char="3743" id="token-31-26" morph="none" pos="word" start_char="3742">to</TOKEN>
<TOKEN end_char="3750" id="token-31-27" morph="none" pos="word" start_char="3745">defeat</TOKEN>
<TOKEN end_char="3755" id="token-31-28" morph="none" pos="word" start_char="3752">them</TOKEN>
<TOKEN end_char="3756" id="token-31-29" morph="none" pos="punct" start_char="3756">.</TOKEN>
</SEG>
<SEG end_char="3792" id="segment-32" start_char="3758">
<ORIGINAL_TEXT>This endeavor will not… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="3761" id="token-32-0" morph="none" pos="word" start_char="3758">This</TOKEN>
<TOKEN end_char="3770" id="token-32-1" morph="none" pos="word" start_char="3763">endeavor</TOKEN>
<TOKEN end_char="3775" id="token-32-2" morph="none" pos="word" start_char="3772">will</TOKEN>
<TOKEN end_char="3779" id="token-32-3" morph="none" pos="word" start_char="3777">not</TOKEN>
<TOKEN end_char="3780" id="token-32-4" morph="none" pos="punct" start_char="3780">…</TOKEN>
<TOKEN end_char="3785" id="token-32-5" morph="none" pos="word" start_char="3782">Read</TOKEN>
<TOKEN end_char="3790" id="token-32-6" morph="none" pos="word" start_char="3787">more</TOKEN>
<TOKEN end_char="3792" id="token-32-7" morph="none" pos="punct" start_char="3792">»</TOKEN>
</SEG>
<SEG end_char="3904" id="segment-33" start_char="3796">
<ORIGINAL_TEXT>absolutely…and yet we see populism and all the bright side of humanity fully at work … in the other direction</ORIGINAL_TEXT>
<TOKEN end_char="3809" id="token-33-0" morph="none" pos="unknown" start_char="3796">absolutely…and</TOKEN>
<TOKEN end_char="3813" id="token-33-1" morph="none" pos="word" start_char="3811">yet</TOKEN>
<TOKEN end_char="3816" id="token-33-2" morph="none" pos="word" start_char="3815">we</TOKEN>
<TOKEN end_char="3820" id="token-33-3" morph="none" pos="word" start_char="3818">see</TOKEN>
<TOKEN end_char="3829" id="token-33-4" morph="none" pos="word" start_char="3822">populism</TOKEN>
<TOKEN end_char="3833" id="token-33-5" morph="none" pos="word" start_char="3831">and</TOKEN>
<TOKEN end_char="3837" id="token-33-6" morph="none" pos="word" start_char="3835">all</TOKEN>
<TOKEN end_char="3841" id="token-33-7" morph="none" pos="word" start_char="3839">the</TOKEN>
<TOKEN end_char="3848" id="token-33-8" morph="none" pos="word" start_char="3843">bright</TOKEN>
<TOKEN end_char="3853" id="token-33-9" morph="none" pos="word" start_char="3850">side</TOKEN>
<TOKEN end_char="3856" id="token-33-10" morph="none" pos="word" start_char="3855">of</TOKEN>
<TOKEN end_char="3865" id="token-33-11" morph="none" pos="word" start_char="3858">humanity</TOKEN>
<TOKEN end_char="3871" id="token-33-12" morph="none" pos="word" start_char="3867">fully</TOKEN>
<TOKEN end_char="3874" id="token-33-13" morph="none" pos="word" start_char="3873">at</TOKEN>
<TOKEN end_char="3879" id="token-33-14" morph="none" pos="word" start_char="3876">work</TOKEN>
<TOKEN end_char="3881" id="token-33-15" morph="none" pos="punct" start_char="3881">…</TOKEN>
<TOKEN end_char="3884" id="token-33-16" morph="none" pos="word" start_char="3883">in</TOKEN>
<TOKEN end_char="3888" id="token-33-17" morph="none" pos="word" start_char="3886">the</TOKEN>
<TOKEN end_char="3894" id="token-33-18" morph="none" pos="word" start_char="3890">other</TOKEN>
<TOKEN end_char="3904" id="token-33-19" morph="none" pos="word" start_char="3896">direction</TOKEN>
</SEG>
<SEG end_char="4136" id="segment-34" start_char="3908">
<ORIGINAL_TEXT>I agree "We all know that high containment labs need to be better managed" and we cannot expect the people working in those labs, and risking causing a pandemic, to be the same people who will suddenly start behaving responsibly.</ORIGINAL_TEXT>
<TOKEN end_char="3908" id="token-34-0" morph="none" pos="word" start_char="3908">I</TOKEN>
<TOKEN end_char="3914" id="token-34-1" morph="none" pos="word" start_char="3910">agree</TOKEN>
<TOKEN end_char="3916" id="token-34-2" morph="none" pos="punct" start_char="3916">"</TOKEN>
<TOKEN end_char="3918" id="token-34-3" morph="none" pos="word" start_char="3917">We</TOKEN>
<TOKEN end_char="3922" id="token-34-4" morph="none" pos="word" start_char="3920">all</TOKEN>
<TOKEN end_char="3927" id="token-34-5" morph="none" pos="word" start_char="3924">know</TOKEN>
<TOKEN end_char="3932" id="token-34-6" morph="none" pos="word" start_char="3929">that</TOKEN>
<TOKEN end_char="3937" id="token-34-7" morph="none" pos="word" start_char="3934">high</TOKEN>
<TOKEN end_char="3949" id="token-34-8" morph="none" pos="word" start_char="3939">containment</TOKEN>
<TOKEN end_char="3954" id="token-34-9" morph="none" pos="word" start_char="3951">labs</TOKEN>
<TOKEN end_char="3959" id="token-34-10" morph="none" pos="word" start_char="3956">need</TOKEN>
<TOKEN end_char="3962" id="token-34-11" morph="none" pos="word" start_char="3961">to</TOKEN>
<TOKEN end_char="3965" id="token-34-12" morph="none" pos="word" start_char="3964">be</TOKEN>
<TOKEN end_char="3972" id="token-34-13" morph="none" pos="word" start_char="3967">better</TOKEN>
<TOKEN end_char="3980" id="token-34-14" morph="none" pos="word" start_char="3974">managed</TOKEN>
<TOKEN end_char="3981" id="token-34-15" morph="none" pos="punct" start_char="3981">"</TOKEN>
<TOKEN end_char="3985" id="token-34-16" morph="none" pos="word" start_char="3983">and</TOKEN>
<TOKEN end_char="3988" id="token-34-17" morph="none" pos="word" start_char="3987">we</TOKEN>
<TOKEN end_char="3995" id="token-34-18" morph="none" pos="word" start_char="3990">cannot</TOKEN>
<TOKEN end_char="4002" id="token-34-19" morph="none" pos="word" start_char="3997">expect</TOKEN>
<TOKEN end_char="4006" id="token-34-20" morph="none" pos="word" start_char="4004">the</TOKEN>
<TOKEN end_char="4013" id="token-34-21" morph="none" pos="word" start_char="4008">people</TOKEN>
<TOKEN end_char="4021" id="token-34-22" morph="none" pos="word" start_char="4015">working</TOKEN>
<TOKEN end_char="4024" id="token-34-23" morph="none" pos="word" start_char="4023">in</TOKEN>
<TOKEN end_char="4030" id="token-34-24" morph="none" pos="word" start_char="4026">those</TOKEN>
<TOKEN end_char="4035" id="token-34-25" morph="none" pos="word" start_char="4032">labs</TOKEN>
<TOKEN end_char="4036" id="token-34-26" morph="none" pos="punct" start_char="4036">,</TOKEN>
<TOKEN end_char="4040" id="token-34-27" morph="none" pos="word" start_char="4038">and</TOKEN>
<TOKEN end_char="4048" id="token-34-28" morph="none" pos="word" start_char="4042">risking</TOKEN>
<TOKEN end_char="4056" id="token-34-29" morph="none" pos="word" start_char="4050">causing</TOKEN>
<TOKEN end_char="4058" id="token-34-30" morph="none" pos="word" start_char="4058">a</TOKEN>
<TOKEN end_char="4067" id="token-34-31" morph="none" pos="word" start_char="4060">pandemic</TOKEN>
<TOKEN end_char="4068" id="token-34-32" morph="none" pos="punct" start_char="4068">,</TOKEN>
<TOKEN end_char="4071" id="token-34-33" morph="none" pos="word" start_char="4070">to</TOKEN>
<TOKEN end_char="4074" id="token-34-34" morph="none" pos="word" start_char="4073">be</TOKEN>
<TOKEN end_char="4078" id="token-34-35" morph="none" pos="word" start_char="4076">the</TOKEN>
<TOKEN end_char="4083" id="token-34-36" morph="none" pos="word" start_char="4080">same</TOKEN>
<TOKEN end_char="4090" id="token-34-37" morph="none" pos="word" start_char="4085">people</TOKEN>
<TOKEN end_char="4094" id="token-34-38" morph="none" pos="word" start_char="4092">who</TOKEN>
<TOKEN end_char="4099" id="token-34-39" morph="none" pos="word" start_char="4096">will</TOKEN>
<TOKEN end_char="4108" id="token-34-40" morph="none" pos="word" start_char="4101">suddenly</TOKEN>
<TOKEN end_char="4114" id="token-34-41" morph="none" pos="word" start_char="4110">start</TOKEN>
<TOKEN end_char="4123" id="token-34-42" morph="none" pos="word" start_char="4116">behaving</TOKEN>
<TOKEN end_char="4135" id="token-34-43" morph="none" pos="word" start_char="4125">responsibly</TOKEN>
<TOKEN end_char="4136" id="token-34-44" morph="none" pos="punct" start_char="4136">.</TOKEN>
</SEG>
<SEG end_char="4261" id="segment-35" start_char="4138">
<ORIGINAL_TEXT>It is laboratories in the USA (and France) that trained some of the scientists in the Wuhan labs working with coronaviruses.</ORIGINAL_TEXT>
<TOKEN end_char="4139" id="token-35-0" morph="none" pos="word" start_char="4138">It</TOKEN>
<TOKEN end_char="4142" id="token-35-1" morph="none" pos="word" start_char="4141">is</TOKEN>
<TOKEN end_char="4155" id="token-35-2" morph="none" pos="word" start_char="4144">laboratories</TOKEN>
<TOKEN end_char="4158" id="token-35-3" morph="none" pos="word" start_char="4157">in</TOKEN>
<TOKEN end_char="4162" id="token-35-4" morph="none" pos="word" start_char="4160">the</TOKEN>
<TOKEN end_char="4166" id="token-35-5" morph="none" pos="word" start_char="4164">USA</TOKEN>
<TOKEN end_char="4168" id="token-35-6" morph="none" pos="punct" start_char="4168">(</TOKEN>
<TOKEN end_char="4171" id="token-35-7" morph="none" pos="word" start_char="4169">and</TOKEN>
<TOKEN end_char="4178" id="token-35-8" morph="none" pos="word" start_char="4173">France</TOKEN>
<TOKEN end_char="4179" id="token-35-9" morph="none" pos="punct" start_char="4179">)</TOKEN>
<TOKEN end_char="4184" id="token-35-10" morph="none" pos="word" start_char="4181">that</TOKEN>
<TOKEN end_char="4192" id="token-35-11" morph="none" pos="word" start_char="4186">trained</TOKEN>
<TOKEN end_char="4197" id="token-35-12" morph="none" pos="word" start_char="4194">some</TOKEN>
<TOKEN end_char="4200" id="token-35-13" morph="none" pos="word" start_char="4199">of</TOKEN>
<TOKEN end_char="4204" id="token-35-14" morph="none" pos="word" start_char="4202">the</TOKEN>
<TOKEN end_char="4215" id="token-35-15" morph="none" pos="word" start_char="4206">scientists</TOKEN>
<TOKEN end_char="4218" id="token-35-16" morph="none" pos="word" start_char="4217">in</TOKEN>
<TOKEN end_char="4222" id="token-35-17" morph="none" pos="word" start_char="4220">the</TOKEN>
<TOKEN end_char="4228" id="token-35-18" morph="none" pos="word" start_char="4224">Wuhan</TOKEN>
<TOKEN end_char="4233" id="token-35-19" morph="none" pos="word" start_char="4230">labs</TOKEN>
<TOKEN end_char="4241" id="token-35-20" morph="none" pos="word" start_char="4235">working</TOKEN>
<TOKEN end_char="4246" id="token-35-21" morph="none" pos="word" start_char="4243">with</TOKEN>
<TOKEN end_char="4260" id="token-35-22" morph="none" pos="word" start_char="4248">coronaviruses</TOKEN>
<TOKEN end_char="4261" id="token-35-23" morph="none" pos="punct" start_char="4261">.</TOKEN>
</SEG>
<SEG end_char="4310" id="segment-36" start_char="4263">
<ORIGINAL_TEXT>There is already too much trust and cooperation!</ORIGINAL_TEXT>
<TOKEN end_char="4267" id="token-36-0" morph="none" pos="word" start_char="4263">There</TOKEN>
<TOKEN end_char="4270" id="token-36-1" morph="none" pos="word" start_char="4269">is</TOKEN>
<TOKEN end_char="4278" id="token-36-2" morph="none" pos="word" start_char="4272">already</TOKEN>
<TOKEN end_char="4282" id="token-36-3" morph="none" pos="word" start_char="4280">too</TOKEN>
<TOKEN end_char="4287" id="token-36-4" morph="none" pos="word" start_char="4284">much</TOKEN>
<TOKEN end_char="4293" id="token-36-5" morph="none" pos="word" start_char="4289">trust</TOKEN>
<TOKEN end_char="4297" id="token-36-6" morph="none" pos="word" start_char="4295">and</TOKEN>
<TOKEN end_char="4309" id="token-36-7" morph="none" pos="word" start_char="4299">cooperation</TOKEN>
<TOKEN end_char="4310" id="token-36-8" morph="none" pos="punct" start_char="4310">!</TOKEN>
</SEG>
<SEG end_char="4453" id="segment-37" start_char="4312">
<ORIGINAL_TEXT>The scientific community needs oversight from people who are not building their careers through the technologies they are claiming to oversee.</ORIGINAL_TEXT>
<TOKEN end_char="4314" id="token-37-0" morph="none" pos="word" start_char="4312">The</TOKEN>
<TOKEN end_char="4325" id="token-37-1" morph="none" pos="word" start_char="4316">scientific</TOKEN>
<TOKEN end_char="4335" id="token-37-2" morph="none" pos="word" start_char="4327">community</TOKEN>
<TOKEN end_char="4341" id="token-37-3" morph="none" pos="word" start_char="4337">needs</TOKEN>
<TOKEN end_char="4351" id="token-37-4" morph="none" pos="word" start_char="4343">oversight</TOKEN>
<TOKEN end_char="4356" id="token-37-5" morph="none" pos="word" start_char="4353">from</TOKEN>
<TOKEN end_char="4363" id="token-37-6" morph="none" pos="word" start_char="4358">people</TOKEN>
<TOKEN end_char="4367" id="token-37-7" morph="none" pos="word" start_char="4365">who</TOKEN>
<TOKEN end_char="4371" id="token-37-8" morph="none" pos="word" start_char="4369">are</TOKEN>
<TOKEN end_char="4375" id="token-37-9" morph="none" pos="word" start_char="4373">not</TOKEN>
<TOKEN end_char="4384" id="token-37-10" morph="none" pos="word" start_char="4377">building</TOKEN>
<TOKEN end_char="4390" id="token-37-11" morph="none" pos="word" start_char="4386">their</TOKEN>
<TOKEN end_char="4398" id="token-37-12" morph="none" pos="word" start_char="4392">careers</TOKEN>
<TOKEN end_char="4406" id="token-37-13" morph="none" pos="word" start_char="4400">through</TOKEN>
<TOKEN end_char="4410" id="token-37-14" morph="none" pos="word" start_char="4408">the</TOKEN>
<TOKEN end_char="4423" id="token-37-15" morph="none" pos="word" start_char="4412">technologies</TOKEN>
<TOKEN end_char="4428" id="token-37-16" morph="none" pos="word" start_char="4425">they</TOKEN>
<TOKEN end_char="4432" id="token-37-17" morph="none" pos="word" start_char="4430">are</TOKEN>
<TOKEN end_char="4441" id="token-37-18" morph="none" pos="word" start_char="4434">claiming</TOKEN>
<TOKEN end_char="4444" id="token-37-19" morph="none" pos="word" start_char="4443">to</TOKEN>
<TOKEN end_char="4452" id="token-37-20" morph="none" pos="word" start_char="4446">oversee</TOKEN>
<TOKEN end_char="4453" id="token-37-21" morph="none" pos="punct" start_char="4453">.</TOKEN>
</SEG>
<SEG end_char="4546" id="segment-38" start_char="4455">
<ORIGINAL_TEXT>For example, gain-of-function research on deadly viruses in unsafe laboratories… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="4457" id="token-38-0" morph="none" pos="word" start_char="4455">For</TOKEN>
<TOKEN end_char="4465" id="token-38-1" morph="none" pos="word" start_char="4459">example</TOKEN>
<TOKEN end_char="4466" id="token-38-2" morph="none" pos="punct" start_char="4466">,</TOKEN>
<TOKEN end_char="4483" id="token-38-3" morph="none" pos="unknown" start_char="4468">gain-of-function</TOKEN>
<TOKEN end_char="4492" id="token-38-4" morph="none" pos="word" start_char="4485">research</TOKEN>
<TOKEN end_char="4495" id="token-38-5" morph="none" pos="word" start_char="4494">on</TOKEN>
<TOKEN end_char="4502" id="token-38-6" morph="none" pos="word" start_char="4497">deadly</TOKEN>
<TOKEN end_char="4510" id="token-38-7" morph="none" pos="word" start_char="4504">viruses</TOKEN>
<TOKEN end_char="4513" id="token-38-8" morph="none" pos="word" start_char="4512">in</TOKEN>
<TOKEN end_char="4520" id="token-38-9" morph="none" pos="word" start_char="4515">unsafe</TOKEN>
<TOKEN end_char="4533" id="token-38-10" morph="none" pos="word" start_char="4522">laboratories</TOKEN>
<TOKEN end_char="4534" id="token-38-11" morph="none" pos="punct" start_char="4534">…</TOKEN>
<TOKEN end_char="4539" id="token-38-12" morph="none" pos="word" start_char="4536">Read</TOKEN>
<TOKEN end_char="4544" id="token-38-13" morph="none" pos="word" start_char="4541">more</TOKEN>
<TOKEN end_char="4546" id="token-38-14" morph="none" pos="punct" start_char="4546">»</TOKEN>
</SEG>
<SEG end_char="4606" id="segment-39" start_char="4550">
<ORIGINAL_TEXT>Yes, cooperation with the Chinese scientist is important.</ORIGINAL_TEXT>
<TOKEN end_char="4552" id="token-39-0" morph="none" pos="word" start_char="4550">Yes</TOKEN>
<TOKEN end_char="4553" id="token-39-1" morph="none" pos="punct" start_char="4553">,</TOKEN>
<TOKEN end_char="4565" id="token-39-2" morph="none" pos="word" start_char="4555">cooperation</TOKEN>
<TOKEN end_char="4570" id="token-39-3" morph="none" pos="word" start_char="4567">with</TOKEN>
<TOKEN end_char="4574" id="token-39-4" morph="none" pos="word" start_char="4572">the</TOKEN>
<TOKEN end_char="4582" id="token-39-5" morph="none" pos="word" start_char="4576">Chinese</TOKEN>
<TOKEN end_char="4592" id="token-39-6" morph="none" pos="word" start_char="4584">scientist</TOKEN>
<TOKEN end_char="4595" id="token-39-7" morph="none" pos="word" start_char="4594">is</TOKEN>
<TOKEN end_char="4605" id="token-39-8" morph="none" pos="word" start_char="4597">important</TOKEN>
<TOKEN end_char="4606" id="token-39-9" morph="none" pos="punct" start_char="4606">.</TOKEN>
</SEG>
<SEG end_char="4727" id="segment-40" start_char="4608">
<ORIGINAL_TEXT>But first we have to get over the hurdle of the Chinese government that doesn’t allow for free speech and communication.</ORIGINAL_TEXT>
<TOKEN end_char="4610" id="token-40-0" morph="none" pos="word" start_char="4608">But</TOKEN>
<TOKEN end_char="4616" id="token-40-1" morph="none" pos="word" start_char="4612">first</TOKEN>
<TOKEN end_char="4619" id="token-40-2" morph="none" pos="word" start_char="4618">we</TOKEN>
<TOKEN end_char="4624" id="token-40-3" morph="none" pos="word" start_char="4621">have</TOKEN>
<TOKEN end_char="4627" id="token-40-4" morph="none" pos="word" start_char="4626">to</TOKEN>
<TOKEN end_char="4631" id="token-40-5" morph="none" pos="word" start_char="4629">get</TOKEN>
<TOKEN end_char="4636" id="token-40-6" morph="none" pos="word" start_char="4633">over</TOKEN>
<TOKEN end_char="4640" id="token-40-7" morph="none" pos="word" start_char="4638">the</TOKEN>
<TOKEN end_char="4647" id="token-40-8" morph="none" pos="word" start_char="4642">hurdle</TOKEN>
<TOKEN end_char="4650" id="token-40-9" morph="none" pos="word" start_char="4649">of</TOKEN>
<TOKEN end_char="4654" id="token-40-10" morph="none" pos="word" start_char="4652">the</TOKEN>
<TOKEN end_char="4662" id="token-40-11" morph="none" pos="word" start_char="4656">Chinese</TOKEN>
<TOKEN end_char="4673" id="token-40-12" morph="none" pos="word" start_char="4664">government</TOKEN>
<TOKEN end_char="4678" id="token-40-13" morph="none" pos="word" start_char="4675">that</TOKEN>
<TOKEN end_char="4686" id="token-40-14" morph="none" pos="word" start_char="4680">doesn’t</TOKEN>
<TOKEN end_char="4692" id="token-40-15" morph="none" pos="word" start_char="4688">allow</TOKEN>
<TOKEN end_char="4696" id="token-40-16" morph="none" pos="word" start_char="4694">for</TOKEN>
<TOKEN end_char="4701" id="token-40-17" morph="none" pos="word" start_char="4698">free</TOKEN>
<TOKEN end_char="4708" id="token-40-18" morph="none" pos="word" start_char="4703">speech</TOKEN>
<TOKEN end_char="4712" id="token-40-19" morph="none" pos="word" start_char="4710">and</TOKEN>
<TOKEN end_char="4726" id="token-40-20" morph="none" pos="word" start_char="4714">communication</TOKEN>
<TOKEN end_char="4727" id="token-40-21" morph="none" pos="punct" start_char="4727">.</TOKEN>
</SEG>
<SEG end_char="4778" id="segment-41" start_char="4729">
<ORIGINAL_TEXT>Until that happens there will always be mysteries.</ORIGINAL_TEXT>
<TOKEN end_char="4733" id="token-41-0" morph="none" pos="word" start_char="4729">Until</TOKEN>
<TOKEN end_char="4738" id="token-41-1" morph="none" pos="word" start_char="4735">that</TOKEN>
<TOKEN end_char="4746" id="token-41-2" morph="none" pos="word" start_char="4740">happens</TOKEN>
<TOKEN end_char="4752" id="token-41-3" morph="none" pos="word" start_char="4748">there</TOKEN>
<TOKEN end_char="4757" id="token-41-4" morph="none" pos="word" start_char="4754">will</TOKEN>
<TOKEN end_char="4764" id="token-41-5" morph="none" pos="word" start_char="4759">always</TOKEN>
<TOKEN end_char="4767" id="token-41-6" morph="none" pos="word" start_char="4766">be</TOKEN>
<TOKEN end_char="4777" id="token-41-7" morph="none" pos="word" start_char="4769">mysteries</TOKEN>
<TOKEN end_char="4778" id="token-41-8" morph="none" pos="punct" start_char="4778">.</TOKEN>
</SEG>
<SEG end_char="4793" id="segment-42" start_char="4782">
<ORIGINAL_TEXT>And US does?</ORIGINAL_TEXT>
<TOKEN end_char="4784" id="token-42-0" morph="none" pos="word" start_char="4782">And</TOKEN>
<TOKEN end_char="4787" id="token-42-1" morph="none" pos="word" start_char="4786">US</TOKEN>
<TOKEN end_char="4792" id="token-42-2" morph="none" pos="word" start_char="4789">does</TOKEN>
<TOKEN end_char="4793" id="token-42-3" morph="none" pos="punct" start_char="4793">?</TOKEN>
<TRANSLATED_TEXT>E gli Stati Uniti?</TRANSLATED_TEXT><DETECTED_LANGUAGE>af</DETECTED_LANGUAGE></SEG>
<SEG end_char="4938" id="segment-43" start_char="4795">
<ORIGINAL_TEXT>It seems far more of a cacophony in place of information, and where information is needed (from the White House), there’s foolhardy podium-love.</ORIGINAL_TEXT>
<TOKEN end_char="4796" id="token-43-0" morph="none" pos="word" start_char="4795">It</TOKEN>
<TOKEN end_char="4802" id="token-43-1" morph="none" pos="word" start_char="4798">seems</TOKEN>
<TOKEN end_char="4806" id="token-43-2" morph="none" pos="word" start_char="4804">far</TOKEN>
<TOKEN end_char="4811" id="token-43-3" morph="none" pos="word" start_char="4808">more</TOKEN>
<TOKEN end_char="4814" id="token-43-4" morph="none" pos="word" start_char="4813">of</TOKEN>
<TOKEN end_char="4816" id="token-43-5" morph="none" pos="word" start_char="4816">a</TOKEN>
<TOKEN end_char="4826" id="token-43-6" morph="none" pos="word" start_char="4818">cacophony</TOKEN>
<TOKEN end_char="4829" id="token-43-7" morph="none" pos="word" start_char="4828">in</TOKEN>
<TOKEN end_char="4835" id="token-43-8" morph="none" pos="word" start_char="4831">place</TOKEN>
<TOKEN end_char="4838" id="token-43-9" morph="none" pos="word" start_char="4837">of</TOKEN>
<TOKEN end_char="4850" id="token-43-10" morph="none" pos="word" start_char="4840">information</TOKEN>
<TOKEN end_char="4851" id="token-43-11" morph="none" pos="punct" start_char="4851">,</TOKEN>
<TOKEN end_char="4855" id="token-43-12" morph="none" pos="word" start_char="4853">and</TOKEN>
<TOKEN end_char="4861" id="token-43-13" morph="none" pos="word" start_char="4857">where</TOKEN>
<TOKEN end_char="4873" id="token-43-14" morph="none" pos="word" start_char="4863">information</TOKEN>
<TOKEN end_char="4876" id="token-43-15" morph="none" pos="word" start_char="4875">is</TOKEN>
<TOKEN end_char="4883" id="token-43-16" morph="none" pos="word" start_char="4878">needed</TOKEN>
<TOKEN end_char="4885" id="token-43-17" morph="none" pos="punct" start_char="4885">(</TOKEN>
<TOKEN end_char="4889" id="token-43-18" morph="none" pos="word" start_char="4886">from</TOKEN>
<TOKEN end_char="4893" id="token-43-19" morph="none" pos="word" start_char="4891">the</TOKEN>
<TOKEN end_char="4899" id="token-43-20" morph="none" pos="word" start_char="4895">White</TOKEN>
<TOKEN end_char="4905" id="token-43-21" morph="none" pos="word" start_char="4901">House</TOKEN>
<TOKEN end_char="4907" id="token-43-22" morph="none" pos="punct" start_char="4906">),</TOKEN>
<TOKEN end_char="4915" id="token-43-23" morph="none" pos="word" start_char="4909">there’s</TOKEN>
<TOKEN end_char="4925" id="token-43-24" morph="none" pos="word" start_char="4917">foolhardy</TOKEN>
<TOKEN end_char="4937" id="token-43-25" morph="none" pos="unknown" start_char="4927">podium-love</TOKEN>
<TOKEN end_char="4938" id="token-43-26" morph="none" pos="punct" start_char="4938">.</TOKEN>
</SEG>
<SEG end_char="5047" id="segment-44" start_char="4942">
<ORIGINAL_TEXT>This kind of report could so quickly become the source of a conspiracy theory in the hands of "Fox News"!!</ORIGINAL_TEXT>
<TOKEN end_char="4945" id="token-44-0" morph="none" pos="word" start_char="4942">This</TOKEN>
<TOKEN end_char="4950" id="token-44-1" morph="none" pos="word" start_char="4947">kind</TOKEN>
<TOKEN end_char="4953" id="token-44-2" morph="none" pos="word" start_char="4952">of</TOKEN>
<TOKEN end_char="4960" id="token-44-3" morph="none" pos="word" start_char="4955">report</TOKEN>
<TOKEN end_char="4966" id="token-44-4" morph="none" pos="word" start_char="4962">could</TOKEN>
<TOKEN end_char="4969" id="token-44-5" morph="none" pos="word" start_char="4968">so</TOKEN>
<TOKEN end_char="4977" id="token-44-6" morph="none" pos="word" start_char="4971">quickly</TOKEN>
<TOKEN end_char="4984" id="token-44-7" morph="none" pos="word" start_char="4979">become</TOKEN>
<TOKEN end_char="4988" id="token-44-8" morph="none" pos="word" start_char="4986">the</TOKEN>
<TOKEN end_char="4995" id="token-44-9" morph="none" pos="word" start_char="4990">source</TOKEN>
<TOKEN end_char="4998" id="token-44-10" morph="none" pos="word" start_char="4997">of</TOKEN>
<TOKEN end_char="5000" id="token-44-11" morph="none" pos="word" start_char="5000">a</TOKEN>
<TOKEN end_char="5011" id="token-44-12" morph="none" pos="word" start_char="5002">conspiracy</TOKEN>
<TOKEN end_char="5018" id="token-44-13" morph="none" pos="word" start_char="5013">theory</TOKEN>
<TOKEN end_char="5021" id="token-44-14" morph="none" pos="word" start_char="5020">in</TOKEN>
<TOKEN end_char="5025" id="token-44-15" morph="none" pos="word" start_char="5023">the</TOKEN>
<TOKEN end_char="5031" id="token-44-16" morph="none" pos="word" start_char="5027">hands</TOKEN>
<TOKEN end_char="5034" id="token-44-17" morph="none" pos="word" start_char="5033">of</TOKEN>
<TOKEN end_char="5036" id="token-44-18" morph="none" pos="punct" start_char="5036">"</TOKEN>
<TOKEN end_char="5039" id="token-44-19" morph="none" pos="word" start_char="5037">Fox</TOKEN>
<TOKEN end_char="5044" id="token-44-20" morph="none" pos="word" start_char="5041">News</TOKEN>
<TOKEN end_char="5047" id="token-44-21" morph="none" pos="punct" start_char="5045">"!!</TOKEN>
</SEG>
<SEG end_char="5110" id="segment-45" start_char="5051">
<ORIGINAL_TEXT>However, that does not mean the issue should not be studied.</ORIGINAL_TEXT>
<TOKEN end_char="5057" id="token-45-0" morph="none" pos="word" start_char="5051">However</TOKEN>
<TOKEN end_char="5058" id="token-45-1" morph="none" pos="punct" start_char="5058">,</TOKEN>
<TOKEN end_char="5063" id="token-45-2" morph="none" pos="word" start_char="5060">that</TOKEN>
<TOKEN end_char="5068" id="token-45-3" morph="none" pos="word" start_char="5065">does</TOKEN>
<TOKEN end_char="5072" id="token-45-4" morph="none" pos="word" start_char="5070">not</TOKEN>
<TOKEN end_char="5077" id="token-45-5" morph="none" pos="word" start_char="5074">mean</TOKEN>
<TOKEN end_char="5081" id="token-45-6" morph="none" pos="word" start_char="5079">the</TOKEN>
<TOKEN end_char="5087" id="token-45-7" morph="none" pos="word" start_char="5083">issue</TOKEN>
<TOKEN end_char="5094" id="token-45-8" morph="none" pos="word" start_char="5089">should</TOKEN>
<TOKEN end_char="5098" id="token-45-9" morph="none" pos="word" start_char="5096">not</TOKEN>
<TOKEN end_char="5101" id="token-45-10" morph="none" pos="word" start_char="5100">be</TOKEN>
<TOKEN end_char="5109" id="token-45-11" morph="none" pos="word" start_char="5103">studied</TOKEN>
<TOKEN end_char="5110" id="token-45-12" morph="none" pos="punct" start_char="5110">.</TOKEN>
</SEG>
<SEG end_char="5180" id="segment-46" start_char="5112">
<ORIGINAL_TEXT>There are already a multitude of conspiracy theories out there on it.</ORIGINAL_TEXT>
<TOKEN end_char="5116" id="token-46-0" morph="none" pos="word" start_char="5112">There</TOKEN>
<TOKEN end_char="5120" id="token-46-1" morph="none" pos="word" start_char="5118">are</TOKEN>
<TOKEN end_char="5128" id="token-46-2" morph="none" pos="word" start_char="5122">already</TOKEN>
<TOKEN end_char="5130" id="token-46-3" morph="none" pos="word" start_char="5130">a</TOKEN>
<TOKEN end_char="5140" id="token-46-4" morph="none" pos="word" start_char="5132">multitude</TOKEN>
<TOKEN end_char="5143" id="token-46-5" morph="none" pos="word" start_char="5142">of</TOKEN>
<TOKEN end_char="5154" id="token-46-6" morph="none" pos="word" start_char="5145">conspiracy</TOKEN>
<TOKEN end_char="5163" id="token-46-7" morph="none" pos="word" start_char="5156">theories</TOKEN>
<TOKEN end_char="5167" id="token-46-8" morph="none" pos="word" start_char="5165">out</TOKEN>
<TOKEN end_char="5173" id="token-46-9" morph="none" pos="word" start_char="5169">there</TOKEN>
<TOKEN end_char="5176" id="token-46-10" morph="none" pos="word" start_char="5175">on</TOKEN>
<TOKEN end_char="5179" id="token-46-11" morph="none" pos="word" start_char="5178">it</TOKEN>
<TOKEN end_char="5180" id="token-46-12" morph="none" pos="punct" start_char="5180">.</TOKEN>
</SEG>
<SEG end_char="5272" id="segment-47" start_char="5182">
<ORIGINAL_TEXT>You don’t find the truth by worrying that someone might misread the purpose of your search.</ORIGINAL_TEXT>
<TOKEN end_char="5184" id="token-47-0" morph="none" pos="word" start_char="5182">You</TOKEN>
<TOKEN end_char="5190" id="token-47-1" morph="none" pos="word" start_char="5186">don’t</TOKEN>
<TOKEN end_char="5195" id="token-47-2" morph="none" pos="word" start_char="5192">find</TOKEN>
<TOKEN end_char="5199" id="token-47-3" morph="none" pos="word" start_char="5197">the</TOKEN>
<TOKEN end_char="5205" id="token-47-4" morph="none" pos="word" start_char="5201">truth</TOKEN>
<TOKEN end_char="5208" id="token-47-5" morph="none" pos="word" start_char="5207">by</TOKEN>
<TOKEN end_char="5217" id="token-47-6" morph="none" pos="word" start_char="5210">worrying</TOKEN>
<TOKEN end_char="5222" id="token-47-7" morph="none" pos="word" start_char="5219">that</TOKEN>
<TOKEN end_char="5230" id="token-47-8" morph="none" pos="word" start_char="5224">someone</TOKEN>
<TOKEN end_char="5236" id="token-47-9" morph="none" pos="word" start_char="5232">might</TOKEN>
<TOKEN end_char="5244" id="token-47-10" morph="none" pos="word" start_char="5238">misread</TOKEN>
<TOKEN end_char="5248" id="token-47-11" morph="none" pos="word" start_char="5246">the</TOKEN>
<TOKEN end_char="5256" id="token-47-12" morph="none" pos="word" start_char="5250">purpose</TOKEN>
<TOKEN end_char="5259" id="token-47-13" morph="none" pos="word" start_char="5258">of</TOKEN>
<TOKEN end_char="5264" id="token-47-14" morph="none" pos="word" start_char="5261">your</TOKEN>
<TOKEN end_char="5271" id="token-47-15" morph="none" pos="word" start_char="5266">search</TOKEN>
<TOKEN end_char="5272" id="token-47-16" morph="none" pos="punct" start_char="5272">.</TOKEN>
</SEG>
<SEG end_char="5349" id="segment-48" start_char="5276">
<ORIGINAL_TEXT>Why is finding the root cause or discussing potential causes inconvenient?</ORIGINAL_TEXT>
<TOKEN end_char="5278" id="token-48-0" morph="none" pos="word" start_char="5276">Why</TOKEN>
<TOKEN end_char="5281" id="token-48-1" morph="none" pos="word" start_char="5280">is</TOKEN>
<TOKEN end_char="5289" id="token-48-2" morph="none" pos="word" start_char="5283">finding</TOKEN>
<TOKEN end_char="5293" id="token-48-3" morph="none" pos="word" start_char="5291">the</TOKEN>
<TOKEN end_char="5298" id="token-48-4" morph="none" pos="word" start_char="5295">root</TOKEN>
<TOKEN end_char="5304" id="token-48-5" morph="none" pos="word" start_char="5300">cause</TOKEN>
<TOKEN end_char="5307" id="token-48-6" morph="none" pos="word" start_char="5306">or</TOKEN>
<TOKEN end_char="5318" id="token-48-7" morph="none" pos="word" start_char="5309">discussing</TOKEN>
<TOKEN end_char="5328" id="token-48-8" morph="none" pos="word" start_char="5320">potential</TOKEN>
<TOKEN end_char="5335" id="token-48-9" morph="none" pos="word" start_char="5330">causes</TOKEN>
<TOKEN end_char="5348" id="token-48-10" morph="none" pos="word" start_char="5337">inconvenient</TOKEN>
<TOKEN end_char="5349" id="token-48-11" morph="none" pos="punct" start_char="5349">?</TOKEN>
</SEG>
<SEG end_char="5421" id="segment-49" start_char="5351">
<ORIGINAL_TEXT>What if all it is proven to be correct, will it be a conspiracy theory?</ORIGINAL_TEXT>
<TOKEN end_char="5354" id="token-49-0" morph="none" pos="word" start_char="5351">What</TOKEN>
<TOKEN end_char="5357" id="token-49-1" morph="none" pos="word" start_char="5356">if</TOKEN>
<TOKEN end_char="5361" id="token-49-2" morph="none" pos="word" start_char="5359">all</TOKEN>
<TOKEN end_char="5364" id="token-49-3" morph="none" pos="word" start_char="5363">it</TOKEN>
<TOKEN end_char="5367" id="token-49-4" morph="none" pos="word" start_char="5366">is</TOKEN>
<TOKEN end_char="5374" id="token-49-5" morph="none" pos="word" start_char="5369">proven</TOKEN>
<TOKEN end_char="5377" id="token-49-6" morph="none" pos="word" start_char="5376">to</TOKEN>
<TOKEN end_char="5380" id="token-49-7" morph="none" pos="word" start_char="5379">be</TOKEN>
<TOKEN end_char="5388" id="token-49-8" morph="none" pos="word" start_char="5382">correct</TOKEN>
<TOKEN end_char="5389" id="token-49-9" morph="none" pos="punct" start_char="5389">,</TOKEN>
<TOKEN end_char="5394" id="token-49-10" morph="none" pos="word" start_char="5391">will</TOKEN>
<TOKEN end_char="5397" id="token-49-11" morph="none" pos="word" start_char="5396">it</TOKEN>
<TOKEN end_char="5400" id="token-49-12" morph="none" pos="word" start_char="5399">be</TOKEN>
<TOKEN end_char="5402" id="token-49-13" morph="none" pos="word" start_char="5402">a</TOKEN>
<TOKEN end_char="5413" id="token-49-14" morph="none" pos="word" start_char="5404">conspiracy</TOKEN>
<TOKEN end_char="5420" id="token-49-15" morph="none" pos="word" start_char="5415">theory</TOKEN>
<TOKEN end_char="5421" id="token-49-16" morph="none" pos="punct" start_char="5421">?</TOKEN>
</SEG>
<SEG end_char="5570" id="segment-50" start_char="5423">
<ORIGINAL_TEXT>Does that bother or worry you MORE than understanding who/how is responsible for what the hell is killing people and shutting down the entire world?</ORIGINAL_TEXT>
<TOKEN end_char="5426" id="token-50-0" morph="none" pos="word" start_char="5423">Does</TOKEN>
<TOKEN end_char="5431" id="token-50-1" morph="none" pos="word" start_char="5428">that</TOKEN>
<TOKEN end_char="5438" id="token-50-2" morph="none" pos="word" start_char="5433">bother</TOKEN>
<TOKEN end_char="5441" id="token-50-3" morph="none" pos="word" start_char="5440">or</TOKEN>
<TOKEN end_char="5447" id="token-50-4" morph="none" pos="word" start_char="5443">worry</TOKEN>
<TOKEN end_char="5451" id="token-50-5" morph="none" pos="word" start_char="5449">you</TOKEN>
<TOKEN end_char="5456" id="token-50-6" morph="none" pos="word" start_char="5453">MORE</TOKEN>
<TOKEN end_char="5461" id="token-50-7" morph="none" pos="word" start_char="5458">than</TOKEN>
<TOKEN end_char="5475" id="token-50-8" morph="none" pos="word" start_char="5463">understanding</TOKEN>
<TOKEN end_char="5483" id="token-50-9" morph="none" pos="unknown" start_char="5477">who/how</TOKEN>
<TOKEN end_char="5486" id="token-50-10" morph="none" pos="word" start_char="5485">is</TOKEN>
<TOKEN end_char="5498" id="token-50-11" morph="none" pos="word" start_char="5488">responsible</TOKEN>
<TOKEN end_char="5502" id="token-50-12" morph="none" pos="word" start_char="5500">for</TOKEN>
<TOKEN end_char="5507" id="token-50-13" morph="none" pos="word" start_char="5504">what</TOKEN>
<TOKEN end_char="5511" id="token-50-14" morph="none" pos="word" start_char="5509">the</TOKEN>
<TOKEN end_char="5516" id="token-50-15" morph="none" pos="word" start_char="5513">hell</TOKEN>
<TOKEN end_char="5519" id="token-50-16" morph="none" pos="word" start_char="5518">is</TOKEN>
<TOKEN end_char="5527" id="token-50-17" morph="none" pos="word" start_char="5521">killing</TOKEN>
<TOKEN end_char="5534" id="token-50-18" morph="none" pos="word" start_char="5529">people</TOKEN>
<TOKEN end_char="5538" id="token-50-19" morph="none" pos="word" start_char="5536">and</TOKEN>
<TOKEN end_char="5547" id="token-50-20" morph="none" pos="word" start_char="5540">shutting</TOKEN>
<TOKEN end_char="5552" id="token-50-21" morph="none" pos="word" start_char="5549">down</TOKEN>
<TOKEN end_char="5556" id="token-50-22" morph="none" pos="word" start_char="5554">the</TOKEN>
<TOKEN end_char="5563" id="token-50-23" morph="none" pos="word" start_char="5558">entire</TOKEN>
<TOKEN end_char="5569" id="token-50-24" morph="none" pos="word" start_char="5565">world</TOKEN>
<TOKEN end_char="5570" id="token-50-25" morph="none" pos="punct" start_char="5570">?</TOKEN>
</SEG>
<SEG end_char="5635" id="segment-51" start_char="5572">
<ORIGINAL_TEXT>It’s time for people to drop the partisan and childish comments…</ORIGINAL_TEXT>
<TOKEN end_char="5575" id="token-51-0" morph="none" pos="word" start_char="5572">It’s</TOKEN>
<TOKEN end_char="5580" id="token-51-1" morph="none" pos="word" start_char="5577">time</TOKEN>
<TOKEN end_char="5584" id="token-51-2" morph="none" pos="word" start_char="5582">for</TOKEN>
<TOKEN end_char="5591" id="token-51-3" morph="none" pos="word" start_char="5586">people</TOKEN>
<TOKEN end_char="5594" id="token-51-4" morph="none" pos="word" start_char="5593">to</TOKEN>
<TOKEN end_char="5599" id="token-51-5" morph="none" pos="word" start_char="5596">drop</TOKEN>
<TOKEN end_char="5603" id="token-51-6" morph="none" pos="word" start_char="5601">the</TOKEN>
<TOKEN end_char="5612" id="token-51-7" morph="none" pos="word" start_char="5605">partisan</TOKEN>
<TOKEN end_char="5616" id="token-51-8" morph="none" pos="word" start_char="5614">and</TOKEN>
<TOKEN end_char="5625" id="token-51-9" morph="none" pos="word" start_char="5618">childish</TOKEN>
<TOKEN end_char="5634" id="token-51-10" morph="none" pos="word" start_char="5627">comments</TOKEN>
<TOKEN end_char="5635" id="token-51-11" morph="none" pos="punct" start_char="5635">…</TOKEN>
</SEG>
<SEG end_char="5715" id="segment-52" start_char="5638">
<ORIGINAL_TEXT>Become a part of the solution instead of just an observer that offers nothing.</ORIGINAL_TEXT>
<TOKEN end_char="5643" id="token-52-0" morph="none" pos="word" start_char="5638">Become</TOKEN>
<TOKEN end_char="5645" id="token-52-1" morph="none" pos="word" start_char="5645">a</TOKEN>
<TOKEN end_char="5650" id="token-52-2" morph="none" pos="word" start_char="5647">part</TOKEN>
<TOKEN end_char="5653" id="token-52-3" morph="none" pos="word" start_char="5652">of</TOKEN>
<TOKEN end_char="5657" id="token-52-4" morph="none" pos="word" start_char="5655">the</TOKEN>
<TOKEN end_char="5666" id="token-52-5" morph="none" pos="word" start_char="5659">solution</TOKEN>
<TOKEN end_char="5674" id="token-52-6" morph="none" pos="word" start_char="5668">instead</TOKEN>
<TOKEN end_char="5677" id="token-52-7" morph="none" pos="word" start_char="5676">of</TOKEN>
<TOKEN end_char="5682" id="token-52-8" morph="none" pos="word" start_char="5679">just</TOKEN>
<TOKEN end_char="5685" id="token-52-9" morph="none" pos="word" start_char="5684">an</TOKEN>
<TOKEN end_char="5694" id="token-52-10" morph="none" pos="word" start_char="5687">observer</TOKEN>
<TOKEN end_char="5699" id="token-52-11" morph="none" pos="word" start_char="5696">that</TOKEN>
<TOKEN end_char="5706" id="token-52-12" morph="none" pos="word" start_char="5701">offers</TOKEN>
<TOKEN end_char="5714" id="token-52-13" morph="none" pos="word" start_char="5708">nothing</TOKEN>
<TOKEN end_char="5715" id="token-52-14" morph="none" pos="punct" start_char="5715">.</TOKEN>
</SEG>
<SEG end_char="5845" id="segment-53" start_char="5719">
<ORIGINAL_TEXT>A question from a layman…in the lab release scenario, what is the source of the virus that might have been released from a lab?</ORIGINAL_TEXT>
<TOKEN end_char="5719" id="token-53-0" morph="none" pos="word" start_char="5719">A</TOKEN>
<TOKEN end_char="5728" id="token-53-1" morph="none" pos="word" start_char="5721">question</TOKEN>
<TOKEN end_char="5733" id="token-53-2" morph="none" pos="word" start_char="5730">from</TOKEN>
<TOKEN end_char="5735" id="token-53-3" morph="none" pos="word" start_char="5735">a</TOKEN>
<TOKEN end_char="5745" id="token-53-4" morph="none" pos="unknown" start_char="5737">layman…in</TOKEN>
<TOKEN end_char="5749" id="token-53-5" morph="none" pos="word" start_char="5747">the</TOKEN>
<TOKEN end_char="5753" id="token-53-6" morph="none" pos="word" start_char="5751">lab</TOKEN>
<TOKEN end_char="5761" id="token-53-7" morph="none" pos="word" start_char="5755">release</TOKEN>
<TOKEN end_char="5770" id="token-53-8" morph="none" pos="word" start_char="5763">scenario</TOKEN>
<TOKEN end_char="5771" id="token-53-9" morph="none" pos="punct" start_char="5771">,</TOKEN>
<TOKEN end_char="5776" id="token-53-10" morph="none" pos="word" start_char="5773">what</TOKEN>
<TOKEN end_char="5779" id="token-53-11" morph="none" pos="word" start_char="5778">is</TOKEN>
<TOKEN end_char="5783" id="token-53-12" morph="none" pos="word" start_char="5781">the</TOKEN>
<TOKEN end_char="5790" id="token-53-13" morph="none" pos="word" start_char="5785">source</TOKEN>
<TOKEN end_char="5793" id="token-53-14" morph="none" pos="word" start_char="5792">of</TOKEN>
<TOKEN end_char="5797" id="token-53-15" morph="none" pos="word" start_char="5795">the</TOKEN>
<TOKEN end_char="5803" id="token-53-16" morph="none" pos="word" start_char="5799">virus</TOKEN>
<TOKEN end_char="5808" id="token-53-17" morph="none" pos="word" start_char="5805">that</TOKEN>
<TOKEN end_char="5814" id="token-53-18" morph="none" pos="word" start_char="5810">might</TOKEN>
<TOKEN end_char="5819" id="token-53-19" morph="none" pos="word" start_char="5816">have</TOKEN>
<TOKEN end_char="5824" id="token-53-20" morph="none" pos="word" start_char="5821">been</TOKEN>
<TOKEN end_char="5833" id="token-53-21" morph="none" pos="word" start_char="5826">released</TOKEN>
<TOKEN end_char="5838" id="token-53-22" morph="none" pos="word" start_char="5835">from</TOKEN>
<TOKEN end_char="5840" id="token-53-23" morph="none" pos="word" start_char="5840">a</TOKEN>
<TOKEN end_char="5844" id="token-53-24" morph="none" pos="word" start_char="5842">lab</TOKEN>
<TOKEN end_char="5845" id="token-53-25" morph="none" pos="punct" start_char="5845">?</TOKEN>
</SEG>
<SEG end_char="5892" id="segment-54" start_char="5847">
<ORIGINAL_TEXT>Are the virus samples collected from the wild?</ORIGINAL_TEXT>
<TOKEN end_char="5849" id="token-54-0" morph="none" pos="word" start_char="5847">Are</TOKEN>
<TOKEN end_char="5853" id="token-54-1" morph="none" pos="word" start_char="5851">the</TOKEN>
<TOKEN end_char="5859" id="token-54-2" morph="none" pos="word" start_char="5855">virus</TOKEN>
<TOKEN end_char="5867" id="token-54-3" morph="none" pos="word" start_char="5861">samples</TOKEN>
<TOKEN end_char="5877" id="token-54-4" morph="none" pos="word" start_char="5869">collected</TOKEN>
<TOKEN end_char="5882" id="token-54-5" morph="none" pos="word" start_char="5879">from</TOKEN>
<TOKEN end_char="5886" id="token-54-6" morph="none" pos="word" start_char="5884">the</TOKEN>
<TOKEN end_char="5891" id="token-54-7" morph="none" pos="word" start_char="5888">wild</TOKEN>
<TOKEN end_char="5892" id="token-54-8" morph="none" pos="punct" start_char="5892">?</TOKEN>
</SEG>
<SEG end_char="6007" id="segment-55" start_char="5894">
<ORIGINAL_TEXT>If so, aren’t the odds much higher that the initial humans infected were exposed to the naturally occurring virus?</ORIGINAL_TEXT>
<TOKEN end_char="5895" id="token-55-0" morph="none" pos="word" start_char="5894">If</TOKEN>
<TOKEN end_char="5898" id="token-55-1" morph="none" pos="word" start_char="5897">so</TOKEN>
<TOKEN end_char="5899" id="token-55-2" morph="none" pos="punct" start_char="5899">,</TOKEN>
<TOKEN end_char="5906" id="token-55-3" morph="none" pos="word" start_char="5901">aren’t</TOKEN>
<TOKEN end_char="5910" id="token-55-4" morph="none" pos="word" start_char="5908">the</TOKEN>
<TOKEN end_char="5915" id="token-55-5" morph="none" pos="word" start_char="5912">odds</TOKEN>
<TOKEN end_char="5920" id="token-55-6" morph="none" pos="word" start_char="5917">much</TOKEN>
<TOKEN end_char="5927" id="token-55-7" morph="none" pos="word" start_char="5922">higher</TOKEN>
<TOKEN end_char="5932" id="token-55-8" morph="none" pos="word" start_char="5929">that</TOKEN>
<TOKEN end_char="5936" id="token-55-9" morph="none" pos="word" start_char="5934">the</TOKEN>
<TOKEN end_char="5944" id="token-55-10" morph="none" pos="word" start_char="5938">initial</TOKEN>
<TOKEN end_char="5951" id="token-55-11" morph="none" pos="word" start_char="5946">humans</TOKEN>
<TOKEN end_char="5960" id="token-55-12" morph="none" pos="word" start_char="5953">infected</TOKEN>
<TOKEN end_char="5965" id="token-55-13" morph="none" pos="word" start_char="5962">were</TOKEN>
<TOKEN end_char="5973" id="token-55-14" morph="none" pos="word" start_char="5967">exposed</TOKEN>
<TOKEN end_char="5976" id="token-55-15" morph="none" pos="word" start_char="5975">to</TOKEN>
<TOKEN end_char="5980" id="token-55-16" morph="none" pos="word" start_char="5978">the</TOKEN>
<TOKEN end_char="5990" id="token-55-17" morph="none" pos="word" start_char="5982">naturally</TOKEN>
<TOKEN end_char="6000" id="token-55-18" morph="none" pos="word" start_char="5992">occurring</TOKEN>
<TOKEN end_char="6006" id="token-55-19" morph="none" pos="word" start_char="6002">virus</TOKEN>
<TOKEN end_char="6007" id="token-55-20" morph="none" pos="punct" start_char="6007">?</TOKEN>
</SEG>
<SEG end_char="6031" id="segment-56" start_char="6009">
<ORIGINAL_TEXT>What am I missing here?</ORIGINAL_TEXT>
<TOKEN end_char="6012" id="token-56-0" morph="none" pos="word" start_char="6009">What</TOKEN>
<TOKEN end_char="6015" id="token-56-1" morph="none" pos="word" start_char="6014">am</TOKEN>
<TOKEN end_char="6017" id="token-56-2" morph="none" pos="word" start_char="6017">I</TOKEN>
<TOKEN end_char="6025" id="token-56-3" morph="none" pos="word" start_char="6019">missing</TOKEN>
<TOKEN end_char="6030" id="token-56-4" morph="none" pos="word" start_char="6027">here</TOKEN>
<TOKEN end_char="6031" id="token-56-5" morph="none" pos="punct" start_char="6031">?</TOKEN>
</SEG>
<SEG end_char="6135" id="segment-57" start_char="6035">
<ORIGINAL_TEXT>Nobody really knows if the source was an accidental lab release or if the cycle was entirely natural.</ORIGINAL_TEXT>
<TOKEN end_char="6040" id="token-57-0" morph="none" pos="word" start_char="6035">Nobody</TOKEN>
<TOKEN end_char="6047" id="token-57-1" morph="none" pos="word" start_char="6042">really</TOKEN>
<TOKEN end_char="6053" id="token-57-2" morph="none" pos="word" start_char="6049">knows</TOKEN>
<TOKEN end_char="6056" id="token-57-3" morph="none" pos="word" start_char="6055">if</TOKEN>
<TOKEN end_char="6060" id="token-57-4" morph="none" pos="word" start_char="6058">the</TOKEN>
<TOKEN end_char="6067" id="token-57-5" morph="none" pos="word" start_char="6062">source</TOKEN>
<TOKEN end_char="6071" id="token-57-6" morph="none" pos="word" start_char="6069">was</TOKEN>
<TOKEN end_char="6074" id="token-57-7" morph="none" pos="word" start_char="6073">an</TOKEN>
<TOKEN end_char="6085" id="token-57-8" morph="none" pos="word" start_char="6076">accidental</TOKEN>
<TOKEN end_char="6089" id="token-57-9" morph="none" pos="word" start_char="6087">lab</TOKEN>
<TOKEN end_char="6097" id="token-57-10" morph="none" pos="word" start_char="6091">release</TOKEN>
<TOKEN end_char="6100" id="token-57-11" morph="none" pos="word" start_char="6099">or</TOKEN>
<TOKEN end_char="6103" id="token-57-12" morph="none" pos="word" start_char="6102">if</TOKEN>
<TOKEN end_char="6107" id="token-57-13" morph="none" pos="word" start_char="6105">the</TOKEN>
<TOKEN end_char="6113" id="token-57-14" morph="none" pos="word" start_char="6109">cycle</TOKEN>
<TOKEN end_char="6117" id="token-57-15" morph="none" pos="word" start_char="6115">was</TOKEN>
<TOKEN end_char="6126" id="token-57-16" morph="none" pos="word" start_char="6119">entirely</TOKEN>
<TOKEN end_char="6134" id="token-57-17" morph="none" pos="word" start_char="6128">natural</TOKEN>
<TOKEN end_char="6135" id="token-57-18" morph="none" pos="punct" start_char="6135">.</TOKEN>
</SEG>
<SEG end_char="6265" id="segment-58" start_char="6137">
<ORIGINAL_TEXT>The problem with the lab scenario is that the virus may have been introduced to a new species that is local to Wuhan as a result.</ORIGINAL_TEXT>
<TOKEN end_char="6139" id="token-58-0" morph="none" pos="word" start_char="6137">The</TOKEN>
<TOKEN end_char="6147" id="token-58-1" morph="none" pos="word" start_char="6141">problem</TOKEN>
<TOKEN end_char="6152" id="token-58-2" morph="none" pos="word" start_char="6149">with</TOKEN>
<TOKEN end_char="6156" id="token-58-3" morph="none" pos="word" start_char="6154">the</TOKEN>
<TOKEN end_char="6160" id="token-58-4" morph="none" pos="word" start_char="6158">lab</TOKEN>
<TOKEN end_char="6169" id="token-58-5" morph="none" pos="word" start_char="6162">scenario</TOKEN>
<TOKEN end_char="6172" id="token-58-6" morph="none" pos="word" start_char="6171">is</TOKEN>
<TOKEN end_char="6177" id="token-58-7" morph="none" pos="word" start_char="6174">that</TOKEN>
<TOKEN end_char="6181" id="token-58-8" morph="none" pos="word" start_char="6179">the</TOKEN>
<TOKEN end_char="6187" id="token-58-9" morph="none" pos="word" start_char="6183">virus</TOKEN>
<TOKEN end_char="6191" id="token-58-10" morph="none" pos="word" start_char="6189">may</TOKEN>
<TOKEN end_char="6196" id="token-58-11" morph="none" pos="word" start_char="6193">have</TOKEN>
<TOKEN end_char="6201" id="token-58-12" morph="none" pos="word" start_char="6198">been</TOKEN>
<TOKEN end_char="6212" id="token-58-13" morph="none" pos="word" start_char="6203">introduced</TOKEN>
<TOKEN end_char="6215" id="token-58-14" morph="none" pos="word" start_char="6214">to</TOKEN>
<TOKEN end_char="6217" id="token-58-15" morph="none" pos="word" start_char="6217">a</TOKEN>
<TOKEN end_char="6221" id="token-58-16" morph="none" pos="word" start_char="6219">new</TOKEN>
<TOKEN end_char="6229" id="token-58-17" morph="none" pos="word" start_char="6223">species</TOKEN>
<TOKEN end_char="6234" id="token-58-18" morph="none" pos="word" start_char="6231">that</TOKEN>
<TOKEN end_char="6237" id="token-58-19" morph="none" pos="word" start_char="6236">is</TOKEN>
<TOKEN end_char="6243" id="token-58-20" morph="none" pos="word" start_char="6239">local</TOKEN>
<TOKEN end_char="6246" id="token-58-21" morph="none" pos="word" start_char="6245">to</TOKEN>
<TOKEN end_char="6252" id="token-58-22" morph="none" pos="word" start_char="6248">Wuhan</TOKEN>
<TOKEN end_char="6255" id="token-58-23" morph="none" pos="word" start_char="6254">as</TOKEN>
<TOKEN end_char="6257" id="token-58-24" morph="none" pos="word" start_char="6257">a</TOKEN>
<TOKEN end_char="6264" id="token-58-25" morph="none" pos="word" start_char="6259">result</TOKEN>
<TOKEN end_char="6265" id="token-58-26" morph="none" pos="punct" start_char="6265">.</TOKEN>
</SEG>
<SEG end_char="6391" id="segment-59" start_char="6267">
<ORIGINAL_TEXT>When viruses are introduced to new species they can evolve differently since different animals have different immune systems.</ORIGINAL_TEXT>
<TOKEN end_char="6270" id="token-59-0" morph="none" pos="word" start_char="6267">When</TOKEN>
<TOKEN end_char="6278" id="token-59-1" morph="none" pos="word" start_char="6272">viruses</TOKEN>
<TOKEN end_char="6282" id="token-59-2" morph="none" pos="word" start_char="6280">are</TOKEN>
<TOKEN end_char="6293" id="token-59-3" morph="none" pos="word" start_char="6284">introduced</TOKEN>
<TOKEN end_char="6296" id="token-59-4" morph="none" pos="word" start_char="6295">to</TOKEN>
<TOKEN end_char="6300" id="token-59-5" morph="none" pos="word" start_char="6298">new</TOKEN>
<TOKEN end_char="6308" id="token-59-6" morph="none" pos="word" start_char="6302">species</TOKEN>
<TOKEN end_char="6313" id="token-59-7" morph="none" pos="word" start_char="6310">they</TOKEN>
<TOKEN end_char="6317" id="token-59-8" morph="none" pos="word" start_char="6315">can</TOKEN>
<TOKEN end_char="6324" id="token-59-9" morph="none" pos="word" start_char="6319">evolve</TOKEN>
<TOKEN end_char="6336" id="token-59-10" morph="none" pos="word" start_char="6326">differently</TOKEN>
<TOKEN end_char="6342" id="token-59-11" morph="none" pos="word" start_char="6338">since</TOKEN>
<TOKEN end_char="6352" id="token-59-12" morph="none" pos="word" start_char="6344">different</TOKEN>
<TOKEN end_char="6360" id="token-59-13" morph="none" pos="word" start_char="6354">animals</TOKEN>
<TOKEN end_char="6365" id="token-59-14" morph="none" pos="word" start_char="6362">have</TOKEN>
<TOKEN end_char="6375" id="token-59-15" morph="none" pos="word" start_char="6367">different</TOKEN>
<TOKEN end_char="6382" id="token-59-16" morph="none" pos="word" start_char="6377">immune</TOKEN>
<TOKEN end_char="6390" id="token-59-17" morph="none" pos="word" start_char="6384">systems</TOKEN>
<TOKEN end_char="6391" id="token-59-18" morph="none" pos="punct" start_char="6391">.</TOKEN>
</SEG>
<SEG end_char="6474" id="segment-60" start_char="6393">
<ORIGINAL_TEXT>It’s an even bigger problem if it’s an animal that is eaten by people in the area.</ORIGINAL_TEXT>
<TOKEN end_char="6396" id="token-60-0" morph="none" pos="word" start_char="6393">It’s</TOKEN>
<TOKEN end_char="6399" id="token-60-1" morph="none" pos="word" start_char="6398">an</TOKEN>
<TOKEN end_char="6404" id="token-60-2" morph="none" pos="word" start_char="6401">even</TOKEN>
<TOKEN end_char="6411" id="token-60-3" morph="none" pos="word" start_char="6406">bigger</TOKEN>
<TOKEN end_char="6419" id="token-60-4" morph="none" pos="word" start_char="6413">problem</TOKEN>
<TOKEN end_char="6422" id="token-60-5" morph="none" pos="word" start_char="6421">if</TOKEN>
<TOKEN end_char="6427" id="token-60-6" morph="none" pos="word" start_char="6424">it’s</TOKEN>
<TOKEN end_char="6430" id="token-60-7" morph="none" pos="word" start_char="6429">an</TOKEN>
<TOKEN end_char="6437" id="token-60-8" morph="none" pos="word" start_char="6432">animal</TOKEN>
<TOKEN end_char="6442" id="token-60-9" morph="none" pos="word" start_char="6439">that</TOKEN>
<TOKEN end_char="6445" id="token-60-10" morph="none" pos="word" start_char="6444">is</TOKEN>
<TOKEN end_char="6451" id="token-60-11" morph="none" pos="word" start_char="6447">eaten</TOKEN>
<TOKEN end_char="6454" id="token-60-12" morph="none" pos="word" start_char="6453">by</TOKEN>
<TOKEN end_char="6461" id="token-60-13" morph="none" pos="word" start_char="6456">people</TOKEN>
<TOKEN end_char="6464" id="token-60-14" morph="none" pos="word" start_char="6463">in</TOKEN>
<TOKEN end_char="6468" id="token-60-15" morph="none" pos="word" start_char="6466">the</TOKEN>
<TOKEN end_char="6473" id="token-60-16" morph="none" pos="word" start_char="6470">area</TOKEN>
<TOKEN end_char="6474" id="token-60-17" morph="none" pos="punct" start_char="6474">.</TOKEN>
</SEG>
<SEG end_char="6593" id="segment-61" start_char="6476">
<ORIGINAL_TEXT>The bats that carry RaTG13 live in Yunan, some 2000 kms away from Wuhan and was discovered in 2013 by the… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="6478" id="token-61-0" morph="none" pos="word" start_char="6476">The</TOKEN>
<TOKEN end_char="6483" id="token-61-1" morph="none" pos="word" start_char="6480">bats</TOKEN>
<TOKEN end_char="6488" id="token-61-2" morph="none" pos="word" start_char="6485">that</TOKEN>
<TOKEN end_char="6494" id="token-61-3" morph="none" pos="word" start_char="6490">carry</TOKEN>
<TOKEN end_char="6501" id="token-61-4" morph="none" pos="word" start_char="6496">RaTG13</TOKEN>
<TOKEN end_char="6506" id="token-61-5" morph="none" pos="word" start_char="6503">live</TOKEN>
<TOKEN end_char="6509" id="token-61-6" morph="none" pos="word" start_char="6508">in</TOKEN>
<TOKEN end_char="6515" id="token-61-7" morph="none" pos="word" start_char="6511">Yunan</TOKEN>
<TOKEN end_char="6516" id="token-61-8" morph="none" pos="punct" start_char="6516">,</TOKEN>
<TOKEN end_char="6521" id="token-61-9" morph="none" pos="word" start_char="6518">some</TOKEN>
<TOKEN end_char="6526" id="token-61-10" morph="none" pos="word" start_char="6523">2000</TOKEN>
<TOKEN end_char="6530" id="token-61-11" morph="none" pos="word" start_char="6528">kms</TOKEN>
<TOKEN end_char="6535" id="token-61-12" morph="none" pos="word" start_char="6532">away</TOKEN>
<TOKEN end_char="6540" id="token-61-13" morph="none" pos="word" start_char="6537">from</TOKEN>
<TOKEN end_char="6546" id="token-61-14" morph="none" pos="word" start_char="6542">Wuhan</TOKEN>
<TOKEN end_char="6550" id="token-61-15" morph="none" pos="word" start_char="6548">and</TOKEN>
<TOKEN end_char="6554" id="token-61-16" morph="none" pos="word" start_char="6552">was</TOKEN>
<TOKEN end_char="6565" id="token-61-17" morph="none" pos="word" start_char="6556">discovered</TOKEN>
<TOKEN end_char="6568" id="token-61-18" morph="none" pos="word" start_char="6567">in</TOKEN>
<TOKEN end_char="6573" id="token-61-19" morph="none" pos="word" start_char="6570">2013</TOKEN>
<TOKEN end_char="6576" id="token-61-20" morph="none" pos="word" start_char="6575">by</TOKEN>
<TOKEN end_char="6580" id="token-61-21" morph="none" pos="word" start_char="6578">the</TOKEN>
<TOKEN end_char="6581" id="token-61-22" morph="none" pos="punct" start_char="6581">…</TOKEN>
<TOKEN end_char="6586" id="token-61-23" morph="none" pos="word" start_char="6583">Read</TOKEN>
<TOKEN end_char="6591" id="token-61-24" morph="none" pos="word" start_char="6588">more</TOKEN>
<TOKEN end_char="6593" id="token-61-25" morph="none" pos="punct" start_char="6593">»</TOKEN>
</SEG>
<SEG end_char="6632" id="segment-62" start_char="6597">
<ORIGINAL_TEXT>Samples are collected from the wild.</ORIGINAL_TEXT>
<TOKEN end_char="6603" id="token-62-0" morph="none" pos="word" start_char="6597">Samples</TOKEN>
<TOKEN end_char="6607" id="token-62-1" morph="none" pos="word" start_char="6605">are</TOKEN>
<TOKEN end_char="6617" id="token-62-2" morph="none" pos="word" start_char="6609">collected</TOKEN>
<TOKEN end_char="6622" id="token-62-3" morph="none" pos="word" start_char="6619">from</TOKEN>
<TOKEN end_char="6626" id="token-62-4" morph="none" pos="word" start_char="6624">the</TOKEN>
<TOKEN end_char="6631" id="token-62-5" morph="none" pos="word" start_char="6628">wild</TOKEN>
<TOKEN end_char="6632" id="token-62-6" morph="none" pos="punct" start_char="6632">.</TOKEN>
</SEG>
<SEG end_char="6771" id="segment-63" start_char="6634">
<ORIGINAL_TEXT>Then those samples can be cultured in the lab and an artificial acceleration of natural mutations can be produced through experimentation.</ORIGINAL_TEXT>
<TOKEN end_char="6637" id="token-63-0" morph="none" pos="word" start_char="6634">Then</TOKEN>
<TOKEN end_char="6643" id="token-63-1" morph="none" pos="word" start_char="6639">those</TOKEN>
<TOKEN end_char="6651" id="token-63-2" morph="none" pos="word" start_char="6645">samples</TOKEN>
<TOKEN end_char="6655" id="token-63-3" morph="none" pos="word" start_char="6653">can</TOKEN>
<TOKEN end_char="6658" id="token-63-4" morph="none" pos="word" start_char="6657">be</TOKEN>
<TOKEN end_char="6667" id="token-63-5" morph="none" pos="word" start_char="6660">cultured</TOKEN>
<TOKEN end_char="6670" id="token-63-6" morph="none" pos="word" start_char="6669">in</TOKEN>
<TOKEN end_char="6674" id="token-63-7" morph="none" pos="word" start_char="6672">the</TOKEN>
<TOKEN end_char="6678" id="token-63-8" morph="none" pos="word" start_char="6676">lab</TOKEN>
<TOKEN end_char="6682" id="token-63-9" morph="none" pos="word" start_char="6680">and</TOKEN>
<TOKEN end_char="6685" id="token-63-10" morph="none" pos="word" start_char="6684">an</TOKEN>
<TOKEN end_char="6696" id="token-63-11" morph="none" pos="word" start_char="6687">artificial</TOKEN>
<TOKEN end_char="6709" id="token-63-12" morph="none" pos="word" start_char="6698">acceleration</TOKEN>
<TOKEN end_char="6712" id="token-63-13" morph="none" pos="word" start_char="6711">of</TOKEN>
<TOKEN end_char="6720" id="token-63-14" morph="none" pos="word" start_char="6714">natural</TOKEN>
<TOKEN end_char="6730" id="token-63-15" morph="none" pos="word" start_char="6722">mutations</TOKEN>
<TOKEN end_char="6734" id="token-63-16" morph="none" pos="word" start_char="6732">can</TOKEN>
<TOKEN end_char="6737" id="token-63-17" morph="none" pos="word" start_char="6736">be</TOKEN>
<TOKEN end_char="6746" id="token-63-18" morph="none" pos="word" start_char="6739">produced</TOKEN>
<TOKEN end_char="6754" id="token-63-19" morph="none" pos="word" start_char="6748">through</TOKEN>
<TOKEN end_char="6770" id="token-63-20" morph="none" pos="word" start_char="6756">experimentation</TOKEN>
<TOKEN end_char="6771" id="token-63-21" morph="none" pos="punct" start_char="6771">.</TOKEN>
</SEG>
<SEG end_char="6885" id="segment-64" start_char="6773">
<ORIGINAL_TEXT>Then new viruses can be selected for new "improved" functions, this filtering can be done through animal testing.</ORIGINAL_TEXT>
<TOKEN end_char="6776" id="token-64-0" morph="none" pos="word" start_char="6773">Then</TOKEN>
<TOKEN end_char="6780" id="token-64-1" morph="none" pos="word" start_char="6778">new</TOKEN>
<TOKEN end_char="6788" id="token-64-2" morph="none" pos="word" start_char="6782">viruses</TOKEN>
<TOKEN end_char="6792" id="token-64-3" morph="none" pos="word" start_char="6790">can</TOKEN>
<TOKEN end_char="6795" id="token-64-4" morph="none" pos="word" start_char="6794">be</TOKEN>
<TOKEN end_char="6804" id="token-64-5" morph="none" pos="word" start_char="6797">selected</TOKEN>
<TOKEN end_char="6808" id="token-64-6" morph="none" pos="word" start_char="6806">for</TOKEN>
<TOKEN end_char="6812" id="token-64-7" morph="none" pos="word" start_char="6810">new</TOKEN>
<TOKEN end_char="6814" id="token-64-8" morph="none" pos="punct" start_char="6814">"</TOKEN>
<TOKEN end_char="6822" id="token-64-9" morph="none" pos="word" start_char="6815">improved</TOKEN>
<TOKEN end_char="6823" id="token-64-10" morph="none" pos="punct" start_char="6823">"</TOKEN>
<TOKEN end_char="6833" id="token-64-11" morph="none" pos="word" start_char="6825">functions</TOKEN>
<TOKEN end_char="6834" id="token-64-12" morph="none" pos="punct" start_char="6834">,</TOKEN>
<TOKEN end_char="6839" id="token-64-13" morph="none" pos="word" start_char="6836">this</TOKEN>
<TOKEN end_char="6849" id="token-64-14" morph="none" pos="word" start_char="6841">filtering</TOKEN>
<TOKEN end_char="6853" id="token-64-15" morph="none" pos="word" start_char="6851">can</TOKEN>
<TOKEN end_char="6856" id="token-64-16" morph="none" pos="word" start_char="6855">be</TOKEN>
<TOKEN end_char="6861" id="token-64-17" morph="none" pos="word" start_char="6858">done</TOKEN>
<TOKEN end_char="6869" id="token-64-18" morph="none" pos="word" start_char="6863">through</TOKEN>
<TOKEN end_char="6876" id="token-64-19" morph="none" pos="word" start_char="6871">animal</TOKEN>
<TOKEN end_char="6884" id="token-64-20" morph="none" pos="word" start_char="6878">testing</TOKEN>
<TOKEN end_char="6885" id="token-64-21" morph="none" pos="punct" start_char="6885">.</TOKEN>
</SEG>
<SEG end_char="6971" id="segment-65" start_char="6887">
<ORIGINAL_TEXT>There is a huge risk that one of the people working in the lab could become infected.</ORIGINAL_TEXT>
<TOKEN end_char="6891" id="token-65-0" morph="none" pos="word" start_char="6887">There</TOKEN>
<TOKEN end_char="6894" id="token-65-1" morph="none" pos="word" start_char="6893">is</TOKEN>
<TOKEN end_char="6896" id="token-65-2" morph="none" pos="word" start_char="6896">a</TOKEN>
<TOKEN end_char="6901" id="token-65-3" morph="none" pos="word" start_char="6898">huge</TOKEN>
<TOKEN end_char="6906" id="token-65-4" morph="none" pos="word" start_char="6903">risk</TOKEN>
<TOKEN end_char="6911" id="token-65-5" morph="none" pos="word" start_char="6908">that</TOKEN>
<TOKEN end_char="6915" id="token-65-6" morph="none" pos="word" start_char="6913">one</TOKEN>
<TOKEN end_char="6918" id="token-65-7" morph="none" pos="word" start_char="6917">of</TOKEN>
<TOKEN end_char="6922" id="token-65-8" morph="none" pos="word" start_char="6920">the</TOKEN>
<TOKEN end_char="6929" id="token-65-9" morph="none" pos="word" start_char="6924">people</TOKEN>
<TOKEN end_char="6937" id="token-65-10" morph="none" pos="word" start_char="6931">working</TOKEN>
<TOKEN end_char="6940" id="token-65-11" morph="none" pos="word" start_char="6939">in</TOKEN>
<TOKEN end_char="6944" id="token-65-12" morph="none" pos="word" start_char="6942">the</TOKEN>
<TOKEN end_char="6948" id="token-65-13" morph="none" pos="word" start_char="6946">lab</TOKEN>
<TOKEN end_char="6954" id="token-65-14" morph="none" pos="word" start_char="6950">could</TOKEN>
<TOKEN end_char="6961" id="token-65-15" morph="none" pos="word" start_char="6956">become</TOKEN>
<TOKEN end_char="6970" id="token-65-16" morph="none" pos="word" start_char="6963">infected</TOKEN>
<TOKEN end_char="6971" id="token-65-17" morph="none" pos="punct" start_char="6971">.</TOKEN>
</SEG>
<SEG end_char="7062" id="segment-66" start_char="6973">
<ORIGINAL_TEXT>For example, this could happen due to human error, it has happened many times in the past.</ORIGINAL_TEXT>
<TOKEN end_char="6975" id="token-66-0" morph="none" pos="word" start_char="6973">For</TOKEN>
<TOKEN end_char="6983" id="token-66-1" morph="none" pos="word" start_char="6977">example</TOKEN>
<TOKEN end_char="6984" id="token-66-2" morph="none" pos="punct" start_char="6984">,</TOKEN>
<TOKEN end_char="6989" id="token-66-3" morph="none" pos="word" start_char="6986">this</TOKEN>
<TOKEN end_char="6995" id="token-66-4" morph="none" pos="word" start_char="6991">could</TOKEN>
<TOKEN end_char="7002" id="token-66-5" morph="none" pos="word" start_char="6997">happen</TOKEN>
<TOKEN end_char="7006" id="token-66-6" morph="none" pos="word" start_char="7004">due</TOKEN>
<TOKEN end_char="7009" id="token-66-7" morph="none" pos="word" start_char="7008">to</TOKEN>
<TOKEN end_char="7015" id="token-66-8" morph="none" pos="word" start_char="7011">human</TOKEN>
<TOKEN end_char="7021" id="token-66-9" morph="none" pos="word" start_char="7017">error</TOKEN>
<TOKEN end_char="7022" id="token-66-10" morph="none" pos="punct" start_char="7022">,</TOKEN>
<TOKEN end_char="7025" id="token-66-11" morph="none" pos="word" start_char="7024">it</TOKEN>
<TOKEN end_char="7029" id="token-66-12" morph="none" pos="word" start_char="7027">has</TOKEN>
<TOKEN end_char="7038" id="token-66-13" morph="none" pos="word" start_char="7031">happened</TOKEN>
<TOKEN end_char="7043" id="token-66-14" morph="none" pos="word" start_char="7040">many</TOKEN>
<TOKEN end_char="7049" id="token-66-15" morph="none" pos="word" start_char="7045">times</TOKEN>
<TOKEN end_char="7052" id="token-66-16" morph="none" pos="word" start_char="7051">in</TOKEN>
<TOKEN end_char="7056" id="token-66-17" morph="none" pos="word" start_char="7054">the</TOKEN>
<TOKEN end_char="7061" id="token-66-18" morph="none" pos="word" start_char="7058">past</TOKEN>
<TOKEN end_char="7062" id="token-66-19" morph="none" pos="punct" start_char="7062">.</TOKEN>
</SEG>
<SEG end_char="7136" id="segment-67" start_char="7064">
<ORIGINAL_TEXT>The above article gives several examples of these mistakes happening e.g.</ORIGINAL_TEXT>
<TOKEN end_char="7066" id="token-67-0" morph="none" pos="word" start_char="7064">The</TOKEN>
<TOKEN end_char="7072" id="token-67-1" morph="none" pos="word" start_char="7068">above</TOKEN>
<TOKEN end_char="7080" id="token-67-2" morph="none" pos="word" start_char="7074">article</TOKEN>
<TOKEN end_char="7086" id="token-67-3" morph="none" pos="word" start_char="7082">gives</TOKEN>
<TOKEN end_char="7094" id="token-67-4" morph="none" pos="word" start_char="7088">several</TOKEN>
<TOKEN end_char="7103" id="token-67-5" morph="none" pos="word" start_char="7096">examples</TOKEN>
<TOKEN end_char="7106" id="token-67-6" morph="none" pos="word" start_char="7105">of</TOKEN>
<TOKEN end_char="7112" id="token-67-7" morph="none" pos="word" start_char="7108">these</TOKEN>
<TOKEN end_char="7121" id="token-67-8" morph="none" pos="word" start_char="7114">mistakes</TOKEN>
<TOKEN end_char="7131" id="token-67-9" morph="none" pos="word" start_char="7123">happening</TOKEN>
<TOKEN end_char="7135" id="token-67-10" morph="none" pos="unknown" start_char="7133">e.g</TOKEN>
<TOKEN end_char="7136" id="token-67-11" morph="none" pos="punct" start_char="7136">.</TOKEN>
</SEG>
<SEG end_char="7152" id="segment-68" start_char="7138">
<ORIGINAL_TEXT>Beijing in 2004</ORIGINAL_TEXT>
<TOKEN end_char="7144" id="token-68-0" morph="none" pos="word" start_char="7138">Beijing</TOKEN>
<TOKEN end_char="7147" id="token-68-1" morph="none" pos="word" start_char="7146">in</TOKEN>
<TOKEN end_char="7152" id="token-68-2" morph="none" pos="word" start_char="7149">2004</TOKEN>
<TRANSLATED_TEXT>Pechino 2004</TRANSLATED_TEXT><DETECTED_LANGUAGE>nl</DETECTED_LANGUAGE></SEG>
<SEG end_char="7374" id="segment-69" start_char="7156">
<ORIGINAL_TEXT>This article mentions the study by Botao Xiao and Lei Xiao entitled "The possible origins of 2019-nCoV coronavirus" as part of the "circumstantial evidence that supports the possibility that a lab release was involved."</ORIGINAL_TEXT>
<TOKEN end_char="7159" id="token-69-0" morph="none" pos="word" start_char="7156">This</TOKEN>
<TOKEN end_char="7167" id="token-69-1" morph="none" pos="word" start_char="7161">article</TOKEN>
<TOKEN end_char="7176" id="token-69-2" morph="none" pos="word" start_char="7169">mentions</TOKEN>
<TOKEN end_char="7180" id="token-69-3" morph="none" pos="word" start_char="7178">the</TOKEN>
<TOKEN end_char="7186" id="token-69-4" morph="none" pos="word" start_char="7182">study</TOKEN>
<TOKEN end_char="7189" id="token-69-5" morph="none" pos="word" start_char="7188">by</TOKEN>
<TOKEN end_char="7195" id="token-69-6" morph="none" pos="word" start_char="7191">Botao</TOKEN>
<TOKEN end_char="7200" id="token-69-7" morph="none" pos="word" start_char="7197">Xiao</TOKEN>
<TOKEN end_char="7204" id="token-69-8" morph="none" pos="word" start_char="7202">and</TOKEN>
<TOKEN end_char="7208" id="token-69-9" morph="none" pos="word" start_char="7206">Lei</TOKEN>
<TOKEN end_char="7213" id="token-69-10" morph="none" pos="word" start_char="7210">Xiao</TOKEN>
<TOKEN end_char="7222" id="token-69-11" morph="none" pos="word" start_char="7215">entitled</TOKEN>
<TOKEN end_char="7224" id="token-69-12" morph="none" pos="punct" start_char="7224">"</TOKEN>
<TOKEN end_char="7227" id="token-69-13" morph="none" pos="word" start_char="7225">The</TOKEN>
<TOKEN end_char="7236" id="token-69-14" morph="none" pos="word" start_char="7229">possible</TOKEN>
<TOKEN end_char="7244" id="token-69-15" morph="none" pos="word" start_char="7238">origins</TOKEN>
<TOKEN end_char="7247" id="token-69-16" morph="none" pos="word" start_char="7246">of</TOKEN>
<TOKEN end_char="7257" id="token-69-17" morph="none" pos="unknown" start_char="7249">2019-nCoV</TOKEN>
<TOKEN end_char="7269" id="token-69-18" morph="none" pos="word" start_char="7259">coronavirus</TOKEN>
<TOKEN end_char="7270" id="token-69-19" morph="none" pos="punct" start_char="7270">"</TOKEN>
<TOKEN end_char="7273" id="token-69-20" morph="none" pos="word" start_char="7272">as</TOKEN>
<TOKEN end_char="7278" id="token-69-21" morph="none" pos="word" start_char="7275">part</TOKEN>
<TOKEN end_char="7281" id="token-69-22" morph="none" pos="word" start_char="7280">of</TOKEN>
<TOKEN end_char="7285" id="token-69-23" morph="none" pos="word" start_char="7283">the</TOKEN>
<TOKEN end_char="7287" id="token-69-24" morph="none" pos="punct" start_char="7287">"</TOKEN>
<TOKEN end_char="7301" id="token-69-25" morph="none" pos="word" start_char="7288">circumstantial</TOKEN>
<TOKEN end_char="7310" id="token-69-26" morph="none" pos="word" start_char="7303">evidence</TOKEN>
<TOKEN end_char="7315" id="token-69-27" morph="none" pos="word" start_char="7312">that</TOKEN>
<TOKEN end_char="7324" id="token-69-28" morph="none" pos="word" start_char="7317">supports</TOKEN>
<TOKEN end_char="7328" id="token-69-29" morph="none" pos="word" start_char="7326">the</TOKEN>
<TOKEN end_char="7340" id="token-69-30" morph="none" pos="word" start_char="7330">possibility</TOKEN>
<TOKEN end_char="7345" id="token-69-31" morph="none" pos="word" start_char="7342">that</TOKEN>
<TOKEN end_char="7347" id="token-69-32" morph="none" pos="word" start_char="7347">a</TOKEN>
<TOKEN end_char="7351" id="token-69-33" morph="none" pos="word" start_char="7349">lab</TOKEN>
<TOKEN end_char="7359" id="token-69-34" morph="none" pos="word" start_char="7353">release</TOKEN>
<TOKEN end_char="7363" id="token-69-35" morph="none" pos="word" start_char="7361">was</TOKEN>
<TOKEN end_char="7372" id="token-69-36" morph="none" pos="word" start_char="7365">involved</TOKEN>
<TOKEN end_char="7374" id="token-69-37" morph="none" pos="punct" start_char="7373">."</TOKEN>
</SEG>
<SEG end_char="7547" id="segment-70" start_char="7376">
<ORIGINAL_TEXT>The following is stated: ‘’"The paper was later removed from ResearchGate, a commercial social-networking site for scientists and researchers to share papers," Huang wrote.</ORIGINAL_TEXT>
<TOKEN end_char="7378" id="token-70-0" morph="none" pos="word" start_char="7376">The</TOKEN>
<TOKEN end_char="7388" id="token-70-1" morph="none" pos="word" start_char="7380">following</TOKEN>
<TOKEN end_char="7391" id="token-70-2" morph="none" pos="word" start_char="7390">is</TOKEN>
<TOKEN end_char="7398" id="token-70-3" morph="none" pos="word" start_char="7393">stated</TOKEN>
<TOKEN end_char="7399" id="token-70-4" morph="none" pos="punct" start_char="7399">:</TOKEN>
<TOKEN end_char="7403" id="token-70-5" morph="none" pos="punct" start_char="7401">‘’"</TOKEN>
<TOKEN end_char="7406" id="token-70-6" morph="none" pos="word" start_char="7404">The</TOKEN>
<TOKEN end_char="7412" id="token-70-7" morph="none" pos="word" start_char="7408">paper</TOKEN>
<TOKEN end_char="7416" id="token-70-8" morph="none" pos="word" start_char="7414">was</TOKEN>
<TOKEN end_char="7422" id="token-70-9" morph="none" pos="word" start_char="7418">later</TOKEN>
<TOKEN end_char="7430" id="token-70-10" morph="none" pos="word" start_char="7424">removed</TOKEN>
<TOKEN end_char="7435" id="token-70-11" morph="none" pos="word" start_char="7432">from</TOKEN>
<TOKEN end_char="7448" id="token-70-12" morph="none" pos="word" start_char="7437">ResearchGate</TOKEN>
<TOKEN end_char="7449" id="token-70-13" morph="none" pos="punct" start_char="7449">,</TOKEN>
<TOKEN end_char="7451" id="token-70-14" morph="none" pos="word" start_char="7451">a</TOKEN>
<TOKEN end_char="7462" id="token-70-15" morph="none" pos="word" start_char="7453">commercial</TOKEN>
<TOKEN end_char="7480" id="token-70-16" morph="none" pos="unknown" start_char="7464">social-networking</TOKEN>
<TOKEN end_char="7485" id="token-70-17" morph="none" pos="word" start_char="7482">site</TOKEN>
<TOKEN end_char="7489" id="token-70-18" morph="none" pos="word" start_char="7487">for</TOKEN>
<TOKEN end_char="7500" id="token-70-19" morph="none" pos="word" start_char="7491">scientists</TOKEN>
<TOKEN end_char="7504" id="token-70-20" morph="none" pos="word" start_char="7502">and</TOKEN>
<TOKEN end_char="7516" id="token-70-21" morph="none" pos="word" start_char="7506">researchers</TOKEN>
<TOKEN end_char="7519" id="token-70-22" morph="none" pos="word" start_char="7518">to</TOKEN>
<TOKEN end_char="7525" id="token-70-23" morph="none" pos="word" start_char="7521">share</TOKEN>
<TOKEN end_char="7532" id="token-70-24" morph="none" pos="word" start_char="7527">papers</TOKEN>
<TOKEN end_char="7534" id="token-70-25" morph="none" pos="punct" start_char="7533">,"</TOKEN>
<TOKEN end_char="7540" id="token-70-26" morph="none" pos="word" start_char="7536">Huang</TOKEN>
<TOKEN end_char="7546" id="token-70-27" morph="none" pos="word" start_char="7542">wrote</TOKEN>
<TOKEN end_char="7547" id="token-70-28" morph="none" pos="punct" start_char="7547">.</TOKEN>
</SEG>
<SEG end_char="7622" id="segment-71" start_char="7549">
<ORIGINAL_TEXT>"Thus far, no scientists have confirmed or refuted the paper’s findings.""</ORIGINAL_TEXT>
<TOKEN end_char="7549" id="token-71-0" morph="none" pos="punct" start_char="7549">"</TOKEN>
<TOKEN end_char="7553" id="token-71-1" morph="none" pos="word" start_char="7550">Thus</TOKEN>
<TOKEN end_char="7557" id="token-71-2" morph="none" pos="word" start_char="7555">far</TOKEN>
<TOKEN end_char="7558" id="token-71-3" morph="none" pos="punct" start_char="7558">,</TOKEN>
<TOKEN end_char="7561" id="token-71-4" morph="none" pos="word" start_char="7560">no</TOKEN>
<TOKEN end_char="7572" id="token-71-5" morph="none" pos="word" start_char="7563">scientists</TOKEN>
<TOKEN end_char="7577" id="token-71-6" morph="none" pos="word" start_char="7574">have</TOKEN>
<TOKEN end_char="7587" id="token-71-7" morph="none" pos="word" start_char="7579">confirmed</TOKEN>
<TOKEN end_char="7590" id="token-71-8" morph="none" pos="word" start_char="7589">or</TOKEN>
<TOKEN end_char="7598" id="token-71-9" morph="none" pos="word" start_char="7592">refuted</TOKEN>
<TOKEN end_char="7602" id="token-71-10" morph="none" pos="word" start_char="7600">the</TOKEN>
<TOKEN end_char="7610" id="token-71-11" morph="none" pos="word" start_char="7604">paper’s</TOKEN>
<TOKEN end_char="7619" id="token-71-12" morph="none" pos="word" start_char="7612">findings</TOKEN>
<TOKEN end_char="7622" id="token-71-13" morph="none" pos="punct" start_char="7620">.""</TOKEN>
</SEG>
<SEG end_char="7803" id="segment-72" start_char="7624">
<ORIGINAL_TEXT>The facts are that this very brief, poor quality paper has not been subjected to the process of peer-review by a scientific journal (or if it has, the world is unaware… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="7626" id="token-72-0" morph="none" pos="word" start_char="7624">The</TOKEN>
<TOKEN end_char="7632" id="token-72-1" morph="none" pos="word" start_char="7628">facts</TOKEN>
<TOKEN end_char="7636" id="token-72-2" morph="none" pos="word" start_char="7634">are</TOKEN>
<TOKEN end_char="7641" id="token-72-3" morph="none" pos="word" start_char="7638">that</TOKEN>
<TOKEN end_char="7646" id="token-72-4" morph="none" pos="word" start_char="7643">this</TOKEN>
<TOKEN end_char="7651" id="token-72-5" morph="none" pos="word" start_char="7648">very</TOKEN>
<TOKEN end_char="7657" id="token-72-6" morph="none" pos="word" start_char="7653">brief</TOKEN>
<TOKEN end_char="7658" id="token-72-7" morph="none" pos="punct" start_char="7658">,</TOKEN>
<TOKEN end_char="7663" id="token-72-8" morph="none" pos="word" start_char="7660">poor</TOKEN>
<TOKEN end_char="7671" id="token-72-9" morph="none" pos="word" start_char="7665">quality</TOKEN>
<TOKEN end_char="7677" id="token-72-10" morph="none" pos="word" start_char="7673">paper</TOKEN>
<TOKEN end_char="7681" id="token-72-11" morph="none" pos="word" start_char="7679">has</TOKEN>
<TOKEN end_char="7685" id="token-72-12" morph="none" pos="word" start_char="7683">not</TOKEN>
<TOKEN end_char="7690" id="token-72-13" morph="none" pos="word" start_char="7687">been</TOKEN>
<TOKEN end_char="7700" id="token-72-14" morph="none" pos="word" start_char="7692">subjected</TOKEN>
<TOKEN end_char="7703" id="token-72-15" morph="none" pos="word" start_char="7702">to</TOKEN>
<TOKEN end_char="7707" id="token-72-16" morph="none" pos="word" start_char="7705">the</TOKEN>
<TOKEN end_char="7715" id="token-72-17" morph="none" pos="word" start_char="7709">process</TOKEN>
<TOKEN end_char="7718" id="token-72-18" morph="none" pos="word" start_char="7717">of</TOKEN>
<TOKEN end_char="7730" id="token-72-19" morph="none" pos="unknown" start_char="7720">peer-review</TOKEN>
<TOKEN end_char="7733" id="token-72-20" morph="none" pos="word" start_char="7732">by</TOKEN>
<TOKEN end_char="7735" id="token-72-21" morph="none" pos="word" start_char="7735">a</TOKEN>
<TOKEN end_char="7746" id="token-72-22" morph="none" pos="word" start_char="7737">scientific</TOKEN>
<TOKEN end_char="7754" id="token-72-23" morph="none" pos="word" start_char="7748">journal</TOKEN>
<TOKEN end_char="7756" id="token-72-24" morph="none" pos="punct" start_char="7756">(</TOKEN>
<TOKEN end_char="7758" id="token-72-25" morph="none" pos="word" start_char="7757">or</TOKEN>
<TOKEN end_char="7761" id="token-72-26" morph="none" pos="word" start_char="7760">if</TOKEN>
<TOKEN end_char="7764" id="token-72-27" morph="none" pos="word" start_char="7763">it</TOKEN>
<TOKEN end_char="7768" id="token-72-28" morph="none" pos="word" start_char="7766">has</TOKEN>
<TOKEN end_char="7769" id="token-72-29" morph="none" pos="punct" start_char="7769">,</TOKEN>
<TOKEN end_char="7773" id="token-72-30" morph="none" pos="word" start_char="7771">the</TOKEN>
<TOKEN end_char="7779" id="token-72-31" morph="none" pos="word" start_char="7775">world</TOKEN>
<TOKEN end_char="7782" id="token-72-32" morph="none" pos="word" start_char="7781">is</TOKEN>
<TOKEN end_char="7790" id="token-72-33" morph="none" pos="word" start_char="7784">unaware</TOKEN>
<TOKEN end_char="7791" id="token-72-34" morph="none" pos="punct" start_char="7791">…</TOKEN>
<TOKEN end_char="7796" id="token-72-35" morph="none" pos="word" start_char="7793">Read</TOKEN>
<TOKEN end_char="7801" id="token-72-36" morph="none" pos="word" start_char="7798">more</TOKEN>
<TOKEN end_char="7803" id="token-72-37" morph="none" pos="punct" start_char="7803">»</TOKEN>
</SEG>
<SEG end_char="8036" id="segment-73" start_char="7807">
<ORIGINAL_TEXT>" However, since we observed all notable SARS-CoV-2 features, including the optimized RBD and polybasic cleavage site, in related coronaviruses in nature, we do not believe that any type of laboratory-based scenario is plausible."</ORIGINAL_TEXT>
<TOKEN end_char="7807" id="token-73-0" morph="none" pos="punct" start_char="7807">"</TOKEN>
<TOKEN end_char="7815" id="token-73-1" morph="none" pos="word" start_char="7809">However</TOKEN>
<TOKEN end_char="7816" id="token-73-2" morph="none" pos="punct" start_char="7816">,</TOKEN>
<TOKEN end_char="7822" id="token-73-3" morph="none" pos="word" start_char="7818">since</TOKEN>
<TOKEN end_char="7825" id="token-73-4" morph="none" pos="word" start_char="7824">we</TOKEN>
<TOKEN end_char="7834" id="token-73-5" morph="none" pos="word" start_char="7827">observed</TOKEN>
<TOKEN end_char="7838" id="token-73-6" morph="none" pos="word" start_char="7836">all</TOKEN>
<TOKEN end_char="7846" id="token-73-7" morph="none" pos="word" start_char="7840">notable</TOKEN>
<TOKEN end_char="7857" id="token-73-8" morph="none" pos="unknown" start_char="7848">SARS-CoV-2</TOKEN>
<TOKEN end_char="7866" id="token-73-9" morph="none" pos="word" start_char="7859">features</TOKEN>
<TOKEN end_char="7867" id="token-73-10" morph="none" pos="punct" start_char="7867">,</TOKEN>
<TOKEN end_char="7877" id="token-73-11" morph="none" pos="word" start_char="7869">including</TOKEN>
<TOKEN end_char="7881" id="token-73-12" morph="none" pos="word" start_char="7879">the</TOKEN>
<TOKEN end_char="7891" id="token-73-13" morph="none" pos="word" start_char="7883">optimized</TOKEN>
<TOKEN end_char="7895" id="token-73-14" morph="none" pos="word" start_char="7893">RBD</TOKEN>
<TOKEN end_char="7899" id="token-73-15" morph="none" pos="word" start_char="7897">and</TOKEN>
<TOKEN end_char="7909" id="token-73-16" morph="none" pos="word" start_char="7901">polybasic</TOKEN>
<TOKEN end_char="7918" id="token-73-17" morph="none" pos="word" start_char="7911">cleavage</TOKEN>
<TOKEN end_char="7923" id="token-73-18" morph="none" pos="word" start_char="7920">site</TOKEN>
<TOKEN end_char="7924" id="token-73-19" morph="none" pos="punct" start_char="7924">,</TOKEN>
<TOKEN end_char="7927" id="token-73-20" morph="none" pos="word" start_char="7926">in</TOKEN>
<TOKEN end_char="7935" id="token-73-21" morph="none" pos="word" start_char="7929">related</TOKEN>
<TOKEN end_char="7949" id="token-73-22" morph="none" pos="word" start_char="7937">coronaviruses</TOKEN>
<TOKEN end_char="7952" id="token-73-23" morph="none" pos="word" start_char="7951">in</TOKEN>
<TOKEN end_char="7959" id="token-73-24" morph="none" pos="word" start_char="7954">nature</TOKEN>
<TOKEN end_char="7960" id="token-73-25" morph="none" pos="punct" start_char="7960">,</TOKEN>
<TOKEN end_char="7963" id="token-73-26" morph="none" pos="word" start_char="7962">we</TOKEN>
<TOKEN end_char="7966" id="token-73-27" morph="none" pos="word" start_char="7965">do</TOKEN>
<TOKEN end_char="7970" id="token-73-28" morph="none" pos="word" start_char="7968">not</TOKEN>
<TOKEN end_char="7978" id="token-73-29" morph="none" pos="word" start_char="7972">believe</TOKEN>
<TOKEN end_char="7983" id="token-73-30" morph="none" pos="word" start_char="7980">that</TOKEN>
<TOKEN end_char="7987" id="token-73-31" morph="none" pos="word" start_char="7985">any</TOKEN>
<TOKEN end_char="7992" id="token-73-32" morph="none" pos="word" start_char="7989">type</TOKEN>
<TOKEN end_char="7995" id="token-73-33" morph="none" pos="word" start_char="7994">of</TOKEN>
<TOKEN end_char="8012" id="token-73-34" morph="none" pos="unknown" start_char="7997">laboratory-based</TOKEN>
<TOKEN end_char="8021" id="token-73-35" morph="none" pos="word" start_char="8014">scenario</TOKEN>
<TOKEN end_char="8024" id="token-73-36" morph="none" pos="word" start_char="8023">is</TOKEN>
<TOKEN end_char="8034" id="token-73-37" morph="none" pos="word" start_char="8026">plausible</TOKEN>
<TOKEN end_char="8036" id="token-73-38" morph="none" pos="punct" start_char="8035">."</TOKEN>
</SEG>
<SEG end_char="8139" id="segment-74" start_char="8039">
<ORIGINAL_TEXT>So all these features are found in nature, but they’re all also part of viruses used in lab research.</ORIGINAL_TEXT>
<TOKEN end_char="8040" id="token-74-0" morph="none" pos="word" start_char="8039">So</TOKEN>
<TOKEN end_char="8044" id="token-74-1" morph="none" pos="word" start_char="8042">all</TOKEN>
<TOKEN end_char="8050" id="token-74-2" morph="none" pos="word" start_char="8046">these</TOKEN>
<TOKEN end_char="8059" id="token-74-3" morph="none" pos="word" start_char="8052">features</TOKEN>
<TOKEN end_char="8063" id="token-74-4" morph="none" pos="word" start_char="8061">are</TOKEN>
<TOKEN end_char="8069" id="token-74-5" morph="none" pos="word" start_char="8065">found</TOKEN>
<TOKEN end_char="8072" id="token-74-6" morph="none" pos="word" start_char="8071">in</TOKEN>
<TOKEN end_char="8079" id="token-74-7" morph="none" pos="word" start_char="8074">nature</TOKEN>
<TOKEN end_char="8080" id="token-74-8" morph="none" pos="punct" start_char="8080">,</TOKEN>
<TOKEN end_char="8084" id="token-74-9" morph="none" pos="word" start_char="8082">but</TOKEN>
<TOKEN end_char="8092" id="token-74-10" morph="none" pos="word" start_char="8086">they’re</TOKEN>
<TOKEN end_char="8096" id="token-74-11" morph="none" pos="word" start_char="8094">all</TOKEN>
<TOKEN end_char="8101" id="token-74-12" morph="none" pos="word" start_char="8098">also</TOKEN>
<TOKEN end_char="8106" id="token-74-13" morph="none" pos="word" start_char="8103">part</TOKEN>
<TOKEN end_char="8109" id="token-74-14" morph="none" pos="word" start_char="8108">of</TOKEN>
<TOKEN end_char="8117" id="token-74-15" morph="none" pos="word" start_char="8111">viruses</TOKEN>
<TOKEN end_char="8122" id="token-74-16" morph="none" pos="word" start_char="8119">used</TOKEN>
<TOKEN end_char="8125" id="token-74-17" morph="none" pos="word" start_char="8124">in</TOKEN>
<TOKEN end_char="8129" id="token-74-18" morph="none" pos="word" start_char="8127">lab</TOKEN>
<TOKEN end_char="8138" id="token-74-19" morph="none" pos="word" start_char="8131">research</TOKEN>
<TOKEN end_char="8139" id="token-74-20" morph="none" pos="punct" start_char="8139">.</TOKEN>
</SEG>
<SEG end_char="8195" id="segment-75" start_char="8141">
<ORIGINAL_TEXT>That’s a terrible argument against a lab-escaped virus.</ORIGINAL_TEXT>
<TOKEN end_char="8146" id="token-75-0" morph="none" pos="word" start_char="8141">That’s</TOKEN>
<TOKEN end_char="8148" id="token-75-1" morph="none" pos="word" start_char="8148">a</TOKEN>
<TOKEN end_char="8157" id="token-75-2" morph="none" pos="word" start_char="8150">terrible</TOKEN>
<TOKEN end_char="8166" id="token-75-3" morph="none" pos="word" start_char="8159">argument</TOKEN>
<TOKEN end_char="8174" id="token-75-4" morph="none" pos="word" start_char="8168">against</TOKEN>
<TOKEN end_char="8176" id="token-75-5" morph="none" pos="word" start_char="8176">a</TOKEN>
<TOKEN end_char="8188" id="token-75-6" morph="none" pos="unknown" start_char="8178">lab-escaped</TOKEN>
<TOKEN end_char="8194" id="token-75-7" morph="none" pos="word" start_char="8190">virus</TOKEN>
<TOKEN end_char="8195" id="token-75-8" morph="none" pos="punct" start_char="8195">.</TOKEN>
<TRANSLATED_TEXT>That's a terrible argument against a lab-escaped virus.</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="8330" id="segment-76" start_char="8199">
<ORIGINAL_TEXT>The line about ResearchGate having removed the South China Institute of technology paper from their site does not appear to be true:</ORIGINAL_TEXT>
<TOKEN end_char="8201" id="token-76-0" morph="none" pos="word" start_char="8199">The</TOKEN>
<TOKEN end_char="8206" id="token-76-1" morph="none" pos="word" start_char="8203">line</TOKEN>
<TOKEN end_char="8212" id="token-76-2" morph="none" pos="word" start_char="8208">about</TOKEN>
<TOKEN end_char="8225" id="token-76-3" morph="none" pos="word" start_char="8214">ResearchGate</TOKEN>
<TOKEN end_char="8232" id="token-76-4" morph="none" pos="word" start_char="8227">having</TOKEN>
<TOKEN end_char="8240" id="token-76-5" morph="none" pos="word" start_char="8234">removed</TOKEN>
<TOKEN end_char="8244" id="token-76-6" morph="none" pos="word" start_char="8242">the</TOKEN>
<TOKEN end_char="8250" id="token-76-7" morph="none" pos="word" start_char="8246">South</TOKEN>
<TOKEN end_char="8256" id="token-76-8" morph="none" pos="word" start_char="8252">China</TOKEN>
<TOKEN end_char="8266" id="token-76-9" morph="none" pos="word" start_char="8258">Institute</TOKEN>
<TOKEN end_char="8269" id="token-76-10" morph="none" pos="word" start_char="8268">of</TOKEN>
<TOKEN end_char="8280" id="token-76-11" morph="none" pos="word" start_char="8271">technology</TOKEN>
<TOKEN end_char="8286" id="token-76-12" morph="none" pos="word" start_char="8282">paper</TOKEN>
<TOKEN end_char="8291" id="token-76-13" morph="none" pos="word" start_char="8288">from</TOKEN>
<TOKEN end_char="8297" id="token-76-14" morph="none" pos="word" start_char="8293">their</TOKEN>
<TOKEN end_char="8302" id="token-76-15" morph="none" pos="word" start_char="8299">site</TOKEN>
<TOKEN end_char="8307" id="token-76-16" morph="none" pos="word" start_char="8304">does</TOKEN>
<TOKEN end_char="8311" id="token-76-17" morph="none" pos="word" start_char="8309">not</TOKEN>
<TOKEN end_char="8318" id="token-76-18" morph="none" pos="word" start_char="8313">appear</TOKEN>
<TOKEN end_char="8321" id="token-76-19" morph="none" pos="word" start_char="8320">to</TOKEN>
<TOKEN end_char="8324" id="token-76-20" morph="none" pos="word" start_char="8323">be</TOKEN>
<TOKEN end_char="8329" id="token-76-21" morph="none" pos="word" start_char="8326">true</TOKEN>
<TOKEN end_char="8330" id="token-76-22" morph="none" pos="punct" start_char="8330">:</TOKEN>
</SEG>
<SEG end_char="8520" id="segment-77" start_char="8333">
<ORIGINAL_TEXT>https://www.researchgate.net/publication/332998684_Novel_Bat_Alphacoronaviruses_in_Southern_China_Support_Chinese_Horseshoe_Bats_as_an_Important_Reservoir_for_Potential_Novel_Coronaviruses</ORIGINAL_TEXT>
<TOKEN end_char="8520" id="token-77-0" morph="none" pos="url" start_char="8333">https://www.researchgate.net/publication/332998684_Novel_Bat_Alphacoronaviruses_in_Southern_China_Support_Chinese_Horseshoe_Bats_as_an_Important_Reservoir_for_Potential_Novel_Coronaviruses</TOKEN>
<TRANSLATED_TEXT>https: / / www.researchgate.net / publication / 332998684 _ Novel _ Bat _ Alphacoronaviruses _ in _ Southern _ China _ Support _ Chinese _ Horseshoe _ Bats _ as _ an _ Important _ Reservoir _ for _ Potential _ Novel _ Coronaviruses</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="8547" id="segment-78" start_char="8523">
<ORIGINAL_TEXT>It still shows up for me.</ORIGINAL_TEXT>
<TOKEN end_char="8524" id="token-78-0" morph="none" pos="word" start_char="8523">It</TOKEN>
<TOKEN end_char="8530" id="token-78-1" morph="none" pos="word" start_char="8526">still</TOKEN>
<TOKEN end_char="8536" id="token-78-2" morph="none" pos="word" start_char="8532">shows</TOKEN>
<TOKEN end_char="8539" id="token-78-3" morph="none" pos="word" start_char="8538">up</TOKEN>
<TOKEN end_char="8543" id="token-78-4" morph="none" pos="word" start_char="8541">for</TOKEN>
<TOKEN end_char="8546" id="token-78-5" morph="none" pos="word" start_char="8545">me</TOKEN>
<TOKEN end_char="8547" id="token-78-6" morph="none" pos="punct" start_char="8547">.</TOKEN>
</SEG>
<SEG end_char="8586" id="segment-79" start_char="8551">
<ORIGINAL_TEXT>That is an entirely different study.</ORIGINAL_TEXT>
<TOKEN end_char="8554" id="token-79-0" morph="none" pos="word" start_char="8551">That</TOKEN>
<TOKEN end_char="8557" id="token-79-1" morph="none" pos="word" start_char="8556">is</TOKEN>
<TOKEN end_char="8560" id="token-79-2" morph="none" pos="word" start_char="8559">an</TOKEN>
<TOKEN end_char="8569" id="token-79-3" morph="none" pos="word" start_char="8562">entirely</TOKEN>
<TOKEN end_char="8579" id="token-79-4" morph="none" pos="word" start_char="8571">different</TOKEN>
<TOKEN end_char="8585" id="token-79-5" morph="none" pos="word" start_char="8581">study</TOKEN>
<TOKEN end_char="8586" id="token-79-6" morph="none" pos="punct" start_char="8586">.</TOKEN>
</SEG>
<SEG end_char="8629" id="segment-80" start_char="8588">
<ORIGINAL_TEXT>Click on hyperlink "study" in the article.</ORIGINAL_TEXT>
<TOKEN end_char="8592" id="token-80-0" morph="none" pos="word" start_char="8588">Click</TOKEN>
<TOKEN end_char="8595" id="token-80-1" morph="none" pos="word" start_char="8594">on</TOKEN>
<TOKEN end_char="8605" id="token-80-2" morph="none" pos="word" start_char="8597">hyperlink</TOKEN>
<TOKEN end_char="8607" id="token-80-3" morph="none" pos="punct" start_char="8607">"</TOKEN>
<TOKEN end_char="8612" id="token-80-4" morph="none" pos="word" start_char="8608">study</TOKEN>
<TOKEN end_char="8613" id="token-80-5" morph="none" pos="punct" start_char="8613">"</TOKEN>
<TOKEN end_char="8616" id="token-80-6" morph="none" pos="word" start_char="8615">in</TOKEN>
<TOKEN end_char="8620" id="token-80-7" morph="none" pos="word" start_char="8618">the</TOKEN>
<TOKEN end_char="8628" id="token-80-8" morph="none" pos="word" start_char="8622">article</TOKEN>
<TOKEN end_char="8629" id="token-80-9" morph="none" pos="punct" start_char="8629">.</TOKEN>
</SEG>
<SEG end_char="8846" id="segment-81" start_char="8633">
<ORIGINAL_TEXT>It is obvious Chinese researchers in Wuhan were studying corona viruses for years and isolated at least one that uses the ACE2 receptor as the binding receptor for the virus, the exact same mechanism Covid-19 uses.</ORIGINAL_TEXT>
<TOKEN end_char="8634" id="token-81-0" morph="none" pos="word" start_char="8633">It</TOKEN>
<TOKEN end_char="8637" id="token-81-1" morph="none" pos="word" start_char="8636">is</TOKEN>
<TOKEN end_char="8645" id="token-81-2" morph="none" pos="word" start_char="8639">obvious</TOKEN>
<TOKEN end_char="8653" id="token-81-3" morph="none" pos="word" start_char="8647">Chinese</TOKEN>
<TOKEN end_char="8665" id="token-81-4" morph="none" pos="word" start_char="8655">researchers</TOKEN>
<TOKEN end_char="8668" id="token-81-5" morph="none" pos="word" start_char="8667">in</TOKEN>
<TOKEN end_char="8674" id="token-81-6" morph="none" pos="word" start_char="8670">Wuhan</TOKEN>
<TOKEN end_char="8679" id="token-81-7" morph="none" pos="word" start_char="8676">were</TOKEN>
<TOKEN end_char="8688" id="token-81-8" morph="none" pos="word" start_char="8681">studying</TOKEN>
<TOKEN end_char="8695" id="token-81-9" morph="none" pos="word" start_char="8690">corona</TOKEN>
<TOKEN end_char="8703" id="token-81-10" morph="none" pos="word" start_char="8697">viruses</TOKEN>
<TOKEN end_char="8707" id="token-81-11" morph="none" pos="word" start_char="8705">for</TOKEN>
<TOKEN end_char="8713" id="token-81-12" morph="none" pos="word" start_char="8709">years</TOKEN>
<TOKEN end_char="8717" id="token-81-13" morph="none" pos="word" start_char="8715">and</TOKEN>
<TOKEN end_char="8726" id="token-81-14" morph="none" pos="word" start_char="8719">isolated</TOKEN>
<TOKEN end_char="8729" id="token-81-15" morph="none" pos="word" start_char="8728">at</TOKEN>
<TOKEN end_char="8735" id="token-81-16" morph="none" pos="word" start_char="8731">least</TOKEN>
<TOKEN end_char="8739" id="token-81-17" morph="none" pos="word" start_char="8737">one</TOKEN>
<TOKEN end_char="8744" id="token-81-18" morph="none" pos="word" start_char="8741">that</TOKEN>
<TOKEN end_char="8749" id="token-81-19" morph="none" pos="word" start_char="8746">uses</TOKEN>
<TOKEN end_char="8753" id="token-81-20" morph="none" pos="word" start_char="8751">the</TOKEN>
<TOKEN end_char="8758" id="token-81-21" morph="none" pos="word" start_char="8755">ACE2</TOKEN>
<TOKEN end_char="8767" id="token-81-22" morph="none" pos="word" start_char="8760">receptor</TOKEN>
<TOKEN end_char="8770" id="token-81-23" morph="none" pos="word" start_char="8769">as</TOKEN>
<TOKEN end_char="8774" id="token-81-24" morph="none" pos="word" start_char="8772">the</TOKEN>
<TOKEN end_char="8782" id="token-81-25" morph="none" pos="word" start_char="8776">binding</TOKEN>
<TOKEN end_char="8791" id="token-81-26" morph="none" pos="word" start_char="8784">receptor</TOKEN>
<TOKEN end_char="8795" id="token-81-27" morph="none" pos="word" start_char="8793">for</TOKEN>
<TOKEN end_char="8799" id="token-81-28" morph="none" pos="word" start_char="8797">the</TOKEN>
<TOKEN end_char="8805" id="token-81-29" morph="none" pos="word" start_char="8801">virus</TOKEN>
<TOKEN end_char="8806" id="token-81-30" morph="none" pos="punct" start_char="8806">,</TOKEN>
<TOKEN end_char="8810" id="token-81-31" morph="none" pos="word" start_char="8808">the</TOKEN>
<TOKEN end_char="8816" id="token-81-32" morph="none" pos="word" start_char="8812">exact</TOKEN>
<TOKEN end_char="8821" id="token-81-33" morph="none" pos="word" start_char="8818">same</TOKEN>
<TOKEN end_char="8831" id="token-81-34" morph="none" pos="word" start_char="8823">mechanism</TOKEN>
<TOKEN end_char="8840" id="token-81-35" morph="none" pos="unknown" start_char="8833">Covid-19</TOKEN>
<TOKEN end_char="8845" id="token-81-36" morph="none" pos="word" start_char="8842">uses</TOKEN>
<TOKEN end_char="8846" id="token-81-37" morph="none" pos="punct" start_char="8846">.</TOKEN>
</SEG>
<SEG end_char="8907" id="segment-82" start_char="8848">
<ORIGINAL_TEXT>see this paper- https://www.ncbi.nlm.nih.gov/pubmed/24172901</ORIGINAL_TEXT>
<TOKEN end_char="8850" id="token-82-0" morph="none" pos="word" start_char="8848">see</TOKEN>
<TOKEN end_char="8855" id="token-82-1" morph="none" pos="word" start_char="8852">this</TOKEN>
<TOKEN end_char="8861" id="token-82-2" morph="none" pos="word" start_char="8857">paper</TOKEN>
<TOKEN end_char="8862" id="token-82-3" morph="none" pos="punct" start_char="8862">-</TOKEN>
<TOKEN end_char="8907" id="token-82-4" morph="none" pos="url" start_char="8864">https://www.ncbi.nlm.nih.gov/pubmed/24172901</TOKEN>
</SEG>
<SEG end_char="9023" id="segment-83" start_char="8911">
<ORIGINAL_TEXT>The best way I guess is to make inventory of Wuhan bio lab scientists /staff whether they are still alive or not.</ORIGINAL_TEXT>
<TOKEN end_char="8913" id="token-83-0" morph="none" pos="word" start_char="8911">The</TOKEN>
<TOKEN end_char="8918" id="token-83-1" morph="none" pos="word" start_char="8915">best</TOKEN>
<TOKEN end_char="8922" id="token-83-2" morph="none" pos="word" start_char="8920">way</TOKEN>
<TOKEN end_char="8924" id="token-83-3" morph="none" pos="word" start_char="8924">I</TOKEN>
<TOKEN end_char="8930" id="token-83-4" morph="none" pos="word" start_char="8926">guess</TOKEN>
<TOKEN end_char="8933" id="token-83-5" morph="none" pos="word" start_char="8932">is</TOKEN>
<TOKEN end_char="8936" id="token-83-6" morph="none" pos="word" start_char="8935">to</TOKEN>
<TOKEN end_char="8941" id="token-83-7" morph="none" pos="word" start_char="8938">make</TOKEN>
<TOKEN end_char="8951" id="token-83-8" morph="none" pos="word" start_char="8943">inventory</TOKEN>
<TOKEN end_char="8954" id="token-83-9" morph="none" pos="word" start_char="8953">of</TOKEN>
<TOKEN end_char="8960" id="token-83-10" morph="none" pos="word" start_char="8956">Wuhan</TOKEN>
<TOKEN end_char="8964" id="token-83-11" morph="none" pos="word" start_char="8962">bio</TOKEN>
<TOKEN end_char="8968" id="token-83-12" morph="none" pos="word" start_char="8966">lab</TOKEN>
<TOKEN end_char="8979" id="token-83-13" morph="none" pos="word" start_char="8970">scientists</TOKEN>
<TOKEN end_char="8981" id="token-83-14" morph="none" pos="punct" start_char="8981">/</TOKEN>
<TOKEN end_char="8986" id="token-83-15" morph="none" pos="word" start_char="8982">staff</TOKEN>
<TOKEN end_char="8994" id="token-83-16" morph="none" pos="word" start_char="8988">whether</TOKEN>
<TOKEN end_char="8999" id="token-83-17" morph="none" pos="word" start_char="8996">they</TOKEN>
<TOKEN end_char="9003" id="token-83-18" morph="none" pos="word" start_char="9001">are</TOKEN>
<TOKEN end_char="9009" id="token-83-19" morph="none" pos="word" start_char="9005">still</TOKEN>
<TOKEN end_char="9015" id="token-83-20" morph="none" pos="word" start_char="9011">alive</TOKEN>
<TOKEN end_char="9018" id="token-83-21" morph="none" pos="word" start_char="9017">or</TOKEN>
<TOKEN end_char="9022" id="token-83-22" morph="none" pos="word" start_char="9020">not</TOKEN>
<TOKEN end_char="9023" id="token-83-23" morph="none" pos="punct" start_char="9023">.</TOKEN>
</SEG>
<SEG end_char="9082" id="segment-84" start_char="9025">
<ORIGINAL_TEXT>And if not, the cause of death and date and time of death.</ORIGINAL_TEXT>
<TOKEN end_char="9027" id="token-84-0" morph="none" pos="word" start_char="9025">And</TOKEN>
<TOKEN end_char="9030" id="token-84-1" morph="none" pos="word" start_char="9029">if</TOKEN>
<TOKEN end_char="9034" id="token-84-2" morph="none" pos="word" start_char="9032">not</TOKEN>
<TOKEN end_char="9035" id="token-84-3" morph="none" pos="punct" start_char="9035">,</TOKEN>
<TOKEN end_char="9039" id="token-84-4" morph="none" pos="word" start_char="9037">the</TOKEN>
<TOKEN end_char="9045" id="token-84-5" morph="none" pos="word" start_char="9041">cause</TOKEN>
<TOKEN end_char="9048" id="token-84-6" morph="none" pos="word" start_char="9047">of</TOKEN>
<TOKEN end_char="9054" id="token-84-7" morph="none" pos="word" start_char="9050">death</TOKEN>
<TOKEN end_char="9058" id="token-84-8" morph="none" pos="word" start_char="9056">and</TOKEN>
<TOKEN end_char="9063" id="token-84-9" morph="none" pos="word" start_char="9060">date</TOKEN>
<TOKEN end_char="9067" id="token-84-10" morph="none" pos="word" start_char="9065">and</TOKEN>
<TOKEN end_char="9072" id="token-84-11" morph="none" pos="word" start_char="9069">time</TOKEN>
<TOKEN end_char="9075" id="token-84-12" morph="none" pos="word" start_char="9074">of</TOKEN>
<TOKEN end_char="9081" id="token-84-13" morph="none" pos="word" start_char="9077">death</TOKEN>
<TOKEN end_char="9082" id="token-84-14" morph="none" pos="punct" start_char="9082">.</TOKEN>
</SEG>
<SEG end_char="9168" id="segment-85" start_char="9084">
<ORIGINAL_TEXT>Experts say it is important to find Patient 0 so they can understand better COVID19..</ORIGINAL_TEXT>
<TOKEN end_char="9090" id="token-85-0" morph="none" pos="word" start_char="9084">Experts</TOKEN>
<TOKEN end_char="9094" id="token-85-1" morph="none" pos="word" start_char="9092">say</TOKEN>
<TOKEN end_char="9097" id="token-85-2" morph="none" pos="word" start_char="9096">it</TOKEN>
<TOKEN end_char="9100" id="token-85-3" morph="none" pos="word" start_char="9099">is</TOKEN>
<TOKEN end_char="9110" id="token-85-4" morph="none" pos="word" start_char="9102">important</TOKEN>
<TOKEN end_char="9113" id="token-85-5" morph="none" pos="word" start_char="9112">to</TOKEN>
<TOKEN end_char="9118" id="token-85-6" morph="none" pos="word" start_char="9115">find</TOKEN>
<TOKEN end_char="9126" id="token-85-7" morph="none" pos="word" start_char="9120">Patient</TOKEN>
<TOKEN end_char="9128" id="token-85-8" morph="none" pos="word" start_char="9128">0</TOKEN>
<TOKEN end_char="9131" id="token-85-9" morph="none" pos="word" start_char="9130">so</TOKEN>
<TOKEN end_char="9136" id="token-85-10" morph="none" pos="word" start_char="9133">they</TOKEN>
<TOKEN end_char="9140" id="token-85-11" morph="none" pos="word" start_char="9138">can</TOKEN>
<TOKEN end_char="9151" id="token-85-12" morph="none" pos="word" start_char="9142">understand</TOKEN>
<TOKEN end_char="9158" id="token-85-13" morph="none" pos="word" start_char="9153">better</TOKEN>
<TOKEN end_char="9166" id="token-85-14" morph="none" pos="word" start_char="9160">COVID19</TOKEN>
<TOKEN end_char="9168" id="token-85-15" morph="none" pos="punct" start_char="9167">..</TOKEN>
</SEG>
<SEG end_char="9251" id="segment-86" start_char="9172">
<ORIGINAL_TEXT>Actually lab-escape theory is so far the more widely accepted conspiracy theory.</ORIGINAL_TEXT>
<TOKEN end_char="9179" id="token-86-0" morph="none" pos="word" start_char="9172">Actually</TOKEN>
<TOKEN end_char="9190" id="token-86-1" morph="none" pos="unknown" start_char="9181">lab-escape</TOKEN>
<TOKEN end_char="9197" id="token-86-2" morph="none" pos="word" start_char="9192">theory</TOKEN>
<TOKEN end_char="9200" id="token-86-3" morph="none" pos="word" start_char="9199">is</TOKEN>
<TOKEN end_char="9203" id="token-86-4" morph="none" pos="word" start_char="9202">so</TOKEN>
<TOKEN end_char="9207" id="token-86-5" morph="none" pos="word" start_char="9205">far</TOKEN>
<TOKEN end_char="9211" id="token-86-6" morph="none" pos="word" start_char="9209">the</TOKEN>
<TOKEN end_char="9216" id="token-86-7" morph="none" pos="word" start_char="9213">more</TOKEN>
<TOKEN end_char="9223" id="token-86-8" morph="none" pos="word" start_char="9218">widely</TOKEN>
<TOKEN end_char="9232" id="token-86-9" morph="none" pos="word" start_char="9225">accepted</TOKEN>
<TOKEN end_char="9243" id="token-86-10" morph="none" pos="word" start_char="9234">conspiracy</TOKEN>
<TOKEN end_char="9250" id="token-86-11" morph="none" pos="word" start_char="9245">theory</TOKEN>
<TOKEN end_char="9251" id="token-86-12" morph="none" pos="punct" start_char="9251">.</TOKEN>
</SEG>
<SEG end_char="9389" id="segment-87" start_char="9253">
<ORIGINAL_TEXT>It had been circulating on social media for weeks, and gained considerable visibility following a New York Post article in late February.</ORIGINAL_TEXT>
<TOKEN end_char="9254" id="token-87-0" morph="none" pos="word" start_char="9253">It</TOKEN>
<TOKEN end_char="9258" id="token-87-1" morph="none" pos="word" start_char="9256">had</TOKEN>
<TOKEN end_char="9263" id="token-87-2" morph="none" pos="word" start_char="9260">been</TOKEN>
<TOKEN end_char="9275" id="token-87-3" morph="none" pos="word" start_char="9265">circulating</TOKEN>
<TOKEN end_char="9278" id="token-87-4" morph="none" pos="word" start_char="9277">on</TOKEN>
<TOKEN end_char="9285" id="token-87-5" morph="none" pos="word" start_char="9280">social</TOKEN>
<TOKEN end_char="9291" id="token-87-6" morph="none" pos="word" start_char="9287">media</TOKEN>
<TOKEN end_char="9295" id="token-87-7" morph="none" pos="word" start_char="9293">for</TOKEN>
<TOKEN end_char="9301" id="token-87-8" morph="none" pos="word" start_char="9297">weeks</TOKEN>
<TOKEN end_char="9302" id="token-87-9" morph="none" pos="punct" start_char="9302">,</TOKEN>
<TOKEN end_char="9306" id="token-87-10" morph="none" pos="word" start_char="9304">and</TOKEN>
<TOKEN end_char="9313" id="token-87-11" morph="none" pos="word" start_char="9308">gained</TOKEN>
<TOKEN end_char="9326" id="token-87-12" morph="none" pos="word" start_char="9315">considerable</TOKEN>
<TOKEN end_char="9337" id="token-87-13" morph="none" pos="word" start_char="9328">visibility</TOKEN>
<TOKEN end_char="9347" id="token-87-14" morph="none" pos="word" start_char="9339">following</TOKEN>
<TOKEN end_char="9349" id="token-87-15" morph="none" pos="word" start_char="9349">a</TOKEN>
<TOKEN end_char="9353" id="token-87-16" morph="none" pos="word" start_char="9351">New</TOKEN>
<TOKEN end_char="9358" id="token-87-17" morph="none" pos="word" start_char="9355">York</TOKEN>
<TOKEN end_char="9363" id="token-87-18" morph="none" pos="word" start_char="9360">Post</TOKEN>
<TOKEN end_char="9371" id="token-87-19" morph="none" pos="word" start_char="9365">article</TOKEN>
<TOKEN end_char="9374" id="token-87-20" morph="none" pos="word" start_char="9373">in</TOKEN>
<TOKEN end_char="9379" id="token-87-21" morph="none" pos="word" start_char="9376">late</TOKEN>
<TOKEN end_char="9388" id="token-87-22" morph="none" pos="word" start_char="9381">February</TOKEN>
<TOKEN end_char="9389" id="token-87-23" morph="none" pos="punct" start_char="9389">.</TOKEN>
</SEG>
<SEG end_char="9498" id="segment-88" start_char="9391">
<ORIGINAL_TEXT>But there is a lot of scholarly evidence to suggest that coronavirus was not manufactured in the laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="9393" id="token-88-0" morph="none" pos="word" start_char="9391">But</TOKEN>
<TOKEN end_char="9399" id="token-88-1" morph="none" pos="word" start_char="9395">there</TOKEN>
<TOKEN end_char="9402" id="token-88-2" morph="none" pos="word" start_char="9401">is</TOKEN>
<TOKEN end_char="9404" id="token-88-3" morph="none" pos="word" start_char="9404">a</TOKEN>
<TOKEN end_char="9408" id="token-88-4" morph="none" pos="word" start_char="9406">lot</TOKEN>
<TOKEN end_char="9411" id="token-88-5" morph="none" pos="word" start_char="9410">of</TOKEN>
<TOKEN end_char="9421" id="token-88-6" morph="none" pos="word" start_char="9413">scholarly</TOKEN>
<TOKEN end_char="9430" id="token-88-7" morph="none" pos="word" start_char="9423">evidence</TOKEN>
<TOKEN end_char="9433" id="token-88-8" morph="none" pos="word" start_char="9432">to</TOKEN>
<TOKEN end_char="9441" id="token-88-9" morph="none" pos="word" start_char="9435">suggest</TOKEN>
<TOKEN end_char="9446" id="token-88-10" morph="none" pos="word" start_char="9443">that</TOKEN>
<TOKEN end_char="9458" id="token-88-11" morph="none" pos="word" start_char="9448">coronavirus</TOKEN>
<TOKEN end_char="9462" id="token-88-12" morph="none" pos="word" start_char="9460">was</TOKEN>
<TOKEN end_char="9466" id="token-88-13" morph="none" pos="word" start_char="9464">not</TOKEN>
<TOKEN end_char="9479" id="token-88-14" morph="none" pos="word" start_char="9468">manufactured</TOKEN>
<TOKEN end_char="9482" id="token-88-15" morph="none" pos="word" start_char="9481">in</TOKEN>
<TOKEN end_char="9486" id="token-88-16" morph="none" pos="word" start_char="9484">the</TOKEN>
<TOKEN end_char="9497" id="token-88-17" morph="none" pos="word" start_char="9488">laboratory</TOKEN>
<TOKEN end_char="9498" id="token-88-18" morph="none" pos="punct" start_char="9498">.</TOKEN>
</SEG>
<SEG end_char="9597" id="segment-89" start_char="9500">
<ORIGINAL_TEXT>Even if the exact source of the disease is not known yet, the virus originally came from wildlife.</ORIGINAL_TEXT>
<TOKEN end_char="9503" id="token-89-0" morph="none" pos="word" start_char="9500">Even</TOKEN>
<TOKEN end_char="9506" id="token-89-1" morph="none" pos="word" start_char="9505">if</TOKEN>
<TOKEN end_char="9510" id="token-89-2" morph="none" pos="word" start_char="9508">the</TOKEN>
<TOKEN end_char="9516" id="token-89-3" morph="none" pos="word" start_char="9512">exact</TOKEN>
<TOKEN end_char="9523" id="token-89-4" morph="none" pos="word" start_char="9518">source</TOKEN>
<TOKEN end_char="9526" id="token-89-5" morph="none" pos="word" start_char="9525">of</TOKEN>
<TOKEN end_char="9530" id="token-89-6" morph="none" pos="word" start_char="9528">the</TOKEN>
<TOKEN end_char="9538" id="token-89-7" morph="none" pos="word" start_char="9532">disease</TOKEN>
<TOKEN end_char="9541" id="token-89-8" morph="none" pos="word" start_char="9540">is</TOKEN>
<TOKEN end_char="9545" id="token-89-9" morph="none" pos="word" start_char="9543">not</TOKEN>
<TOKEN end_char="9551" id="token-89-10" morph="none" pos="word" start_char="9547">known</TOKEN>
<TOKEN end_char="9555" id="token-89-11" morph="none" pos="word" start_char="9553">yet</TOKEN>
<TOKEN end_char="9556" id="token-89-12" morph="none" pos="punct" start_char="9556">,</TOKEN>
<TOKEN end_char="9560" id="token-89-13" morph="none" pos="word" start_char="9558">the</TOKEN>
<TOKEN end_char="9566" id="token-89-14" morph="none" pos="word" start_char="9562">virus</TOKEN>
<TOKEN end_char="9577" id="token-89-15" morph="none" pos="word" start_char="9568">originally</TOKEN>
<TOKEN end_char="9582" id="token-89-16" morph="none" pos="word" start_char="9579">came</TOKEN>
<TOKEN end_char="9587" id="token-89-17" morph="none" pos="word" start_char="9584">from</TOKEN>
<TOKEN end_char="9596" id="token-89-18" morph="none" pos="word" start_char="9589">wildlife</TOKEN>
<TOKEN end_char="9597" id="token-89-19" morph="none" pos="punct" start_char="9597">.</TOKEN>
</SEG>
<SEG end_char="9747" id="segment-90" start_char="9599">
<ORIGINAL_TEXT>In the past , these viruses have spread through wild bats that infect another type of animal – an intermediate host – that then spreads it to humans.</ORIGINAL_TEXT>
<TOKEN end_char="9600" id="token-90-0" morph="none" pos="word" start_char="9599">In</TOKEN>
<TOKEN end_char="9604" id="token-90-1" morph="none" pos="word" start_char="9602">the</TOKEN>
<TOKEN end_char="9609" id="token-90-2" morph="none" pos="word" start_char="9606">past</TOKEN>
<TOKEN end_char="9611" id="token-90-3" morph="none" pos="punct" start_char="9611">,</TOKEN>
<TOKEN end_char="9617" id="token-90-4" morph="none" pos="word" start_char="9613">these</TOKEN>
<TOKEN end_char="9625" id="token-90-5" morph="none" pos="word" start_char="9619">viruses</TOKEN>
<TOKEN end_char="9630" id="token-90-6" morph="none" pos="word" start_char="9627">have</TOKEN>
<TOKEN end_char="9637" id="token-90-7" morph="none" pos="word" start_char="9632">spread</TOKEN>
<TOKEN end_char="9645" id="token-90-8" morph="none" pos="word" start_char="9639">through</TOKEN>
<TOKEN end_char="9650" id="token-90-9" morph="none" pos="word" start_char="9647">wild</TOKEN>
<TOKEN end_char="9655" id="token-90-10" morph="none" pos="word" start_char="9652">bats</TOKEN>
<TOKEN end_char="9660" id="token-90-11" morph="none" pos="word" start_char="9657">that</TOKEN>
<TOKEN end_char="9667" id="token-90-12" morph="none" pos="word" start_char="9662">infect</TOKEN>
<TOKEN end_char="9675" id="token-90-13" morph="none" pos="word" start_char="9669">another</TOKEN>
<TOKEN end_char="9680" id="token-90-14" morph="none" pos="word" start_char="9677">type</TOKEN>
<TOKEN end_char="9683" id="token-90-15" morph="none" pos="word" start_char="9682">of</TOKEN>
<TOKEN end_char="9690" id="token-90-16" morph="none" pos="word" start_char="9685">animal</TOKEN>
<TOKEN end_char="9692" id="token-90-17" morph="none" pos="punct" start_char="9692">–</TOKEN>
<TOKEN end_char="9695" id="token-90-18" morph="none" pos="word" start_char="9694">an</TOKEN>
<TOKEN end_char="9708" id="token-90-19" morph="none" pos="word" start_char="9697">intermediate</TOKEN>
<TOKEN end_char="9713" id="token-90-20" morph="none" pos="word" start_char="9710">host</TOKEN>
<TOKEN end_char="9715" id="token-90-21" morph="none" pos="punct" start_char="9715">–</TOKEN>
<TOKEN end_char="9720" id="token-90-22" morph="none" pos="word" start_char="9717">that</TOKEN>
<TOKEN end_char="9725" id="token-90-23" morph="none" pos="word" start_char="9722">then</TOKEN>
<TOKEN end_char="9733" id="token-90-24" morph="none" pos="word" start_char="9727">spreads</TOKEN>
<TOKEN end_char="9736" id="token-90-25" morph="none" pos="word" start_char="9735">it</TOKEN>
<TOKEN end_char="9739" id="token-90-26" morph="none" pos="word" start_char="9738">to</TOKEN>
<TOKEN end_char="9746" id="token-90-27" morph="none" pos="word" start_char="9741">humans</TOKEN>
<TOKEN end_char="9747" id="token-90-28" morph="none" pos="punct" start_char="9747">.</TOKEN>
</SEG>
<SEG end_char="9860" id="segment-91" start_char="9751">
<ORIGINAL_TEXT>So first you make sure to point out the "social media conspiracy theory" to set the stage for what comes next.</ORIGINAL_TEXT>
<TOKEN end_char="9752" id="token-91-0" morph="none" pos="word" start_char="9751">So</TOKEN>
<TOKEN end_char="9758" id="token-91-1" morph="none" pos="word" start_char="9754">first</TOKEN>
<TOKEN end_char="9762" id="token-91-2" morph="none" pos="word" start_char="9760">you</TOKEN>
<TOKEN end_char="9767" id="token-91-3" morph="none" pos="word" start_char="9764">make</TOKEN>
<TOKEN end_char="9772" id="token-91-4" morph="none" pos="word" start_char="9769">sure</TOKEN>
<TOKEN end_char="9775" id="token-91-5" morph="none" pos="word" start_char="9774">to</TOKEN>
<TOKEN end_char="9781" id="token-91-6" morph="none" pos="word" start_char="9777">point</TOKEN>
<TOKEN end_char="9785" id="token-91-7" morph="none" pos="word" start_char="9783">out</TOKEN>
<TOKEN end_char="9789" id="token-91-8" morph="none" pos="word" start_char="9787">the</TOKEN>
<TOKEN end_char="9791" id="token-91-9" morph="none" pos="punct" start_char="9791">"</TOKEN>
<TOKEN end_char="9797" id="token-91-10" morph="none" pos="word" start_char="9792">social</TOKEN>
<TOKEN end_char="9803" id="token-91-11" morph="none" pos="word" start_char="9799">media</TOKEN>
<TOKEN end_char="9814" id="token-91-12" morph="none" pos="word" start_char="9805">conspiracy</TOKEN>
<TOKEN end_char="9821" id="token-91-13" morph="none" pos="word" start_char="9816">theory</TOKEN>
<TOKEN end_char="9822" id="token-91-14" morph="none" pos="punct" start_char="9822">"</TOKEN>
<TOKEN end_char="9825" id="token-91-15" morph="none" pos="word" start_char="9824">to</TOKEN>
<TOKEN end_char="9829" id="token-91-16" morph="none" pos="word" start_char="9827">set</TOKEN>
<TOKEN end_char="9833" id="token-91-17" morph="none" pos="word" start_char="9831">the</TOKEN>
<TOKEN end_char="9839" id="token-91-18" morph="none" pos="word" start_char="9835">stage</TOKEN>
<TOKEN end_char="9843" id="token-91-19" morph="none" pos="word" start_char="9841">for</TOKEN>
<TOKEN end_char="9848" id="token-91-20" morph="none" pos="word" start_char="9845">what</TOKEN>
<TOKEN end_char="9854" id="token-91-21" morph="none" pos="word" start_char="9850">comes</TOKEN>
<TOKEN end_char="9859" id="token-91-22" morph="none" pos="word" start_char="9856">next</TOKEN>
<TOKEN end_char="9860" id="token-91-23" morph="none" pos="punct" start_char="9860">.</TOKEN>
</SEG>
<SEG end_char="10072" id="segment-92" start_char="9862">
<ORIGINAL_TEXT>What comes next is your refutation of the possibility of a lab escape accident, with this statement… "But there is a lot of scholarly evidence to suggest that coronavirus was not manufactured in the laboratory."</ORIGINAL_TEXT>
<TOKEN end_char="9865" id="token-92-0" morph="none" pos="word" start_char="9862">What</TOKEN>
<TOKEN end_char="9871" id="token-92-1" morph="none" pos="word" start_char="9867">comes</TOKEN>
<TOKEN end_char="9876" id="token-92-2" morph="none" pos="word" start_char="9873">next</TOKEN>
<TOKEN end_char="9879" id="token-92-3" morph="none" pos="word" start_char="9878">is</TOKEN>
<TOKEN end_char="9884" id="token-92-4" morph="none" pos="word" start_char="9881">your</TOKEN>
<TOKEN end_char="9895" id="token-92-5" morph="none" pos="word" start_char="9886">refutation</TOKEN>
<TOKEN end_char="9898" id="token-92-6" morph="none" pos="word" start_char="9897">of</TOKEN>
<TOKEN end_char="9902" id="token-92-7" morph="none" pos="word" start_char="9900">the</TOKEN>
<TOKEN end_char="9914" id="token-92-8" morph="none" pos="word" start_char="9904">possibility</TOKEN>
<TOKEN end_char="9917" id="token-92-9" morph="none" pos="word" start_char="9916">of</TOKEN>
<TOKEN end_char="9919" id="token-92-10" morph="none" pos="word" start_char="9919">a</TOKEN>
<TOKEN end_char="9923" id="token-92-11" morph="none" pos="word" start_char="9921">lab</TOKEN>
<TOKEN end_char="9930" id="token-92-12" morph="none" pos="word" start_char="9925">escape</TOKEN>
<TOKEN end_char="9939" id="token-92-13" morph="none" pos="word" start_char="9932">accident</TOKEN>
<TOKEN end_char="9940" id="token-92-14" morph="none" pos="punct" start_char="9940">,</TOKEN>
<TOKEN end_char="9945" id="token-92-15" morph="none" pos="word" start_char="9942">with</TOKEN>
<TOKEN end_char="9950" id="token-92-16" morph="none" pos="word" start_char="9947">this</TOKEN>
<TOKEN end_char="9960" id="token-92-17" morph="none" pos="word" start_char="9952">statement</TOKEN>
<TOKEN end_char="9961" id="token-92-18" morph="none" pos="punct" start_char="9961">…</TOKEN>
<TOKEN end_char="9963" id="token-92-19" morph="none" pos="punct" start_char="9963">"</TOKEN>
<TOKEN end_char="9966" id="token-92-20" morph="none" pos="word" start_char="9964">But</TOKEN>
<TOKEN end_char="9972" id="token-92-21" morph="none" pos="word" start_char="9968">there</TOKEN>
<TOKEN end_char="9975" id="token-92-22" morph="none" pos="word" start_char="9974">is</TOKEN>
<TOKEN end_char="9977" id="token-92-23" morph="none" pos="word" start_char="9977">a</TOKEN>
<TOKEN end_char="9981" id="token-92-24" morph="none" pos="word" start_char="9979">lot</TOKEN>
<TOKEN end_char="9984" id="token-92-25" morph="none" pos="word" start_char="9983">of</TOKEN>
<TOKEN end_char="9994" id="token-92-26" morph="none" pos="word" start_char="9986">scholarly</TOKEN>
<TOKEN end_char="10003" id="token-92-27" morph="none" pos="word" start_char="9996">evidence</TOKEN>
<TOKEN end_char="10006" id="token-92-28" morph="none" pos="word" start_char="10005">to</TOKEN>
<TOKEN end_char="10014" id="token-92-29" morph="none" pos="word" start_char="10008">suggest</TOKEN>
<TOKEN end_char="10019" id="token-92-30" morph="none" pos="word" start_char="10016">that</TOKEN>
<TOKEN end_char="10031" id="token-92-31" morph="none" pos="word" start_char="10021">coronavirus</TOKEN>
<TOKEN end_char="10035" id="token-92-32" morph="none" pos="word" start_char="10033">was</TOKEN>
<TOKEN end_char="10039" id="token-92-33" morph="none" pos="word" start_char="10037">not</TOKEN>
<TOKEN end_char="10052" id="token-92-34" morph="none" pos="word" start_char="10041">manufactured</TOKEN>
<TOKEN end_char="10055" id="token-92-35" morph="none" pos="word" start_char="10054">in</TOKEN>
<TOKEN end_char="10059" id="token-92-36" morph="none" pos="word" start_char="10057">the</TOKEN>
<TOKEN end_char="10070" id="token-92-37" morph="none" pos="word" start_char="10061">laboratory</TOKEN>
<TOKEN end_char="10072" id="token-92-38" morph="none" pos="punct" start_char="10071">."</TOKEN>
</SEG>
<SEG end_char="10123" id="segment-93" start_char="10074">
<ORIGINAL_TEXT>That is a textbook example of a strawman argument.</ORIGINAL_TEXT>
<TOKEN end_char="10077" id="token-93-0" morph="none" pos="word" start_char="10074">That</TOKEN>
<TOKEN end_char="10080" id="token-93-1" morph="none" pos="word" start_char="10079">is</TOKEN>
<TOKEN end_char="10082" id="token-93-2" morph="none" pos="word" start_char="10082">a</TOKEN>
<TOKEN end_char="10091" id="token-93-3" morph="none" pos="word" start_char="10084">textbook</TOKEN>
<TOKEN end_char="10099" id="token-93-4" morph="none" pos="word" start_char="10093">example</TOKEN>
<TOKEN end_char="10102" id="token-93-5" morph="none" pos="word" start_char="10101">of</TOKEN>
<TOKEN end_char="10104" id="token-93-6" morph="none" pos="word" start_char="10104">a</TOKEN>
<TOKEN end_char="10113" id="token-93-7" morph="none" pos="word" start_char="10106">strawman</TOKEN>
<TOKEN end_char="10122" id="token-93-8" morph="none" pos="word" start_char="10115">argument</TOKEN>
<TOKEN end_char="10123" id="token-93-9" morph="none" pos="punct" start_char="10123">.</TOKEN>
</SEG>
<SEG end_char="10206" id="segment-94" start_char="10125">
<ORIGINAL_TEXT>The lab escape theory in no way depends upon the virus being a man-made construct.</ORIGINAL_TEXT>
<TOKEN end_char="10127" id="token-94-0" morph="none" pos="word" start_char="10125">The</TOKEN>
<TOKEN end_char="10131" id="token-94-1" morph="none" pos="word" start_char="10129">lab</TOKEN>
<TOKEN end_char="10138" id="token-94-2" morph="none" pos="word" start_char="10133">escape</TOKEN>
<TOKEN end_char="10145" id="token-94-3" morph="none" pos="word" start_char="10140">theory</TOKEN>
<TOKEN end_char="10148" id="token-94-4" morph="none" pos="word" start_char="10147">in</TOKEN>
<TOKEN end_char="10151" id="token-94-5" morph="none" pos="word" start_char="10150">no</TOKEN>
<TOKEN end_char="10155" id="token-94-6" morph="none" pos="word" start_char="10153">way</TOKEN>
<TOKEN end_char="10163" id="token-94-7" morph="none" pos="word" start_char="10157">depends</TOKEN>
<TOKEN end_char="10168" id="token-94-8" morph="none" pos="word" start_char="10165">upon</TOKEN>
<TOKEN end_char="10172" id="token-94-9" morph="none" pos="word" start_char="10170">the</TOKEN>
<TOKEN end_char="10178" id="token-94-10" morph="none" pos="word" start_char="10174">virus</TOKEN>
<TOKEN end_char="10184" id="token-94-11" morph="none" pos="word" start_char="10180">being</TOKEN>
<TOKEN end_char="10186" id="token-94-12" morph="none" pos="word" start_char="10186">a</TOKEN>
<TOKEN end_char="10195" id="token-94-13" morph="none" pos="unknown" start_char="10188">man-made</TOKEN>
<TOKEN end_char="10205" id="token-94-14" morph="none" pos="word" start_char="10197">construct</TOKEN>
<TOKEN end_char="10206" id="token-94-15" morph="none" pos="punct" start_char="10206">.</TOKEN>
</SEG>
<SEG end_char="10302" id="segment-95" start_char="10208">
<ORIGINAL_TEXT>Conclusively proving that the virus was not man-made in no way disproves the lab escape theory.</ORIGINAL_TEXT>
<TOKEN end_char="10219" id="token-95-0" morph="none" pos="word" start_char="10208">Conclusively</TOKEN>
<TOKEN end_char="10227" id="token-95-1" morph="none" pos="word" start_char="10221">proving</TOKEN>
<TOKEN end_char="10232" id="token-95-2" morph="none" pos="word" start_char="10229">that</TOKEN>
<TOKEN end_char="10236" id="token-95-3" morph="none" pos="word" start_char="10234">the</TOKEN>
<TOKEN end_char="10242" id="token-95-4" morph="none" pos="word" start_char="10238">virus</TOKEN>
<TOKEN end_char="10246" id="token-95-5" morph="none" pos="word" start_char="10244">was</TOKEN>
<TOKEN end_char="10250" id="token-95-6" morph="none" pos="word" start_char="10248">not</TOKEN>
<TOKEN end_char="10259" id="token-95-7" morph="none" pos="unknown" start_char="10252">man-made</TOKEN>
<TOKEN end_char="10262" id="token-95-8" morph="none" pos="word" start_char="10261">in</TOKEN>
<TOKEN end_char="10265" id="token-95-9" morph="none" pos="word" start_char="10264">no</TOKEN>
<TOKEN end_char="10269" id="token-95-10" morph="none" pos="word" start_char="10267">way</TOKEN>
<TOKEN end_char="10279" id="token-95-11" morph="none" pos="word" start_char="10271">disproves</TOKEN>
<TOKEN end_char="10283" id="token-95-12" morph="none" pos="word" start_char="10281">the</TOKEN>
<TOKEN end_char="10287" id="token-95-13" morph="none" pos="word" start_char="10285">lab</TOKEN>
<TOKEN end_char="10294" id="token-95-14" morph="none" pos="word" start_char="10289">escape</TOKEN>
<TOKEN end_char="10301" id="token-95-15" morph="none" pos="word" start_char="10296">theory</TOKEN>
<TOKEN end_char="10302" id="token-95-16" morph="none" pos="punct" start_char="10302">.</TOKEN>
</SEG>
<SEG end_char="10329" id="segment-96" start_char="10304">
<ORIGINAL_TEXT>It is a known… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="10305" id="token-96-0" morph="none" pos="word" start_char="10304">It</TOKEN>
<TOKEN end_char="10308" id="token-96-1" morph="none" pos="word" start_char="10307">is</TOKEN>
<TOKEN end_char="10310" id="token-96-2" morph="none" pos="word" start_char="10310">a</TOKEN>
<TOKEN end_char="10316" id="token-96-3" morph="none" pos="word" start_char="10312">known</TOKEN>
<TOKEN end_char="10317" id="token-96-4" morph="none" pos="punct" start_char="10317">…</TOKEN>
<TOKEN end_char="10322" id="token-96-5" morph="none" pos="word" start_char="10319">Read</TOKEN>
<TOKEN end_char="10327" id="token-96-6" morph="none" pos="word" start_char="10324">more</TOKEN>
<TOKEN end_char="10329" id="token-96-7" morph="none" pos="punct" start_char="10329">»</TOKEN>
</SEG>
<SEG end_char="10390" id="segment-97" start_char="10333">
<ORIGINAL_TEXT>in case of a lab leak, wound many lab workers be infected?</ORIGINAL_TEXT>
<TOKEN end_char="10334" id="token-97-0" morph="none" pos="word" start_char="10333">in</TOKEN>
<TOKEN end_char="10339" id="token-97-1" morph="none" pos="word" start_char="10336">case</TOKEN>
<TOKEN end_char="10342" id="token-97-2" morph="none" pos="word" start_char="10341">of</TOKEN>
<TOKEN end_char="10344" id="token-97-3" morph="none" pos="word" start_char="10344">a</TOKEN>
<TOKEN end_char="10348" id="token-97-4" morph="none" pos="word" start_char="10346">lab</TOKEN>
<TOKEN end_char="10353" id="token-97-5" morph="none" pos="word" start_char="10350">leak</TOKEN>
<TOKEN end_char="10354" id="token-97-6" morph="none" pos="punct" start_char="10354">,</TOKEN>
<TOKEN end_char="10360" id="token-97-7" morph="none" pos="word" start_char="10356">wound</TOKEN>
<TOKEN end_char="10365" id="token-97-8" morph="none" pos="word" start_char="10362">many</TOKEN>
<TOKEN end_char="10369" id="token-97-9" morph="none" pos="word" start_char="10367">lab</TOKEN>
<TOKEN end_char="10377" id="token-97-10" morph="none" pos="word" start_char="10371">workers</TOKEN>
<TOKEN end_char="10380" id="token-97-11" morph="none" pos="word" start_char="10379">be</TOKEN>
<TOKEN end_char="10389" id="token-97-12" morph="none" pos="word" start_char="10382">infected</TOKEN>
<TOKEN end_char="10390" id="token-97-13" morph="none" pos="punct" start_char="10390">?</TOKEN>
</SEG>
<SEG end_char="10469" id="segment-98" start_char="10394">
<ORIGINAL_TEXT>Why doesn’t anyone talk about the CDC closing down the bioweapons lab at Ft.</ORIGINAL_TEXT>
<TOKEN end_char="10396" id="token-98-0" morph="none" pos="word" start_char="10394">Why</TOKEN>
<TOKEN end_char="10404" id="token-98-1" morph="none" pos="word" start_char="10398">doesn’t</TOKEN>
<TOKEN end_char="10411" id="token-98-2" morph="none" pos="word" start_char="10406">anyone</TOKEN>
<TOKEN end_char="10416" id="token-98-3" morph="none" pos="word" start_char="10413">talk</TOKEN>
<TOKEN end_char="10422" id="token-98-4" morph="none" pos="word" start_char="10418">about</TOKEN>
<TOKEN end_char="10426" id="token-98-5" morph="none" pos="word" start_char="10424">the</TOKEN>
<TOKEN end_char="10430" id="token-98-6" morph="none" pos="word" start_char="10428">CDC</TOKEN>
<TOKEN end_char="10438" id="token-98-7" morph="none" pos="word" start_char="10432">closing</TOKEN>
<TOKEN end_char="10443" id="token-98-8" morph="none" pos="word" start_char="10440">down</TOKEN>
<TOKEN end_char="10447" id="token-98-9" morph="none" pos="word" start_char="10445">the</TOKEN>
<TOKEN end_char="10458" id="token-98-10" morph="none" pos="word" start_char="10449">bioweapons</TOKEN>
<TOKEN end_char="10462" id="token-98-11" morph="none" pos="word" start_char="10460">lab</TOKEN>
<TOKEN end_char="10465" id="token-98-12" morph="none" pos="word" start_char="10464">at</TOKEN>
<TOKEN end_char="10468" id="token-98-13" morph="none" pos="word" start_char="10467">Ft</TOKEN>
<TOKEN end_char="10469" id="token-98-14" morph="none" pos="punct" start_char="10469">.</TOKEN>
</SEG>
<SEG end_char="10534" id="segment-99" start_char="10471">
<ORIGINAL_TEXT>Detrick for sloppy practices last August (reopened in November).</ORIGINAL_TEXT>
<TOKEN end_char="10477" id="token-99-0" morph="none" pos="word" start_char="10471">Detrick</TOKEN>
<TOKEN end_char="10481" id="token-99-1" morph="none" pos="word" start_char="10479">for</TOKEN>
<TOKEN end_char="10488" id="token-99-2" morph="none" pos="word" start_char="10483">sloppy</TOKEN>
<TOKEN end_char="10498" id="token-99-3" morph="none" pos="word" start_char="10490">practices</TOKEN>
<TOKEN end_char="10503" id="token-99-4" morph="none" pos="word" start_char="10500">last</TOKEN>
<TOKEN end_char="10510" id="token-99-5" morph="none" pos="word" start_char="10505">August</TOKEN>
<TOKEN end_char="10512" id="token-99-6" morph="none" pos="punct" start_char="10512">(</TOKEN>
<TOKEN end_char="10520" id="token-99-7" morph="none" pos="word" start_char="10513">reopened</TOKEN>
<TOKEN end_char="10523" id="token-99-8" morph="none" pos="word" start_char="10522">in</TOKEN>
<TOKEN end_char="10532" id="token-99-9" morph="none" pos="word" start_char="10525">November</TOKEN>
<TOKEN end_char="10534" id="token-99-10" morph="none" pos="punct" start_char="10533">).</TOKEN>
</SEG>
<SEG end_char="10558" id="segment-100" start_char="10536">
<ORIGINAL_TEXT>Remarkable coincidence.</ORIGINAL_TEXT>
<TOKEN end_char="10545" id="token-100-0" morph="none" pos="word" start_char="10536">Remarkable</TOKEN>
<TOKEN end_char="10557" id="token-100-1" morph="none" pos="word" start_char="10547">coincidence</TOKEN>
<TOKEN end_char="10558" id="token-100-2" morph="none" pos="punct" start_char="10558">.</TOKEN>
</SEG>
<SEG end_char="10767" id="segment-101" start_char="10560">
<ORIGINAL_TEXT>(See https://www.fredericknewspost.com/news/politics_and_government/military/fort-detrick-laboratory-restored-to-full-operations-after-being-shut-down-by-cdc/article_fcee204f-1493-52fb-ba9b-f80cb00da727.html)</ORIGINAL_TEXT>
<TOKEN end_char="10560" id="token-101-0" morph="none" pos="punct" start_char="10560">(</TOKEN>
<TOKEN end_char="10563" id="token-101-1" morph="none" pos="word" start_char="10561">See</TOKEN>
<TOKEN end_char="10767" id="token-101-2" morph="none" pos="url" start_char="10565">https://www.fredericknewspost.com/news/politics_and_government/military/fort-detrick-laboratory-restored-to-full-operations-after-being-shut-down-by-cdc/article_fcee204f-1493-52fb-ba9b-f80cb00da727.html)</TOKEN>
<TRANSLATED_TEXT>(See htps: / / www.fredericknowest.com / news / politics _ and _ government / military / fort-detrick-laboratory-restored-to-full-operations-after-being-shut-down-by-cdc / article _ fcee204f-1493-52fb-ba9b-f80cb00da727.html)</TRANSLATED_TEXT><DETECTED_LANGUAGE>af</DETECTED_LANGUAGE></SEG>
<SEG end_char="10825" id="segment-102" start_char="10771">
<ORIGINAL_TEXT>Lab leakage in secured labs outside China has occurred.</ORIGINAL_TEXT>
<TOKEN end_char="10773" id="token-102-0" morph="none" pos="word" start_char="10771">Lab</TOKEN>
<TOKEN end_char="10781" id="token-102-1" morph="none" pos="word" start_char="10775">leakage</TOKEN>
<TOKEN end_char="10784" id="token-102-2" morph="none" pos="word" start_char="10783">in</TOKEN>
<TOKEN end_char="10792" id="token-102-3" morph="none" pos="word" start_char="10786">secured</TOKEN>
<TOKEN end_char="10797" id="token-102-4" morph="none" pos="word" start_char="10794">labs</TOKEN>
<TOKEN end_char="10805" id="token-102-5" morph="none" pos="word" start_char="10799">outside</TOKEN>
<TOKEN end_char="10811" id="token-102-6" morph="none" pos="word" start_char="10807">China</TOKEN>
<TOKEN end_char="10815" id="token-102-7" morph="none" pos="word" start_char="10813">has</TOKEN>
<TOKEN end_char="10824" id="token-102-8" morph="none" pos="word" start_char="10817">occurred</TOKEN>
<TOKEN end_char="10825" id="token-102-9" morph="none" pos="punct" start_char="10825">.</TOKEN>
</SEG>
<SEG end_char="10866" id="segment-103" start_char="10827">
<ORIGINAL_TEXT>It happened in Detrick and in Singapore.</ORIGINAL_TEXT>
<TOKEN end_char="10828" id="token-103-0" morph="none" pos="word" start_char="10827">It</TOKEN>
<TOKEN end_char="10837" id="token-103-1" morph="none" pos="word" start_char="10830">happened</TOKEN>
<TOKEN end_char="10840" id="token-103-2" morph="none" pos="word" start_char="10839">in</TOKEN>
<TOKEN end_char="10848" id="token-103-3" morph="none" pos="word" start_char="10842">Detrick</TOKEN>
<TOKEN end_char="10852" id="token-103-4" morph="none" pos="word" start_char="10850">and</TOKEN>
<TOKEN end_char="10855" id="token-103-5" morph="none" pos="word" start_char="10854">in</TOKEN>
<TOKEN end_char="10865" id="token-103-6" morph="none" pos="word" start_char="10857">Singapore</TOKEN>
<TOKEN end_char="10866" id="token-103-7" morph="none" pos="punct" start_char="10866">.</TOKEN>
</SEG>
<SEG end_char="10905" id="segment-104" start_char="10870">
<ORIGINAL_TEXT>We humans are our own worst enemies.</ORIGINAL_TEXT>
<TOKEN end_char="10871" id="token-104-0" morph="none" pos="word" start_char="10870">We</TOKEN>
<TOKEN end_char="10878" id="token-104-1" morph="none" pos="word" start_char="10873">humans</TOKEN>
<TOKEN end_char="10882" id="token-104-2" morph="none" pos="word" start_char="10880">are</TOKEN>
<TOKEN end_char="10886" id="token-104-3" morph="none" pos="word" start_char="10884">our</TOKEN>
<TOKEN end_char="10890" id="token-104-4" morph="none" pos="word" start_char="10888">own</TOKEN>
<TOKEN end_char="10896" id="token-104-5" morph="none" pos="word" start_char="10892">worst</TOKEN>
<TOKEN end_char="10904" id="token-104-6" morph="none" pos="word" start_char="10898">enemies</TOKEN>
<TOKEN end_char="10905" id="token-104-7" morph="none" pos="punct" start_char="10905">.</TOKEN>
<TRANSLATED_TEXT>Wij mensen zijn onze eigen slechtste nemingen.</TRANSLATED_TEXT><DETECTED_LANGUAGE>af</DETECTED_LANGUAGE></SEG>
<SEG end_char="11015" id="segment-105" start_char="10907">
<ORIGINAL_TEXT>What the hell are scientists (anywhere) doing fooling around with viruses that have no benefit to human life?</ORIGINAL_TEXT>
<TOKEN end_char="10910" id="token-105-0" morph="none" pos="word" start_char="10907">What</TOKEN>
<TOKEN end_char="10914" id="token-105-1" morph="none" pos="word" start_char="10912">the</TOKEN>
<TOKEN end_char="10919" id="token-105-2" morph="none" pos="word" start_char="10916">hell</TOKEN>
<TOKEN end_char="10923" id="token-105-3" morph="none" pos="word" start_char="10921">are</TOKEN>
<TOKEN end_char="10934" id="token-105-4" morph="none" pos="word" start_char="10925">scientists</TOKEN>
<TOKEN end_char="10936" id="token-105-5" morph="none" pos="punct" start_char="10936">(</TOKEN>
<TOKEN end_char="10944" id="token-105-6" morph="none" pos="word" start_char="10937">anywhere</TOKEN>
<TOKEN end_char="10945" id="token-105-7" morph="none" pos="punct" start_char="10945">)</TOKEN>
<TOKEN end_char="10951" id="token-105-8" morph="none" pos="word" start_char="10947">doing</TOKEN>
<TOKEN end_char="10959" id="token-105-9" morph="none" pos="word" start_char="10953">fooling</TOKEN>
<TOKEN end_char="10966" id="token-105-10" morph="none" pos="word" start_char="10961">around</TOKEN>
<TOKEN end_char="10971" id="token-105-11" morph="none" pos="word" start_char="10968">with</TOKEN>
<TOKEN end_char="10979" id="token-105-12" morph="none" pos="word" start_char="10973">viruses</TOKEN>
<TOKEN end_char="10984" id="token-105-13" morph="none" pos="word" start_char="10981">that</TOKEN>
<TOKEN end_char="10989" id="token-105-14" morph="none" pos="word" start_char="10986">have</TOKEN>
<TOKEN end_char="10992" id="token-105-15" morph="none" pos="word" start_char="10991">no</TOKEN>
<TOKEN end_char="11000" id="token-105-16" morph="none" pos="word" start_char="10994">benefit</TOKEN>
<TOKEN end_char="11003" id="token-105-17" morph="none" pos="word" start_char="11002">to</TOKEN>
<TOKEN end_char="11009" id="token-105-18" morph="none" pos="word" start_char="11005">human</TOKEN>
<TOKEN end_char="11014" id="token-105-19" morph="none" pos="word" start_char="11011">life</TOKEN>
<TOKEN end_char="11015" id="token-105-20" morph="none" pos="punct" start_char="11015">?</TOKEN>
</SEG>
<SEG end_char="11148" id="segment-106" start_char="11017">
<ORIGINAL_TEXT>This virus is more dangerous than Chernobyl…it’s probably going to reappear each year until more sensible scientists find a vaccine.</ORIGINAL_TEXT>
<TOKEN end_char="11020" id="token-106-0" morph="none" pos="word" start_char="11017">This</TOKEN>
<TOKEN end_char="11026" id="token-106-1" morph="none" pos="word" start_char="11022">virus</TOKEN>
<TOKEN end_char="11029" id="token-106-2" morph="none" pos="word" start_char="11028">is</TOKEN>
<TOKEN end_char="11034" id="token-106-3" morph="none" pos="word" start_char="11031">more</TOKEN>
<TOKEN end_char="11044" id="token-106-4" morph="none" pos="word" start_char="11036">dangerous</TOKEN>
<TOKEN end_char="11049" id="token-106-5" morph="none" pos="word" start_char="11046">than</TOKEN>
<TOKEN end_char="11064" id="token-106-6" morph="none" pos="unknown" start_char="11051">Chernobyl…it’s</TOKEN>
<TOKEN end_char="11073" id="token-106-7" morph="none" pos="word" start_char="11066">probably</TOKEN>
<TOKEN end_char="11079" id="token-106-8" morph="none" pos="word" start_char="11075">going</TOKEN>
<TOKEN end_char="11082" id="token-106-9" morph="none" pos="word" start_char="11081">to</TOKEN>
<TOKEN end_char="11091" id="token-106-10" morph="none" pos="word" start_char="11084">reappear</TOKEN>
<TOKEN end_char="11096" id="token-106-11" morph="none" pos="word" start_char="11093">each</TOKEN>
<TOKEN end_char="11101" id="token-106-12" morph="none" pos="word" start_char="11098">year</TOKEN>
<TOKEN end_char="11107" id="token-106-13" morph="none" pos="word" start_char="11103">until</TOKEN>
<TOKEN end_char="11112" id="token-106-14" morph="none" pos="word" start_char="11109">more</TOKEN>
<TOKEN end_char="11121" id="token-106-15" morph="none" pos="word" start_char="11114">sensible</TOKEN>
<TOKEN end_char="11132" id="token-106-16" morph="none" pos="word" start_char="11123">scientists</TOKEN>
<TOKEN end_char="11137" id="token-106-17" morph="none" pos="word" start_char="11134">find</TOKEN>
<TOKEN end_char="11139" id="token-106-18" morph="none" pos="word" start_char="11139">a</TOKEN>
<TOKEN end_char="11147" id="token-106-19" morph="none" pos="word" start_char="11141">vaccine</TOKEN>
<TOKEN end_char="11148" id="token-106-20" morph="none" pos="punct" start_char="11148">.</TOKEN>
</SEG>
<SEG end_char="11489" id="segment-107" start_char="11150">
<ORIGINAL_TEXT>It is one thing to have it be an ‘accident’ it is also another having/allowing ‘children’ to play around with viruses when they are too incompetent to handle such tests securely (accident free)…but when they KNEW it was an issue they (Chinese government) then hoarded all the PPE shipments going out of their country…cause they… Read more »</ORIGINAL_TEXT>
<TOKEN end_char="11151" id="token-107-0" morph="none" pos="word" start_char="11150">It</TOKEN>
<TOKEN end_char="11154" id="token-107-1" morph="none" pos="word" start_char="11153">is</TOKEN>
<TOKEN end_char="11158" id="token-107-2" morph="none" pos="word" start_char="11156">one</TOKEN>
<TOKEN end_char="11164" id="token-107-3" morph="none" pos="word" start_char="11160">thing</TOKEN>
<TOKEN end_char="11167" id="token-107-4" morph="none" pos="word" start_char="11166">to</TOKEN>
<TOKEN end_char="11172" id="token-107-5" morph="none" pos="word" start_char="11169">have</TOKEN>
<TOKEN end_char="11175" id="token-107-6" morph="none" pos="word" start_char="11174">it</TOKEN>
<TOKEN end_char="11178" id="token-107-7" morph="none" pos="word" start_char="11177">be</TOKEN>
<TOKEN end_char="11181" id="token-107-8" morph="none" pos="word" start_char="11180">an</TOKEN>
<TOKEN end_char="11183" id="token-107-9" morph="none" pos="punct" start_char="11183">‘</TOKEN>
<TOKEN end_char="11191" id="token-107-10" morph="none" pos="word" start_char="11184">accident</TOKEN>
<TOKEN end_char="11192" id="token-107-11" morph="none" pos="punct" start_char="11192">’</TOKEN>
<TOKEN end_char="11195" id="token-107-12" morph="none" pos="word" start_char="11194">it</TOKEN>
<TOKEN end_char="11198" id="token-107-13" morph="none" pos="word" start_char="11197">is</TOKEN>
<TOKEN end_char="11203" id="token-107-14" morph="none" pos="word" start_char="11200">also</TOKEN>
<TOKEN end_char="11211" id="token-107-15" morph="none" pos="word" start_char="11205">another</TOKEN>
<TOKEN end_char="11227" id="token-107-16" morph="none" pos="unknown" start_char="11213">having/allowing</TOKEN>
<TOKEN end_char="11229" id="token-107-17" morph="none" pos="punct" start_char="11229">‘</TOKEN>
<TOKEN end_char="11237" id="token-107-18" morph="none" pos="word" start_char="11230">children</TOKEN>
<TOKEN end_char="11238" id="token-107-19" morph="none" pos="punct" start_char="11238">’</TOKEN>
<TOKEN end_char="11241" id="token-107-20" morph="none" pos="word" start_char="11240">to</TOKEN>
<TOKEN end_char="11246" id="token-107-21" morph="none" pos="word" start_char="11243">play</TOKEN>
<TOKEN end_char="11253" id="token-107-22" morph="none" pos="word" start_char="11248">around</TOKEN>
<TOKEN end_char="11258" id="token-107-23" morph="none" pos="word" start_char="11255">with</TOKEN>
<TOKEN end_char="11266" id="token-107-24" morph="none" pos="word" start_char="11260">viruses</TOKEN>
<TOKEN end_char="11271" id="token-107-25" morph="none" pos="word" start_char="11268">when</TOKEN>
<TOKEN end_char="11276" id="token-107-26" morph="none" pos="word" start_char="11273">they</TOKEN>
<TOKEN end_char="11280" id="token-107-27" morph="none" pos="word" start_char="11278">are</TOKEN>
<TOKEN end_char="11284" id="token-107-28" morph="none" pos="word" start_char="11282">too</TOKEN>
<TOKEN end_char="11296" id="token-107-29" morph="none" pos="word" start_char="11286">incompetent</TOKEN>
<TOKEN end_char="11299" id="token-107-30" morph="none" pos="word" start_char="11298">to</TOKEN>
<TOKEN end_char="11306" id="token-107-31" morph="none" pos="word" start_char="11301">handle</TOKEN>
<TOKEN end_char="11311" id="token-107-32" morph="none" pos="word" start_char="11308">such</TOKEN>
<TOKEN end_char="11317" id="token-107-33" morph="none" pos="word" start_char="11313">tests</TOKEN>
<TOKEN end_char="11326" id="token-107-34" morph="none" pos="word" start_char="11319">securely</TOKEN>
<TOKEN end_char="11328" id="token-107-35" morph="none" pos="punct" start_char="11328">(</TOKEN>
<TOKEN end_char="11336" id="token-107-36" morph="none" pos="word" start_char="11329">accident</TOKEN>
<TOKEN end_char="11346" id="token-107-37" morph="none" pos="unknown" start_char="11338">free)…but</TOKEN>
<TOKEN end_char="11351" id="token-107-38" morph="none" pos="word" start_char="11348">when</TOKEN>
<TOKEN end_char="11356" id="token-107-39" morph="none" pos="word" start_char="11353">they</TOKEN>
<TOKEN end_char="11361" id="token-107-40" morph="none" pos="word" start_char="11358">KNEW</TOKEN>
<TOKEN end_char="11364" id="token-107-41" morph="none" pos="word" start_char="11363">it</TOKEN>
<TOKEN end_char="11368" id="token-107-42" morph="none" pos="word" start_char="11366">was</TOKEN>
<TOKEN end_char="11371" id="token-107-43" morph="none" pos="word" start_char="11370">an</TOKEN>
<TOKEN end_char="11377" id="token-107-44" morph="none" pos="word" start_char="11373">issue</TOKEN>
<TOKEN end_char="11382" id="token-107-45" morph="none" pos="word" start_char="11379">they</TOKEN>
<TOKEN end_char="11384" id="token-107-46" morph="none" pos="punct" start_char="11384">(</TOKEN>
<TOKEN end_char="11391" id="token-107-47" morph="none" pos="word" start_char="11385">Chinese</TOKEN>
<TOKEN end_char="11402" id="token-107-48" morph="none" pos="word" start_char="11393">government</TOKEN>
<TOKEN end_char="11403" id="token-107-49" morph="none" pos="punct" start_char="11403">)</TOKEN>
<TOKEN end_char="11408" id="token-107-50" morph="none" pos="word" start_char="11405">then</TOKEN>
<TOKEN end_char="11416" id="token-107-51" morph="none" pos="word" start_char="11410">hoarded</TOKEN>
<TOKEN end_char="11420" id="token-107-52" morph="none" pos="word" start_char="11418">all</TOKEN>
<TOKEN end_char="11424" id="token-107-53" morph="none" pos="word" start_char="11422">the</TOKEN>
<TOKEN end_char="11428" id="token-107-54" morph="none" pos="word" start_char="11426">PPE</TOKEN>
<TOKEN end_char="11438" id="token-107-55" morph="none" pos="word" start_char="11430">shipments</TOKEN>
<TOKEN end_char="11444" id="token-107-56" morph="none" pos="word" start_char="11440">going</TOKEN>
<TOKEN end_char="11448" id="token-107-57" morph="none" pos="word" start_char="11446">out</TOKEN>
<TOKEN end_char="11451" id="token-107-58" morph="none" pos="word" start_char="11450">of</TOKEN>
<TOKEN end_char="11457" id="token-107-59" morph="none" pos="word" start_char="11453">their</TOKEN>
<TOKEN end_char="11471" id="token-107-60" morph="none" pos="unknown" start_char="11459">country…cause</TOKEN>
<TOKEN end_char="11476" id="token-107-61" morph="none" pos="word" start_char="11473">they</TOKEN>
<TOKEN end_char="11477" id="token-107-62" morph="none" pos="punct" start_char="11477">…</TOKEN>
<TOKEN end_char="11482" id="token-107-63" morph="none" pos="word" start_char="11479">Read</TOKEN>
<TOKEN end_char="11487" id="token-107-64" morph="none" pos="word" start_char="11484">more</TOKEN>
<TOKEN end_char="11489" id="token-107-65" morph="none" pos="punct" start_char="11489">»</TOKEN>
</SEG>
<SEG end_char="11560" id="segment-108" start_char="11493">
<ORIGINAL_TEXT>Wuhan was known to collect bats and study their SARS-family viruses.</ORIGINAL_TEXT>
<TOKEN end_char="11497" id="token-108-0" morph="none" pos="word" start_char="11493">Wuhan</TOKEN>
<TOKEN end_char="11501" id="token-108-1" morph="none" pos="word" start_char="11499">was</TOKEN>
<TOKEN end_char="11507" id="token-108-2" morph="none" pos="word" start_char="11503">known</TOKEN>
<TOKEN end_char="11510" id="token-108-3" morph="none" pos="word" start_char="11509">to</TOKEN>
<TOKEN end_char="11518" id="token-108-4" morph="none" pos="word" start_char="11512">collect</TOKEN>
<TOKEN end_char="11523" id="token-108-5" morph="none" pos="word" start_char="11520">bats</TOKEN>
<TOKEN end_char="11527" id="token-108-6" morph="none" pos="word" start_char="11525">and</TOKEN>
<TOKEN end_char="11533" id="token-108-7" morph="none" pos="word" start_char="11529">study</TOKEN>
<TOKEN end_char="11539" id="token-108-8" morph="none" pos="word" start_char="11535">their</TOKEN>
<TOKEN end_char="11551" id="token-108-9" morph="none" pos="unknown" start_char="11541">SARS-family</TOKEN>
<TOKEN end_char="11559" id="token-108-10" morph="none" pos="word" start_char="11553">viruses</TOKEN>
<TOKEN end_char="11560" id="token-108-11" morph="none" pos="punct" start_char="11560">.</TOKEN>
</SEG>
<SEG end_char="11689" id="segment-109" start_char="11562">
<ORIGINAL_TEXT>A virus the result of "serial passage" animal breeding would have no trace of genetic modification because it wouldn’t have any.</ORIGINAL_TEXT>
<TOKEN end_char="11562" id="token-109-0" morph="none" pos="word" start_char="11562">A</TOKEN>
<TOKEN end_char="11568" id="token-109-1" morph="none" pos="word" start_char="11564">virus</TOKEN>
<TOKEN end_char="11572" id="token-109-2" morph="none" pos="word" start_char="11570">the</TOKEN>
<TOKEN end_char="11579" id="token-109-3" morph="none" pos="word" start_char="11574">result</TOKEN>
<TOKEN end_char="11582" id="token-109-4" morph="none" pos="word" start_char="11581">of</TOKEN>
<TOKEN end_char="11584" id="token-109-5" morph="none" pos="punct" start_char="11584">"</TOKEN>
<TOKEN end_char="11590" id="token-109-6" morph="none" pos="word" start_char="11585">serial</TOKEN>
<TOKEN end_char="11598" id="token-109-7" morph="none" pos="word" start_char="11592">passage</TOKEN>
<TOKEN end_char="11599" id="token-109-8" morph="none" pos="punct" start_char="11599">"</TOKEN>
<TOKEN end_char="11606" id="token-109-9" morph="none" pos="word" start_char="11601">animal</TOKEN>
<TOKEN end_char="11615" id="token-109-10" morph="none" pos="word" start_char="11608">breeding</TOKEN>
<TOKEN end_char="11621" id="token-109-11" morph="none" pos="word" start_char="11617">would</TOKEN>
<TOKEN end_char="11626" id="token-109-12" morph="none" pos="word" start_char="11623">have</TOKEN>
<TOKEN end_char="11629" id="token-109-13" morph="none" pos="word" start_char="11628">no</TOKEN>
<TOKEN end_char="11635" id="token-109-14" morph="none" pos="word" start_char="11631">trace</TOKEN>
<TOKEN end_char="11638" id="token-109-15" morph="none" pos="word" start_char="11637">of</TOKEN>
<TOKEN end_char="11646" id="token-109-16" morph="none" pos="word" start_char="11640">genetic</TOKEN>
<TOKEN end_char="11659" id="token-109-17" morph="none" pos="word" start_char="11648">modification</TOKEN>
<TOKEN end_char="11667" id="token-109-18" morph="none" pos="word" start_char="11661">because</TOKEN>
<TOKEN end_char="11670" id="token-109-19" morph="none" pos="word" start_char="11669">it</TOKEN>
<TOKEN end_char="11679" id="token-109-20" morph="none" pos="word" start_char="11672">wouldn’t</TOKEN>
<TOKEN end_char="11684" id="token-109-21" morph="none" pos="word" start_char="11681">have</TOKEN>
<TOKEN end_char="11688" id="token-109-22" morph="none" pos="word" start_char="11686">any</TOKEN>
<TOKEN end_char="11689" id="token-109-23" morph="none" pos="punct" start_char="11689">.</TOKEN>
</SEG>
<SEG end_char="11822" id="segment-110" start_char="11691">
<ORIGINAL_TEXT>The most damning evidence in favor of lab origin is, ironically, China’s draconian "sanitizing" of the lab after the outbreak began.</ORIGINAL_TEXT>
<TOKEN end_char="11693" id="token-110-0" morph="none" pos="word" start_char="11691">The</TOKEN>
<TOKEN end_char="11698" id="token-110-1" morph="none" pos="word" start_char="11695">most</TOKEN>
<TOKEN end_char="11706" id="token-110-2" morph="none" pos="word" start_char="11700">damning</TOKEN>
<TOKEN end_char="11715" id="token-110-3" morph="none" pos="word" start_char="11708">evidence</TOKEN>
<TOKEN end_char="11718" id="token-110-4" morph="none" pos="word" start_char="11717">in</TOKEN>
<TOKEN end_char="11724" id="token-110-5" morph="none" pos="word" start_char="11720">favor</TOKEN>
<TOKEN end_char="11727" id="token-110-6" morph="none" pos="word" start_char="11726">of</TOKEN>
<TOKEN end_char="11731" id="token-110-7" morph="none" pos="word" start_char="11729">lab</TOKEN>
<TOKEN end_char="11738" id="token-110-8" morph="none" pos="word" start_char="11733">origin</TOKEN>
<TOKEN end_char="11741" id="token-110-9" morph="none" pos="word" start_char="11740">is</TOKEN>
<TOKEN end_char="11742" id="token-110-10" morph="none" pos="punct" start_char="11742">,</TOKEN>
<TOKEN end_char="11753" id="token-110-11" morph="none" pos="word" start_char="11744">ironically</TOKEN>
<TOKEN end_char="11754" id="token-110-12" morph="none" pos="punct" start_char="11754">,</TOKEN>
<TOKEN end_char="11762" id="token-110-13" morph="none" pos="word" start_char="11756">China’s</TOKEN>
<TOKEN end_char="11772" id="token-110-14" morph="none" pos="word" start_char="11764">draconian</TOKEN>
<TOKEN end_char="11774" id="token-110-15" morph="none" pos="punct" start_char="11774">"</TOKEN>
<TOKEN end_char="11784" id="token-110-16" morph="none" pos="word" start_char="11775">sanitizing</TOKEN>
<TOKEN end_char="11785" id="token-110-17" morph="none" pos="punct" start_char="11785">"</TOKEN>
<TOKEN end_char="11788" id="token-110-18" morph="none" pos="word" start_char="11787">of</TOKEN>
<TOKEN end_char="11792" id="token-110-19" morph="none" pos="word" start_char="11790">the</TOKEN>
<TOKEN end_char="11796" id="token-110-20" morph="none" pos="word" start_char="11794">lab</TOKEN>
<TOKEN end_char="11802" id="token-110-21" morph="none" pos="word" start_char="11798">after</TOKEN>
<TOKEN end_char="11806" id="token-110-22" morph="none" pos="word" start_char="11804">the</TOKEN>
<TOKEN end_char="11815" id="token-110-23" morph="none" pos="word" start_char="11808">outbreak</TOKEN>
<TOKEN end_char="11821" id="token-110-24" morph="none" pos="word" start_char="11817">began</TOKEN>
<TOKEN end_char="11822" id="token-110-25" morph="none" pos="punct" start_char="11822">.</TOKEN>
</SEG>
<SEG end_char="11898" id="segment-111" start_char="11824">
<ORIGINAL_TEXT>Circumstantial evidence points to the Wuhan lab, no gene sequencing needed.</ORIGINAL_TEXT>
<TOKEN end_char="11837" id="token-111-0" morph="none" pos="word" start_char="11824">Circumstantial</TOKEN>
<TOKEN end_char="11846" id="token-111-1" morph="none" pos="word" start_char="11839">evidence</TOKEN>
<TOKEN end_char="11853" id="token-111-2" morph="none" pos="word" start_char="11848">points</TOKEN>
<TOKEN end_char="11856" id="token-111-3" morph="none" pos="word" start_char="11855">to</TOKEN>
<TOKEN end_char="11860" id="token-111-4" morph="none" pos="word" start_char="11858">the</TOKEN>
<TOKEN end_char="11866" id="token-111-5" morph="none" pos="word" start_char="11862">Wuhan</TOKEN>
<TOKEN end_char="11870" id="token-111-6" morph="none" pos="word" start_char="11868">lab</TOKEN>
<TOKEN end_char="11871" id="token-111-7" morph="none" pos="punct" start_char="11871">,</TOKEN>
<TOKEN end_char="11874" id="token-111-8" morph="none" pos="word" start_char="11873">no</TOKEN>
<TOKEN end_char="11879" id="token-111-9" morph="none" pos="word" start_char="11876">gene</TOKEN>
<TOKEN end_char="11890" id="token-111-10" morph="none" pos="word" start_char="11881">sequencing</TOKEN>
<TOKEN end_char="11897" id="token-111-11" morph="none" pos="word" start_char="11892">needed</TOKEN>
<TOKEN end_char="11898" id="token-111-12" morph="none" pos="punct" start_char="11898">.</TOKEN>
</SEG>
<SEG end_char="12042" id="segment-112" start_char="11900">
<ORIGINAL_TEXT>I’m still looking for proof they were doing GoF research, preferably with a (hypothetical) SARS-CoV-2 ancestor, selecting for transmissibility.</ORIGINAL_TEXT>
<TOKEN end_char="11902" id="token-112-0" morph="none" pos="word" start_char="11900">I’m</TOKEN>
<TOKEN end_char="11908" id="token-112-1" morph="none" pos="word" start_char="11904">still</TOKEN>
<TOKEN end_char="11916" id="token-112-2" morph="none" pos="word" start_char="11910">looking</TOKEN>
<TOKEN end_char="11920" id="token-112-3" morph="none" pos="word" start_char="11918">for</TOKEN>
<TOKEN end_char="11926" id="token-112-4" morph="none" pos="word" start_char="11922">proof</TOKEN>
<TOKEN end_char="11931" id="token-112-5" morph="none" pos="word" start_char="11928">they</TOKEN>
<TOKEN end_char="11936" id="token-112-6" morph="none" pos="word" start_char="11933">were</TOKEN>
<TOKEN end_char="11942" id="token-112-7" morph="none" pos="word" start_char="11938">doing</TOKEN>
<TOKEN end_char="11946" id="token-112-8" morph="none" pos="word" start_char="11944">GoF</TOKEN>
<TOKEN end_char="11955" id="token-112-9" morph="none" pos="word" start_char="11948">research</TOKEN>
<TOKEN end_char="11956" id="token-112-10" morph="none" pos="punct" start_char="11956">,</TOKEN>
<TOKEN end_char="11967" id="token-112-11" morph="none" pos="word" start_char="11958">preferably</TOKEN>
<TOKEN end_char="11972" id="token-112-12" morph="none" pos="word" start_char="11969">with</TOKEN>
<TOKEN end_char="11974" id="token-112-13" morph="none" pos="word" start_char="11974">a</TOKEN>
<TOKEN end_char="11976" id="token-112-14" morph="none" pos="punct" start_char="11976">(</TOKEN>
<TOKEN end_char="11988" id="token-112-15" morph="none" pos="word" start_char="11977">hypothetical</TOKEN>
<TOKEN end_char="11989" id="token-112-16" morph="none" pos="punct" start_char="11989">)</TOKEN>
<TOKEN end_char="12000" id="token-112-17" morph="none" pos="unknown" start_char="11991">SARS-CoV-2</TOKEN>
<TOKEN end_char="12009" id="token-112-18" morph="none" pos="word" start_char="12002">ancestor</TOKEN>
<TOKEN end_char="12010" id="token-112-19" morph="none" pos="punct" start_char="12010">,</TOKEN>
<TOKEN end_char="12020" id="token-112-20" morph="none" pos="word" start_char="12012">selecting</TOKEN>
<TOKEN end_char="12024" id="token-112-21" morph="none" pos="word" start_char="12022">for</TOKEN>
<TOKEN end_char="12041" id="token-112-22" morph="none" pos="word" start_char="12026">transmissibility</TOKEN>
<TOKEN end_char="12042" id="token-112-23" morph="none" pos="punct" start_char="12042">.</TOKEN>
</SEG>
<SEG end_char="12122" id="segment-113" start_char="12044">
<ORIGINAL_TEXT>They could destroy evidence in the lab; published acadmeic papers, not so easy.</ORIGINAL_TEXT>
<TOKEN end_char="12047" id="token-113-0" morph="none" pos="word" start_char="12044">They</TOKEN>
<TOKEN end_char="12053" id="token-113-1" morph="none" pos="word" start_char="12049">could</TOKEN>
<TOKEN end_char="12061" id="token-113-2" morph="none" pos="word" start_char="12055">destroy</TOKEN>
<TOKEN end_char="12070" id="token-113-3" morph="none" pos="word" start_char="12063">evidence</TOKEN>
<TOKEN end_char="12073" id="token-113-4" morph="none" pos="word" start_char="12072">in</TOKEN>
<TOKEN end_char="12077" id="token-113-5" morph="none" pos="word" start_char="12075">the</TOKEN>
<TOKEN end_char="12081" id="token-113-6" morph="none" pos="word" start_char="12079">lab</TOKEN>
<TOKEN end_char="12082" id="token-113-7" morph="none" pos="punct" start_char="12082">;</TOKEN>
<TOKEN end_char="12092" id="token-113-8" morph="none" pos="word" start_char="12084">published</TOKEN>
<TOKEN end_char="12101" id="token-113-9" morph="none" pos="word" start_char="12094">acadmeic</TOKEN>
<TOKEN end_char="12108" id="token-113-10" morph="none" pos="word" start_char="12103">papers</TOKEN>
<TOKEN end_char="12109" id="token-113-11" morph="none" pos="punct" start_char="12109">,</TOKEN>
<TOKEN end_char="12113" id="token-113-12" morph="none" pos="word" start_char="12111">not</TOKEN>
<TOKEN end_char="12116" id="token-113-13" morph="none" pos="word" start_char="12115">so</TOKEN>
<TOKEN end_char="12121" id="token-113-14" morph="none" pos="word" start_char="12118">easy</TOKEN>
<TOKEN end_char="12122" id="token-113-15" morph="none" pos="punct" start_char="12122">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>