<LCTL_TEXT lang="ukr">
<DOC grammar="none" id="L0C049PKB" lang="ukr" raw_text_char_length="313" raw_text_md5="18eed8c6389905024608eb71fee91668" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="43" id="segment-0" start_char="1">
<ORIGINAL_TEXT>All the Ways to Kill a Coronavirus (So Far)</ORIGINAL_TEXT>
<TOKEN end_char="3" id="token-0-0" morph="none" pos="word" start_char="1">All</TOKEN>
<TOKEN end_char="7" id="token-0-1" morph="none" pos="word" start_char="5">the</TOKEN>
<TOKEN end_char="12" id="token-0-2" morph="none" pos="word" start_char="9">Ways</TOKEN>
<TOKEN end_char="15" id="token-0-3" morph="none" pos="word" start_char="14">to</TOKEN>
<TOKEN end_char="20" id="token-0-4" morph="none" pos="word" start_char="17">Kill</TOKEN>
<TOKEN end_char="22" id="token-0-5" morph="none" pos="word" start_char="22">a</TOKEN>
<TOKEN end_char="34" id="token-0-6" morph="none" pos="word" start_char="24">Coronavirus</TOKEN>
<TOKEN end_char="36" id="token-0-7" morph="none" pos="punct" start_char="36">(</TOKEN>
<TOKEN end_char="38" id="token-0-8" morph="none" pos="word" start_char="37">So</TOKEN>
<TOKEN end_char="42" id="token-0-9" morph="none" pos="word" start_char="40">Far</TOKEN>
<TOKEN end_char="43" id="token-0-10" morph="none" pos="punct" start_char="43">)</TOKEN>
</SEG>
<SEG end_char="59" id="segment-1" start_char="47">
<ORIGINAL_TEXT>Sara Harrison</ORIGINAL_TEXT>
<TOKEN end_char="50" id="token-1-0" morph="none" pos="word" start_char="47">Sara</TOKEN>
<TOKEN end_char="59" id="token-1-1" morph="none" pos="word" start_char="52">Harrison</TOKEN>
<TRANSLATED_TEXT>Sara Harrison.</TRANSLATED_TEXT><DETECTED_LANGUAGE>sv</DETECTED_LANGUAGE></SEG>
<SEG end_char="68" id="segment-2" start_char="62">
<ORIGINAL_TEXT>Science</ORIGINAL_TEXT>
<TOKEN end_char="68" id="token-2-0" morph="none" pos="word" start_char="62">Science</TOKEN>
<TRANSLATED_TEXT>Wetenschap</TRANSLATED_TEXT><DETECTED_LANGUAGE>es</DETECTED_LANGUAGE></SEG>
<SEG end_char="89" id="segment-3" start_char="71">
<ORIGINAL_TEXT>04.23.2020 09:00 AM</ORIGINAL_TEXT>
<TOKEN end_char="80" id="token-3-0" morph="none" pos="unknown" start_char="71">04.23.2020</TOKEN>
<TOKEN end_char="86" id="token-3-1" morph="none" pos="unknown" start_char="82">09:00</TOKEN>
<TOKEN end_char="89" id="token-3-2" morph="none" pos="word" start_char="88">AM</TOKEN>
<TRANSLATED_TEXT>04.23.2020 09: 00 PM</TRANSLATED_TEXT><DETECTED_LANGUAGE>tl</DETECTED_LANGUAGE></SEG>
<SEG end_char="134" id="segment-4" start_char="92">
<ORIGINAL_TEXT>All the Ways to Kill a Coronavirus (So Far)</ORIGINAL_TEXT>
<TOKEN end_char="94" id="token-4-0" morph="none" pos="word" start_char="92">All</TOKEN>
<TOKEN end_char="98" id="token-4-1" morph="none" pos="word" start_char="96">the</TOKEN>
<TOKEN end_char="103" id="token-4-2" morph="none" pos="word" start_char="100">Ways</TOKEN>
<TOKEN end_char="106" id="token-4-3" morph="none" pos="word" start_char="105">to</TOKEN>
<TOKEN end_char="111" id="token-4-4" morph="none" pos="word" start_char="108">Kill</TOKEN>
<TOKEN end_char="113" id="token-4-5" morph="none" pos="word" start_char="113">a</TOKEN>
<TOKEN end_char="125" id="token-4-6" morph="none" pos="word" start_char="115">Coronavirus</TOKEN>
<TOKEN end_char="127" id="token-4-7" morph="none" pos="punct" start_char="127">(</TOKEN>
<TOKEN end_char="129" id="token-4-8" morph="none" pos="word" start_char="128">So</TOKEN>
<TOKEN end_char="133" id="token-4-9" morph="none" pos="word" start_char="131">Far</TOKEN>
<TOKEN end_char="134" id="token-4-10" morph="none" pos="punct" start_char="134">)</TOKEN>
</SEG>
<SEG end_char="200" id="segment-5" start_char="138">
<ORIGINAL_TEXT>Even a bug this ruthless has a few fatal weaknesses of its own.</ORIGINAL_TEXT>
<TOKEN end_char="141" id="token-5-0" morph="none" pos="word" start_char="138">Even</TOKEN>
<TOKEN end_char="143" id="token-5-1" morph="none" pos="word" start_char="143">a</TOKEN>
<TOKEN end_char="147" id="token-5-2" morph="none" pos="word" start_char="145">bug</TOKEN>
<TOKEN end_char="152" id="token-5-3" morph="none" pos="word" start_char="149">this</TOKEN>
<TOKEN end_char="161" id="token-5-4" morph="none" pos="word" start_char="154">ruthless</TOKEN>
<TOKEN end_char="165" id="token-5-5" morph="none" pos="word" start_char="163">has</TOKEN>
<TOKEN end_char="167" id="token-5-6" morph="none" pos="word" start_char="167">a</TOKEN>
<TOKEN end_char="171" id="token-5-7" morph="none" pos="word" start_char="169">few</TOKEN>
<TOKEN end_char="177" id="token-5-8" morph="none" pos="word" start_char="173">fatal</TOKEN>
<TOKEN end_char="188" id="token-5-9" morph="none" pos="word" start_char="179">weaknesses</TOKEN>
<TOKEN end_char="191" id="token-5-10" morph="none" pos="word" start_char="190">of</TOKEN>
<TOKEN end_char="195" id="token-5-11" morph="none" pos="word" start_char="193">its</TOKEN>
<TOKEN end_char="199" id="token-5-12" morph="none" pos="word" start_char="197">own</TOKEN>
<TOKEN end_char="200" id="token-5-13" morph="none" pos="punct" start_char="200">.</TOKEN>
</SEG>
<SEG end_char="228" id="segment-6" start_char="203">
<ORIGINAL_TEXT>Save this story for later.</ORIGINAL_TEXT>
<TOKEN end_char="206" id="token-6-0" morph="none" pos="word" start_char="203">Save</TOKEN>
<TOKEN end_char="211" id="token-6-1" morph="none" pos="word" start_char="208">this</TOKEN>
<TOKEN end_char="217" id="token-6-2" morph="none" pos="word" start_char="213">story</TOKEN>
<TOKEN end_char="221" id="token-6-3" morph="none" pos="word" start_char="219">for</TOKEN>
<TOKEN end_char="227" id="token-6-4" morph="none" pos="word" start_char="223">later</TOKEN>
<TOKEN end_char="228" id="token-6-5" morph="none" pos="punct" start_char="228">.</TOKEN>
</SEG>
<SEG end_char="309" id="segment-7" start_char="232">
<ORIGINAL_TEXT>pattern":"Caption"}" data-include-experiments="true"&gt;ILLUSTRATION: SIMOUL ALVA</ORIGINAL_TEXT>
<TOKEN end_char="248" id="token-7-0" morph="none" pos="unknown" start_char="232">pattern":"Caption</TOKEN>
<TOKEN end_char="251" id="token-7-1" morph="none" pos="punct" start_char="249">"}"</TOKEN>
<TOKEN end_char="296" id="token-7-2" morph="none" pos="unknown" start_char="253">data-include-experiments="true"&gt;ILLUSTRATION</TOKEN>
<TOKEN end_char="297" id="token-7-3" morph="none" pos="punct" start_char="297">:</TOKEN>
<TOKEN end_char="304" id="token-7-4" morph="none" pos="word" start_char="299">SIMOUL</TOKEN>
<TOKEN end_char="309" id="token-7-5" morph="none" pos="word" start_char="306">ALVA</TOKEN>
<TRANSLATED_TEXT>pattern ":" Caption "}" data-include-experiments = "true" &gt; ILLUSTRATION: SIMOUL ALVA</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
</TEXT>
</DOC>
</LCTL_TEXT>