<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C049P3R" lang="eng" raw_text_char_length="18846" raw_text_md5="866997e1b450525fad01b57a8a1b6fdd" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="37" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Why isn't Asia susceptible to Covid ?</ORIGINAL_TEXT>
<TOKEN end_char="3" id="token-0-0" morph="none" pos="word" start_char="1">Why</TOKEN>
<TOKEN end_char="9" id="token-0-1" morph="none" pos="word" start_char="5">isn't</TOKEN>
<TOKEN end_char="14" id="token-0-2" morph="none" pos="word" start_char="11">Asia</TOKEN>
<TOKEN end_char="26" id="token-0-3" morph="none" pos="word" start_char="16">susceptible</TOKEN>
<TOKEN end_char="29" id="token-0-4" morph="none" pos="word" start_char="28">to</TOKEN>
<TOKEN end_char="35" id="token-0-5" morph="none" pos="word" start_char="31">Covid</TOKEN>
<TOKEN end_char="37" id="token-0-6" morph="none" pos="punct" start_char="37">?</TOKEN>
</SEG>
<SEG end_char="168" id="segment-1" start_char="41">
<ORIGINAL_TEXT>Looking at Worldometer when switched to show Asia and, with a couple exceptions, the presence of Covid outbreaks is pretty tiny.</ORIGINAL_TEXT>
<TOKEN end_char="47" id="token-1-0" morph="none" pos="word" start_char="41">Looking</TOKEN>
<TOKEN end_char="50" id="token-1-1" morph="none" pos="word" start_char="49">at</TOKEN>
<TOKEN end_char="62" id="token-1-2" morph="none" pos="word" start_char="52">Worldometer</TOKEN>
<TOKEN end_char="67" id="token-1-3" morph="none" pos="word" start_char="64">when</TOKEN>
<TOKEN end_char="76" id="token-1-4" morph="none" pos="word" start_char="69">switched</TOKEN>
<TOKEN end_char="79" id="token-1-5" morph="none" pos="word" start_char="78">to</TOKEN>
<TOKEN end_char="84" id="token-1-6" morph="none" pos="word" start_char="81">show</TOKEN>
<TOKEN end_char="89" id="token-1-7" morph="none" pos="word" start_char="86">Asia</TOKEN>
<TOKEN end_char="93" id="token-1-8" morph="none" pos="word" start_char="91">and</TOKEN>
<TOKEN end_char="94" id="token-1-9" morph="none" pos="punct" start_char="94">,</TOKEN>
<TOKEN end_char="99" id="token-1-10" morph="none" pos="word" start_char="96">with</TOKEN>
<TOKEN end_char="101" id="token-1-11" morph="none" pos="word" start_char="101">a</TOKEN>
<TOKEN end_char="108" id="token-1-12" morph="none" pos="word" start_char="103">couple</TOKEN>
<TOKEN end_char="119" id="token-1-13" morph="none" pos="word" start_char="110">exceptions</TOKEN>
<TOKEN end_char="120" id="token-1-14" morph="none" pos="punct" start_char="120">,</TOKEN>
<TOKEN end_char="124" id="token-1-15" morph="none" pos="word" start_char="122">the</TOKEN>
<TOKEN end_char="133" id="token-1-16" morph="none" pos="word" start_char="126">presence</TOKEN>
<TOKEN end_char="136" id="token-1-17" morph="none" pos="word" start_char="135">of</TOKEN>
<TOKEN end_char="142" id="token-1-18" morph="none" pos="word" start_char="138">Covid</TOKEN>
<TOKEN end_char="152" id="token-1-19" morph="none" pos="word" start_char="144">outbreaks</TOKEN>
<TOKEN end_char="155" id="token-1-20" morph="none" pos="word" start_char="154">is</TOKEN>
<TOKEN end_char="162" id="token-1-21" morph="none" pos="word" start_char="157">pretty</TOKEN>
<TOKEN end_char="167" id="token-1-22" morph="none" pos="word" start_char="164">tiny</TOKEN>
<TOKEN end_char="168" id="token-1-23" morph="none" pos="punct" start_char="168">.</TOKEN>
</SEG>
<SEG end_char="278" id="segment-2" start_char="171">
<ORIGINAL_TEXT>Coronavirus Update (Live): 124,390,921 Cases and 2,737,714 Deaths from COVID-19 Virus Pandemic - Worldometer</ORIGINAL_TEXT>
<TOKEN end_char="181" id="token-2-0" morph="none" pos="word" start_char="171">Coronavirus</TOKEN>
<TOKEN end_char="188" id="token-2-1" morph="none" pos="word" start_char="183">Update</TOKEN>
<TOKEN end_char="190" id="token-2-2" morph="none" pos="punct" start_char="190">(</TOKEN>
<TOKEN end_char="194" id="token-2-3" morph="none" pos="word" start_char="191">Live</TOKEN>
<TOKEN end_char="196" id="token-2-4" morph="none" pos="punct" start_char="195">):</TOKEN>
<TOKEN end_char="208" id="token-2-5" morph="none" pos="unknown" start_char="198">124,390,921</TOKEN>
<TOKEN end_char="214" id="token-2-6" morph="none" pos="word" start_char="210">Cases</TOKEN>
<TOKEN end_char="218" id="token-2-7" morph="none" pos="word" start_char="216">and</TOKEN>
<TOKEN end_char="228" id="token-2-8" morph="none" pos="unknown" start_char="220">2,737,714</TOKEN>
<TOKEN end_char="235" id="token-2-9" morph="none" pos="word" start_char="230">Deaths</TOKEN>
<TOKEN end_char="240" id="token-2-10" morph="none" pos="word" start_char="237">from</TOKEN>
<TOKEN end_char="249" id="token-2-11" morph="none" pos="unknown" start_char="242">COVID-19</TOKEN>
<TOKEN end_char="255" id="token-2-12" morph="none" pos="word" start_char="251">Virus</TOKEN>
<TOKEN end_char="264" id="token-2-13" morph="none" pos="word" start_char="257">Pandemic</TOKEN>
<TOKEN end_char="266" id="token-2-14" morph="none" pos="punct" start_char="266">-</TOKEN>
<TOKEN end_char="278" id="token-2-15" morph="none" pos="word" start_char="268">Worldometer</TOKEN>
</SEG>
<SEG end_char="450" id="segment-3" start_char="282">
<ORIGINAL_TEXT>Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China.</ORIGINAL_TEXT>
<TOKEN end_char="285" id="token-3-0" morph="none" pos="word" start_char="282">Live</TOKEN>
<TOKEN end_char="296" id="token-3-1" morph="none" pos="word" start_char="287">statistics</TOKEN>
<TOKEN end_char="300" id="token-3-2" morph="none" pos="word" start_char="298">and</TOKEN>
<TOKEN end_char="312" id="token-3-3" morph="none" pos="word" start_char="302">coronavirus</TOKEN>
<TOKEN end_char="317" id="token-3-4" morph="none" pos="word" start_char="314">news</TOKEN>
<TOKEN end_char="326" id="token-3-5" morph="none" pos="word" start_char="319">tracking</TOKEN>
<TOKEN end_char="330" id="token-3-6" morph="none" pos="word" start_char="328">the</TOKEN>
<TOKEN end_char="337" id="token-3-7" morph="none" pos="word" start_char="332">number</TOKEN>
<TOKEN end_char="340" id="token-3-8" morph="none" pos="word" start_char="339">of</TOKEN>
<TOKEN end_char="350" id="token-3-9" morph="none" pos="word" start_char="342">confirmed</TOKEN>
<TOKEN end_char="356" id="token-3-10" morph="none" pos="word" start_char="352">cases</TOKEN>
<TOKEN end_char="357" id="token-3-11" morph="none" pos="punct" start_char="357">,</TOKEN>
<TOKEN end_char="367" id="token-3-12" morph="none" pos="word" start_char="359">recovered</TOKEN>
<TOKEN end_char="376" id="token-3-13" morph="none" pos="word" start_char="369">patients</TOKEN>
<TOKEN end_char="377" id="token-3-14" morph="none" pos="punct" start_char="377">,</TOKEN>
<TOKEN end_char="383" id="token-3-15" morph="none" pos="word" start_char="379">tests</TOKEN>
<TOKEN end_char="384" id="token-3-16" morph="none" pos="punct" start_char="384">,</TOKEN>
<TOKEN end_char="388" id="token-3-17" morph="none" pos="word" start_char="386">and</TOKEN>
<TOKEN end_char="394" id="token-3-18" morph="none" pos="word" start_char="390">death</TOKEN>
<TOKEN end_char="399" id="token-3-19" morph="none" pos="word" start_char="396">toll</TOKEN>
<TOKEN end_char="403" id="token-3-20" morph="none" pos="word" start_char="401">due</TOKEN>
<TOKEN end_char="406" id="token-3-21" morph="none" pos="word" start_char="405">to</TOKEN>
<TOKEN end_char="410" id="token-3-22" morph="none" pos="word" start_char="408">the</TOKEN>
<TOKEN end_char="419" id="token-3-23" morph="none" pos="unknown" start_char="412">COVID-19</TOKEN>
<TOKEN end_char="431" id="token-3-24" morph="none" pos="word" start_char="421">coronavirus</TOKEN>
<TOKEN end_char="436" id="token-3-25" morph="none" pos="word" start_char="433">from</TOKEN>
<TOKEN end_char="442" id="token-3-26" morph="none" pos="word" start_char="438">Wuhan</TOKEN>
<TOKEN end_char="443" id="token-3-27" morph="none" pos="punct" start_char="443">,</TOKEN>
<TOKEN end_char="449" id="token-3-28" morph="none" pos="word" start_char="445">China</TOKEN>
<TOKEN end_char="450" id="token-3-29" morph="none" pos="punct" start_char="450">.</TOKEN>
</SEG>
<SEG end_char="540" id="segment-4" start_char="452">
<ORIGINAL_TEXT>Coronavirus counter with new cases, deaths, and number of tests per 1 Million population.</ORIGINAL_TEXT>
<TOKEN end_char="462" id="token-4-0" morph="none" pos="word" start_char="452">Coronavirus</TOKEN>
<TOKEN end_char="470" id="token-4-1" morph="none" pos="word" start_char="464">counter</TOKEN>
<TOKEN end_char="475" id="token-4-2" morph="none" pos="word" start_char="472">with</TOKEN>
<TOKEN end_char="479" id="token-4-3" morph="none" pos="word" start_char="477">new</TOKEN>
<TOKEN end_char="485" id="token-4-4" morph="none" pos="word" start_char="481">cases</TOKEN>
<TOKEN end_char="486" id="token-4-5" morph="none" pos="punct" start_char="486">,</TOKEN>
<TOKEN end_char="493" id="token-4-6" morph="none" pos="word" start_char="488">deaths</TOKEN>
<TOKEN end_char="494" id="token-4-7" morph="none" pos="punct" start_char="494">,</TOKEN>
<TOKEN end_char="498" id="token-4-8" morph="none" pos="word" start_char="496">and</TOKEN>
<TOKEN end_char="505" id="token-4-9" morph="none" pos="word" start_char="500">number</TOKEN>
<TOKEN end_char="508" id="token-4-10" morph="none" pos="word" start_char="507">of</TOKEN>
<TOKEN end_char="514" id="token-4-11" morph="none" pos="word" start_char="510">tests</TOKEN>
<TOKEN end_char="518" id="token-4-12" morph="none" pos="word" start_char="516">per</TOKEN>
<TOKEN end_char="520" id="token-4-13" morph="none" pos="word" start_char="520">1</TOKEN>
<TOKEN end_char="528" id="token-4-14" morph="none" pos="word" start_char="522">Million</TOKEN>
<TOKEN end_char="539" id="token-4-15" morph="none" pos="word" start_char="530">population</TOKEN>
<TOKEN end_char="540" id="token-4-16" morph="none" pos="punct" start_char="540">.</TOKEN>
</SEG>
<SEG end_char="566" id="segment-5" start_char="542">
<ORIGINAL_TEXT>Historical data and info.</ORIGINAL_TEXT>
<TOKEN end_char="551" id="token-5-0" morph="none" pos="word" start_char="542">Historical</TOKEN>
<TOKEN end_char="556" id="token-5-1" morph="none" pos="word" start_char="553">data</TOKEN>
<TOKEN end_char="560" id="token-5-2" morph="none" pos="word" start_char="558">and</TOKEN>
<TOKEN end_char="565" id="token-5-3" morph="none" pos="word" start_char="562">info</TOKEN>
<TOKEN end_char="566" id="token-5-4" morph="none" pos="punct" start_char="566">.</TOKEN>
</SEG>
<SEG end_char="575" id="segment-6" start_char="568">
<ORIGINAL_TEXT>Daily...</ORIGINAL_TEXT>
<TOKEN end_char="572" id="token-6-0" morph="none" pos="word" start_char="568">Daily</TOKEN>
<TOKEN end_char="575" id="token-6-1" morph="none" pos="punct" start_char="573">...</TOKEN>
<TRANSLATED_TEXT>Daily.</TRANSLATED_TEXT><DETECTED_LANGUAGE>cy</DETECTED_LANGUAGE></SEG>
<SEG end_char="598" id="segment-7" start_char="578">
<ORIGINAL_TEXT>www.worldometers.info</ORIGINAL_TEXT>
<TOKEN end_char="598" id="token-7-0" morph="none" pos="url" start_char="578">www.worldometers.info</TOKEN>
</SEG>
<SEG end_char="612" id="segment-8" start_char="601">
<ORIGINAL_TEXT>How is this?</ORIGINAL_TEXT>
<TOKEN end_char="603" id="token-8-0" morph="none" pos="word" start_char="601">How</TOKEN>
<TOKEN end_char="606" id="token-8-1" morph="none" pos="word" start_char="605">is</TOKEN>
<TOKEN end_char="611" id="token-8-2" morph="none" pos="word" start_char="608">this</TOKEN>
<TOKEN end_char="612" id="token-8-3" morph="none" pos="punct" start_char="612">?</TOKEN>
</SEG>
<SEG end_char="743" id="segment-9" start_char="615">
<ORIGINAL_TEXT>I refuse to believe that all of those countries have better health systems or are implementing measures any better than the west.</ORIGINAL_TEXT>
<TOKEN end_char="615" id="token-9-0" morph="none" pos="word" start_char="615">I</TOKEN>
<TOKEN end_char="622" id="token-9-1" morph="none" pos="word" start_char="617">refuse</TOKEN>
<TOKEN end_char="625" id="token-9-2" morph="none" pos="word" start_char="624">to</TOKEN>
<TOKEN end_char="633" id="token-9-3" morph="none" pos="word" start_char="627">believe</TOKEN>
<TOKEN end_char="638" id="token-9-4" morph="none" pos="word" start_char="635">that</TOKEN>
<TOKEN end_char="642" id="token-9-5" morph="none" pos="word" start_char="640">all</TOKEN>
<TOKEN end_char="645" id="token-9-6" morph="none" pos="word" start_char="644">of</TOKEN>
<TOKEN end_char="651" id="token-9-7" morph="none" pos="word" start_char="647">those</TOKEN>
<TOKEN end_char="661" id="token-9-8" morph="none" pos="word" start_char="653">countries</TOKEN>
<TOKEN end_char="666" id="token-9-9" morph="none" pos="word" start_char="663">have</TOKEN>
<TOKEN end_char="673" id="token-9-10" morph="none" pos="word" start_char="668">better</TOKEN>
<TOKEN end_char="680" id="token-9-11" morph="none" pos="word" start_char="675">health</TOKEN>
<TOKEN end_char="688" id="token-9-12" morph="none" pos="word" start_char="682">systems</TOKEN>
<TOKEN end_char="691" id="token-9-13" morph="none" pos="word" start_char="690">or</TOKEN>
<TOKEN end_char="695" id="token-9-14" morph="none" pos="word" start_char="693">are</TOKEN>
<TOKEN end_char="708" id="token-9-15" morph="none" pos="word" start_char="697">implementing</TOKEN>
<TOKEN end_char="717" id="token-9-16" morph="none" pos="word" start_char="710">measures</TOKEN>
<TOKEN end_char="721" id="token-9-17" morph="none" pos="word" start_char="719">any</TOKEN>
<TOKEN end_char="728" id="token-9-18" morph="none" pos="word" start_char="723">better</TOKEN>
<TOKEN end_char="733" id="token-9-19" morph="none" pos="word" start_char="730">than</TOKEN>
<TOKEN end_char="737" id="token-9-20" morph="none" pos="word" start_char="735">the</TOKEN>
<TOKEN end_char="742" id="token-9-21" morph="none" pos="word" start_char="739">west</TOKEN>
<TOKEN end_char="743" id="token-9-22" morph="none" pos="punct" start_char="743">.</TOKEN>
</SEG>
<SEG end_char="824" id="segment-10" start_char="746">
<ORIGINAL_TEXT>Either they are all lying with their stats or there is something else going on.</ORIGINAL_TEXT>
<TOKEN end_char="751" id="token-10-0" morph="none" pos="word" start_char="746">Either</TOKEN>
<TOKEN end_char="756" id="token-10-1" morph="none" pos="word" start_char="753">they</TOKEN>
<TOKEN end_char="760" id="token-10-2" morph="none" pos="word" start_char="758">are</TOKEN>
<TOKEN end_char="764" id="token-10-3" morph="none" pos="word" start_char="762">all</TOKEN>
<TOKEN end_char="770" id="token-10-4" morph="none" pos="word" start_char="766">lying</TOKEN>
<TOKEN end_char="775" id="token-10-5" morph="none" pos="word" start_char="772">with</TOKEN>
<TOKEN end_char="781" id="token-10-6" morph="none" pos="word" start_char="777">their</TOKEN>
<TOKEN end_char="787" id="token-10-7" morph="none" pos="word" start_char="783">stats</TOKEN>
<TOKEN end_char="790" id="token-10-8" morph="none" pos="word" start_char="789">or</TOKEN>
<TOKEN end_char="796" id="token-10-9" morph="none" pos="word" start_char="792">there</TOKEN>
<TOKEN end_char="799" id="token-10-10" morph="none" pos="word" start_char="798">is</TOKEN>
<TOKEN end_char="809" id="token-10-11" morph="none" pos="word" start_char="801">something</TOKEN>
<TOKEN end_char="814" id="token-10-12" morph="none" pos="word" start_char="811">else</TOKEN>
<TOKEN end_char="820" id="token-10-13" morph="none" pos="word" start_char="816">going</TOKEN>
<TOKEN end_char="823" id="token-10-14" morph="none" pos="word" start_char="822">on</TOKEN>
<TOKEN end_char="824" id="token-10-15" morph="none" pos="punct" start_char="824">.</TOKEN>
</SEG>
<SEG end_char="850" id="segment-11" start_char="828">
<ORIGINAL_TEXT>They were quick to act.</ORIGINAL_TEXT>
<TOKEN end_char="831" id="token-11-0" morph="none" pos="word" start_char="828">They</TOKEN>
<TOKEN end_char="836" id="token-11-1" morph="none" pos="word" start_char="833">were</TOKEN>
<TOKEN end_char="842" id="token-11-2" morph="none" pos="word" start_char="838">quick</TOKEN>
<TOKEN end_char="845" id="token-11-3" morph="none" pos="word" start_char="844">to</TOKEN>
<TOKEN end_char="849" id="token-11-4" morph="none" pos="word" start_char="847">act</TOKEN>
<TOKEN end_char="850" id="token-11-5" morph="none" pos="punct" start_char="850">.</TOKEN>
</SEG>
<SEG end_char="883" id="segment-12" start_char="852">
<ORIGINAL_TEXT>Test n Trace and border control.</ORIGINAL_TEXT>
<TOKEN end_char="855" id="token-12-0" morph="none" pos="word" start_char="852">Test</TOKEN>
<TOKEN end_char="857" id="token-12-1" morph="none" pos="word" start_char="857">n</TOKEN>
<TOKEN end_char="863" id="token-12-2" morph="none" pos="word" start_char="859">Trace</TOKEN>
<TOKEN end_char="867" id="token-12-3" morph="none" pos="word" start_char="865">and</TOKEN>
<TOKEN end_char="874" id="token-12-4" morph="none" pos="word" start_char="869">border</TOKEN>
<TOKEN end_char="882" id="token-12-5" morph="none" pos="word" start_char="876">control</TOKEN>
<TOKEN end_char="883" id="token-12-6" morph="none" pos="punct" start_char="883">.</TOKEN>
</SEG>
<SEG end_char="900" id="segment-13" start_char="886">
<ORIGINAL_TEXT>Good article at</ORIGINAL_TEXT>
<TOKEN end_char="889" id="token-13-0" morph="none" pos="word" start_char="886">Good</TOKEN>
<TOKEN end_char="897" id="token-13-1" morph="none" pos="word" start_char="891">article</TOKEN>
<TOKEN end_char="900" id="token-13-2" morph="none" pos="word" start_char="899">at</TOKEN>
</SEG>
<SEG end_char="920" id="segment-14" start_char="903">
<ORIGINAL_TEXT>Regional Immunity?</ORIGINAL_TEXT>
<TOKEN end_char="910" id="token-14-0" morph="none" pos="word" start_char="903">Regional</TOKEN>
<TOKEN end_char="919" id="token-14-1" morph="none" pos="word" start_char="912">Immunity</TOKEN>
<TOKEN end_char="920" id="token-14-2" morph="none" pos="punct" start_char="920">?</TOKEN>
</SEG>
<SEG end_char="963" id="segment-15" start_char="922">
<ORIGINAL_TEXT>Why Asia Has Avoided The Worst Of COVID-19</ORIGINAL_TEXT>
<TOKEN end_char="924" id="token-15-0" morph="none" pos="word" start_char="922">Why</TOKEN>
<TOKEN end_char="929" id="token-15-1" morph="none" pos="word" start_char="926">Asia</TOKEN>
<TOKEN end_char="933" id="token-15-2" morph="none" pos="word" start_char="931">Has</TOKEN>
<TOKEN end_char="941" id="token-15-3" morph="none" pos="word" start_char="935">Avoided</TOKEN>
<TOKEN end_char="945" id="token-15-4" morph="none" pos="word" start_char="943">The</TOKEN>
<TOKEN end_char="951" id="token-15-5" morph="none" pos="word" start_char="947">Worst</TOKEN>
<TOKEN end_char="954" id="token-15-6" morph="none" pos="word" start_char="953">Of</TOKEN>
<TOKEN end_char="963" id="token-15-7" morph="none" pos="unknown" start_char="956">COVID-19</TOKEN>
</SEG>
<SEG end_char="1078" id="segment-16" start_char="967">
<ORIGINAL_TEXT>East Asia is home to 30% of the world's population but has recorded only 2.4% of the COVID-19 global death toll.</ORIGINAL_TEXT>
<TOKEN end_char="970" id="token-16-0" morph="none" pos="word" start_char="967">East</TOKEN>
<TOKEN end_char="975" id="token-16-1" morph="none" pos="word" start_char="972">Asia</TOKEN>
<TOKEN end_char="978" id="token-16-2" morph="none" pos="word" start_char="977">is</TOKEN>
<TOKEN end_char="983" id="token-16-3" morph="none" pos="word" start_char="980">home</TOKEN>
<TOKEN end_char="986" id="token-16-4" morph="none" pos="word" start_char="985">to</TOKEN>
<TOKEN end_char="989" id="token-16-5" morph="none" pos="word" start_char="988">30</TOKEN>
<TOKEN end_char="990" id="token-16-6" morph="none" pos="punct" start_char="990">%</TOKEN>
<TOKEN end_char="993" id="token-16-7" morph="none" pos="word" start_char="992">of</TOKEN>
<TOKEN end_char="997" id="token-16-8" morph="none" pos="word" start_char="995">the</TOKEN>
<TOKEN end_char="1005" id="token-16-9" morph="none" pos="word" start_char="999">world's</TOKEN>
<TOKEN end_char="1016" id="token-16-10" morph="none" pos="word" start_char="1007">population</TOKEN>
<TOKEN end_char="1020" id="token-16-11" morph="none" pos="word" start_char="1018">but</TOKEN>
<TOKEN end_char="1024" id="token-16-12" morph="none" pos="word" start_char="1022">has</TOKEN>
<TOKEN end_char="1033" id="token-16-13" morph="none" pos="word" start_char="1026">recorded</TOKEN>
<TOKEN end_char="1038" id="token-16-14" morph="none" pos="word" start_char="1035">only</TOKEN>
<TOKEN end_char="1042" id="token-16-15" morph="none" pos="unknown" start_char="1040">2.4</TOKEN>
<TOKEN end_char="1043" id="token-16-16" morph="none" pos="punct" start_char="1043">%</TOKEN>
<TOKEN end_char="1046" id="token-16-17" morph="none" pos="word" start_char="1045">of</TOKEN>
<TOKEN end_char="1050" id="token-16-18" morph="none" pos="word" start_char="1048">the</TOKEN>
<TOKEN end_char="1059" id="token-16-19" morph="none" pos="unknown" start_char="1052">COVID-19</TOKEN>
<TOKEN end_char="1066" id="token-16-20" morph="none" pos="word" start_char="1061">global</TOKEN>
<TOKEN end_char="1072" id="token-16-21" morph="none" pos="word" start_char="1068">death</TOKEN>
<TOKEN end_char="1077" id="token-16-22" morph="none" pos="word" start_char="1074">toll</TOKEN>
<TOKEN end_char="1078" id="token-16-23" morph="none" pos="punct" start_char="1078">.</TOKEN>
</SEG>
<SEG end_char="1160" id="segment-17" start_char="1080">
<ORIGINAL_TEXT>Scientists are looking at possible immunity from past epidemics or even genetics.</ORIGINAL_TEXT>
<TOKEN end_char="1089" id="token-17-0" morph="none" pos="word" start_char="1080">Scientists</TOKEN>
<TOKEN end_char="1093" id="token-17-1" morph="none" pos="word" start_char="1091">are</TOKEN>
<TOKEN end_char="1101" id="token-17-2" morph="none" pos="word" start_char="1095">looking</TOKEN>
<TOKEN end_char="1104" id="token-17-3" morph="none" pos="word" start_char="1103">at</TOKEN>
<TOKEN end_char="1113" id="token-17-4" morph="none" pos="word" start_char="1106">possible</TOKEN>
<TOKEN end_char="1122" id="token-17-5" morph="none" pos="word" start_char="1115">immunity</TOKEN>
<TOKEN end_char="1127" id="token-17-6" morph="none" pos="word" start_char="1124">from</TOKEN>
<TOKEN end_char="1132" id="token-17-7" morph="none" pos="word" start_char="1129">past</TOKEN>
<TOKEN end_char="1142" id="token-17-8" morph="none" pos="word" start_char="1134">epidemics</TOKEN>
<TOKEN end_char="1145" id="token-17-9" morph="none" pos="word" start_char="1144">or</TOKEN>
<TOKEN end_char="1150" id="token-17-10" morph="none" pos="word" start_char="1147">even</TOKEN>
<TOKEN end_char="1159" id="token-17-11" morph="none" pos="word" start_char="1152">genetics</TOKEN>
<TOKEN end_char="1160" id="token-17-12" morph="none" pos="punct" start_char="1160">.</TOKEN>
</SEG>
<SEG end_char="1177" id="segment-18" start_char="1163">
<ORIGINAL_TEXT>worldcrunch.com</ORIGINAL_TEXT>
<TOKEN end_char="1177" id="token-18-0" morph="none" pos="unknown" start_char="1163">worldcrunch.com</TOKEN>
</SEG>
<SEG end_char="1344" id="segment-19" start_char="1180">
<ORIGINAL_TEXT>Across the continent, people were quick to adopt mask-wearing without question, as wearing this protective gear was already common practice in Japan and South Korea.</ORIGINAL_TEXT>
<TOKEN end_char="1185" id="token-19-0" morph="none" pos="word" start_char="1180">Across</TOKEN>
<TOKEN end_char="1189" id="token-19-1" morph="none" pos="word" start_char="1187">the</TOKEN>
<TOKEN end_char="1199" id="token-19-2" morph="none" pos="word" start_char="1191">continent</TOKEN>
<TOKEN end_char="1200" id="token-19-3" morph="none" pos="punct" start_char="1200">,</TOKEN>
<TOKEN end_char="1207" id="token-19-4" morph="none" pos="word" start_char="1202">people</TOKEN>
<TOKEN end_char="1212" id="token-19-5" morph="none" pos="word" start_char="1209">were</TOKEN>
<TOKEN end_char="1218" id="token-19-6" morph="none" pos="word" start_char="1214">quick</TOKEN>
<TOKEN end_char="1221" id="token-19-7" morph="none" pos="word" start_char="1220">to</TOKEN>
<TOKEN end_char="1227" id="token-19-8" morph="none" pos="word" start_char="1223">adopt</TOKEN>
<TOKEN end_char="1240" id="token-19-9" morph="none" pos="unknown" start_char="1229">mask-wearing</TOKEN>
<TOKEN end_char="1248" id="token-19-10" morph="none" pos="word" start_char="1242">without</TOKEN>
<TOKEN end_char="1257" id="token-19-11" morph="none" pos="word" start_char="1250">question</TOKEN>
<TOKEN end_char="1258" id="token-19-12" morph="none" pos="punct" start_char="1258">,</TOKEN>
<TOKEN end_char="1261" id="token-19-13" morph="none" pos="word" start_char="1260">as</TOKEN>
<TOKEN end_char="1269" id="token-19-14" morph="none" pos="word" start_char="1263">wearing</TOKEN>
<TOKEN end_char="1274" id="token-19-15" morph="none" pos="word" start_char="1271">this</TOKEN>
<TOKEN end_char="1285" id="token-19-16" morph="none" pos="word" start_char="1276">protective</TOKEN>
<TOKEN end_char="1290" id="token-19-17" morph="none" pos="word" start_char="1287">gear</TOKEN>
<TOKEN end_char="1294" id="token-19-18" morph="none" pos="word" start_char="1292">was</TOKEN>
<TOKEN end_char="1302" id="token-19-19" morph="none" pos="word" start_char="1296">already</TOKEN>
<TOKEN end_char="1309" id="token-19-20" morph="none" pos="word" start_char="1304">common</TOKEN>
<TOKEN end_char="1318" id="token-19-21" morph="none" pos="word" start_char="1311">practice</TOKEN>
<TOKEN end_char="1321" id="token-19-22" morph="none" pos="word" start_char="1320">in</TOKEN>
<TOKEN end_char="1327" id="token-19-23" morph="none" pos="word" start_char="1323">Japan</TOKEN>
<TOKEN end_char="1331" id="token-19-24" morph="none" pos="word" start_char="1329">and</TOKEN>
<TOKEN end_char="1337" id="token-19-25" morph="none" pos="word" start_char="1333">South</TOKEN>
<TOKEN end_char="1343" id="token-19-26" morph="none" pos="word" start_char="1339">Korea</TOKEN>
<TOKEN end_char="1344" id="token-19-27" morph="none" pos="punct" start_char="1344">.</TOKEN>
</SEG>
<SEG end_char="1624" id="segment-20" start_char="1346">
<ORIGINAL_TEXT>"The big differences with the West have been the strict border shutdowns and the rigorous testing of infected people, whether they were sick or asymptomatic," notes Professor Teo Yik Ying, dean of the Saw Swee Hock School of Public Health at the National University of Singapore.</ORIGINAL_TEXT>
<TOKEN end_char="1346" id="token-20-0" morph="none" pos="punct" start_char="1346">"</TOKEN>
<TOKEN end_char="1349" id="token-20-1" morph="none" pos="word" start_char="1347">The</TOKEN>
<TOKEN end_char="1353" id="token-20-2" morph="none" pos="word" start_char="1351">big</TOKEN>
<TOKEN end_char="1365" id="token-20-3" morph="none" pos="word" start_char="1355">differences</TOKEN>
<TOKEN end_char="1370" id="token-20-4" morph="none" pos="word" start_char="1367">with</TOKEN>
<TOKEN end_char="1374" id="token-20-5" morph="none" pos="word" start_char="1372">the</TOKEN>
<TOKEN end_char="1379" id="token-20-6" morph="none" pos="word" start_char="1376">West</TOKEN>
<TOKEN end_char="1384" id="token-20-7" morph="none" pos="word" start_char="1381">have</TOKEN>
<TOKEN end_char="1389" id="token-20-8" morph="none" pos="word" start_char="1386">been</TOKEN>
<TOKEN end_char="1393" id="token-20-9" morph="none" pos="word" start_char="1391">the</TOKEN>
<TOKEN end_char="1400" id="token-20-10" morph="none" pos="word" start_char="1395">strict</TOKEN>
<TOKEN end_char="1407" id="token-20-11" morph="none" pos="word" start_char="1402">border</TOKEN>
<TOKEN end_char="1417" id="token-20-12" morph="none" pos="word" start_char="1409">shutdowns</TOKEN>
<TOKEN end_char="1421" id="token-20-13" morph="none" pos="word" start_char="1419">and</TOKEN>
<TOKEN end_char="1425" id="token-20-14" morph="none" pos="word" start_char="1423">the</TOKEN>
<TOKEN end_char="1434" id="token-20-15" morph="none" pos="word" start_char="1427">rigorous</TOKEN>
<TOKEN end_char="1442" id="token-20-16" morph="none" pos="word" start_char="1436">testing</TOKEN>
<TOKEN end_char="1445" id="token-20-17" morph="none" pos="word" start_char="1444">of</TOKEN>
<TOKEN end_char="1454" id="token-20-18" morph="none" pos="word" start_char="1447">infected</TOKEN>
<TOKEN end_char="1461" id="token-20-19" morph="none" pos="word" start_char="1456">people</TOKEN>
<TOKEN end_char="1462" id="token-20-20" morph="none" pos="punct" start_char="1462">,</TOKEN>
<TOKEN end_char="1470" id="token-20-21" morph="none" pos="word" start_char="1464">whether</TOKEN>
<TOKEN end_char="1475" id="token-20-22" morph="none" pos="word" start_char="1472">they</TOKEN>
<TOKEN end_char="1480" id="token-20-23" morph="none" pos="word" start_char="1477">were</TOKEN>
<TOKEN end_char="1485" id="token-20-24" morph="none" pos="word" start_char="1482">sick</TOKEN>
<TOKEN end_char="1488" id="token-20-25" morph="none" pos="word" start_char="1487">or</TOKEN>
<TOKEN end_char="1501" id="token-20-26" morph="none" pos="word" start_char="1490">asymptomatic</TOKEN>
<TOKEN end_char="1503" id="token-20-27" morph="none" pos="punct" start_char="1502">,"</TOKEN>
<TOKEN end_char="1509" id="token-20-28" morph="none" pos="word" start_char="1505">notes</TOKEN>
<TOKEN end_char="1519" id="token-20-29" morph="none" pos="word" start_char="1511">Professor</TOKEN>
<TOKEN end_char="1523" id="token-20-30" morph="none" pos="word" start_char="1521">Teo</TOKEN>
<TOKEN end_char="1527" id="token-20-31" morph="none" pos="word" start_char="1525">Yik</TOKEN>
<TOKEN end_char="1532" id="token-20-32" morph="none" pos="word" start_char="1529">Ying</TOKEN>
<TOKEN end_char="1533" id="token-20-33" morph="none" pos="punct" start_char="1533">,</TOKEN>
<TOKEN end_char="1538" id="token-20-34" morph="none" pos="word" start_char="1535">dean</TOKEN>
<TOKEN end_char="1541" id="token-20-35" morph="none" pos="word" start_char="1540">of</TOKEN>
<TOKEN end_char="1545" id="token-20-36" morph="none" pos="word" start_char="1543">the</TOKEN>
<TOKEN end_char="1549" id="token-20-37" morph="none" pos="word" start_char="1547">Saw</TOKEN>
<TOKEN end_char="1554" id="token-20-38" morph="none" pos="word" start_char="1551">Swee</TOKEN>
<TOKEN end_char="1559" id="token-20-39" morph="none" pos="word" start_char="1556">Hock</TOKEN>
<TOKEN end_char="1566" id="token-20-40" morph="none" pos="word" start_char="1561">School</TOKEN>
<TOKEN end_char="1569" id="token-20-41" morph="none" pos="word" start_char="1568">of</TOKEN>
<TOKEN end_char="1576" id="token-20-42" morph="none" pos="word" start_char="1571">Public</TOKEN>
<TOKEN end_char="1583" id="token-20-43" morph="none" pos="word" start_char="1578">Health</TOKEN>
<TOKEN end_char="1586" id="token-20-44" morph="none" pos="word" start_char="1585">at</TOKEN>
<TOKEN end_char="1590" id="token-20-45" morph="none" pos="word" start_char="1588">the</TOKEN>
<TOKEN end_char="1599" id="token-20-46" morph="none" pos="word" start_char="1592">National</TOKEN>
<TOKEN end_char="1610" id="token-20-47" morph="none" pos="word" start_char="1601">University</TOKEN>
<TOKEN end_char="1613" id="token-20-48" morph="none" pos="word" start_char="1612">of</TOKEN>
<TOKEN end_char="1623" id="token-20-49" morph="none" pos="word" start_char="1615">Singapore</TOKEN>
<TOKEN end_char="1624" id="token-20-50" morph="none" pos="punct" start_char="1624">.</TOKEN>
</SEG>
<SEG end_char="1654" id="segment-21" start_char="1628">
<ORIGINAL_TEXT>Great school name there BTW</ORIGINAL_TEXT>
<TOKEN end_char="1632" id="token-21-0" morph="none" pos="word" start_char="1628">Great</TOKEN>
<TOKEN end_char="1639" id="token-21-1" morph="none" pos="word" start_char="1634">school</TOKEN>
<TOKEN end_char="1644" id="token-21-2" morph="none" pos="word" start_char="1641">name</TOKEN>
<TOKEN end_char="1650" id="token-21-3" morph="none" pos="word" start_char="1646">there</TOKEN>
<TOKEN end_char="1654" id="token-21-4" morph="none" pos="word" start_char="1652">BTW</TOKEN>
</SEG>
<SEG end_char="1732" id="segment-22" start_char="1657">
<ORIGINAL_TEXT>So yes they implemented measures much better then the West and much quicker.</ORIGINAL_TEXT>
<TOKEN end_char="1658" id="token-22-0" morph="none" pos="word" start_char="1657">So</TOKEN>
<TOKEN end_char="1662" id="token-22-1" morph="none" pos="word" start_char="1660">yes</TOKEN>
<TOKEN end_char="1667" id="token-22-2" morph="none" pos="word" start_char="1664">they</TOKEN>
<TOKEN end_char="1679" id="token-22-3" morph="none" pos="word" start_char="1669">implemented</TOKEN>
<TOKEN end_char="1688" id="token-22-4" morph="none" pos="word" start_char="1681">measures</TOKEN>
<TOKEN end_char="1693" id="token-22-5" morph="none" pos="word" start_char="1690">much</TOKEN>
<TOKEN end_char="1700" id="token-22-6" morph="none" pos="word" start_char="1695">better</TOKEN>
<TOKEN end_char="1705" id="token-22-7" morph="none" pos="word" start_char="1702">then</TOKEN>
<TOKEN end_char="1709" id="token-22-8" morph="none" pos="word" start_char="1707">the</TOKEN>
<TOKEN end_char="1714" id="token-22-9" morph="none" pos="word" start_char="1711">West</TOKEN>
<TOKEN end_char="1718" id="token-22-10" morph="none" pos="word" start_char="1716">and</TOKEN>
<TOKEN end_char="1723" id="token-22-11" morph="none" pos="word" start_char="1720">much</TOKEN>
<TOKEN end_char="1731" id="token-22-12" morph="none" pos="word" start_char="1725">quicker</TOKEN>
<TOKEN end_char="1732" id="token-22-13" morph="none" pos="punct" start_char="1732">.</TOKEN>
</SEG>
<SEG end_char="1768" id="segment-23" start_char="1735">
<ORIGINAL_TEXT>Sometimes I pretend I am a carrot.</ORIGINAL_TEXT>
<TOKEN end_char="1743" id="token-23-0" morph="none" pos="word" start_char="1735">Sometimes</TOKEN>
<TOKEN end_char="1745" id="token-23-1" morph="none" pos="word" start_char="1745">I</TOKEN>
<TOKEN end_char="1753" id="token-23-2" morph="none" pos="word" start_char="1747">pretend</TOKEN>
<TOKEN end_char="1755" id="token-23-3" morph="none" pos="word" start_char="1755">I</TOKEN>
<TOKEN end_char="1758" id="token-23-4" morph="none" pos="word" start_char="1757">am</TOKEN>
<TOKEN end_char="1760" id="token-23-5" morph="none" pos="word" start_char="1760">a</TOKEN>
<TOKEN end_char="1767" id="token-23-6" morph="none" pos="word" start_char="1762">carrot</TOKEN>
<TOKEN end_char="1768" id="token-23-7" morph="none" pos="punct" start_char="1768">.</TOKEN>
</SEG>
<SEG end_char="1817" id="segment-24" start_char="1773">
<ORIGINAL_TEXT>realfrankturner said: They were quick to act.</ORIGINAL_TEXT>
<TOKEN end_char="1787" id="token-24-0" morph="none" pos="word" start_char="1773">realfrankturner</TOKEN>
<TOKEN end_char="1792" id="token-24-1" morph="none" pos="word" start_char="1789">said</TOKEN>
<TOKEN end_char="1793" id="token-24-2" morph="none" pos="punct" start_char="1793">:</TOKEN>
<TOKEN end_char="1798" id="token-24-3" morph="none" pos="word" start_char="1795">They</TOKEN>
<TOKEN end_char="1803" id="token-24-4" morph="none" pos="word" start_char="1800">were</TOKEN>
<TOKEN end_char="1809" id="token-24-5" morph="none" pos="word" start_char="1805">quick</TOKEN>
<TOKEN end_char="1812" id="token-24-6" morph="none" pos="word" start_char="1811">to</TOKEN>
<TOKEN end_char="1816" id="token-24-7" morph="none" pos="word" start_char="1814">act</TOKEN>
<TOKEN end_char="1817" id="token-24-8" morph="none" pos="punct" start_char="1817">.</TOKEN>
</SEG>
<SEG end_char="1850" id="segment-25" start_char="1819">
<ORIGINAL_TEXT>Test n Trace and border control.</ORIGINAL_TEXT>
<TOKEN end_char="1822" id="token-25-0" morph="none" pos="word" start_char="1819">Test</TOKEN>
<TOKEN end_char="1824" id="token-25-1" morph="none" pos="word" start_char="1824">n</TOKEN>
<TOKEN end_char="1830" id="token-25-2" morph="none" pos="word" start_char="1826">Trace</TOKEN>
<TOKEN end_char="1834" id="token-25-3" morph="none" pos="word" start_char="1832">and</TOKEN>
<TOKEN end_char="1841" id="token-25-4" morph="none" pos="word" start_char="1836">border</TOKEN>
<TOKEN end_char="1849" id="token-25-5" morph="none" pos="word" start_char="1843">control</TOKEN>
<TOKEN end_char="1850" id="token-25-6" morph="none" pos="punct" start_char="1850">.</TOKEN>
</SEG>
<SEG end_char="1866" id="segment-26" start_char="1852">
<ORIGINAL_TEXT>Good article at</ORIGINAL_TEXT>
<TOKEN end_char="1855" id="token-26-0" morph="none" pos="word" start_char="1852">Good</TOKEN>
<TOKEN end_char="1863" id="token-26-1" morph="none" pos="word" start_char="1857">article</TOKEN>
<TOKEN end_char="1866" id="token-26-2" morph="none" pos="word" start_char="1865">at</TOKEN>
</SEG>
<SEG end_char="1886" id="segment-27" start_char="1869">
<ORIGINAL_TEXT>Regional Immunity?</ORIGINAL_TEXT>
<TOKEN end_char="1876" id="token-27-0" morph="none" pos="word" start_char="1869">Regional</TOKEN>
<TOKEN end_char="1885" id="token-27-1" morph="none" pos="word" start_char="1878">Immunity</TOKEN>
<TOKEN end_char="1886" id="token-27-2" morph="none" pos="punct" start_char="1886">?</TOKEN>
</SEG>
<SEG end_char="1929" id="segment-28" start_char="1888">
<ORIGINAL_TEXT>Why Asia Has Avoided The Worst Of COVID-19</ORIGINAL_TEXT>
<TOKEN end_char="1890" id="token-28-0" morph="none" pos="word" start_char="1888">Why</TOKEN>
<TOKEN end_char="1895" id="token-28-1" morph="none" pos="word" start_char="1892">Asia</TOKEN>
<TOKEN end_char="1899" id="token-28-2" morph="none" pos="word" start_char="1897">Has</TOKEN>
<TOKEN end_char="1907" id="token-28-3" morph="none" pos="word" start_char="1901">Avoided</TOKEN>
<TOKEN end_char="1911" id="token-28-4" morph="none" pos="word" start_char="1909">The</TOKEN>
<TOKEN end_char="1917" id="token-28-5" morph="none" pos="word" start_char="1913">Worst</TOKEN>
<TOKEN end_char="1920" id="token-28-6" morph="none" pos="word" start_char="1919">Of</TOKEN>
<TOKEN end_char="1929" id="token-28-7" morph="none" pos="unknown" start_char="1922">COVID-19</TOKEN>
</SEG>
<SEG end_char="2043" id="segment-29" start_char="1932">
<ORIGINAL_TEXT>East Asia is home to 30% of the world's population but has recorded only 2.4% of the COVID-19 global death toll.</ORIGINAL_TEXT>
<TOKEN end_char="1935" id="token-29-0" morph="none" pos="word" start_char="1932">East</TOKEN>
<TOKEN end_char="1940" id="token-29-1" morph="none" pos="word" start_char="1937">Asia</TOKEN>
<TOKEN end_char="1943" id="token-29-2" morph="none" pos="word" start_char="1942">is</TOKEN>
<TOKEN end_char="1948" id="token-29-3" morph="none" pos="word" start_char="1945">home</TOKEN>
<TOKEN end_char="1951" id="token-29-4" morph="none" pos="word" start_char="1950">to</TOKEN>
<TOKEN end_char="1954" id="token-29-5" morph="none" pos="word" start_char="1953">30</TOKEN>
<TOKEN end_char="1955" id="token-29-6" morph="none" pos="punct" start_char="1955">%</TOKEN>
<TOKEN end_char="1958" id="token-29-7" morph="none" pos="word" start_char="1957">of</TOKEN>
<TOKEN end_char="1962" id="token-29-8" morph="none" pos="word" start_char="1960">the</TOKEN>
<TOKEN end_char="1970" id="token-29-9" morph="none" pos="word" start_char="1964">world's</TOKEN>
<TOKEN end_char="1981" id="token-29-10" morph="none" pos="word" start_char="1972">population</TOKEN>
<TOKEN end_char="1985" id="token-29-11" morph="none" pos="word" start_char="1983">but</TOKEN>
<TOKEN end_char="1989" id="token-29-12" morph="none" pos="word" start_char="1987">has</TOKEN>
<TOKEN end_char="1998" id="token-29-13" morph="none" pos="word" start_char="1991">recorded</TOKEN>
<TOKEN end_char="2003" id="token-29-14" morph="none" pos="word" start_char="2000">only</TOKEN>
<TOKEN end_char="2007" id="token-29-15" morph="none" pos="unknown" start_char="2005">2.4</TOKEN>
<TOKEN end_char="2008" id="token-29-16" morph="none" pos="punct" start_char="2008">%</TOKEN>
<TOKEN end_char="2011" id="token-29-17" morph="none" pos="word" start_char="2010">of</TOKEN>
<TOKEN end_char="2015" id="token-29-18" morph="none" pos="word" start_char="2013">the</TOKEN>
<TOKEN end_char="2024" id="token-29-19" morph="none" pos="unknown" start_char="2017">COVID-19</TOKEN>
<TOKEN end_char="2031" id="token-29-20" morph="none" pos="word" start_char="2026">global</TOKEN>
<TOKEN end_char="2037" id="token-29-21" morph="none" pos="word" start_char="2033">death</TOKEN>
<TOKEN end_char="2042" id="token-29-22" morph="none" pos="word" start_char="2039">toll</TOKEN>
<TOKEN end_char="2043" id="token-29-23" morph="none" pos="punct" start_char="2043">.</TOKEN>
</SEG>
<SEG end_char="2125" id="segment-30" start_char="2045">
<ORIGINAL_TEXT>Scientists are looking at possible immunity from past epidemics or even genetics.</ORIGINAL_TEXT>
<TOKEN end_char="2054" id="token-30-0" morph="none" pos="word" start_char="2045">Scientists</TOKEN>
<TOKEN end_char="2058" id="token-30-1" morph="none" pos="word" start_char="2056">are</TOKEN>
<TOKEN end_char="2066" id="token-30-2" morph="none" pos="word" start_char="2060">looking</TOKEN>
<TOKEN end_char="2069" id="token-30-3" morph="none" pos="word" start_char="2068">at</TOKEN>
<TOKEN end_char="2078" id="token-30-4" morph="none" pos="word" start_char="2071">possible</TOKEN>
<TOKEN end_char="2087" id="token-30-5" morph="none" pos="word" start_char="2080">immunity</TOKEN>
<TOKEN end_char="2092" id="token-30-6" morph="none" pos="word" start_char="2089">from</TOKEN>
<TOKEN end_char="2097" id="token-30-7" morph="none" pos="word" start_char="2094">past</TOKEN>
<TOKEN end_char="2107" id="token-30-8" morph="none" pos="word" start_char="2099">epidemics</TOKEN>
<TOKEN end_char="2110" id="token-30-9" morph="none" pos="word" start_char="2109">or</TOKEN>
<TOKEN end_char="2115" id="token-30-10" morph="none" pos="word" start_char="2112">even</TOKEN>
<TOKEN end_char="2124" id="token-30-11" morph="none" pos="word" start_char="2117">genetics</TOKEN>
<TOKEN end_char="2125" id="token-30-12" morph="none" pos="punct" start_char="2125">.</TOKEN>
</SEG>
<SEG end_char="2246" id="segment-31" start_char="2127">
<ORIGINAL_TEXT>worldcrunch.com Great school name there BTW So yes they implemented measures much better then the West and much quicker.</ORIGINAL_TEXT>
<TOKEN end_char="2141" id="token-31-0" morph="none" pos="unknown" start_char="2127">worldcrunch.com</TOKEN>
<TOKEN end_char="2147" id="token-31-1" morph="none" pos="word" start_char="2143">Great</TOKEN>
<TOKEN end_char="2154" id="token-31-2" morph="none" pos="word" start_char="2149">school</TOKEN>
<TOKEN end_char="2159" id="token-31-3" morph="none" pos="word" start_char="2156">name</TOKEN>
<TOKEN end_char="2165" id="token-31-4" morph="none" pos="word" start_char="2161">there</TOKEN>
<TOKEN end_char="2169" id="token-31-5" morph="none" pos="word" start_char="2167">BTW</TOKEN>
<TOKEN end_char="2172" id="token-31-6" morph="none" pos="word" start_char="2171">So</TOKEN>
<TOKEN end_char="2176" id="token-31-7" morph="none" pos="word" start_char="2174">yes</TOKEN>
<TOKEN end_char="2181" id="token-31-8" morph="none" pos="word" start_char="2178">they</TOKEN>
<TOKEN end_char="2193" id="token-31-9" morph="none" pos="word" start_char="2183">implemented</TOKEN>
<TOKEN end_char="2202" id="token-31-10" morph="none" pos="word" start_char="2195">measures</TOKEN>
<TOKEN end_char="2207" id="token-31-11" morph="none" pos="word" start_char="2204">much</TOKEN>
<TOKEN end_char="2214" id="token-31-12" morph="none" pos="word" start_char="2209">better</TOKEN>
<TOKEN end_char="2219" id="token-31-13" morph="none" pos="word" start_char="2216">then</TOKEN>
<TOKEN end_char="2223" id="token-31-14" morph="none" pos="word" start_char="2221">the</TOKEN>
<TOKEN end_char="2228" id="token-31-15" morph="none" pos="word" start_char="2225">West</TOKEN>
<TOKEN end_char="2232" id="token-31-16" morph="none" pos="word" start_char="2230">and</TOKEN>
<TOKEN end_char="2237" id="token-31-17" morph="none" pos="word" start_char="2234">much</TOKEN>
<TOKEN end_char="2245" id="token-31-18" morph="none" pos="word" start_char="2239">quicker</TOKEN>
<TOKEN end_char="2246" id="token-31-19" morph="none" pos="punct" start_char="2246">.</TOKEN>
</SEG>
<SEG end_char="2265" id="segment-32" start_char="2248">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="2252" id="token-32-0" morph="none" pos="word" start_char="2248">Click</TOKEN>
<TOKEN end_char="2255" id="token-32-1" morph="none" pos="word" start_char="2254">to</TOKEN>
<TOKEN end_char="2262" id="token-32-2" morph="none" pos="word" start_char="2257">expand</TOKEN>
<TOKEN end_char="2265" id="token-32-3" morph="none" pos="punct" start_char="2263">...</TOKEN>
</SEG>
<SEG end_char="2354" id="segment-33" start_char="2268">
<ORIGINAL_TEXT>I don't believe every single country in Asia did that or has the capability to do that.</ORIGINAL_TEXT>
<TOKEN end_char="2268" id="token-33-0" morph="none" pos="word" start_char="2268">I</TOKEN>
<TOKEN end_char="2274" id="token-33-1" morph="none" pos="word" start_char="2270">don't</TOKEN>
<TOKEN end_char="2282" id="token-33-2" morph="none" pos="word" start_char="2276">believe</TOKEN>
<TOKEN end_char="2288" id="token-33-3" morph="none" pos="word" start_char="2284">every</TOKEN>
<TOKEN end_char="2295" id="token-33-4" morph="none" pos="word" start_char="2290">single</TOKEN>
<TOKEN end_char="2303" id="token-33-5" morph="none" pos="word" start_char="2297">country</TOKEN>
<TOKEN end_char="2306" id="token-33-6" morph="none" pos="word" start_char="2305">in</TOKEN>
<TOKEN end_char="2311" id="token-33-7" morph="none" pos="word" start_char="2308">Asia</TOKEN>
<TOKEN end_char="2315" id="token-33-8" morph="none" pos="word" start_char="2313">did</TOKEN>
<TOKEN end_char="2320" id="token-33-9" morph="none" pos="word" start_char="2317">that</TOKEN>
<TOKEN end_char="2323" id="token-33-10" morph="none" pos="word" start_char="2322">or</TOKEN>
<TOKEN end_char="2327" id="token-33-11" morph="none" pos="word" start_char="2325">has</TOKEN>
<TOKEN end_char="2331" id="token-33-12" morph="none" pos="word" start_char="2329">the</TOKEN>
<TOKEN end_char="2342" id="token-33-13" morph="none" pos="word" start_char="2333">capability</TOKEN>
<TOKEN end_char="2345" id="token-33-14" morph="none" pos="word" start_char="2344">to</TOKEN>
<TOKEN end_char="2348" id="token-33-15" morph="none" pos="word" start_char="2347">do</TOKEN>
<TOKEN end_char="2353" id="token-33-16" morph="none" pos="word" start_char="2350">that</TOKEN>
<TOKEN end_char="2354" id="token-33-17" morph="none" pos="punct" start_char="2354">.</TOKEN>
</SEG>
<SEG end_char="2395" id="segment-34" start_char="2357">
<ORIGINAL_TEXT>Good article though so thanks for that.</ORIGINAL_TEXT>
<TOKEN end_char="2360" id="token-34-0" morph="none" pos="word" start_char="2357">Good</TOKEN>
<TOKEN end_char="2368" id="token-34-1" morph="none" pos="word" start_char="2362">article</TOKEN>
<TOKEN end_char="2375" id="token-34-2" morph="none" pos="word" start_char="2370">though</TOKEN>
<TOKEN end_char="2378" id="token-34-3" morph="none" pos="word" start_char="2377">so</TOKEN>
<TOKEN end_char="2385" id="token-34-4" morph="none" pos="word" start_char="2380">thanks</TOKEN>
<TOKEN end_char="2389" id="token-34-5" morph="none" pos="word" start_char="2387">for</TOKEN>
<TOKEN end_char="2394" id="token-34-6" morph="none" pos="word" start_char="2391">that</TOKEN>
<TOKEN end_char="2395" id="token-34-7" morph="none" pos="punct" start_char="2395">.</TOKEN>
</SEG>
<SEG end_char="2444" id="segment-35" start_char="2398">
<ORIGINAL_TEXT>So it looks like asians are genetically immune.</ORIGINAL_TEXT>
<TOKEN end_char="2399" id="token-35-0" morph="none" pos="word" start_char="2398">So</TOKEN>
<TOKEN end_char="2402" id="token-35-1" morph="none" pos="word" start_char="2401">it</TOKEN>
<TOKEN end_char="2408" id="token-35-2" morph="none" pos="word" start_char="2404">looks</TOKEN>
<TOKEN end_char="2413" id="token-35-3" morph="none" pos="word" start_char="2410">like</TOKEN>
<TOKEN end_char="2420" id="token-35-4" morph="none" pos="word" start_char="2415">asians</TOKEN>
<TOKEN end_char="2424" id="token-35-5" morph="none" pos="word" start_char="2422">are</TOKEN>
<TOKEN end_char="2436" id="token-35-6" morph="none" pos="word" start_char="2426">genetically</TOKEN>
<TOKEN end_char="2443" id="token-35-7" morph="none" pos="word" start_char="2438">immune</TOKEN>
<TOKEN end_char="2444" id="token-35-8" morph="none" pos="punct" start_char="2444">.</TOKEN>
</SEG>
<SEG end_char="2515" id="segment-36" start_char="2447">
<ORIGINAL_TEXT>and yes had a little trouble spelling the user name when registering.</ORIGINAL_TEXT>
<TOKEN end_char="2449" id="token-36-0" morph="none" pos="word" start_char="2447">and</TOKEN>
<TOKEN end_char="2453" id="token-36-1" morph="none" pos="word" start_char="2451">yes</TOKEN>
<TOKEN end_char="2457" id="token-36-2" morph="none" pos="word" start_char="2455">had</TOKEN>
<TOKEN end_char="2459" id="token-36-3" morph="none" pos="word" start_char="2459">a</TOKEN>
<TOKEN end_char="2466" id="token-36-4" morph="none" pos="word" start_char="2461">little</TOKEN>
<TOKEN end_char="2474" id="token-36-5" morph="none" pos="word" start_char="2468">trouble</TOKEN>
<TOKEN end_char="2483" id="token-36-6" morph="none" pos="word" start_char="2476">spelling</TOKEN>
<TOKEN end_char="2487" id="token-36-7" morph="none" pos="word" start_char="2485">the</TOKEN>
<TOKEN end_char="2492" id="token-36-8" morph="none" pos="word" start_char="2489">user</TOKEN>
<TOKEN end_char="2497" id="token-36-9" morph="none" pos="word" start_char="2494">name</TOKEN>
<TOKEN end_char="2502" id="token-36-10" morph="none" pos="word" start_char="2499">when</TOKEN>
<TOKEN end_char="2514" id="token-36-11" morph="none" pos="word" start_char="2504">registering</TOKEN>
<TOKEN end_char="2515" id="token-36-12" morph="none" pos="punct" start_char="2515">.</TOKEN>
</SEG>
<SEG end_char="2550" id="segment-37" start_char="2518">
<ORIGINAL_TEXT>It caused much amusement to some.</ORIGINAL_TEXT>
<TOKEN end_char="2519" id="token-37-0" morph="none" pos="word" start_char="2518">It</TOKEN>
<TOKEN end_char="2526" id="token-37-1" morph="none" pos="word" start_char="2521">caused</TOKEN>
<TOKEN end_char="2531" id="token-37-2" morph="none" pos="word" start_char="2528">much</TOKEN>
<TOKEN end_char="2541" id="token-37-3" morph="none" pos="word" start_char="2533">amusement</TOKEN>
<TOKEN end_char="2544" id="token-37-4" morph="none" pos="word" start_char="2543">to</TOKEN>
<TOKEN end_char="2549" id="token-37-5" morph="none" pos="word" start_char="2546">some</TOKEN>
<TOKEN end_char="2550" id="token-37-6" morph="none" pos="punct" start_char="2550">.</TOKEN>
</SEG>
<SEG end_char="2605" id="segment-38" start_char="2555">
<ORIGINAL_TEXT>I said school name, from the article, not user name</ORIGINAL_TEXT>
<TOKEN end_char="2555" id="token-38-0" morph="none" pos="word" start_char="2555">I</TOKEN>
<TOKEN end_char="2560" id="token-38-1" morph="none" pos="word" start_char="2557">said</TOKEN>
<TOKEN end_char="2567" id="token-38-2" morph="none" pos="word" start_char="2562">school</TOKEN>
<TOKEN end_char="2572" id="token-38-3" morph="none" pos="word" start_char="2569">name</TOKEN>
<TOKEN end_char="2573" id="token-38-4" morph="none" pos="punct" start_char="2573">,</TOKEN>
<TOKEN end_char="2578" id="token-38-5" morph="none" pos="word" start_char="2575">from</TOKEN>
<TOKEN end_char="2582" id="token-38-6" morph="none" pos="word" start_char="2580">the</TOKEN>
<TOKEN end_char="2590" id="token-38-7" morph="none" pos="word" start_char="2584">article</TOKEN>
<TOKEN end_char="2591" id="token-38-8" morph="none" pos="punct" start_char="2591">,</TOKEN>
<TOKEN end_char="2595" id="token-38-9" morph="none" pos="word" start_char="2593">not</TOKEN>
<TOKEN end_char="2600" id="token-38-10" morph="none" pos="word" start_char="2597">user</TOKEN>
<TOKEN end_char="2605" id="token-38-11" morph="none" pos="word" start_char="2602">name</TOKEN>
</SEG>
<SEG end_char="2658" id="segment-39" start_char="2608">
<ORIGINAL_TEXT>So if you don't believe it, why do you think it is?</ORIGINAL_TEXT>
<TOKEN end_char="2609" id="token-39-0" morph="none" pos="word" start_char="2608">So</TOKEN>
<TOKEN end_char="2612" id="token-39-1" morph="none" pos="word" start_char="2611">if</TOKEN>
<TOKEN end_char="2616" id="token-39-2" morph="none" pos="word" start_char="2614">you</TOKEN>
<TOKEN end_char="2622" id="token-39-3" morph="none" pos="word" start_char="2618">don't</TOKEN>
<TOKEN end_char="2630" id="token-39-4" morph="none" pos="word" start_char="2624">believe</TOKEN>
<TOKEN end_char="2633" id="token-39-5" morph="none" pos="word" start_char="2632">it</TOKEN>
<TOKEN end_char="2634" id="token-39-6" morph="none" pos="punct" start_char="2634">,</TOKEN>
<TOKEN end_char="2638" id="token-39-7" morph="none" pos="word" start_char="2636">why</TOKEN>
<TOKEN end_char="2641" id="token-39-8" morph="none" pos="word" start_char="2640">do</TOKEN>
<TOKEN end_char="2645" id="token-39-9" morph="none" pos="word" start_char="2643">you</TOKEN>
<TOKEN end_char="2651" id="token-39-10" morph="none" pos="word" start_char="2647">think</TOKEN>
<TOKEN end_char="2654" id="token-39-11" morph="none" pos="word" start_char="2653">it</TOKEN>
<TOKEN end_char="2657" id="token-39-12" morph="none" pos="word" start_char="2656">is</TOKEN>
<TOKEN end_char="2658" id="token-39-13" morph="none" pos="punct" start_char="2658">?</TOKEN>
</SEG>
<SEG end_char="2692" id="segment-40" start_char="2661">
<ORIGINAL_TEXT>Also look at places like NZ too.</ORIGINAL_TEXT>
<TOKEN end_char="2664" id="token-40-0" morph="none" pos="word" start_char="2661">Also</TOKEN>
<TOKEN end_char="2669" id="token-40-1" morph="none" pos="word" start_char="2666">look</TOKEN>
<TOKEN end_char="2672" id="token-40-2" morph="none" pos="word" start_char="2671">at</TOKEN>
<TOKEN end_char="2679" id="token-40-3" morph="none" pos="word" start_char="2674">places</TOKEN>
<TOKEN end_char="2684" id="token-40-4" morph="none" pos="word" start_char="2681">like</TOKEN>
<TOKEN end_char="2687" id="token-40-5" morph="none" pos="word" start_char="2686">NZ</TOKEN>
<TOKEN end_char="2691" id="token-40-6" morph="none" pos="word" start_char="2689">too</TOKEN>
<TOKEN end_char="2692" id="token-40-7" morph="none" pos="punct" start_char="2692">.</TOKEN>
</SEG>
<SEG end_char="2786" id="segment-41" start_char="2695">
<ORIGINAL_TEXT>The countries that acted the quickest and did the right things prospered better in some ways</ORIGINAL_TEXT>
<TOKEN end_char="2697" id="token-41-0" morph="none" pos="word" start_char="2695">The</TOKEN>
<TOKEN end_char="2707" id="token-41-1" morph="none" pos="word" start_char="2699">countries</TOKEN>
<TOKEN end_char="2712" id="token-41-2" morph="none" pos="word" start_char="2709">that</TOKEN>
<TOKEN end_char="2718" id="token-41-3" morph="none" pos="word" start_char="2714">acted</TOKEN>
<TOKEN end_char="2722" id="token-41-4" morph="none" pos="word" start_char="2720">the</TOKEN>
<TOKEN end_char="2731" id="token-41-5" morph="none" pos="word" start_char="2724">quickest</TOKEN>
<TOKEN end_char="2735" id="token-41-6" morph="none" pos="word" start_char="2733">and</TOKEN>
<TOKEN end_char="2739" id="token-41-7" morph="none" pos="word" start_char="2737">did</TOKEN>
<TOKEN end_char="2743" id="token-41-8" morph="none" pos="word" start_char="2741">the</TOKEN>
<TOKEN end_char="2749" id="token-41-9" morph="none" pos="word" start_char="2745">right</TOKEN>
<TOKEN end_char="2756" id="token-41-10" morph="none" pos="word" start_char="2751">things</TOKEN>
<TOKEN end_char="2766" id="token-41-11" morph="none" pos="word" start_char="2758">prospered</TOKEN>
<TOKEN end_char="2773" id="token-41-12" morph="none" pos="word" start_char="2768">better</TOKEN>
<TOKEN end_char="2776" id="token-41-13" morph="none" pos="word" start_char="2775">in</TOKEN>
<TOKEN end_char="2781" id="token-41-14" morph="none" pos="word" start_char="2778">some</TOKEN>
<TOKEN end_char="2786" id="token-41-15" morph="none" pos="word" start_char="2783">ways</TOKEN>
</SEG>
<SEG end_char="2899" id="segment-42" start_char="2789">
<ORIGINAL_TEXT>Border control and test n trace are the two biggies, look at how long it took us and how slow we were to react.</ORIGINAL_TEXT>
<TOKEN end_char="2794" id="token-42-0" morph="none" pos="word" start_char="2789">Border</TOKEN>
<TOKEN end_char="2802" id="token-42-1" morph="none" pos="word" start_char="2796">control</TOKEN>
<TOKEN end_char="2806" id="token-42-2" morph="none" pos="word" start_char="2804">and</TOKEN>
<TOKEN end_char="2811" id="token-42-3" morph="none" pos="word" start_char="2808">test</TOKEN>
<TOKEN end_char="2813" id="token-42-4" morph="none" pos="word" start_char="2813">n</TOKEN>
<TOKEN end_char="2819" id="token-42-5" morph="none" pos="word" start_char="2815">trace</TOKEN>
<TOKEN end_char="2823" id="token-42-6" morph="none" pos="word" start_char="2821">are</TOKEN>
<TOKEN end_char="2827" id="token-42-7" morph="none" pos="word" start_char="2825">the</TOKEN>
<TOKEN end_char="2831" id="token-42-8" morph="none" pos="word" start_char="2829">two</TOKEN>
<TOKEN end_char="2839" id="token-42-9" morph="none" pos="word" start_char="2833">biggies</TOKEN>
<TOKEN end_char="2840" id="token-42-10" morph="none" pos="punct" start_char="2840">,</TOKEN>
<TOKEN end_char="2845" id="token-42-11" morph="none" pos="word" start_char="2842">look</TOKEN>
<TOKEN end_char="2848" id="token-42-12" morph="none" pos="word" start_char="2847">at</TOKEN>
<TOKEN end_char="2852" id="token-42-13" morph="none" pos="word" start_char="2850">how</TOKEN>
<TOKEN end_char="2857" id="token-42-14" morph="none" pos="word" start_char="2854">long</TOKEN>
<TOKEN end_char="2860" id="token-42-15" morph="none" pos="word" start_char="2859">it</TOKEN>
<TOKEN end_char="2865" id="token-42-16" morph="none" pos="word" start_char="2862">took</TOKEN>
<TOKEN end_char="2868" id="token-42-17" morph="none" pos="word" start_char="2867">us</TOKEN>
<TOKEN end_char="2872" id="token-42-18" morph="none" pos="word" start_char="2870">and</TOKEN>
<TOKEN end_char="2876" id="token-42-19" morph="none" pos="word" start_char="2874">how</TOKEN>
<TOKEN end_char="2881" id="token-42-20" morph="none" pos="word" start_char="2878">slow</TOKEN>
<TOKEN end_char="2884" id="token-42-21" morph="none" pos="word" start_char="2883">we</TOKEN>
<TOKEN end_char="2889" id="token-42-22" morph="none" pos="word" start_char="2886">were</TOKEN>
<TOKEN end_char="2892" id="token-42-23" morph="none" pos="word" start_char="2891">to</TOKEN>
<TOKEN end_char="2898" id="token-42-24" morph="none" pos="word" start_char="2894">react</TOKEN>
<TOKEN end_char="2899" id="token-42-25" morph="none" pos="punct" start_char="2899">.</TOKEN>
</SEG>
<SEG end_char="2935" id="segment-43" start_char="2902">
<ORIGINAL_TEXT>Sometimes I pretend I am a carrot.</ORIGINAL_TEXT>
<TOKEN end_char="2910" id="token-43-0" morph="none" pos="word" start_char="2902">Sometimes</TOKEN>
<TOKEN end_char="2912" id="token-43-1" morph="none" pos="word" start_char="2912">I</TOKEN>
<TOKEN end_char="2920" id="token-43-2" morph="none" pos="word" start_char="2914">pretend</TOKEN>
<TOKEN end_char="2922" id="token-43-3" morph="none" pos="word" start_char="2922">I</TOKEN>
<TOKEN end_char="2925" id="token-43-4" morph="none" pos="word" start_char="2924">am</TOKEN>
<TOKEN end_char="2927" id="token-43-5" morph="none" pos="word" start_char="2927">a</TOKEN>
<TOKEN end_char="2934" id="token-43-6" morph="none" pos="word" start_char="2929">carrot</TOKEN>
<TOKEN end_char="2935" id="token-43-7" morph="none" pos="punct" start_char="2935">.</TOKEN>
<TRANSLATED_TEXT>Sometimes I pretend I have a carrot.</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="3026" id="segment-44" start_char="2940">
<ORIGINAL_TEXT>BorkenArrow said: and yes had a little trouble spelling the user name when registering.</ORIGINAL_TEXT>
<TOKEN end_char="2950" id="token-44-0" morph="none" pos="word" start_char="2940">BorkenArrow</TOKEN>
<TOKEN end_char="2955" id="token-44-1" morph="none" pos="word" start_char="2952">said</TOKEN>
<TOKEN end_char="2956" id="token-44-2" morph="none" pos="punct" start_char="2956">:</TOKEN>
<TOKEN end_char="2960" id="token-44-3" morph="none" pos="word" start_char="2958">and</TOKEN>
<TOKEN end_char="2964" id="token-44-4" morph="none" pos="word" start_char="2962">yes</TOKEN>
<TOKEN end_char="2968" id="token-44-5" morph="none" pos="word" start_char="2966">had</TOKEN>
<TOKEN end_char="2970" id="token-44-6" morph="none" pos="word" start_char="2970">a</TOKEN>
<TOKEN end_char="2977" id="token-44-7" morph="none" pos="word" start_char="2972">little</TOKEN>
<TOKEN end_char="2985" id="token-44-8" morph="none" pos="word" start_char="2979">trouble</TOKEN>
<TOKEN end_char="2994" id="token-44-9" morph="none" pos="word" start_char="2987">spelling</TOKEN>
<TOKEN end_char="2998" id="token-44-10" morph="none" pos="word" start_char="2996">the</TOKEN>
<TOKEN end_char="3003" id="token-44-11" morph="none" pos="word" start_char="3000">user</TOKEN>
<TOKEN end_char="3008" id="token-44-12" morph="none" pos="word" start_char="3005">name</TOKEN>
<TOKEN end_char="3013" id="token-44-13" morph="none" pos="word" start_char="3010">when</TOKEN>
<TOKEN end_char="3025" id="token-44-14" morph="none" pos="word" start_char="3015">registering</TOKEN>
<TOKEN end_char="3026" id="token-44-15" morph="none" pos="punct" start_char="3026">.</TOKEN>
</SEG>
<SEG end_char="3060" id="segment-45" start_char="3028">
<ORIGINAL_TEXT>It caused much amusement to some.</ORIGINAL_TEXT>
<TOKEN end_char="3029" id="token-45-0" morph="none" pos="word" start_char="3028">It</TOKEN>
<TOKEN end_char="3036" id="token-45-1" morph="none" pos="word" start_char="3031">caused</TOKEN>
<TOKEN end_char="3041" id="token-45-2" morph="none" pos="word" start_char="3038">much</TOKEN>
<TOKEN end_char="3051" id="token-45-3" morph="none" pos="word" start_char="3043">amusement</TOKEN>
<TOKEN end_char="3054" id="token-45-4" morph="none" pos="word" start_char="3053">to</TOKEN>
<TOKEN end_char="3059" id="token-45-5" morph="none" pos="word" start_char="3056">some</TOKEN>
<TOKEN end_char="3060" id="token-45-6" morph="none" pos="punct" start_char="3060">.</TOKEN>
</SEG>
<SEG end_char="3161" id="segment-46" start_char="3063">
<ORIGINAL_TEXT>You can apply for a name change in your preferences if you want to correct it, or has it stuck now?</ORIGINAL_TEXT>
<TOKEN end_char="3065" id="token-46-0" morph="none" pos="word" start_char="3063">You</TOKEN>
<TOKEN end_char="3069" id="token-46-1" morph="none" pos="word" start_char="3067">can</TOKEN>
<TOKEN end_char="3075" id="token-46-2" morph="none" pos="word" start_char="3071">apply</TOKEN>
<TOKEN end_char="3079" id="token-46-3" morph="none" pos="word" start_char="3077">for</TOKEN>
<TOKEN end_char="3081" id="token-46-4" morph="none" pos="word" start_char="3081">a</TOKEN>
<TOKEN end_char="3086" id="token-46-5" morph="none" pos="word" start_char="3083">name</TOKEN>
<TOKEN end_char="3093" id="token-46-6" morph="none" pos="word" start_char="3088">change</TOKEN>
<TOKEN end_char="3096" id="token-46-7" morph="none" pos="word" start_char="3095">in</TOKEN>
<TOKEN end_char="3101" id="token-46-8" morph="none" pos="word" start_char="3098">your</TOKEN>
<TOKEN end_char="3113" id="token-46-9" morph="none" pos="word" start_char="3103">preferences</TOKEN>
<TOKEN end_char="3116" id="token-46-10" morph="none" pos="word" start_char="3115">if</TOKEN>
<TOKEN end_char="3120" id="token-46-11" morph="none" pos="word" start_char="3118">you</TOKEN>
<TOKEN end_char="3125" id="token-46-12" morph="none" pos="word" start_char="3122">want</TOKEN>
<TOKEN end_char="3128" id="token-46-13" morph="none" pos="word" start_char="3127">to</TOKEN>
<TOKEN end_char="3136" id="token-46-14" morph="none" pos="word" start_char="3130">correct</TOKEN>
<TOKEN end_char="3139" id="token-46-15" morph="none" pos="word" start_char="3138">it</TOKEN>
<TOKEN end_char="3140" id="token-46-16" morph="none" pos="punct" start_char="3140">,</TOKEN>
<TOKEN end_char="3143" id="token-46-17" morph="none" pos="word" start_char="3142">or</TOKEN>
<TOKEN end_char="3147" id="token-46-18" morph="none" pos="word" start_char="3145">has</TOKEN>
<TOKEN end_char="3150" id="token-46-19" morph="none" pos="word" start_char="3149">it</TOKEN>
<TOKEN end_char="3156" id="token-46-20" morph="none" pos="word" start_char="3152">stuck</TOKEN>
<TOKEN end_char="3160" id="token-46-21" morph="none" pos="word" start_char="3158">now</TOKEN>
<TOKEN end_char="3161" id="token-46-22" morph="none" pos="punct" start_char="3161">?</TOKEN>
</SEG>
<SEG end_char="3256" id="segment-47" start_char="3164">
<ORIGINAL_TEXT>My opinions are my own and do not represent those of the AVForums or its associated websites.</ORIGINAL_TEXT>
<TOKEN end_char="3165" id="token-47-0" morph="none" pos="word" start_char="3164">My</TOKEN>
<TOKEN end_char="3174" id="token-47-1" morph="none" pos="word" start_char="3167">opinions</TOKEN>
<TOKEN end_char="3178" id="token-47-2" morph="none" pos="word" start_char="3176">are</TOKEN>
<TOKEN end_char="3181" id="token-47-3" morph="none" pos="word" start_char="3180">my</TOKEN>
<TOKEN end_char="3185" id="token-47-4" morph="none" pos="word" start_char="3183">own</TOKEN>
<TOKEN end_char="3189" id="token-47-5" morph="none" pos="word" start_char="3187">and</TOKEN>
<TOKEN end_char="3192" id="token-47-6" morph="none" pos="word" start_char="3191">do</TOKEN>
<TOKEN end_char="3196" id="token-47-7" morph="none" pos="word" start_char="3194">not</TOKEN>
<TOKEN end_char="3206" id="token-47-8" morph="none" pos="word" start_char="3198">represent</TOKEN>
<TOKEN end_char="3212" id="token-47-9" morph="none" pos="word" start_char="3208">those</TOKEN>
<TOKEN end_char="3215" id="token-47-10" morph="none" pos="word" start_char="3214">of</TOKEN>
<TOKEN end_char="3219" id="token-47-11" morph="none" pos="word" start_char="3217">the</TOKEN>
<TOKEN end_char="3228" id="token-47-12" morph="none" pos="word" start_char="3221">AVForums</TOKEN>
<TOKEN end_char="3231" id="token-47-13" morph="none" pos="word" start_char="3230">or</TOKEN>
<TOKEN end_char="3235" id="token-47-14" morph="none" pos="word" start_char="3233">its</TOKEN>
<TOKEN end_char="3246" id="token-47-15" morph="none" pos="word" start_char="3237">associated</TOKEN>
<TOKEN end_char="3255" id="token-47-16" morph="none" pos="word" start_char="3248">websites</TOKEN>
<TOKEN end_char="3256" id="token-47-17" morph="none" pos="punct" start_char="3256">.</TOKEN>
</SEG>
<SEG end_char="3407" id="segment-48" start_char="3262">
<ORIGINAL_TEXT>BorkenArrow said: Looking at Worldometer when switched to show Asia and, with a couple exceptions, the presence of Covid outbreaks is pretty tiny.</ORIGINAL_TEXT>
<TOKEN end_char="3272" id="token-48-0" morph="none" pos="word" start_char="3262">BorkenArrow</TOKEN>
<TOKEN end_char="3277" id="token-48-1" morph="none" pos="word" start_char="3274">said</TOKEN>
<TOKEN end_char="3278" id="token-48-2" morph="none" pos="punct" start_char="3278">:</TOKEN>
<TOKEN end_char="3286" id="token-48-3" morph="none" pos="word" start_char="3280">Looking</TOKEN>
<TOKEN end_char="3289" id="token-48-4" morph="none" pos="word" start_char="3288">at</TOKEN>
<TOKEN end_char="3301" id="token-48-5" morph="none" pos="word" start_char="3291">Worldometer</TOKEN>
<TOKEN end_char="3306" id="token-48-6" morph="none" pos="word" start_char="3303">when</TOKEN>
<TOKEN end_char="3315" id="token-48-7" morph="none" pos="word" start_char="3308">switched</TOKEN>
<TOKEN end_char="3318" id="token-48-8" morph="none" pos="word" start_char="3317">to</TOKEN>
<TOKEN end_char="3323" id="token-48-9" morph="none" pos="word" start_char="3320">show</TOKEN>
<TOKEN end_char="3328" id="token-48-10" morph="none" pos="word" start_char="3325">Asia</TOKEN>
<TOKEN end_char="3332" id="token-48-11" morph="none" pos="word" start_char="3330">and</TOKEN>
<TOKEN end_char="3333" id="token-48-12" morph="none" pos="punct" start_char="3333">,</TOKEN>
<TOKEN end_char="3338" id="token-48-13" morph="none" pos="word" start_char="3335">with</TOKEN>
<TOKEN end_char="3340" id="token-48-14" morph="none" pos="word" start_char="3340">a</TOKEN>
<TOKEN end_char="3347" id="token-48-15" morph="none" pos="word" start_char="3342">couple</TOKEN>
<TOKEN end_char="3358" id="token-48-16" morph="none" pos="word" start_char="3349">exceptions</TOKEN>
<TOKEN end_char="3359" id="token-48-17" morph="none" pos="punct" start_char="3359">,</TOKEN>
<TOKEN end_char="3363" id="token-48-18" morph="none" pos="word" start_char="3361">the</TOKEN>
<TOKEN end_char="3372" id="token-48-19" morph="none" pos="word" start_char="3365">presence</TOKEN>
<TOKEN end_char="3375" id="token-48-20" morph="none" pos="word" start_char="3374">of</TOKEN>
<TOKEN end_char="3381" id="token-48-21" morph="none" pos="word" start_char="3377">Covid</TOKEN>
<TOKEN end_char="3391" id="token-48-22" morph="none" pos="word" start_char="3383">outbreaks</TOKEN>
<TOKEN end_char="3394" id="token-48-23" morph="none" pos="word" start_char="3393">is</TOKEN>
<TOKEN end_char="3401" id="token-48-24" morph="none" pos="word" start_char="3396">pretty</TOKEN>
<TOKEN end_char="3406" id="token-48-25" morph="none" pos="word" start_char="3403">tiny</TOKEN>
<TOKEN end_char="3407" id="token-48-26" morph="none" pos="punct" start_char="3407">.</TOKEN>
</SEG>
<SEG end_char="3517" id="segment-49" start_char="3410">
<ORIGINAL_TEXT>Coronavirus Update (Live): 124,390,921 Cases and 2,737,714 Deaths from COVID-19 Virus Pandemic - Worldometer</ORIGINAL_TEXT>
<TOKEN end_char="3420" id="token-49-0" morph="none" pos="word" start_char="3410">Coronavirus</TOKEN>
<TOKEN end_char="3427" id="token-49-1" morph="none" pos="word" start_char="3422">Update</TOKEN>
<TOKEN end_char="3429" id="token-49-2" morph="none" pos="punct" start_char="3429">(</TOKEN>
<TOKEN end_char="3433" id="token-49-3" morph="none" pos="word" start_char="3430">Live</TOKEN>
<TOKEN end_char="3435" id="token-49-4" morph="none" pos="punct" start_char="3434">):</TOKEN>
<TOKEN end_char="3447" id="token-49-5" morph="none" pos="unknown" start_char="3437">124,390,921</TOKEN>
<TOKEN end_char="3453" id="token-49-6" morph="none" pos="word" start_char="3449">Cases</TOKEN>
<TOKEN end_char="3457" id="token-49-7" morph="none" pos="word" start_char="3455">and</TOKEN>
<TOKEN end_char="3467" id="token-49-8" morph="none" pos="unknown" start_char="3459">2,737,714</TOKEN>
<TOKEN end_char="3474" id="token-49-9" morph="none" pos="word" start_char="3469">Deaths</TOKEN>
<TOKEN end_char="3479" id="token-49-10" morph="none" pos="word" start_char="3476">from</TOKEN>
<TOKEN end_char="3488" id="token-49-11" morph="none" pos="unknown" start_char="3481">COVID-19</TOKEN>
<TOKEN end_char="3494" id="token-49-12" morph="none" pos="word" start_char="3490">Virus</TOKEN>
<TOKEN end_char="3503" id="token-49-13" morph="none" pos="word" start_char="3496">Pandemic</TOKEN>
<TOKEN end_char="3505" id="token-49-14" morph="none" pos="punct" start_char="3505">-</TOKEN>
<TOKEN end_char="3517" id="token-49-15" morph="none" pos="word" start_char="3507">Worldometer</TOKEN>
</SEG>
<SEG end_char="3688" id="segment-50" start_char="3520">
<ORIGINAL_TEXT>Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China.</ORIGINAL_TEXT>
<TOKEN end_char="3523" id="token-50-0" morph="none" pos="word" start_char="3520">Live</TOKEN>
<TOKEN end_char="3534" id="token-50-1" morph="none" pos="word" start_char="3525">statistics</TOKEN>
<TOKEN end_char="3538" id="token-50-2" morph="none" pos="word" start_char="3536">and</TOKEN>
<TOKEN end_char="3550" id="token-50-3" morph="none" pos="word" start_char="3540">coronavirus</TOKEN>
<TOKEN end_char="3555" id="token-50-4" morph="none" pos="word" start_char="3552">news</TOKEN>
<TOKEN end_char="3564" id="token-50-5" morph="none" pos="word" start_char="3557">tracking</TOKEN>
<TOKEN end_char="3568" id="token-50-6" morph="none" pos="word" start_char="3566">the</TOKEN>
<TOKEN end_char="3575" id="token-50-7" morph="none" pos="word" start_char="3570">number</TOKEN>
<TOKEN end_char="3578" id="token-50-8" morph="none" pos="word" start_char="3577">of</TOKEN>
<TOKEN end_char="3588" id="token-50-9" morph="none" pos="word" start_char="3580">confirmed</TOKEN>
<TOKEN end_char="3594" id="token-50-10" morph="none" pos="word" start_char="3590">cases</TOKEN>
<TOKEN end_char="3595" id="token-50-11" morph="none" pos="punct" start_char="3595">,</TOKEN>
<TOKEN end_char="3605" id="token-50-12" morph="none" pos="word" start_char="3597">recovered</TOKEN>
<TOKEN end_char="3614" id="token-50-13" morph="none" pos="word" start_char="3607">patients</TOKEN>
<TOKEN end_char="3615" id="token-50-14" morph="none" pos="punct" start_char="3615">,</TOKEN>
<TOKEN end_char="3621" id="token-50-15" morph="none" pos="word" start_char="3617">tests</TOKEN>
<TOKEN end_char="3622" id="token-50-16" morph="none" pos="punct" start_char="3622">,</TOKEN>
<TOKEN end_char="3626" id="token-50-17" morph="none" pos="word" start_char="3624">and</TOKEN>
<TOKEN end_char="3632" id="token-50-18" morph="none" pos="word" start_char="3628">death</TOKEN>
<TOKEN end_char="3637" id="token-50-19" morph="none" pos="word" start_char="3634">toll</TOKEN>
<TOKEN end_char="3641" id="token-50-20" morph="none" pos="word" start_char="3639">due</TOKEN>
<TOKEN end_char="3644" id="token-50-21" morph="none" pos="word" start_char="3643">to</TOKEN>
<TOKEN end_char="3648" id="token-50-22" morph="none" pos="word" start_char="3646">the</TOKEN>
<TOKEN end_char="3657" id="token-50-23" morph="none" pos="unknown" start_char="3650">COVID-19</TOKEN>
<TOKEN end_char="3669" id="token-50-24" morph="none" pos="word" start_char="3659">coronavirus</TOKEN>
<TOKEN end_char="3674" id="token-50-25" morph="none" pos="word" start_char="3671">from</TOKEN>
<TOKEN end_char="3680" id="token-50-26" morph="none" pos="word" start_char="3676">Wuhan</TOKEN>
<TOKEN end_char="3681" id="token-50-27" morph="none" pos="punct" start_char="3681">,</TOKEN>
<TOKEN end_char="3687" id="token-50-28" morph="none" pos="word" start_char="3683">China</TOKEN>
<TOKEN end_char="3688" id="token-50-29" morph="none" pos="punct" start_char="3688">.</TOKEN>
</SEG>
<SEG end_char="3778" id="segment-51" start_char="3690">
<ORIGINAL_TEXT>Coronavirus counter with new cases, deaths, and number of tests per 1 Million population.</ORIGINAL_TEXT>
<TOKEN end_char="3700" id="token-51-0" morph="none" pos="word" start_char="3690">Coronavirus</TOKEN>
<TOKEN end_char="3708" id="token-51-1" morph="none" pos="word" start_char="3702">counter</TOKEN>
<TOKEN end_char="3713" id="token-51-2" morph="none" pos="word" start_char="3710">with</TOKEN>
<TOKEN end_char="3717" id="token-51-3" morph="none" pos="word" start_char="3715">new</TOKEN>
<TOKEN end_char="3723" id="token-51-4" morph="none" pos="word" start_char="3719">cases</TOKEN>
<TOKEN end_char="3724" id="token-51-5" morph="none" pos="punct" start_char="3724">,</TOKEN>
<TOKEN end_char="3731" id="token-51-6" morph="none" pos="word" start_char="3726">deaths</TOKEN>
<TOKEN end_char="3732" id="token-51-7" morph="none" pos="punct" start_char="3732">,</TOKEN>
<TOKEN end_char="3736" id="token-51-8" morph="none" pos="word" start_char="3734">and</TOKEN>
<TOKEN end_char="3743" id="token-51-9" morph="none" pos="word" start_char="3738">number</TOKEN>
<TOKEN end_char="3746" id="token-51-10" morph="none" pos="word" start_char="3745">of</TOKEN>
<TOKEN end_char="3752" id="token-51-11" morph="none" pos="word" start_char="3748">tests</TOKEN>
<TOKEN end_char="3756" id="token-51-12" morph="none" pos="word" start_char="3754">per</TOKEN>
<TOKEN end_char="3758" id="token-51-13" morph="none" pos="word" start_char="3758">1</TOKEN>
<TOKEN end_char="3766" id="token-51-14" morph="none" pos="word" start_char="3760">Million</TOKEN>
<TOKEN end_char="3777" id="token-51-15" morph="none" pos="word" start_char="3768">population</TOKEN>
<TOKEN end_char="3778" id="token-51-16" morph="none" pos="punct" start_char="3778">.</TOKEN>
</SEG>
<SEG end_char="3804" id="segment-52" start_char="3780">
<ORIGINAL_TEXT>Historical data and info.</ORIGINAL_TEXT>
<TOKEN end_char="3789" id="token-52-0" morph="none" pos="word" start_char="3780">Historical</TOKEN>
<TOKEN end_char="3794" id="token-52-1" morph="none" pos="word" start_char="3791">data</TOKEN>
<TOKEN end_char="3798" id="token-52-2" morph="none" pos="word" start_char="3796">and</TOKEN>
<TOKEN end_char="3803" id="token-52-3" morph="none" pos="word" start_char="3800">info</TOKEN>
<TOKEN end_char="3804" id="token-52-4" morph="none" pos="punct" start_char="3804">.</TOKEN>
<TRANSLATED_TEXT>Historische gegevens en informatie.</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="3848" id="segment-53" start_char="3806">
<ORIGINAL_TEXT>Daily... www.worldometers.info How is this?</ORIGINAL_TEXT>
<TOKEN end_char="3810" id="token-53-0" morph="none" pos="word" start_char="3806">Daily</TOKEN>
<TOKEN end_char="3813" id="token-53-1" morph="none" pos="punct" start_char="3811">...</TOKEN>
<TOKEN end_char="3835" id="token-53-2" morph="none" pos="url" start_char="3815">www.worldometers.info</TOKEN>
<TOKEN end_char="3839" id="token-53-3" morph="none" pos="word" start_char="3837">How</TOKEN>
<TOKEN end_char="3842" id="token-53-4" morph="none" pos="word" start_char="3841">is</TOKEN>
<TOKEN end_char="3847" id="token-53-5" morph="none" pos="word" start_char="3844">this</TOKEN>
<TOKEN end_char="3848" id="token-53-6" morph="none" pos="punct" start_char="3848">?</TOKEN>
</SEG>
<SEG end_char="3978" id="segment-54" start_char="3850">
<ORIGINAL_TEXT>I refuse to believe that all of those countries have better health systems or are implementing measures any better than the west.</ORIGINAL_TEXT>
<TOKEN end_char="3850" id="token-54-0" morph="none" pos="word" start_char="3850">I</TOKEN>
<TOKEN end_char="3857" id="token-54-1" morph="none" pos="word" start_char="3852">refuse</TOKEN>
<TOKEN end_char="3860" id="token-54-2" morph="none" pos="word" start_char="3859">to</TOKEN>
<TOKEN end_char="3868" id="token-54-3" morph="none" pos="word" start_char="3862">believe</TOKEN>
<TOKEN end_char="3873" id="token-54-4" morph="none" pos="word" start_char="3870">that</TOKEN>
<TOKEN end_char="3877" id="token-54-5" morph="none" pos="word" start_char="3875">all</TOKEN>
<TOKEN end_char="3880" id="token-54-6" morph="none" pos="word" start_char="3879">of</TOKEN>
<TOKEN end_char="3886" id="token-54-7" morph="none" pos="word" start_char="3882">those</TOKEN>
<TOKEN end_char="3896" id="token-54-8" morph="none" pos="word" start_char="3888">countries</TOKEN>
<TOKEN end_char="3901" id="token-54-9" morph="none" pos="word" start_char="3898">have</TOKEN>
<TOKEN end_char="3908" id="token-54-10" morph="none" pos="word" start_char="3903">better</TOKEN>
<TOKEN end_char="3915" id="token-54-11" morph="none" pos="word" start_char="3910">health</TOKEN>
<TOKEN end_char="3923" id="token-54-12" morph="none" pos="word" start_char="3917">systems</TOKEN>
<TOKEN end_char="3926" id="token-54-13" morph="none" pos="word" start_char="3925">or</TOKEN>
<TOKEN end_char="3930" id="token-54-14" morph="none" pos="word" start_char="3928">are</TOKEN>
<TOKEN end_char="3943" id="token-54-15" morph="none" pos="word" start_char="3932">implementing</TOKEN>
<TOKEN end_char="3952" id="token-54-16" morph="none" pos="word" start_char="3945">measures</TOKEN>
<TOKEN end_char="3956" id="token-54-17" morph="none" pos="word" start_char="3954">any</TOKEN>
<TOKEN end_char="3963" id="token-54-18" morph="none" pos="word" start_char="3958">better</TOKEN>
<TOKEN end_char="3968" id="token-54-19" morph="none" pos="word" start_char="3965">than</TOKEN>
<TOKEN end_char="3972" id="token-54-20" morph="none" pos="word" start_char="3970">the</TOKEN>
<TOKEN end_char="3977" id="token-54-21" morph="none" pos="word" start_char="3974">west</TOKEN>
<TOKEN end_char="3978" id="token-54-22" morph="none" pos="punct" start_char="3978">.</TOKEN>
</SEG>
<SEG end_char="4058" id="segment-55" start_char="3980">
<ORIGINAL_TEXT>Either they are all lying with their stats or there is something else going on.</ORIGINAL_TEXT>
<TOKEN end_char="3985" id="token-55-0" morph="none" pos="word" start_char="3980">Either</TOKEN>
<TOKEN end_char="3990" id="token-55-1" morph="none" pos="word" start_char="3987">they</TOKEN>
<TOKEN end_char="3994" id="token-55-2" morph="none" pos="word" start_char="3992">are</TOKEN>
<TOKEN end_char="3998" id="token-55-3" morph="none" pos="word" start_char="3996">all</TOKEN>
<TOKEN end_char="4004" id="token-55-4" morph="none" pos="word" start_char="4000">lying</TOKEN>
<TOKEN end_char="4009" id="token-55-5" morph="none" pos="word" start_char="4006">with</TOKEN>
<TOKEN end_char="4015" id="token-55-6" morph="none" pos="word" start_char="4011">their</TOKEN>
<TOKEN end_char="4021" id="token-55-7" morph="none" pos="word" start_char="4017">stats</TOKEN>
<TOKEN end_char="4024" id="token-55-8" morph="none" pos="word" start_char="4023">or</TOKEN>
<TOKEN end_char="4030" id="token-55-9" morph="none" pos="word" start_char="4026">there</TOKEN>
<TOKEN end_char="4033" id="token-55-10" morph="none" pos="word" start_char="4032">is</TOKEN>
<TOKEN end_char="4043" id="token-55-11" morph="none" pos="word" start_char="4035">something</TOKEN>
<TOKEN end_char="4048" id="token-55-12" morph="none" pos="word" start_char="4045">else</TOKEN>
<TOKEN end_char="4054" id="token-55-13" morph="none" pos="word" start_char="4050">going</TOKEN>
<TOKEN end_char="4057" id="token-55-14" morph="none" pos="word" start_char="4056">on</TOKEN>
<TOKEN end_char="4058" id="token-55-15" morph="none" pos="punct" start_char="4058">.</TOKEN>
</SEG>
<SEG end_char="4077" id="segment-56" start_char="4060">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="4064" id="token-56-0" morph="none" pos="word" start_char="4060">Click</TOKEN>
<TOKEN end_char="4067" id="token-56-1" morph="none" pos="word" start_char="4066">to</TOKEN>
<TOKEN end_char="4074" id="token-56-2" morph="none" pos="word" start_char="4069">expand</TOKEN>
<TOKEN end_char="4077" id="token-56-3" morph="none" pos="punct" start_char="4075">...</TOKEN>
</SEG>
<SEG end_char="4195" id="segment-57" start_char="4080">
<ORIGINAL_TEXT>The closer a country is to both the equator and the international date line the lower the incidence it has of covid.</ORIGINAL_TEXT>
<TOKEN end_char="4082" id="token-57-0" morph="none" pos="word" start_char="4080">The</TOKEN>
<TOKEN end_char="4089" id="token-57-1" morph="none" pos="word" start_char="4084">closer</TOKEN>
<TOKEN end_char="4091" id="token-57-2" morph="none" pos="word" start_char="4091">a</TOKEN>
<TOKEN end_char="4099" id="token-57-3" morph="none" pos="word" start_char="4093">country</TOKEN>
<TOKEN end_char="4102" id="token-57-4" morph="none" pos="word" start_char="4101">is</TOKEN>
<TOKEN end_char="4105" id="token-57-5" morph="none" pos="word" start_char="4104">to</TOKEN>
<TOKEN end_char="4110" id="token-57-6" morph="none" pos="word" start_char="4107">both</TOKEN>
<TOKEN end_char="4114" id="token-57-7" morph="none" pos="word" start_char="4112">the</TOKEN>
<TOKEN end_char="4122" id="token-57-8" morph="none" pos="word" start_char="4116">equator</TOKEN>
<TOKEN end_char="4126" id="token-57-9" morph="none" pos="word" start_char="4124">and</TOKEN>
<TOKEN end_char="4130" id="token-57-10" morph="none" pos="word" start_char="4128">the</TOKEN>
<TOKEN end_char="4144" id="token-57-11" morph="none" pos="word" start_char="4132">international</TOKEN>
<TOKEN end_char="4149" id="token-57-12" morph="none" pos="word" start_char="4146">date</TOKEN>
<TOKEN end_char="4154" id="token-57-13" morph="none" pos="word" start_char="4151">line</TOKEN>
<TOKEN end_char="4158" id="token-57-14" morph="none" pos="word" start_char="4156">the</TOKEN>
<TOKEN end_char="4164" id="token-57-15" morph="none" pos="word" start_char="4160">lower</TOKEN>
<TOKEN end_char="4168" id="token-57-16" morph="none" pos="word" start_char="4166">the</TOKEN>
<TOKEN end_char="4178" id="token-57-17" morph="none" pos="word" start_char="4170">incidence</TOKEN>
<TOKEN end_char="4181" id="token-57-18" morph="none" pos="word" start_char="4180">it</TOKEN>
<TOKEN end_char="4185" id="token-57-19" morph="none" pos="word" start_char="4183">has</TOKEN>
<TOKEN end_char="4188" id="token-57-20" morph="none" pos="word" start_char="4187">of</TOKEN>
<TOKEN end_char="4194" id="token-57-21" morph="none" pos="word" start_char="4190">covid</TOKEN>
<TOKEN end_char="4195" id="token-57-22" morph="none" pos="punct" start_char="4195">.</TOKEN>
</SEG>
<SEG end_char="4299" id="segment-58" start_char="4197">
<ORIGINAL_TEXT>Of the USA states the ones with two out of the three lowest deaths per million fall into that category.</ORIGINAL_TEXT>
<TOKEN end_char="4198" id="token-58-0" morph="none" pos="word" start_char="4197">Of</TOKEN>
<TOKEN end_char="4202" id="token-58-1" morph="none" pos="word" start_char="4200">the</TOKEN>
<TOKEN end_char="4206" id="token-58-2" morph="none" pos="word" start_char="4204">USA</TOKEN>
<TOKEN end_char="4213" id="token-58-3" morph="none" pos="word" start_char="4208">states</TOKEN>
<TOKEN end_char="4217" id="token-58-4" morph="none" pos="word" start_char="4215">the</TOKEN>
<TOKEN end_char="4222" id="token-58-5" morph="none" pos="word" start_char="4219">ones</TOKEN>
<TOKEN end_char="4227" id="token-58-6" morph="none" pos="word" start_char="4224">with</TOKEN>
<TOKEN end_char="4231" id="token-58-7" morph="none" pos="word" start_char="4229">two</TOKEN>
<TOKEN end_char="4235" id="token-58-8" morph="none" pos="word" start_char="4233">out</TOKEN>
<TOKEN end_char="4238" id="token-58-9" morph="none" pos="word" start_char="4237">of</TOKEN>
<TOKEN end_char="4242" id="token-58-10" morph="none" pos="word" start_char="4240">the</TOKEN>
<TOKEN end_char="4248" id="token-58-11" morph="none" pos="word" start_char="4244">three</TOKEN>
<TOKEN end_char="4255" id="token-58-12" morph="none" pos="word" start_char="4250">lowest</TOKEN>
<TOKEN end_char="4262" id="token-58-13" morph="none" pos="word" start_char="4257">deaths</TOKEN>
<TOKEN end_char="4266" id="token-58-14" morph="none" pos="word" start_char="4264">per</TOKEN>
<TOKEN end_char="4274" id="token-58-15" morph="none" pos="word" start_char="4268">million</TOKEN>
<TOKEN end_char="4279" id="token-58-16" morph="none" pos="word" start_char="4276">fall</TOKEN>
<TOKEN end_char="4284" id="token-58-17" morph="none" pos="word" start_char="4281">into</TOKEN>
<TOKEN end_char="4289" id="token-58-18" morph="none" pos="word" start_char="4286">that</TOKEN>
<TOKEN end_char="4298" id="token-58-19" morph="none" pos="word" start_char="4291">category</TOKEN>
<TOKEN end_char="4299" id="token-58-20" morph="none" pos="punct" start_char="4299">.</TOKEN>
</SEG>
<SEG end_char="4331" id="segment-59" start_char="4301">
<ORIGINAL_TEXT>Correlation is not causation...</ORIGINAL_TEXT>
<TOKEN end_char="4311" id="token-59-0" morph="none" pos="word" start_char="4301">Correlation</TOKEN>
<TOKEN end_char="4314" id="token-59-1" morph="none" pos="word" start_char="4313">is</TOKEN>
<TOKEN end_char="4318" id="token-59-2" morph="none" pos="word" start_char="4316">not</TOKEN>
<TOKEN end_char="4328" id="token-59-3" morph="none" pos="word" start_char="4320">causation</TOKEN>
<TOKEN end_char="4331" id="token-59-4" morph="none" pos="punct" start_char="4329">...</TOKEN>
</SEG>
<SEG end_char="4461" id="segment-60" start_char="4337">
<ORIGINAL_TEXT>realfrankturner said: I said school name, from the article, not user name So if you don't believe it, why do you think it is?</ORIGINAL_TEXT>
<TOKEN end_char="4351" id="token-60-0" morph="none" pos="word" start_char="4337">realfrankturner</TOKEN>
<TOKEN end_char="4356" id="token-60-1" morph="none" pos="word" start_char="4353">said</TOKEN>
<TOKEN end_char="4357" id="token-60-2" morph="none" pos="punct" start_char="4357">:</TOKEN>
<TOKEN end_char="4359" id="token-60-3" morph="none" pos="word" start_char="4359">I</TOKEN>
<TOKEN end_char="4364" id="token-60-4" morph="none" pos="word" start_char="4361">said</TOKEN>
<TOKEN end_char="4371" id="token-60-5" morph="none" pos="word" start_char="4366">school</TOKEN>
<TOKEN end_char="4376" id="token-60-6" morph="none" pos="word" start_char="4373">name</TOKEN>
<TOKEN end_char="4377" id="token-60-7" morph="none" pos="punct" start_char="4377">,</TOKEN>
<TOKEN end_char="4382" id="token-60-8" morph="none" pos="word" start_char="4379">from</TOKEN>
<TOKEN end_char="4386" id="token-60-9" morph="none" pos="word" start_char="4384">the</TOKEN>
<TOKEN end_char="4394" id="token-60-10" morph="none" pos="word" start_char="4388">article</TOKEN>
<TOKEN end_char="4395" id="token-60-11" morph="none" pos="punct" start_char="4395">,</TOKEN>
<TOKEN end_char="4399" id="token-60-12" morph="none" pos="word" start_char="4397">not</TOKEN>
<TOKEN end_char="4404" id="token-60-13" morph="none" pos="word" start_char="4401">user</TOKEN>
<TOKEN end_char="4409" id="token-60-14" morph="none" pos="word" start_char="4406">name</TOKEN>
<TOKEN end_char="4412" id="token-60-15" morph="none" pos="word" start_char="4411">So</TOKEN>
<TOKEN end_char="4415" id="token-60-16" morph="none" pos="word" start_char="4414">if</TOKEN>
<TOKEN end_char="4419" id="token-60-17" morph="none" pos="word" start_char="4417">you</TOKEN>
<TOKEN end_char="4425" id="token-60-18" morph="none" pos="word" start_char="4421">don't</TOKEN>
<TOKEN end_char="4433" id="token-60-19" morph="none" pos="word" start_char="4427">believe</TOKEN>
<TOKEN end_char="4436" id="token-60-20" morph="none" pos="word" start_char="4435">it</TOKEN>
<TOKEN end_char="4437" id="token-60-21" morph="none" pos="punct" start_char="4437">,</TOKEN>
<TOKEN end_char="4441" id="token-60-22" morph="none" pos="word" start_char="4439">why</TOKEN>
<TOKEN end_char="4444" id="token-60-23" morph="none" pos="word" start_char="4443">do</TOKEN>
<TOKEN end_char="4448" id="token-60-24" morph="none" pos="word" start_char="4446">you</TOKEN>
<TOKEN end_char="4454" id="token-60-25" morph="none" pos="word" start_char="4450">think</TOKEN>
<TOKEN end_char="4457" id="token-60-26" morph="none" pos="word" start_char="4456">it</TOKEN>
<TOKEN end_char="4460" id="token-60-27" morph="none" pos="word" start_char="4459">is</TOKEN>
<TOKEN end_char="4461" id="token-60-28" morph="none" pos="punct" start_char="4461">?</TOKEN>
</SEG>
<SEG end_char="4500" id="segment-61" start_char="4464">
<ORIGINAL_TEXT>The article says why it thinks it is.</ORIGINAL_TEXT>
<TOKEN end_char="4466" id="token-61-0" morph="none" pos="word" start_char="4464">The</TOKEN>
<TOKEN end_char="4474" id="token-61-1" morph="none" pos="word" start_char="4468">article</TOKEN>
<TOKEN end_char="4479" id="token-61-2" morph="none" pos="word" start_char="4476">says</TOKEN>
<TOKEN end_char="4483" id="token-61-3" morph="none" pos="word" start_char="4481">why</TOKEN>
<TOKEN end_char="4486" id="token-61-4" morph="none" pos="word" start_char="4485">it</TOKEN>
<TOKEN end_char="4493" id="token-61-5" morph="none" pos="word" start_char="4488">thinks</TOKEN>
<TOKEN end_char="4496" id="token-61-6" morph="none" pos="word" start_char="4495">it</TOKEN>
<TOKEN end_char="4499" id="token-61-7" morph="none" pos="word" start_char="4498">is</TOKEN>
<TOKEN end_char="4500" id="token-61-8" morph="none" pos="punct" start_char="4500">.</TOKEN>
</SEG>
<SEG end_char="4519" id="segment-62" start_char="4503">
<ORIGINAL_TEXT>Genetic immunity.</ORIGINAL_TEXT>
<TOKEN end_char="4509" id="token-62-0" morph="none" pos="word" start_char="4503">Genetic</TOKEN>
<TOKEN end_char="4518" id="token-62-1" morph="none" pos="word" start_char="4511">immunity</TOKEN>
<TOKEN end_char="4519" id="token-62-2" morph="none" pos="punct" start_char="4519">.</TOKEN>
</SEG>
<SEG end_char="4637" id="segment-63" start_char="4522">
<ORIGINAL_TEXT>Sounds the most rational because there is no way every single country managed to lock down early and test and trace.</ORIGINAL_TEXT>
<TOKEN end_char="4527" id="token-63-0" morph="none" pos="word" start_char="4522">Sounds</TOKEN>
<TOKEN end_char="4531" id="token-63-1" morph="none" pos="word" start_char="4529">the</TOKEN>
<TOKEN end_char="4536" id="token-63-2" morph="none" pos="word" start_char="4533">most</TOKEN>
<TOKEN end_char="4545" id="token-63-3" morph="none" pos="word" start_char="4538">rational</TOKEN>
<TOKEN end_char="4553" id="token-63-4" morph="none" pos="word" start_char="4547">because</TOKEN>
<TOKEN end_char="4559" id="token-63-5" morph="none" pos="word" start_char="4555">there</TOKEN>
<TOKEN end_char="4562" id="token-63-6" morph="none" pos="word" start_char="4561">is</TOKEN>
<TOKEN end_char="4565" id="token-63-7" morph="none" pos="word" start_char="4564">no</TOKEN>
<TOKEN end_char="4569" id="token-63-8" morph="none" pos="word" start_char="4567">way</TOKEN>
<TOKEN end_char="4575" id="token-63-9" morph="none" pos="word" start_char="4571">every</TOKEN>
<TOKEN end_char="4582" id="token-63-10" morph="none" pos="word" start_char="4577">single</TOKEN>
<TOKEN end_char="4590" id="token-63-11" morph="none" pos="word" start_char="4584">country</TOKEN>
<TOKEN end_char="4598" id="token-63-12" morph="none" pos="word" start_char="4592">managed</TOKEN>
<TOKEN end_char="4601" id="token-63-13" morph="none" pos="word" start_char="4600">to</TOKEN>
<TOKEN end_char="4606" id="token-63-14" morph="none" pos="word" start_char="4603">lock</TOKEN>
<TOKEN end_char="4611" id="token-63-15" morph="none" pos="word" start_char="4608">down</TOKEN>
<TOKEN end_char="4617" id="token-63-16" morph="none" pos="word" start_char="4613">early</TOKEN>
<TOKEN end_char="4621" id="token-63-17" morph="none" pos="word" start_char="4619">and</TOKEN>
<TOKEN end_char="4626" id="token-63-18" morph="none" pos="word" start_char="4623">test</TOKEN>
<TOKEN end_char="4630" id="token-63-19" morph="none" pos="word" start_char="4628">and</TOKEN>
<TOKEN end_char="4636" id="token-63-20" morph="none" pos="word" start_char="4632">trace</TOKEN>
<TOKEN end_char="4637" id="token-63-21" morph="none" pos="punct" start_char="4637">.</TOKEN>
</SEG>
<SEG end_char="4697" id="segment-64" start_char="4643">
<ORIGINAL_TEXT>BorkenArrow said: The article says why it thinks it is.</ORIGINAL_TEXT>
<TOKEN end_char="4653" id="token-64-0" morph="none" pos="word" start_char="4643">BorkenArrow</TOKEN>
<TOKEN end_char="4658" id="token-64-1" morph="none" pos="word" start_char="4655">said</TOKEN>
<TOKEN end_char="4659" id="token-64-2" morph="none" pos="punct" start_char="4659">:</TOKEN>
<TOKEN end_char="4663" id="token-64-3" morph="none" pos="word" start_char="4661">The</TOKEN>
<TOKEN end_char="4671" id="token-64-4" morph="none" pos="word" start_char="4665">article</TOKEN>
<TOKEN end_char="4676" id="token-64-5" morph="none" pos="word" start_char="4673">says</TOKEN>
<TOKEN end_char="4680" id="token-64-6" morph="none" pos="word" start_char="4678">why</TOKEN>
<TOKEN end_char="4683" id="token-64-7" morph="none" pos="word" start_char="4682">it</TOKEN>
<TOKEN end_char="4690" id="token-64-8" morph="none" pos="word" start_char="4685">thinks</TOKEN>
<TOKEN end_char="4693" id="token-64-9" morph="none" pos="word" start_char="4692">it</TOKEN>
<TOKEN end_char="4696" id="token-64-10" morph="none" pos="word" start_char="4695">is</TOKEN>
<TOKEN end_char="4697" id="token-64-11" morph="none" pos="punct" start_char="4697">.</TOKEN>
</SEG>
<SEG end_char="4715" id="segment-65" start_char="4699">
<ORIGINAL_TEXT>Genetic immunity.</ORIGINAL_TEXT>
<TOKEN end_char="4705" id="token-65-0" morph="none" pos="word" start_char="4699">Genetic</TOKEN>
<TOKEN end_char="4714" id="token-65-1" morph="none" pos="word" start_char="4707">immunity</TOKEN>
<TOKEN end_char="4715" id="token-65-2" morph="none" pos="punct" start_char="4715">.</TOKEN>
</SEG>
<SEG end_char="4803" id="segment-66" start_char="4718">
<ORIGINAL_TEXT>Right, wasn't sure if that's what you believed or just quoting a bit from the article.</ORIGINAL_TEXT>
<TOKEN end_char="4722" id="token-66-0" morph="none" pos="word" start_char="4718">Right</TOKEN>
<TOKEN end_char="4723" id="token-66-1" morph="none" pos="punct" start_char="4723">,</TOKEN>
<TOKEN end_char="4730" id="token-66-2" morph="none" pos="word" start_char="4725">wasn't</TOKEN>
<TOKEN end_char="4735" id="token-66-3" morph="none" pos="word" start_char="4732">sure</TOKEN>
<TOKEN end_char="4738" id="token-66-4" morph="none" pos="word" start_char="4737">if</TOKEN>
<TOKEN end_char="4745" id="token-66-5" morph="none" pos="word" start_char="4740">that's</TOKEN>
<TOKEN end_char="4750" id="token-66-6" morph="none" pos="word" start_char="4747">what</TOKEN>
<TOKEN end_char="4754" id="token-66-7" morph="none" pos="word" start_char="4752">you</TOKEN>
<TOKEN end_char="4763" id="token-66-8" morph="none" pos="word" start_char="4756">believed</TOKEN>
<TOKEN end_char="4766" id="token-66-9" morph="none" pos="word" start_char="4765">or</TOKEN>
<TOKEN end_char="4771" id="token-66-10" morph="none" pos="word" start_char="4768">just</TOKEN>
<TOKEN end_char="4779" id="token-66-11" morph="none" pos="word" start_char="4773">quoting</TOKEN>
<TOKEN end_char="4781" id="token-66-12" morph="none" pos="word" start_char="4781">a</TOKEN>
<TOKEN end_char="4785" id="token-66-13" morph="none" pos="word" start_char="4783">bit</TOKEN>
<TOKEN end_char="4790" id="token-66-14" morph="none" pos="word" start_char="4787">from</TOKEN>
<TOKEN end_char="4794" id="token-66-15" morph="none" pos="word" start_char="4792">the</TOKEN>
<TOKEN end_char="4802" id="token-66-16" morph="none" pos="word" start_char="4796">article</TOKEN>
<TOKEN end_char="4803" id="token-66-17" morph="none" pos="punct" start_char="4803">.</TOKEN>
</SEG>
<SEG end_char="4839" id="segment-67" start_char="4806">
<ORIGINAL_TEXT>Sometimes I pretend I am a carrot.</ORIGINAL_TEXT>
<TOKEN end_char="4814" id="token-67-0" morph="none" pos="word" start_char="4806">Sometimes</TOKEN>
<TOKEN end_char="4816" id="token-67-1" morph="none" pos="word" start_char="4816">I</TOKEN>
<TOKEN end_char="4824" id="token-67-2" morph="none" pos="word" start_char="4818">pretend</TOKEN>
<TOKEN end_char="4826" id="token-67-3" morph="none" pos="word" start_char="4826">I</TOKEN>
<TOKEN end_char="4829" id="token-67-4" morph="none" pos="word" start_char="4828">am</TOKEN>
<TOKEN end_char="4831" id="token-67-5" morph="none" pos="word" start_char="4831">a</TOKEN>
<TOKEN end_char="4838" id="token-67-6" morph="none" pos="word" start_char="4833">carrot</TOKEN>
<TOKEN end_char="4839" id="token-67-7" morph="none" pos="punct" start_char="4839">.</TOKEN>
</SEG>
<SEG end_char="4958" id="segment-68" start_char="4844">
<ORIGINAL_TEXT>IronGiant said: You can apply for a name change in your preferences if you want to correct it, or has it stuck now?</ORIGINAL_TEXT>
<TOKEN end_char="4852" id="token-68-0" morph="none" pos="word" start_char="4844">IronGiant</TOKEN>
<TOKEN end_char="4857" id="token-68-1" morph="none" pos="word" start_char="4854">said</TOKEN>
<TOKEN end_char="4858" id="token-68-2" morph="none" pos="punct" start_char="4858">:</TOKEN>
<TOKEN end_char="4862" id="token-68-3" morph="none" pos="word" start_char="4860">You</TOKEN>
<TOKEN end_char="4866" id="token-68-4" morph="none" pos="word" start_char="4864">can</TOKEN>
<TOKEN end_char="4872" id="token-68-5" morph="none" pos="word" start_char="4868">apply</TOKEN>
<TOKEN end_char="4876" id="token-68-6" morph="none" pos="word" start_char="4874">for</TOKEN>
<TOKEN end_char="4878" id="token-68-7" morph="none" pos="word" start_char="4878">a</TOKEN>
<TOKEN end_char="4883" id="token-68-8" morph="none" pos="word" start_char="4880">name</TOKEN>
<TOKEN end_char="4890" id="token-68-9" morph="none" pos="word" start_char="4885">change</TOKEN>
<TOKEN end_char="4893" id="token-68-10" morph="none" pos="word" start_char="4892">in</TOKEN>
<TOKEN end_char="4898" id="token-68-11" morph="none" pos="word" start_char="4895">your</TOKEN>
<TOKEN end_char="4910" id="token-68-12" morph="none" pos="word" start_char="4900">preferences</TOKEN>
<TOKEN end_char="4913" id="token-68-13" morph="none" pos="word" start_char="4912">if</TOKEN>
<TOKEN end_char="4917" id="token-68-14" morph="none" pos="word" start_char="4915">you</TOKEN>
<TOKEN end_char="4922" id="token-68-15" morph="none" pos="word" start_char="4919">want</TOKEN>
<TOKEN end_char="4925" id="token-68-16" morph="none" pos="word" start_char="4924">to</TOKEN>
<TOKEN end_char="4933" id="token-68-17" morph="none" pos="word" start_char="4927">correct</TOKEN>
<TOKEN end_char="4936" id="token-68-18" morph="none" pos="word" start_char="4935">it</TOKEN>
<TOKEN end_char="4937" id="token-68-19" morph="none" pos="punct" start_char="4937">,</TOKEN>
<TOKEN end_char="4940" id="token-68-20" morph="none" pos="word" start_char="4939">or</TOKEN>
<TOKEN end_char="4944" id="token-68-21" morph="none" pos="word" start_char="4942">has</TOKEN>
<TOKEN end_char="4947" id="token-68-22" morph="none" pos="word" start_char="4946">it</TOKEN>
<TOKEN end_char="4953" id="token-68-23" morph="none" pos="word" start_char="4949">stuck</TOKEN>
<TOKEN end_char="4957" id="token-68-24" morph="none" pos="word" start_char="4955">now</TOKEN>
<TOKEN end_char="4958" id="token-68-25" morph="none" pos="punct" start_char="4958">?</TOKEN>
</SEG>
<SEG end_char="4990" id="segment-69" start_char="4962">
<ORIGINAL_TEXT>Thanks, I got used to it now.</ORIGINAL_TEXT>
<TOKEN end_char="4967" id="token-69-0" morph="none" pos="word" start_char="4962">Thanks</TOKEN>
<TOKEN end_char="4968" id="token-69-1" morph="none" pos="punct" start_char="4968">,</TOKEN>
<TOKEN end_char="4970" id="token-69-2" morph="none" pos="word" start_char="4970">I</TOKEN>
<TOKEN end_char="4974" id="token-69-3" morph="none" pos="word" start_char="4972">got</TOKEN>
<TOKEN end_char="4979" id="token-69-4" morph="none" pos="word" start_char="4976">used</TOKEN>
<TOKEN end_char="4982" id="token-69-5" morph="none" pos="word" start_char="4981">to</TOKEN>
<TOKEN end_char="4985" id="token-69-6" morph="none" pos="word" start_char="4984">it</TOKEN>
<TOKEN end_char="4989" id="token-69-7" morph="none" pos="word" start_char="4987">now</TOKEN>
<TOKEN end_char="4990" id="token-69-8" morph="none" pos="punct" start_char="4990">.</TOKEN>
</SEG>
<SEG end_char="5171" id="segment-70" start_char="4994">
<ORIGINAL_TEXT>A big thing is it's been a constant threat for the Far East and they've been through SARS and MERS so much easier to convince a populace to take basic measures and do it quickly.</ORIGINAL_TEXT>
<TOKEN end_char="4994" id="token-70-0" morph="none" pos="word" start_char="4994">A</TOKEN>
<TOKEN end_char="4998" id="token-70-1" morph="none" pos="word" start_char="4996">big</TOKEN>
<TOKEN end_char="5004" id="token-70-2" morph="none" pos="word" start_char="5000">thing</TOKEN>
<TOKEN end_char="5007" id="token-70-3" morph="none" pos="word" start_char="5006">is</TOKEN>
<TOKEN end_char="5012" id="token-70-4" morph="none" pos="word" start_char="5009">it's</TOKEN>
<TOKEN end_char="5017" id="token-70-5" morph="none" pos="word" start_char="5014">been</TOKEN>
<TOKEN end_char="5019" id="token-70-6" morph="none" pos="word" start_char="5019">a</TOKEN>
<TOKEN end_char="5028" id="token-70-7" morph="none" pos="word" start_char="5021">constant</TOKEN>
<TOKEN end_char="5035" id="token-70-8" morph="none" pos="word" start_char="5030">threat</TOKEN>
<TOKEN end_char="5039" id="token-70-9" morph="none" pos="word" start_char="5037">for</TOKEN>
<TOKEN end_char="5043" id="token-70-10" morph="none" pos="word" start_char="5041">the</TOKEN>
<TOKEN end_char="5047" id="token-70-11" morph="none" pos="word" start_char="5045">Far</TOKEN>
<TOKEN end_char="5052" id="token-70-12" morph="none" pos="word" start_char="5049">East</TOKEN>
<TOKEN end_char="5056" id="token-70-13" morph="none" pos="word" start_char="5054">and</TOKEN>
<TOKEN end_char="5064" id="token-70-14" morph="none" pos="word" start_char="5058">they've</TOKEN>
<TOKEN end_char="5069" id="token-70-15" morph="none" pos="word" start_char="5066">been</TOKEN>
<TOKEN end_char="5077" id="token-70-16" morph="none" pos="word" start_char="5071">through</TOKEN>
<TOKEN end_char="5082" id="token-70-17" morph="none" pos="word" start_char="5079">SARS</TOKEN>
<TOKEN end_char="5086" id="token-70-18" morph="none" pos="word" start_char="5084">and</TOKEN>
<TOKEN end_char="5091" id="token-70-19" morph="none" pos="word" start_char="5088">MERS</TOKEN>
<TOKEN end_char="5094" id="token-70-20" morph="none" pos="word" start_char="5093">so</TOKEN>
<TOKEN end_char="5099" id="token-70-21" morph="none" pos="word" start_char="5096">much</TOKEN>
<TOKEN end_char="5106" id="token-70-22" morph="none" pos="word" start_char="5101">easier</TOKEN>
<TOKEN end_char="5109" id="token-70-23" morph="none" pos="word" start_char="5108">to</TOKEN>
<TOKEN end_char="5118" id="token-70-24" morph="none" pos="word" start_char="5111">convince</TOKEN>
<TOKEN end_char="5120" id="token-70-25" morph="none" pos="word" start_char="5120">a</TOKEN>
<TOKEN end_char="5129" id="token-70-26" morph="none" pos="word" start_char="5122">populace</TOKEN>
<TOKEN end_char="5132" id="token-70-27" morph="none" pos="word" start_char="5131">to</TOKEN>
<TOKEN end_char="5137" id="token-70-28" morph="none" pos="word" start_char="5134">take</TOKEN>
<TOKEN end_char="5143" id="token-70-29" morph="none" pos="word" start_char="5139">basic</TOKEN>
<TOKEN end_char="5152" id="token-70-30" morph="none" pos="word" start_char="5145">measures</TOKEN>
<TOKEN end_char="5156" id="token-70-31" morph="none" pos="word" start_char="5154">and</TOKEN>
<TOKEN end_char="5159" id="token-70-32" morph="none" pos="word" start_char="5158">do</TOKEN>
<TOKEN end_char="5162" id="token-70-33" morph="none" pos="word" start_char="5161">it</TOKEN>
<TOKEN end_char="5170" id="token-70-34" morph="none" pos="word" start_char="5164">quickly</TOKEN>
<TOKEN end_char="5171" id="token-70-35" morph="none" pos="punct" start_char="5171">.</TOKEN>
</SEG>
<SEG end_char="5298" id="segment-71" start_char="5174">
<ORIGINAL_TEXT>There's also something to be said for building up a very strong immune system due to poor quality sanitation for generations.</ORIGINAL_TEXT>
<TOKEN end_char="5180" id="token-71-0" morph="none" pos="word" start_char="5174">There's</TOKEN>
<TOKEN end_char="5185" id="token-71-1" morph="none" pos="word" start_char="5182">also</TOKEN>
<TOKEN end_char="5195" id="token-71-2" morph="none" pos="word" start_char="5187">something</TOKEN>
<TOKEN end_char="5198" id="token-71-3" morph="none" pos="word" start_char="5197">to</TOKEN>
<TOKEN end_char="5201" id="token-71-4" morph="none" pos="word" start_char="5200">be</TOKEN>
<TOKEN end_char="5206" id="token-71-5" morph="none" pos="word" start_char="5203">said</TOKEN>
<TOKEN end_char="5210" id="token-71-6" morph="none" pos="word" start_char="5208">for</TOKEN>
<TOKEN end_char="5219" id="token-71-7" morph="none" pos="word" start_char="5212">building</TOKEN>
<TOKEN end_char="5222" id="token-71-8" morph="none" pos="word" start_char="5221">up</TOKEN>
<TOKEN end_char="5224" id="token-71-9" morph="none" pos="word" start_char="5224">a</TOKEN>
<TOKEN end_char="5229" id="token-71-10" morph="none" pos="word" start_char="5226">very</TOKEN>
<TOKEN end_char="5236" id="token-71-11" morph="none" pos="word" start_char="5231">strong</TOKEN>
<TOKEN end_char="5243" id="token-71-12" morph="none" pos="word" start_char="5238">immune</TOKEN>
<TOKEN end_char="5250" id="token-71-13" morph="none" pos="word" start_char="5245">system</TOKEN>
<TOKEN end_char="5254" id="token-71-14" morph="none" pos="word" start_char="5252">due</TOKEN>
<TOKEN end_char="5257" id="token-71-15" morph="none" pos="word" start_char="5256">to</TOKEN>
<TOKEN end_char="5262" id="token-71-16" morph="none" pos="word" start_char="5259">poor</TOKEN>
<TOKEN end_char="5270" id="token-71-17" morph="none" pos="word" start_char="5264">quality</TOKEN>
<TOKEN end_char="5281" id="token-71-18" morph="none" pos="word" start_char="5272">sanitation</TOKEN>
<TOKEN end_char="5285" id="token-71-19" morph="none" pos="word" start_char="5283">for</TOKEN>
<TOKEN end_char="5297" id="token-71-20" morph="none" pos="word" start_char="5287">generations</TOKEN>
<TOKEN end_char="5298" id="token-71-21" morph="none" pos="punct" start_char="5298">.</TOKEN>
</SEG>
<SEG end_char="5382" id="segment-72" start_char="5301">
<ORIGINAL_TEXT>We probably over use bleach at home at a guess, it certainly has its place though.</ORIGINAL_TEXT>
<TOKEN end_char="5302" id="token-72-0" morph="none" pos="word" start_char="5301">We</TOKEN>
<TOKEN end_char="5311" id="token-72-1" morph="none" pos="word" start_char="5304">probably</TOKEN>
<TOKEN end_char="5316" id="token-72-2" morph="none" pos="word" start_char="5313">over</TOKEN>
<TOKEN end_char="5320" id="token-72-3" morph="none" pos="word" start_char="5318">use</TOKEN>
<TOKEN end_char="5327" id="token-72-4" morph="none" pos="word" start_char="5322">bleach</TOKEN>
<TOKEN end_char="5330" id="token-72-5" morph="none" pos="word" start_char="5329">at</TOKEN>
<TOKEN end_char="5335" id="token-72-6" morph="none" pos="word" start_char="5332">home</TOKEN>
<TOKEN end_char="5338" id="token-72-7" morph="none" pos="word" start_char="5337">at</TOKEN>
<TOKEN end_char="5340" id="token-72-8" morph="none" pos="word" start_char="5340">a</TOKEN>
<TOKEN end_char="5346" id="token-72-9" morph="none" pos="word" start_char="5342">guess</TOKEN>
<TOKEN end_char="5347" id="token-72-10" morph="none" pos="punct" start_char="5347">,</TOKEN>
<TOKEN end_char="5350" id="token-72-11" morph="none" pos="word" start_char="5349">it</TOKEN>
<TOKEN end_char="5360" id="token-72-12" morph="none" pos="word" start_char="5352">certainly</TOKEN>
<TOKEN end_char="5364" id="token-72-13" morph="none" pos="word" start_char="5362">has</TOKEN>
<TOKEN end_char="5368" id="token-72-14" morph="none" pos="word" start_char="5366">its</TOKEN>
<TOKEN end_char="5374" id="token-72-15" morph="none" pos="word" start_char="5370">place</TOKEN>
<TOKEN end_char="5381" id="token-72-16" morph="none" pos="word" start_char="5376">though</TOKEN>
<TOKEN end_char="5382" id="token-72-17" morph="none" pos="punct" start_char="5382">.</TOKEN>
</SEG>
<SEG end_char="5577" id="segment-73" start_char="5387">
<ORIGINAL_TEXT>Belzok said: A big thing is it's been a constant threat for the Far East and they've been through SARS and MERS so much easier to convince a populace to take basic measures and do it quickly.</ORIGINAL_TEXT>
<TOKEN end_char="5392" id="token-73-0" morph="none" pos="word" start_char="5387">Belzok</TOKEN>
<TOKEN end_char="5397" id="token-73-1" morph="none" pos="word" start_char="5394">said</TOKEN>
<TOKEN end_char="5398" id="token-73-2" morph="none" pos="punct" start_char="5398">:</TOKEN>
<TOKEN end_char="5400" id="token-73-3" morph="none" pos="word" start_char="5400">A</TOKEN>
<TOKEN end_char="5404" id="token-73-4" morph="none" pos="word" start_char="5402">big</TOKEN>
<TOKEN end_char="5410" id="token-73-5" morph="none" pos="word" start_char="5406">thing</TOKEN>
<TOKEN end_char="5413" id="token-73-6" morph="none" pos="word" start_char="5412">is</TOKEN>
<TOKEN end_char="5418" id="token-73-7" morph="none" pos="word" start_char="5415">it's</TOKEN>
<TOKEN end_char="5423" id="token-73-8" morph="none" pos="word" start_char="5420">been</TOKEN>
<TOKEN end_char="5425" id="token-73-9" morph="none" pos="word" start_char="5425">a</TOKEN>
<TOKEN end_char="5434" id="token-73-10" morph="none" pos="word" start_char="5427">constant</TOKEN>
<TOKEN end_char="5441" id="token-73-11" morph="none" pos="word" start_char="5436">threat</TOKEN>
<TOKEN end_char="5445" id="token-73-12" morph="none" pos="word" start_char="5443">for</TOKEN>
<TOKEN end_char="5449" id="token-73-13" morph="none" pos="word" start_char="5447">the</TOKEN>
<TOKEN end_char="5453" id="token-73-14" morph="none" pos="word" start_char="5451">Far</TOKEN>
<TOKEN end_char="5458" id="token-73-15" morph="none" pos="word" start_char="5455">East</TOKEN>
<TOKEN end_char="5462" id="token-73-16" morph="none" pos="word" start_char="5460">and</TOKEN>
<TOKEN end_char="5470" id="token-73-17" morph="none" pos="word" start_char="5464">they've</TOKEN>
<TOKEN end_char="5475" id="token-73-18" morph="none" pos="word" start_char="5472">been</TOKEN>
<TOKEN end_char="5483" id="token-73-19" morph="none" pos="word" start_char="5477">through</TOKEN>
<TOKEN end_char="5488" id="token-73-20" morph="none" pos="word" start_char="5485">SARS</TOKEN>
<TOKEN end_char="5492" id="token-73-21" morph="none" pos="word" start_char="5490">and</TOKEN>
<TOKEN end_char="5497" id="token-73-22" morph="none" pos="word" start_char="5494">MERS</TOKEN>
<TOKEN end_char="5500" id="token-73-23" morph="none" pos="word" start_char="5499">so</TOKEN>
<TOKEN end_char="5505" id="token-73-24" morph="none" pos="word" start_char="5502">much</TOKEN>
<TOKEN end_char="5512" id="token-73-25" morph="none" pos="word" start_char="5507">easier</TOKEN>
<TOKEN end_char="5515" id="token-73-26" morph="none" pos="word" start_char="5514">to</TOKEN>
<TOKEN end_char="5524" id="token-73-27" morph="none" pos="word" start_char="5517">convince</TOKEN>
<TOKEN end_char="5526" id="token-73-28" morph="none" pos="word" start_char="5526">a</TOKEN>
<TOKEN end_char="5535" id="token-73-29" morph="none" pos="word" start_char="5528">populace</TOKEN>
<TOKEN end_char="5538" id="token-73-30" morph="none" pos="word" start_char="5537">to</TOKEN>
<TOKEN end_char="5543" id="token-73-31" morph="none" pos="word" start_char="5540">take</TOKEN>
<TOKEN end_char="5549" id="token-73-32" morph="none" pos="word" start_char="5545">basic</TOKEN>
<TOKEN end_char="5558" id="token-73-33" morph="none" pos="word" start_char="5551">measures</TOKEN>
<TOKEN end_char="5562" id="token-73-34" morph="none" pos="word" start_char="5560">and</TOKEN>
<TOKEN end_char="5565" id="token-73-35" morph="none" pos="word" start_char="5564">do</TOKEN>
<TOKEN end_char="5568" id="token-73-36" morph="none" pos="word" start_char="5567">it</TOKEN>
<TOKEN end_char="5576" id="token-73-37" morph="none" pos="word" start_char="5570">quickly</TOKEN>
<TOKEN end_char="5577" id="token-73-38" morph="none" pos="punct" start_char="5577">.</TOKEN>
</SEG>
<SEG end_char="5703" id="segment-74" start_char="5579">
<ORIGINAL_TEXT>There's also something to be said for building up a very strong immune system due to poor quality sanitation for generations.</ORIGINAL_TEXT>
<TOKEN end_char="5585" id="token-74-0" morph="none" pos="word" start_char="5579">There's</TOKEN>
<TOKEN end_char="5590" id="token-74-1" morph="none" pos="word" start_char="5587">also</TOKEN>
<TOKEN end_char="5600" id="token-74-2" morph="none" pos="word" start_char="5592">something</TOKEN>
<TOKEN end_char="5603" id="token-74-3" morph="none" pos="word" start_char="5602">to</TOKEN>
<TOKEN end_char="5606" id="token-74-4" morph="none" pos="word" start_char="5605">be</TOKEN>
<TOKEN end_char="5611" id="token-74-5" morph="none" pos="word" start_char="5608">said</TOKEN>
<TOKEN end_char="5615" id="token-74-6" morph="none" pos="word" start_char="5613">for</TOKEN>
<TOKEN end_char="5624" id="token-74-7" morph="none" pos="word" start_char="5617">building</TOKEN>
<TOKEN end_char="5627" id="token-74-8" morph="none" pos="word" start_char="5626">up</TOKEN>
<TOKEN end_char="5629" id="token-74-9" morph="none" pos="word" start_char="5629">a</TOKEN>
<TOKEN end_char="5634" id="token-74-10" morph="none" pos="word" start_char="5631">very</TOKEN>
<TOKEN end_char="5641" id="token-74-11" morph="none" pos="word" start_char="5636">strong</TOKEN>
<TOKEN end_char="5648" id="token-74-12" morph="none" pos="word" start_char="5643">immune</TOKEN>
<TOKEN end_char="5655" id="token-74-13" morph="none" pos="word" start_char="5650">system</TOKEN>
<TOKEN end_char="5659" id="token-74-14" morph="none" pos="word" start_char="5657">due</TOKEN>
<TOKEN end_char="5662" id="token-74-15" morph="none" pos="word" start_char="5661">to</TOKEN>
<TOKEN end_char="5667" id="token-74-16" morph="none" pos="word" start_char="5664">poor</TOKEN>
<TOKEN end_char="5675" id="token-74-17" morph="none" pos="word" start_char="5669">quality</TOKEN>
<TOKEN end_char="5686" id="token-74-18" morph="none" pos="word" start_char="5677">sanitation</TOKEN>
<TOKEN end_char="5690" id="token-74-19" morph="none" pos="word" start_char="5688">for</TOKEN>
<TOKEN end_char="5702" id="token-74-20" morph="none" pos="word" start_char="5692">generations</TOKEN>
<TOKEN end_char="5703" id="token-74-21" morph="none" pos="punct" start_char="5703">.</TOKEN>
</SEG>
<SEG end_char="5786" id="segment-75" start_char="5705">
<ORIGINAL_TEXT>We probably over use bleach at home at a guess, it certainly has its place though.</ORIGINAL_TEXT>
<TOKEN end_char="5706" id="token-75-0" morph="none" pos="word" start_char="5705">We</TOKEN>
<TOKEN end_char="5715" id="token-75-1" morph="none" pos="word" start_char="5708">probably</TOKEN>
<TOKEN end_char="5720" id="token-75-2" morph="none" pos="word" start_char="5717">over</TOKEN>
<TOKEN end_char="5724" id="token-75-3" morph="none" pos="word" start_char="5722">use</TOKEN>
<TOKEN end_char="5731" id="token-75-4" morph="none" pos="word" start_char="5726">bleach</TOKEN>
<TOKEN end_char="5734" id="token-75-5" morph="none" pos="word" start_char="5733">at</TOKEN>
<TOKEN end_char="5739" id="token-75-6" morph="none" pos="word" start_char="5736">home</TOKEN>
<TOKEN end_char="5742" id="token-75-7" morph="none" pos="word" start_char="5741">at</TOKEN>
<TOKEN end_char="5744" id="token-75-8" morph="none" pos="word" start_char="5744">a</TOKEN>
<TOKEN end_char="5750" id="token-75-9" morph="none" pos="word" start_char="5746">guess</TOKEN>
<TOKEN end_char="5751" id="token-75-10" morph="none" pos="punct" start_char="5751">,</TOKEN>
<TOKEN end_char="5754" id="token-75-11" morph="none" pos="word" start_char="5753">it</TOKEN>
<TOKEN end_char="5764" id="token-75-12" morph="none" pos="word" start_char="5756">certainly</TOKEN>
<TOKEN end_char="5768" id="token-75-13" morph="none" pos="word" start_char="5766">has</TOKEN>
<TOKEN end_char="5772" id="token-75-14" morph="none" pos="word" start_char="5770">its</TOKEN>
<TOKEN end_char="5778" id="token-75-15" morph="none" pos="word" start_char="5774">place</TOKEN>
<TOKEN end_char="5785" id="token-75-16" morph="none" pos="word" start_char="5780">though</TOKEN>
<TOKEN end_char="5786" id="token-75-17" morph="none" pos="punct" start_char="5786">.</TOKEN>
</SEG>
<SEG end_char="6043" id="segment-76" start_char="5789">
<ORIGINAL_TEXT>I heard something the other day on the World Service that was making exactly this point...they had both people/equipment/testing/tracing systems ready to go because of SARS and MERS...both of which are also types of Coronavirus (and the common cold IIRC).</ORIGINAL_TEXT>
<TOKEN end_char="5789" id="token-76-0" morph="none" pos="word" start_char="5789">I</TOKEN>
<TOKEN end_char="5795" id="token-76-1" morph="none" pos="word" start_char="5791">heard</TOKEN>
<TOKEN end_char="5805" id="token-76-2" morph="none" pos="word" start_char="5797">something</TOKEN>
<TOKEN end_char="5809" id="token-76-3" morph="none" pos="word" start_char="5807">the</TOKEN>
<TOKEN end_char="5815" id="token-76-4" morph="none" pos="word" start_char="5811">other</TOKEN>
<TOKEN end_char="5819" id="token-76-5" morph="none" pos="word" start_char="5817">day</TOKEN>
<TOKEN end_char="5822" id="token-76-6" morph="none" pos="word" start_char="5821">on</TOKEN>
<TOKEN end_char="5826" id="token-76-7" morph="none" pos="word" start_char="5824">the</TOKEN>
<TOKEN end_char="5832" id="token-76-8" morph="none" pos="word" start_char="5828">World</TOKEN>
<TOKEN end_char="5840" id="token-76-9" morph="none" pos="word" start_char="5834">Service</TOKEN>
<TOKEN end_char="5845" id="token-76-10" morph="none" pos="word" start_char="5842">that</TOKEN>
<TOKEN end_char="5849" id="token-76-11" morph="none" pos="word" start_char="5847">was</TOKEN>
<TOKEN end_char="5856" id="token-76-12" morph="none" pos="word" start_char="5851">making</TOKEN>
<TOKEN end_char="5864" id="token-76-13" morph="none" pos="word" start_char="5858">exactly</TOKEN>
<TOKEN end_char="5869" id="token-76-14" morph="none" pos="word" start_char="5866">this</TOKEN>
<TOKEN end_char="5882" id="token-76-15" morph="none" pos="unknown" start_char="5871">point...they</TOKEN>
<TOKEN end_char="5886" id="token-76-16" morph="none" pos="word" start_char="5884">had</TOKEN>
<TOKEN end_char="5891" id="token-76-17" morph="none" pos="word" start_char="5888">both</TOKEN>
<TOKEN end_char="5924" id="token-76-18" morph="none" pos="unknown" start_char="5893">people/equipment/testing/tracing</TOKEN>
<TOKEN end_char="5932" id="token-76-19" morph="none" pos="word" start_char="5926">systems</TOKEN>
<TOKEN end_char="5938" id="token-76-20" morph="none" pos="word" start_char="5934">ready</TOKEN>
<TOKEN end_char="5941" id="token-76-21" morph="none" pos="word" start_char="5940">to</TOKEN>
<TOKEN end_char="5944" id="token-76-22" morph="none" pos="word" start_char="5943">go</TOKEN>
<TOKEN end_char="5952" id="token-76-23" morph="none" pos="word" start_char="5946">because</TOKEN>
<TOKEN end_char="5955" id="token-76-24" morph="none" pos="word" start_char="5954">of</TOKEN>
<TOKEN end_char="5960" id="token-76-25" morph="none" pos="word" start_char="5957">SARS</TOKEN>
<TOKEN end_char="5964" id="token-76-26" morph="none" pos="word" start_char="5962">and</TOKEN>
<TOKEN end_char="5976" id="token-76-27" morph="none" pos="unknown" start_char="5966">MERS...both</TOKEN>
<TOKEN end_char="5979" id="token-76-28" morph="none" pos="word" start_char="5978">of</TOKEN>
<TOKEN end_char="5985" id="token-76-29" morph="none" pos="word" start_char="5981">which</TOKEN>
<TOKEN end_char="5989" id="token-76-30" morph="none" pos="word" start_char="5987">are</TOKEN>
<TOKEN end_char="5994" id="token-76-31" morph="none" pos="word" start_char="5991">also</TOKEN>
<TOKEN end_char="6000" id="token-76-32" morph="none" pos="word" start_char="5996">types</TOKEN>
<TOKEN end_char="6003" id="token-76-33" morph="none" pos="word" start_char="6002">of</TOKEN>
<TOKEN end_char="6015" id="token-76-34" morph="none" pos="word" start_char="6005">Coronavirus</TOKEN>
<TOKEN end_char="6017" id="token-76-35" morph="none" pos="punct" start_char="6017">(</TOKEN>
<TOKEN end_char="6020" id="token-76-36" morph="none" pos="word" start_char="6018">and</TOKEN>
<TOKEN end_char="6024" id="token-76-37" morph="none" pos="word" start_char="6022">the</TOKEN>
<TOKEN end_char="6031" id="token-76-38" morph="none" pos="word" start_char="6026">common</TOKEN>
<TOKEN end_char="6036" id="token-76-39" morph="none" pos="word" start_char="6033">cold</TOKEN>
<TOKEN end_char="6041" id="token-76-40" morph="none" pos="word" start_char="6038">IIRC</TOKEN>
<TOKEN end_char="6043" id="token-76-41" morph="none" pos="punct" start_char="6042">).</TOKEN>
</SEG>
<SEG end_char="6289" id="segment-77" start_char="6046">
<ORIGINAL_TEXT>Past history tells us that Corona variations are highly likely to emerge in that part of the world first so they are always on a higher level of alert/preparedness and so is the general population a percentage of whom went through those events.</ORIGINAL_TEXT>
<TOKEN end_char="6049" id="token-77-0" morph="none" pos="word" start_char="6046">Past</TOKEN>
<TOKEN end_char="6057" id="token-77-1" morph="none" pos="word" start_char="6051">history</TOKEN>
<TOKEN end_char="6063" id="token-77-2" morph="none" pos="word" start_char="6059">tells</TOKEN>
<TOKEN end_char="6066" id="token-77-3" morph="none" pos="word" start_char="6065">us</TOKEN>
<TOKEN end_char="6071" id="token-77-4" morph="none" pos="word" start_char="6068">that</TOKEN>
<TOKEN end_char="6078" id="token-77-5" morph="none" pos="word" start_char="6073">Corona</TOKEN>
<TOKEN end_char="6089" id="token-77-6" morph="none" pos="word" start_char="6080">variations</TOKEN>
<TOKEN end_char="6093" id="token-77-7" morph="none" pos="word" start_char="6091">are</TOKEN>
<TOKEN end_char="6100" id="token-77-8" morph="none" pos="word" start_char="6095">highly</TOKEN>
<TOKEN end_char="6107" id="token-77-9" morph="none" pos="word" start_char="6102">likely</TOKEN>
<TOKEN end_char="6110" id="token-77-10" morph="none" pos="word" start_char="6109">to</TOKEN>
<TOKEN end_char="6117" id="token-77-11" morph="none" pos="word" start_char="6112">emerge</TOKEN>
<TOKEN end_char="6120" id="token-77-12" morph="none" pos="word" start_char="6119">in</TOKEN>
<TOKEN end_char="6125" id="token-77-13" morph="none" pos="word" start_char="6122">that</TOKEN>
<TOKEN end_char="6130" id="token-77-14" morph="none" pos="word" start_char="6127">part</TOKEN>
<TOKEN end_char="6133" id="token-77-15" morph="none" pos="word" start_char="6132">of</TOKEN>
<TOKEN end_char="6137" id="token-77-16" morph="none" pos="word" start_char="6135">the</TOKEN>
<TOKEN end_char="6143" id="token-77-17" morph="none" pos="word" start_char="6139">world</TOKEN>
<TOKEN end_char="6149" id="token-77-18" morph="none" pos="word" start_char="6145">first</TOKEN>
<TOKEN end_char="6152" id="token-77-19" morph="none" pos="word" start_char="6151">so</TOKEN>
<TOKEN end_char="6157" id="token-77-20" morph="none" pos="word" start_char="6154">they</TOKEN>
<TOKEN end_char="6161" id="token-77-21" morph="none" pos="word" start_char="6159">are</TOKEN>
<TOKEN end_char="6168" id="token-77-22" morph="none" pos="word" start_char="6163">always</TOKEN>
<TOKEN end_char="6171" id="token-77-23" morph="none" pos="word" start_char="6170">on</TOKEN>
<TOKEN end_char="6173" id="token-77-24" morph="none" pos="word" start_char="6173">a</TOKEN>
<TOKEN end_char="6180" id="token-77-25" morph="none" pos="word" start_char="6175">higher</TOKEN>
<TOKEN end_char="6186" id="token-77-26" morph="none" pos="word" start_char="6182">level</TOKEN>
<TOKEN end_char="6189" id="token-77-27" morph="none" pos="word" start_char="6188">of</TOKEN>
<TOKEN end_char="6208" id="token-77-28" morph="none" pos="unknown" start_char="6191">alert/preparedness</TOKEN>
<TOKEN end_char="6212" id="token-77-29" morph="none" pos="word" start_char="6210">and</TOKEN>
<TOKEN end_char="6215" id="token-77-30" morph="none" pos="word" start_char="6214">so</TOKEN>
<TOKEN end_char="6218" id="token-77-31" morph="none" pos="word" start_char="6217">is</TOKEN>
<TOKEN end_char="6222" id="token-77-32" morph="none" pos="word" start_char="6220">the</TOKEN>
<TOKEN end_char="6230" id="token-77-33" morph="none" pos="word" start_char="6224">general</TOKEN>
<TOKEN end_char="6241" id="token-77-34" morph="none" pos="word" start_char="6232">population</TOKEN>
<TOKEN end_char="6243" id="token-77-35" morph="none" pos="word" start_char="6243">a</TOKEN>
<TOKEN end_char="6254" id="token-77-36" morph="none" pos="word" start_char="6245">percentage</TOKEN>
<TOKEN end_char="6257" id="token-77-37" morph="none" pos="word" start_char="6256">of</TOKEN>
<TOKEN end_char="6262" id="token-77-38" morph="none" pos="word" start_char="6259">whom</TOKEN>
<TOKEN end_char="6267" id="token-77-39" morph="none" pos="word" start_char="6264">went</TOKEN>
<TOKEN end_char="6275" id="token-77-40" morph="none" pos="word" start_char="6269">through</TOKEN>
<TOKEN end_char="6281" id="token-77-41" morph="none" pos="word" start_char="6277">those</TOKEN>
<TOKEN end_char="6288" id="token-77-42" morph="none" pos="word" start_char="6283">events</TOKEN>
<TOKEN end_char="6289" id="token-77-43" morph="none" pos="punct" start_char="6289">.</TOKEN>
</SEG>
<SEG end_char="6426" id="segment-78" start_char="6292">
<ORIGINAL_TEXT>Maybe too those past events also mean that politicians are more willing to be "strict" sooner and less worried about staying "popular".</ORIGINAL_TEXT>
<TOKEN end_char="6296" id="token-78-0" morph="none" pos="word" start_char="6292">Maybe</TOKEN>
<TOKEN end_char="6300" id="token-78-1" morph="none" pos="word" start_char="6298">too</TOKEN>
<TOKEN end_char="6306" id="token-78-2" morph="none" pos="word" start_char="6302">those</TOKEN>
<TOKEN end_char="6311" id="token-78-3" morph="none" pos="word" start_char="6308">past</TOKEN>
<TOKEN end_char="6318" id="token-78-4" morph="none" pos="word" start_char="6313">events</TOKEN>
<TOKEN end_char="6323" id="token-78-5" morph="none" pos="word" start_char="6320">also</TOKEN>
<TOKEN end_char="6328" id="token-78-6" morph="none" pos="word" start_char="6325">mean</TOKEN>
<TOKEN end_char="6333" id="token-78-7" morph="none" pos="word" start_char="6330">that</TOKEN>
<TOKEN end_char="6345" id="token-78-8" morph="none" pos="word" start_char="6335">politicians</TOKEN>
<TOKEN end_char="6349" id="token-78-9" morph="none" pos="word" start_char="6347">are</TOKEN>
<TOKEN end_char="6354" id="token-78-10" morph="none" pos="word" start_char="6351">more</TOKEN>
<TOKEN end_char="6362" id="token-78-11" morph="none" pos="word" start_char="6356">willing</TOKEN>
<TOKEN end_char="6365" id="token-78-12" morph="none" pos="word" start_char="6364">to</TOKEN>
<TOKEN end_char="6368" id="token-78-13" morph="none" pos="word" start_char="6367">be</TOKEN>
<TOKEN end_char="6370" id="token-78-14" morph="none" pos="punct" start_char="6370">"</TOKEN>
<TOKEN end_char="6376" id="token-78-15" morph="none" pos="word" start_char="6371">strict</TOKEN>
<TOKEN end_char="6377" id="token-78-16" morph="none" pos="punct" start_char="6377">"</TOKEN>
<TOKEN end_char="6384" id="token-78-17" morph="none" pos="word" start_char="6379">sooner</TOKEN>
<TOKEN end_char="6388" id="token-78-18" morph="none" pos="word" start_char="6386">and</TOKEN>
<TOKEN end_char="6393" id="token-78-19" morph="none" pos="word" start_char="6390">less</TOKEN>
<TOKEN end_char="6401" id="token-78-20" morph="none" pos="word" start_char="6395">worried</TOKEN>
<TOKEN end_char="6407" id="token-78-21" morph="none" pos="word" start_char="6403">about</TOKEN>
<TOKEN end_char="6415" id="token-78-22" morph="none" pos="word" start_char="6409">staying</TOKEN>
<TOKEN end_char="6417" id="token-78-23" morph="none" pos="punct" start_char="6417">"</TOKEN>
<TOKEN end_char="6424" id="token-78-24" morph="none" pos="word" start_char="6418">popular</TOKEN>
<TOKEN end_char="6426" id="token-78-25" morph="none" pos="punct" start_char="6425">".</TOKEN>
</SEG>
<SEG end_char="6724" id="segment-79" start_char="6428">
<ORIGINAL_TEXT>And whilst it is a sweeping generalisation many parts of the Far East are fairly monocultural with more of a tradition of "compliance"...and some of them not slow to send out law enforcement who do not "engage with people" or "advise" them to "go home" but show them the sharp end of bamboo canes.</ORIGINAL_TEXT>
<TOKEN end_char="6430" id="token-79-0" morph="none" pos="word" start_char="6428">And</TOKEN>
<TOKEN end_char="6437" id="token-79-1" morph="none" pos="word" start_char="6432">whilst</TOKEN>
<TOKEN end_char="6440" id="token-79-2" morph="none" pos="word" start_char="6439">it</TOKEN>
<TOKEN end_char="6443" id="token-79-3" morph="none" pos="word" start_char="6442">is</TOKEN>
<TOKEN end_char="6445" id="token-79-4" morph="none" pos="word" start_char="6445">a</TOKEN>
<TOKEN end_char="6454" id="token-79-5" morph="none" pos="word" start_char="6447">sweeping</TOKEN>
<TOKEN end_char="6469" id="token-79-6" morph="none" pos="word" start_char="6456">generalisation</TOKEN>
<TOKEN end_char="6474" id="token-79-7" morph="none" pos="word" start_char="6471">many</TOKEN>
<TOKEN end_char="6480" id="token-79-8" morph="none" pos="word" start_char="6476">parts</TOKEN>
<TOKEN end_char="6483" id="token-79-9" morph="none" pos="word" start_char="6482">of</TOKEN>
<TOKEN end_char="6487" id="token-79-10" morph="none" pos="word" start_char="6485">the</TOKEN>
<TOKEN end_char="6491" id="token-79-11" morph="none" pos="word" start_char="6489">Far</TOKEN>
<TOKEN end_char="6496" id="token-79-12" morph="none" pos="word" start_char="6493">East</TOKEN>
<TOKEN end_char="6500" id="token-79-13" morph="none" pos="word" start_char="6498">are</TOKEN>
<TOKEN end_char="6507" id="token-79-14" morph="none" pos="word" start_char="6502">fairly</TOKEN>
<TOKEN end_char="6520" id="token-79-15" morph="none" pos="word" start_char="6509">monocultural</TOKEN>
<TOKEN end_char="6525" id="token-79-16" morph="none" pos="word" start_char="6522">with</TOKEN>
<TOKEN end_char="6530" id="token-79-17" morph="none" pos="word" start_char="6527">more</TOKEN>
<TOKEN end_char="6533" id="token-79-18" morph="none" pos="word" start_char="6532">of</TOKEN>
<TOKEN end_char="6535" id="token-79-19" morph="none" pos="word" start_char="6535">a</TOKEN>
<TOKEN end_char="6545" id="token-79-20" morph="none" pos="word" start_char="6537">tradition</TOKEN>
<TOKEN end_char="6548" id="token-79-21" morph="none" pos="word" start_char="6547">of</TOKEN>
<TOKEN end_char="6550" id="token-79-22" morph="none" pos="punct" start_char="6550">"</TOKEN>
<TOKEN end_char="6567" id="token-79-23" morph="none" pos="unknown" start_char="6551">compliance"...and</TOKEN>
<TOKEN end_char="6572" id="token-79-24" morph="none" pos="word" start_char="6569">some</TOKEN>
<TOKEN end_char="6575" id="token-79-25" morph="none" pos="word" start_char="6574">of</TOKEN>
<TOKEN end_char="6580" id="token-79-26" morph="none" pos="word" start_char="6577">them</TOKEN>
<TOKEN end_char="6584" id="token-79-27" morph="none" pos="word" start_char="6582">not</TOKEN>
<TOKEN end_char="6589" id="token-79-28" morph="none" pos="word" start_char="6586">slow</TOKEN>
<TOKEN end_char="6592" id="token-79-29" morph="none" pos="word" start_char="6591">to</TOKEN>
<TOKEN end_char="6597" id="token-79-30" morph="none" pos="word" start_char="6594">send</TOKEN>
<TOKEN end_char="6601" id="token-79-31" morph="none" pos="word" start_char="6599">out</TOKEN>
<TOKEN end_char="6605" id="token-79-32" morph="none" pos="word" start_char="6603">law</TOKEN>
<TOKEN end_char="6617" id="token-79-33" morph="none" pos="word" start_char="6607">enforcement</TOKEN>
<TOKEN end_char="6621" id="token-79-34" morph="none" pos="word" start_char="6619">who</TOKEN>
<TOKEN end_char="6624" id="token-79-35" morph="none" pos="word" start_char="6623">do</TOKEN>
<TOKEN end_char="6628" id="token-79-36" morph="none" pos="word" start_char="6626">not</TOKEN>
<TOKEN end_char="6630" id="token-79-37" morph="none" pos="punct" start_char="6630">"</TOKEN>
<TOKEN end_char="6636" id="token-79-38" morph="none" pos="word" start_char="6631">engage</TOKEN>
<TOKEN end_char="6641" id="token-79-39" morph="none" pos="word" start_char="6638">with</TOKEN>
<TOKEN end_char="6648" id="token-79-40" morph="none" pos="word" start_char="6643">people</TOKEN>
<TOKEN end_char="6649" id="token-79-41" morph="none" pos="punct" start_char="6649">"</TOKEN>
<TOKEN end_char="6652" id="token-79-42" morph="none" pos="word" start_char="6651">or</TOKEN>
<TOKEN end_char="6654" id="token-79-43" morph="none" pos="punct" start_char="6654">"</TOKEN>
<TOKEN end_char="6660" id="token-79-44" morph="none" pos="word" start_char="6655">advise</TOKEN>
<TOKEN end_char="6661" id="token-79-45" morph="none" pos="punct" start_char="6661">"</TOKEN>
<TOKEN end_char="6666" id="token-79-46" morph="none" pos="word" start_char="6663">them</TOKEN>
<TOKEN end_char="6669" id="token-79-47" morph="none" pos="word" start_char="6668">to</TOKEN>
<TOKEN end_char="6671" id="token-79-48" morph="none" pos="punct" start_char="6671">"</TOKEN>
<TOKEN end_char="6673" id="token-79-49" morph="none" pos="word" start_char="6672">go</TOKEN>
<TOKEN end_char="6678" id="token-79-50" morph="none" pos="word" start_char="6675">home</TOKEN>
<TOKEN end_char="6679" id="token-79-51" morph="none" pos="punct" start_char="6679">"</TOKEN>
<TOKEN end_char="6683" id="token-79-52" morph="none" pos="word" start_char="6681">but</TOKEN>
<TOKEN end_char="6688" id="token-79-53" morph="none" pos="word" start_char="6685">show</TOKEN>
<TOKEN end_char="6693" id="token-79-54" morph="none" pos="word" start_char="6690">them</TOKEN>
<TOKEN end_char="6697" id="token-79-55" morph="none" pos="word" start_char="6695">the</TOKEN>
<TOKEN end_char="6703" id="token-79-56" morph="none" pos="word" start_char="6699">sharp</TOKEN>
<TOKEN end_char="6707" id="token-79-57" morph="none" pos="word" start_char="6705">end</TOKEN>
<TOKEN end_char="6710" id="token-79-58" morph="none" pos="word" start_char="6709">of</TOKEN>
<TOKEN end_char="6717" id="token-79-59" morph="none" pos="word" start_char="6712">bamboo</TOKEN>
<TOKEN end_char="6723" id="token-79-60" morph="none" pos="word" start_char="6719">canes</TOKEN>
<TOKEN end_char="6724" id="token-79-61" morph="none" pos="punct" start_char="6724">.</TOKEN>
</SEG>
<SEG end_char="7005" id="segment-80" start_char="6730">
<ORIGINAL_TEXT>glasshalfempty said: I heard something the other day on the World Service that was making exactly this point...they had both people/equipment/testing/tracing systems ready to go because of SARS and MERS...both of which are also types of Coronavirus (and the common cold IIRC).</ORIGINAL_TEXT>
<TOKEN end_char="6743" id="token-80-0" morph="none" pos="word" start_char="6730">glasshalfempty</TOKEN>
<TOKEN end_char="6748" id="token-80-1" morph="none" pos="word" start_char="6745">said</TOKEN>
<TOKEN end_char="6749" id="token-80-2" morph="none" pos="punct" start_char="6749">:</TOKEN>
<TOKEN end_char="6751" id="token-80-3" morph="none" pos="word" start_char="6751">I</TOKEN>
<TOKEN end_char="6757" id="token-80-4" morph="none" pos="word" start_char="6753">heard</TOKEN>
<TOKEN end_char="6767" id="token-80-5" morph="none" pos="word" start_char="6759">something</TOKEN>
<TOKEN end_char="6771" id="token-80-6" morph="none" pos="word" start_char="6769">the</TOKEN>
<TOKEN end_char="6777" id="token-80-7" morph="none" pos="word" start_char="6773">other</TOKEN>
<TOKEN end_char="6781" id="token-80-8" morph="none" pos="word" start_char="6779">day</TOKEN>
<TOKEN end_char="6784" id="token-80-9" morph="none" pos="word" start_char="6783">on</TOKEN>
<TOKEN end_char="6788" id="token-80-10" morph="none" pos="word" start_char="6786">the</TOKEN>
<TOKEN end_char="6794" id="token-80-11" morph="none" pos="word" start_char="6790">World</TOKEN>
<TOKEN end_char="6802" id="token-80-12" morph="none" pos="word" start_char="6796">Service</TOKEN>
<TOKEN end_char="6807" id="token-80-13" morph="none" pos="word" start_char="6804">that</TOKEN>
<TOKEN end_char="6811" id="token-80-14" morph="none" pos="word" start_char="6809">was</TOKEN>
<TOKEN end_char="6818" id="token-80-15" morph="none" pos="word" start_char="6813">making</TOKEN>
<TOKEN end_char="6826" id="token-80-16" morph="none" pos="word" start_char="6820">exactly</TOKEN>
<TOKEN end_char="6831" id="token-80-17" morph="none" pos="word" start_char="6828">this</TOKEN>
<TOKEN end_char="6844" id="token-80-18" morph="none" pos="unknown" start_char="6833">point...they</TOKEN>
<TOKEN end_char="6848" id="token-80-19" morph="none" pos="word" start_char="6846">had</TOKEN>
<TOKEN end_char="6853" id="token-80-20" morph="none" pos="word" start_char="6850">both</TOKEN>
<TOKEN end_char="6886" id="token-80-21" morph="none" pos="unknown" start_char="6855">people/equipment/testing/tracing</TOKEN>
<TOKEN end_char="6894" id="token-80-22" morph="none" pos="word" start_char="6888">systems</TOKEN>
<TOKEN end_char="6900" id="token-80-23" morph="none" pos="word" start_char="6896">ready</TOKEN>
<TOKEN end_char="6903" id="token-80-24" morph="none" pos="word" start_char="6902">to</TOKEN>
<TOKEN end_char="6906" id="token-80-25" morph="none" pos="word" start_char="6905">go</TOKEN>
<TOKEN end_char="6914" id="token-80-26" morph="none" pos="word" start_char="6908">because</TOKEN>
<TOKEN end_char="6917" id="token-80-27" morph="none" pos="word" start_char="6916">of</TOKEN>
<TOKEN end_char="6922" id="token-80-28" morph="none" pos="word" start_char="6919">SARS</TOKEN>
<TOKEN end_char="6926" id="token-80-29" morph="none" pos="word" start_char="6924">and</TOKEN>
<TOKEN end_char="6938" id="token-80-30" morph="none" pos="unknown" start_char="6928">MERS...both</TOKEN>
<TOKEN end_char="6941" id="token-80-31" morph="none" pos="word" start_char="6940">of</TOKEN>
<TOKEN end_char="6947" id="token-80-32" morph="none" pos="word" start_char="6943">which</TOKEN>
<TOKEN end_char="6951" id="token-80-33" morph="none" pos="word" start_char="6949">are</TOKEN>
<TOKEN end_char="6956" id="token-80-34" morph="none" pos="word" start_char="6953">also</TOKEN>
<TOKEN end_char="6962" id="token-80-35" morph="none" pos="word" start_char="6958">types</TOKEN>
<TOKEN end_char="6965" id="token-80-36" morph="none" pos="word" start_char="6964">of</TOKEN>
<TOKEN end_char="6977" id="token-80-37" morph="none" pos="word" start_char="6967">Coronavirus</TOKEN>
<TOKEN end_char="6979" id="token-80-38" morph="none" pos="punct" start_char="6979">(</TOKEN>
<TOKEN end_char="6982" id="token-80-39" morph="none" pos="word" start_char="6980">and</TOKEN>
<TOKEN end_char="6986" id="token-80-40" morph="none" pos="word" start_char="6984">the</TOKEN>
<TOKEN end_char="6993" id="token-80-41" morph="none" pos="word" start_char="6988">common</TOKEN>
<TOKEN end_char="6998" id="token-80-42" morph="none" pos="word" start_char="6995">cold</TOKEN>
<TOKEN end_char="7003" id="token-80-43" morph="none" pos="word" start_char="7000">IIRC</TOKEN>
<TOKEN end_char="7005" id="token-80-44" morph="none" pos="punct" start_char="7004">).</TOKEN>
</SEG>
<SEG end_char="7250" id="segment-81" start_char="7007">
<ORIGINAL_TEXT>Past history tells us that Corona variations are highly likely to emerge in that part of the world first so they are always on a higher level of alert/preparedness and so is the general population a percentage of whom went through those events.</ORIGINAL_TEXT>
<TOKEN end_char="7010" id="token-81-0" morph="none" pos="word" start_char="7007">Past</TOKEN>
<TOKEN end_char="7018" id="token-81-1" morph="none" pos="word" start_char="7012">history</TOKEN>
<TOKEN end_char="7024" id="token-81-2" morph="none" pos="word" start_char="7020">tells</TOKEN>
<TOKEN end_char="7027" id="token-81-3" morph="none" pos="word" start_char="7026">us</TOKEN>
<TOKEN end_char="7032" id="token-81-4" morph="none" pos="word" start_char="7029">that</TOKEN>
<TOKEN end_char="7039" id="token-81-5" morph="none" pos="word" start_char="7034">Corona</TOKEN>
<TOKEN end_char="7050" id="token-81-6" morph="none" pos="word" start_char="7041">variations</TOKEN>
<TOKEN end_char="7054" id="token-81-7" morph="none" pos="word" start_char="7052">are</TOKEN>
<TOKEN end_char="7061" id="token-81-8" morph="none" pos="word" start_char="7056">highly</TOKEN>
<TOKEN end_char="7068" id="token-81-9" morph="none" pos="word" start_char="7063">likely</TOKEN>
<TOKEN end_char="7071" id="token-81-10" morph="none" pos="word" start_char="7070">to</TOKEN>
<TOKEN end_char="7078" id="token-81-11" morph="none" pos="word" start_char="7073">emerge</TOKEN>
<TOKEN end_char="7081" id="token-81-12" morph="none" pos="word" start_char="7080">in</TOKEN>
<TOKEN end_char="7086" id="token-81-13" morph="none" pos="word" start_char="7083">that</TOKEN>
<TOKEN end_char="7091" id="token-81-14" morph="none" pos="word" start_char="7088">part</TOKEN>
<TOKEN end_char="7094" id="token-81-15" morph="none" pos="word" start_char="7093">of</TOKEN>
<TOKEN end_char="7098" id="token-81-16" morph="none" pos="word" start_char="7096">the</TOKEN>
<TOKEN end_char="7104" id="token-81-17" morph="none" pos="word" start_char="7100">world</TOKEN>
<TOKEN end_char="7110" id="token-81-18" morph="none" pos="word" start_char="7106">first</TOKEN>
<TOKEN end_char="7113" id="token-81-19" morph="none" pos="word" start_char="7112">so</TOKEN>
<TOKEN end_char="7118" id="token-81-20" morph="none" pos="word" start_char="7115">they</TOKEN>
<TOKEN end_char="7122" id="token-81-21" morph="none" pos="word" start_char="7120">are</TOKEN>
<TOKEN end_char="7129" id="token-81-22" morph="none" pos="word" start_char="7124">always</TOKEN>
<TOKEN end_char="7132" id="token-81-23" morph="none" pos="word" start_char="7131">on</TOKEN>
<TOKEN end_char="7134" id="token-81-24" morph="none" pos="word" start_char="7134">a</TOKEN>
<TOKEN end_char="7141" id="token-81-25" morph="none" pos="word" start_char="7136">higher</TOKEN>
<TOKEN end_char="7147" id="token-81-26" morph="none" pos="word" start_char="7143">level</TOKEN>
<TOKEN end_char="7150" id="token-81-27" morph="none" pos="word" start_char="7149">of</TOKEN>
<TOKEN end_char="7169" id="token-81-28" morph="none" pos="unknown" start_char="7152">alert/preparedness</TOKEN>
<TOKEN end_char="7173" id="token-81-29" morph="none" pos="word" start_char="7171">and</TOKEN>
<TOKEN end_char="7176" id="token-81-30" morph="none" pos="word" start_char="7175">so</TOKEN>
<TOKEN end_char="7179" id="token-81-31" morph="none" pos="word" start_char="7178">is</TOKEN>
<TOKEN end_char="7183" id="token-81-32" morph="none" pos="word" start_char="7181">the</TOKEN>
<TOKEN end_char="7191" id="token-81-33" morph="none" pos="word" start_char="7185">general</TOKEN>
<TOKEN end_char="7202" id="token-81-34" morph="none" pos="word" start_char="7193">population</TOKEN>
<TOKEN end_char="7204" id="token-81-35" morph="none" pos="word" start_char="7204">a</TOKEN>
<TOKEN end_char="7215" id="token-81-36" morph="none" pos="word" start_char="7206">percentage</TOKEN>
<TOKEN end_char="7218" id="token-81-37" morph="none" pos="word" start_char="7217">of</TOKEN>
<TOKEN end_char="7223" id="token-81-38" morph="none" pos="word" start_char="7220">whom</TOKEN>
<TOKEN end_char="7228" id="token-81-39" morph="none" pos="word" start_char="7225">went</TOKEN>
<TOKEN end_char="7236" id="token-81-40" morph="none" pos="word" start_char="7230">through</TOKEN>
<TOKEN end_char="7242" id="token-81-41" morph="none" pos="word" start_char="7238">those</TOKEN>
<TOKEN end_char="7249" id="token-81-42" morph="none" pos="word" start_char="7244">events</TOKEN>
<TOKEN end_char="7250" id="token-81-43" morph="none" pos="punct" start_char="7250">.</TOKEN>
</SEG>
<SEG end_char="7386" id="segment-82" start_char="7252">
<ORIGINAL_TEXT>Maybe too those past events also mean that politicians are more willing to be "strict" sooner and less worried about staying "popular".</ORIGINAL_TEXT>
<TOKEN end_char="7256" id="token-82-0" morph="none" pos="word" start_char="7252">Maybe</TOKEN>
<TOKEN end_char="7260" id="token-82-1" morph="none" pos="word" start_char="7258">too</TOKEN>
<TOKEN end_char="7266" id="token-82-2" morph="none" pos="word" start_char="7262">those</TOKEN>
<TOKEN end_char="7271" id="token-82-3" morph="none" pos="word" start_char="7268">past</TOKEN>
<TOKEN end_char="7278" id="token-82-4" morph="none" pos="word" start_char="7273">events</TOKEN>
<TOKEN end_char="7283" id="token-82-5" morph="none" pos="word" start_char="7280">also</TOKEN>
<TOKEN end_char="7288" id="token-82-6" morph="none" pos="word" start_char="7285">mean</TOKEN>
<TOKEN end_char="7293" id="token-82-7" morph="none" pos="word" start_char="7290">that</TOKEN>
<TOKEN end_char="7305" id="token-82-8" morph="none" pos="word" start_char="7295">politicians</TOKEN>
<TOKEN end_char="7309" id="token-82-9" morph="none" pos="word" start_char="7307">are</TOKEN>
<TOKEN end_char="7314" id="token-82-10" morph="none" pos="word" start_char="7311">more</TOKEN>
<TOKEN end_char="7322" id="token-82-11" morph="none" pos="word" start_char="7316">willing</TOKEN>
<TOKEN end_char="7325" id="token-82-12" morph="none" pos="word" start_char="7324">to</TOKEN>
<TOKEN end_char="7328" id="token-82-13" morph="none" pos="word" start_char="7327">be</TOKEN>
<TOKEN end_char="7330" id="token-82-14" morph="none" pos="punct" start_char="7330">"</TOKEN>
<TOKEN end_char="7336" id="token-82-15" morph="none" pos="word" start_char="7331">strict</TOKEN>
<TOKEN end_char="7337" id="token-82-16" morph="none" pos="punct" start_char="7337">"</TOKEN>
<TOKEN end_char="7344" id="token-82-17" morph="none" pos="word" start_char="7339">sooner</TOKEN>
<TOKEN end_char="7348" id="token-82-18" morph="none" pos="word" start_char="7346">and</TOKEN>
<TOKEN end_char="7353" id="token-82-19" morph="none" pos="word" start_char="7350">less</TOKEN>
<TOKEN end_char="7361" id="token-82-20" morph="none" pos="word" start_char="7355">worried</TOKEN>
<TOKEN end_char="7367" id="token-82-21" morph="none" pos="word" start_char="7363">about</TOKEN>
<TOKEN end_char="7375" id="token-82-22" morph="none" pos="word" start_char="7369">staying</TOKEN>
<TOKEN end_char="7377" id="token-82-23" morph="none" pos="punct" start_char="7377">"</TOKEN>
<TOKEN end_char="7384" id="token-82-24" morph="none" pos="word" start_char="7378">popular</TOKEN>
<TOKEN end_char="7386" id="token-82-25" morph="none" pos="punct" start_char="7385">".</TOKEN>
</SEG>
<SEG end_char="7684" id="segment-83" start_char="7388">
<ORIGINAL_TEXT>And whilst it is a sweeping generalisation many parts of the Far East are fairly monocultural with more of a tradition of "compliance"...and some of them not slow to send out law enforcement who do not "engage with people" or "advise" them to "go home" but show them the sharp end of bamboo canes.</ORIGINAL_TEXT>
<TOKEN end_char="7390" id="token-83-0" morph="none" pos="word" start_char="7388">And</TOKEN>
<TOKEN end_char="7397" id="token-83-1" morph="none" pos="word" start_char="7392">whilst</TOKEN>
<TOKEN end_char="7400" id="token-83-2" morph="none" pos="word" start_char="7399">it</TOKEN>
<TOKEN end_char="7403" id="token-83-3" morph="none" pos="word" start_char="7402">is</TOKEN>
<TOKEN end_char="7405" id="token-83-4" morph="none" pos="word" start_char="7405">a</TOKEN>
<TOKEN end_char="7414" id="token-83-5" morph="none" pos="word" start_char="7407">sweeping</TOKEN>
<TOKEN end_char="7429" id="token-83-6" morph="none" pos="word" start_char="7416">generalisation</TOKEN>
<TOKEN end_char="7434" id="token-83-7" morph="none" pos="word" start_char="7431">many</TOKEN>
<TOKEN end_char="7440" id="token-83-8" morph="none" pos="word" start_char="7436">parts</TOKEN>
<TOKEN end_char="7443" id="token-83-9" morph="none" pos="word" start_char="7442">of</TOKEN>
<TOKEN end_char="7447" id="token-83-10" morph="none" pos="word" start_char="7445">the</TOKEN>
<TOKEN end_char="7451" id="token-83-11" morph="none" pos="word" start_char="7449">Far</TOKEN>
<TOKEN end_char="7456" id="token-83-12" morph="none" pos="word" start_char="7453">East</TOKEN>
<TOKEN end_char="7460" id="token-83-13" morph="none" pos="word" start_char="7458">are</TOKEN>
<TOKEN end_char="7467" id="token-83-14" morph="none" pos="word" start_char="7462">fairly</TOKEN>
<TOKEN end_char="7480" id="token-83-15" morph="none" pos="word" start_char="7469">monocultural</TOKEN>
<TOKEN end_char="7485" id="token-83-16" morph="none" pos="word" start_char="7482">with</TOKEN>
<TOKEN end_char="7490" id="token-83-17" morph="none" pos="word" start_char="7487">more</TOKEN>
<TOKEN end_char="7493" id="token-83-18" morph="none" pos="word" start_char="7492">of</TOKEN>
<TOKEN end_char="7495" id="token-83-19" morph="none" pos="word" start_char="7495">a</TOKEN>
<TOKEN end_char="7505" id="token-83-20" morph="none" pos="word" start_char="7497">tradition</TOKEN>
<TOKEN end_char="7508" id="token-83-21" morph="none" pos="word" start_char="7507">of</TOKEN>
<TOKEN end_char="7510" id="token-83-22" morph="none" pos="punct" start_char="7510">"</TOKEN>
<TOKEN end_char="7527" id="token-83-23" morph="none" pos="unknown" start_char="7511">compliance"...and</TOKEN>
<TOKEN end_char="7532" id="token-83-24" morph="none" pos="word" start_char="7529">some</TOKEN>
<TOKEN end_char="7535" id="token-83-25" morph="none" pos="word" start_char="7534">of</TOKEN>
<TOKEN end_char="7540" id="token-83-26" morph="none" pos="word" start_char="7537">them</TOKEN>
<TOKEN end_char="7544" id="token-83-27" morph="none" pos="word" start_char="7542">not</TOKEN>
<TOKEN end_char="7549" id="token-83-28" morph="none" pos="word" start_char="7546">slow</TOKEN>
<TOKEN end_char="7552" id="token-83-29" morph="none" pos="word" start_char="7551">to</TOKEN>
<TOKEN end_char="7557" id="token-83-30" morph="none" pos="word" start_char="7554">send</TOKEN>
<TOKEN end_char="7561" id="token-83-31" morph="none" pos="word" start_char="7559">out</TOKEN>
<TOKEN end_char="7565" id="token-83-32" morph="none" pos="word" start_char="7563">law</TOKEN>
<TOKEN end_char="7577" id="token-83-33" morph="none" pos="word" start_char="7567">enforcement</TOKEN>
<TOKEN end_char="7581" id="token-83-34" morph="none" pos="word" start_char="7579">who</TOKEN>
<TOKEN end_char="7584" id="token-83-35" morph="none" pos="word" start_char="7583">do</TOKEN>
<TOKEN end_char="7588" id="token-83-36" morph="none" pos="word" start_char="7586">not</TOKEN>
<TOKEN end_char="7590" id="token-83-37" morph="none" pos="punct" start_char="7590">"</TOKEN>
<TOKEN end_char="7596" id="token-83-38" morph="none" pos="word" start_char="7591">engage</TOKEN>
<TOKEN end_char="7601" id="token-83-39" morph="none" pos="word" start_char="7598">with</TOKEN>
<TOKEN end_char="7608" id="token-83-40" morph="none" pos="word" start_char="7603">people</TOKEN>
<TOKEN end_char="7609" id="token-83-41" morph="none" pos="punct" start_char="7609">"</TOKEN>
<TOKEN end_char="7612" id="token-83-42" morph="none" pos="word" start_char="7611">or</TOKEN>
<TOKEN end_char="7614" id="token-83-43" morph="none" pos="punct" start_char="7614">"</TOKEN>
<TOKEN end_char="7620" id="token-83-44" morph="none" pos="word" start_char="7615">advise</TOKEN>
<TOKEN end_char="7621" id="token-83-45" morph="none" pos="punct" start_char="7621">"</TOKEN>
<TOKEN end_char="7626" id="token-83-46" morph="none" pos="word" start_char="7623">them</TOKEN>
<TOKEN end_char="7629" id="token-83-47" morph="none" pos="word" start_char="7628">to</TOKEN>
<TOKEN end_char="7631" id="token-83-48" morph="none" pos="punct" start_char="7631">"</TOKEN>
<TOKEN end_char="7633" id="token-83-49" morph="none" pos="word" start_char="7632">go</TOKEN>
<TOKEN end_char="7638" id="token-83-50" morph="none" pos="word" start_char="7635">home</TOKEN>
<TOKEN end_char="7639" id="token-83-51" morph="none" pos="punct" start_char="7639">"</TOKEN>
<TOKEN end_char="7643" id="token-83-52" morph="none" pos="word" start_char="7641">but</TOKEN>
<TOKEN end_char="7648" id="token-83-53" morph="none" pos="word" start_char="7645">show</TOKEN>
<TOKEN end_char="7653" id="token-83-54" morph="none" pos="word" start_char="7650">them</TOKEN>
<TOKEN end_char="7657" id="token-83-55" morph="none" pos="word" start_char="7655">the</TOKEN>
<TOKEN end_char="7663" id="token-83-56" morph="none" pos="word" start_char="7659">sharp</TOKEN>
<TOKEN end_char="7667" id="token-83-57" morph="none" pos="word" start_char="7665">end</TOKEN>
<TOKEN end_char="7670" id="token-83-58" morph="none" pos="word" start_char="7669">of</TOKEN>
<TOKEN end_char="7677" id="token-83-59" morph="none" pos="word" start_char="7672">bamboo</TOKEN>
<TOKEN end_char="7683" id="token-83-60" morph="none" pos="word" start_char="7679">canes</TOKEN>
<TOKEN end_char="7684" id="token-83-61" morph="none" pos="punct" start_char="7684">.</TOKEN>
</SEG>
<SEG end_char="7703" id="segment-84" start_char="7686">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="7690" id="token-84-0" morph="none" pos="word" start_char="7686">Click</TOKEN>
<TOKEN end_char="7693" id="token-84-1" morph="none" pos="word" start_char="7692">to</TOKEN>
<TOKEN end_char="7700" id="token-84-2" morph="none" pos="word" start_char="7695">expand</TOKEN>
<TOKEN end_char="7703" id="token-84-3" morph="none" pos="punct" start_char="7701">...</TOKEN>
</SEG>
<SEG end_char="7785" id="segment-85" start_char="7706">
<ORIGINAL_TEXT>Compliance is a massive consideration in government's response around the world.</ORIGINAL_TEXT>
<TOKEN end_char="7715" id="token-85-0" morph="none" pos="word" start_char="7706">Compliance</TOKEN>
<TOKEN end_char="7718" id="token-85-1" morph="none" pos="word" start_char="7717">is</TOKEN>
<TOKEN end_char="7720" id="token-85-2" morph="none" pos="word" start_char="7720">a</TOKEN>
<TOKEN end_char="7728" id="token-85-3" morph="none" pos="word" start_char="7722">massive</TOKEN>
<TOKEN end_char="7742" id="token-85-4" morph="none" pos="word" start_char="7730">consideration</TOKEN>
<TOKEN end_char="7745" id="token-85-5" morph="none" pos="word" start_char="7744">in</TOKEN>
<TOKEN end_char="7758" id="token-85-6" morph="none" pos="word" start_char="7747">government's</TOKEN>
<TOKEN end_char="7767" id="token-85-7" morph="none" pos="word" start_char="7760">response</TOKEN>
<TOKEN end_char="7774" id="token-85-8" morph="none" pos="word" start_char="7769">around</TOKEN>
<TOKEN end_char="7778" id="token-85-9" morph="none" pos="word" start_char="7776">the</TOKEN>
<TOKEN end_char="7784" id="token-85-10" morph="none" pos="word" start_char="7780">world</TOKEN>
<TOKEN end_char="7785" id="token-85-11" morph="none" pos="punct" start_char="7785">.</TOKEN>
</SEG>
<SEG end_char="7921" id="segment-86" start_char="7788">
<ORIGINAL_TEXT>You can see in Germany, fairly will known to be rule followers, have done well whereas the likes of us and the US haven't faired well.</ORIGINAL_TEXT>
<TOKEN end_char="7790" id="token-86-0" morph="none" pos="word" start_char="7788">You</TOKEN>
<TOKEN end_char="7794" id="token-86-1" morph="none" pos="word" start_char="7792">can</TOKEN>
<TOKEN end_char="7798" id="token-86-2" morph="none" pos="word" start_char="7796">see</TOKEN>
<TOKEN end_char="7801" id="token-86-3" morph="none" pos="word" start_char="7800">in</TOKEN>
<TOKEN end_char="7809" id="token-86-4" morph="none" pos="word" start_char="7803">Germany</TOKEN>
<TOKEN end_char="7810" id="token-86-5" morph="none" pos="punct" start_char="7810">,</TOKEN>
<TOKEN end_char="7817" id="token-86-6" morph="none" pos="word" start_char="7812">fairly</TOKEN>
<TOKEN end_char="7822" id="token-86-7" morph="none" pos="word" start_char="7819">will</TOKEN>
<TOKEN end_char="7828" id="token-86-8" morph="none" pos="word" start_char="7824">known</TOKEN>
<TOKEN end_char="7831" id="token-86-9" morph="none" pos="word" start_char="7830">to</TOKEN>
<TOKEN end_char="7834" id="token-86-10" morph="none" pos="word" start_char="7833">be</TOKEN>
<TOKEN end_char="7839" id="token-86-11" morph="none" pos="word" start_char="7836">rule</TOKEN>
<TOKEN end_char="7849" id="token-86-12" morph="none" pos="word" start_char="7841">followers</TOKEN>
<TOKEN end_char="7850" id="token-86-13" morph="none" pos="punct" start_char="7850">,</TOKEN>
<TOKEN end_char="7855" id="token-86-14" morph="none" pos="word" start_char="7852">have</TOKEN>
<TOKEN end_char="7860" id="token-86-15" morph="none" pos="word" start_char="7857">done</TOKEN>
<TOKEN end_char="7865" id="token-86-16" morph="none" pos="word" start_char="7862">well</TOKEN>
<TOKEN end_char="7873" id="token-86-17" morph="none" pos="word" start_char="7867">whereas</TOKEN>
<TOKEN end_char="7877" id="token-86-18" morph="none" pos="word" start_char="7875">the</TOKEN>
<TOKEN end_char="7883" id="token-86-19" morph="none" pos="word" start_char="7879">likes</TOKEN>
<TOKEN end_char="7886" id="token-86-20" morph="none" pos="word" start_char="7885">of</TOKEN>
<TOKEN end_char="7889" id="token-86-21" morph="none" pos="word" start_char="7888">us</TOKEN>
<TOKEN end_char="7893" id="token-86-22" morph="none" pos="word" start_char="7891">and</TOKEN>
<TOKEN end_char="7897" id="token-86-23" morph="none" pos="word" start_char="7895">the</TOKEN>
<TOKEN end_char="7900" id="token-86-24" morph="none" pos="word" start_char="7899">US</TOKEN>
<TOKEN end_char="7908" id="token-86-25" morph="none" pos="word" start_char="7902">haven't</TOKEN>
<TOKEN end_char="7915" id="token-86-26" morph="none" pos="word" start_char="7910">faired</TOKEN>
<TOKEN end_char="7920" id="token-86-27" morph="none" pos="word" start_char="7917">well</TOKEN>
<TOKEN end_char="7921" id="token-86-28" morph="none" pos="punct" start_char="7921">.</TOKEN>
</SEG>
<SEG end_char="8131" id="segment-87" start_char="7924">
<ORIGINAL_TEXT>We've sadly not got enough jail cells to house those who flout the law so dramatically, such as the organisers of parties, weddings and so on where there have been hundreds in attendance during the lockdowns.</ORIGINAL_TEXT>
<TOKEN end_char="7928" id="token-87-0" morph="none" pos="word" start_char="7924">We've</TOKEN>
<TOKEN end_char="7934" id="token-87-1" morph="none" pos="word" start_char="7930">sadly</TOKEN>
<TOKEN end_char="7938" id="token-87-2" morph="none" pos="word" start_char="7936">not</TOKEN>
<TOKEN end_char="7942" id="token-87-3" morph="none" pos="word" start_char="7940">got</TOKEN>
<TOKEN end_char="7949" id="token-87-4" morph="none" pos="word" start_char="7944">enough</TOKEN>
<TOKEN end_char="7954" id="token-87-5" morph="none" pos="word" start_char="7951">jail</TOKEN>
<TOKEN end_char="7960" id="token-87-6" morph="none" pos="word" start_char="7956">cells</TOKEN>
<TOKEN end_char="7963" id="token-87-7" morph="none" pos="word" start_char="7962">to</TOKEN>
<TOKEN end_char="7969" id="token-87-8" morph="none" pos="word" start_char="7965">house</TOKEN>
<TOKEN end_char="7975" id="token-87-9" morph="none" pos="word" start_char="7971">those</TOKEN>
<TOKEN end_char="7979" id="token-87-10" morph="none" pos="word" start_char="7977">who</TOKEN>
<TOKEN end_char="7985" id="token-87-11" morph="none" pos="word" start_char="7981">flout</TOKEN>
<TOKEN end_char="7989" id="token-87-12" morph="none" pos="word" start_char="7987">the</TOKEN>
<TOKEN end_char="7993" id="token-87-13" morph="none" pos="word" start_char="7991">law</TOKEN>
<TOKEN end_char="7996" id="token-87-14" morph="none" pos="word" start_char="7995">so</TOKEN>
<TOKEN end_char="8009" id="token-87-15" morph="none" pos="word" start_char="7998">dramatically</TOKEN>
<TOKEN end_char="8010" id="token-87-16" morph="none" pos="punct" start_char="8010">,</TOKEN>
<TOKEN end_char="8015" id="token-87-17" morph="none" pos="word" start_char="8012">such</TOKEN>
<TOKEN end_char="8018" id="token-87-18" morph="none" pos="word" start_char="8017">as</TOKEN>
<TOKEN end_char="8022" id="token-87-19" morph="none" pos="word" start_char="8020">the</TOKEN>
<TOKEN end_char="8033" id="token-87-20" morph="none" pos="word" start_char="8024">organisers</TOKEN>
<TOKEN end_char="8036" id="token-87-21" morph="none" pos="word" start_char="8035">of</TOKEN>
<TOKEN end_char="8044" id="token-87-22" morph="none" pos="word" start_char="8038">parties</TOKEN>
<TOKEN end_char="8045" id="token-87-23" morph="none" pos="punct" start_char="8045">,</TOKEN>
<TOKEN end_char="8054" id="token-87-24" morph="none" pos="word" start_char="8047">weddings</TOKEN>
<TOKEN end_char="8058" id="token-87-25" morph="none" pos="word" start_char="8056">and</TOKEN>
<TOKEN end_char="8061" id="token-87-26" morph="none" pos="word" start_char="8060">so</TOKEN>
<TOKEN end_char="8064" id="token-87-27" morph="none" pos="word" start_char="8063">on</TOKEN>
<TOKEN end_char="8070" id="token-87-28" morph="none" pos="word" start_char="8066">where</TOKEN>
<TOKEN end_char="8076" id="token-87-29" morph="none" pos="word" start_char="8072">there</TOKEN>
<TOKEN end_char="8081" id="token-87-30" morph="none" pos="word" start_char="8078">have</TOKEN>
<TOKEN end_char="8086" id="token-87-31" morph="none" pos="word" start_char="8083">been</TOKEN>
<TOKEN end_char="8095" id="token-87-32" morph="none" pos="word" start_char="8088">hundreds</TOKEN>
<TOKEN end_char="8098" id="token-87-33" morph="none" pos="word" start_char="8097">in</TOKEN>
<TOKEN end_char="8109" id="token-87-34" morph="none" pos="word" start_char="8100">attendance</TOKEN>
<TOKEN end_char="8116" id="token-87-35" morph="none" pos="word" start_char="8111">during</TOKEN>
<TOKEN end_char="8120" id="token-87-36" morph="none" pos="word" start_char="8118">the</TOKEN>
<TOKEN end_char="8130" id="token-87-37" morph="none" pos="word" start_char="8122">lockdowns</TOKEN>
<TOKEN end_char="8131" id="token-87-38" morph="none" pos="punct" start_char="8131">.</TOKEN>
</SEG>
<SEG end_char="8412" id="segment-88" start_char="8137">
<ORIGINAL_TEXT>glasshalfempty said: I heard something the other day on the World Service that was making exactly this point...they had both people/equipment/testing/tracing systems ready to go because of SARS and MERS...both of which are also types of Coronavirus (and the common cold IIRC).</ORIGINAL_TEXT>
<TOKEN end_char="8150" id="token-88-0" morph="none" pos="word" start_char="8137">glasshalfempty</TOKEN>
<TOKEN end_char="8155" id="token-88-1" morph="none" pos="word" start_char="8152">said</TOKEN>
<TOKEN end_char="8156" id="token-88-2" morph="none" pos="punct" start_char="8156">:</TOKEN>
<TOKEN end_char="8158" id="token-88-3" morph="none" pos="word" start_char="8158">I</TOKEN>
<TOKEN end_char="8164" id="token-88-4" morph="none" pos="word" start_char="8160">heard</TOKEN>
<TOKEN end_char="8174" id="token-88-5" morph="none" pos="word" start_char="8166">something</TOKEN>
<TOKEN end_char="8178" id="token-88-6" morph="none" pos="word" start_char="8176">the</TOKEN>
<TOKEN end_char="8184" id="token-88-7" morph="none" pos="word" start_char="8180">other</TOKEN>
<TOKEN end_char="8188" id="token-88-8" morph="none" pos="word" start_char="8186">day</TOKEN>
<TOKEN end_char="8191" id="token-88-9" morph="none" pos="word" start_char="8190">on</TOKEN>
<TOKEN end_char="8195" id="token-88-10" morph="none" pos="word" start_char="8193">the</TOKEN>
<TOKEN end_char="8201" id="token-88-11" morph="none" pos="word" start_char="8197">World</TOKEN>
<TOKEN end_char="8209" id="token-88-12" morph="none" pos="word" start_char="8203">Service</TOKEN>
<TOKEN end_char="8214" id="token-88-13" morph="none" pos="word" start_char="8211">that</TOKEN>
<TOKEN end_char="8218" id="token-88-14" morph="none" pos="word" start_char="8216">was</TOKEN>
<TOKEN end_char="8225" id="token-88-15" morph="none" pos="word" start_char="8220">making</TOKEN>
<TOKEN end_char="8233" id="token-88-16" morph="none" pos="word" start_char="8227">exactly</TOKEN>
<TOKEN end_char="8238" id="token-88-17" morph="none" pos="word" start_char="8235">this</TOKEN>
<TOKEN end_char="8251" id="token-88-18" morph="none" pos="unknown" start_char="8240">point...they</TOKEN>
<TOKEN end_char="8255" id="token-88-19" morph="none" pos="word" start_char="8253">had</TOKEN>
<TOKEN end_char="8260" id="token-88-20" morph="none" pos="word" start_char="8257">both</TOKEN>
<TOKEN end_char="8293" id="token-88-21" morph="none" pos="unknown" start_char="8262">people/equipment/testing/tracing</TOKEN>
<TOKEN end_char="8301" id="token-88-22" morph="none" pos="word" start_char="8295">systems</TOKEN>
<TOKEN end_char="8307" id="token-88-23" morph="none" pos="word" start_char="8303">ready</TOKEN>
<TOKEN end_char="8310" id="token-88-24" morph="none" pos="word" start_char="8309">to</TOKEN>
<TOKEN end_char="8313" id="token-88-25" morph="none" pos="word" start_char="8312">go</TOKEN>
<TOKEN end_char="8321" id="token-88-26" morph="none" pos="word" start_char="8315">because</TOKEN>
<TOKEN end_char="8324" id="token-88-27" morph="none" pos="word" start_char="8323">of</TOKEN>
<TOKEN end_char="8329" id="token-88-28" morph="none" pos="word" start_char="8326">SARS</TOKEN>
<TOKEN end_char="8333" id="token-88-29" morph="none" pos="word" start_char="8331">and</TOKEN>
<TOKEN end_char="8345" id="token-88-30" morph="none" pos="unknown" start_char="8335">MERS...both</TOKEN>
<TOKEN end_char="8348" id="token-88-31" morph="none" pos="word" start_char="8347">of</TOKEN>
<TOKEN end_char="8354" id="token-88-32" morph="none" pos="word" start_char="8350">which</TOKEN>
<TOKEN end_char="8358" id="token-88-33" morph="none" pos="word" start_char="8356">are</TOKEN>
<TOKEN end_char="8363" id="token-88-34" morph="none" pos="word" start_char="8360">also</TOKEN>
<TOKEN end_char="8369" id="token-88-35" morph="none" pos="word" start_char="8365">types</TOKEN>
<TOKEN end_char="8372" id="token-88-36" morph="none" pos="word" start_char="8371">of</TOKEN>
<TOKEN end_char="8384" id="token-88-37" morph="none" pos="word" start_char="8374">Coronavirus</TOKEN>
<TOKEN end_char="8386" id="token-88-38" morph="none" pos="punct" start_char="8386">(</TOKEN>
<TOKEN end_char="8389" id="token-88-39" morph="none" pos="word" start_char="8387">and</TOKEN>
<TOKEN end_char="8393" id="token-88-40" morph="none" pos="word" start_char="8391">the</TOKEN>
<TOKEN end_char="8400" id="token-88-41" morph="none" pos="word" start_char="8395">common</TOKEN>
<TOKEN end_char="8405" id="token-88-42" morph="none" pos="word" start_char="8402">cold</TOKEN>
<TOKEN end_char="8410" id="token-88-43" morph="none" pos="word" start_char="8407">IIRC</TOKEN>
<TOKEN end_char="8412" id="token-88-44" morph="none" pos="punct" start_char="8411">).</TOKEN>
</SEG>
<SEG end_char="8657" id="segment-89" start_char="8414">
<ORIGINAL_TEXT>Past history tells us that Corona variations are highly likely to emerge in that part of the world first so they are always on a higher level of alert/preparedness and so is the general population a percentage of whom went through those events.</ORIGINAL_TEXT>
<TOKEN end_char="8417" id="token-89-0" morph="none" pos="word" start_char="8414">Past</TOKEN>
<TOKEN end_char="8425" id="token-89-1" morph="none" pos="word" start_char="8419">history</TOKEN>
<TOKEN end_char="8431" id="token-89-2" morph="none" pos="word" start_char="8427">tells</TOKEN>
<TOKEN end_char="8434" id="token-89-3" morph="none" pos="word" start_char="8433">us</TOKEN>
<TOKEN end_char="8439" id="token-89-4" morph="none" pos="word" start_char="8436">that</TOKEN>
<TOKEN end_char="8446" id="token-89-5" morph="none" pos="word" start_char="8441">Corona</TOKEN>
<TOKEN end_char="8457" id="token-89-6" morph="none" pos="word" start_char="8448">variations</TOKEN>
<TOKEN end_char="8461" id="token-89-7" morph="none" pos="word" start_char="8459">are</TOKEN>
<TOKEN end_char="8468" id="token-89-8" morph="none" pos="word" start_char="8463">highly</TOKEN>
<TOKEN end_char="8475" id="token-89-9" morph="none" pos="word" start_char="8470">likely</TOKEN>
<TOKEN end_char="8478" id="token-89-10" morph="none" pos="word" start_char="8477">to</TOKEN>
<TOKEN end_char="8485" id="token-89-11" morph="none" pos="word" start_char="8480">emerge</TOKEN>
<TOKEN end_char="8488" id="token-89-12" morph="none" pos="word" start_char="8487">in</TOKEN>
<TOKEN end_char="8493" id="token-89-13" morph="none" pos="word" start_char="8490">that</TOKEN>
<TOKEN end_char="8498" id="token-89-14" morph="none" pos="word" start_char="8495">part</TOKEN>
<TOKEN end_char="8501" id="token-89-15" morph="none" pos="word" start_char="8500">of</TOKEN>
<TOKEN end_char="8505" id="token-89-16" morph="none" pos="word" start_char="8503">the</TOKEN>
<TOKEN end_char="8511" id="token-89-17" morph="none" pos="word" start_char="8507">world</TOKEN>
<TOKEN end_char="8517" id="token-89-18" morph="none" pos="word" start_char="8513">first</TOKEN>
<TOKEN end_char="8520" id="token-89-19" morph="none" pos="word" start_char="8519">so</TOKEN>
<TOKEN end_char="8525" id="token-89-20" morph="none" pos="word" start_char="8522">they</TOKEN>
<TOKEN end_char="8529" id="token-89-21" morph="none" pos="word" start_char="8527">are</TOKEN>
<TOKEN end_char="8536" id="token-89-22" morph="none" pos="word" start_char="8531">always</TOKEN>
<TOKEN end_char="8539" id="token-89-23" morph="none" pos="word" start_char="8538">on</TOKEN>
<TOKEN end_char="8541" id="token-89-24" morph="none" pos="word" start_char="8541">a</TOKEN>
<TOKEN end_char="8548" id="token-89-25" morph="none" pos="word" start_char="8543">higher</TOKEN>
<TOKEN end_char="8554" id="token-89-26" morph="none" pos="word" start_char="8550">level</TOKEN>
<TOKEN end_char="8557" id="token-89-27" morph="none" pos="word" start_char="8556">of</TOKEN>
<TOKEN end_char="8576" id="token-89-28" morph="none" pos="unknown" start_char="8559">alert/preparedness</TOKEN>
<TOKEN end_char="8580" id="token-89-29" morph="none" pos="word" start_char="8578">and</TOKEN>
<TOKEN end_char="8583" id="token-89-30" morph="none" pos="word" start_char="8582">so</TOKEN>
<TOKEN end_char="8586" id="token-89-31" morph="none" pos="word" start_char="8585">is</TOKEN>
<TOKEN end_char="8590" id="token-89-32" morph="none" pos="word" start_char="8588">the</TOKEN>
<TOKEN end_char="8598" id="token-89-33" morph="none" pos="word" start_char="8592">general</TOKEN>
<TOKEN end_char="8609" id="token-89-34" morph="none" pos="word" start_char="8600">population</TOKEN>
<TOKEN end_char="8611" id="token-89-35" morph="none" pos="word" start_char="8611">a</TOKEN>
<TOKEN end_char="8622" id="token-89-36" morph="none" pos="word" start_char="8613">percentage</TOKEN>
<TOKEN end_char="8625" id="token-89-37" morph="none" pos="word" start_char="8624">of</TOKEN>
<TOKEN end_char="8630" id="token-89-38" morph="none" pos="word" start_char="8627">whom</TOKEN>
<TOKEN end_char="8635" id="token-89-39" morph="none" pos="word" start_char="8632">went</TOKEN>
<TOKEN end_char="8643" id="token-89-40" morph="none" pos="word" start_char="8637">through</TOKEN>
<TOKEN end_char="8649" id="token-89-41" morph="none" pos="word" start_char="8645">those</TOKEN>
<TOKEN end_char="8656" id="token-89-42" morph="none" pos="word" start_char="8651">events</TOKEN>
<TOKEN end_char="8657" id="token-89-43" morph="none" pos="punct" start_char="8657">.</TOKEN>
</SEG>
<SEG end_char="8793" id="segment-90" start_char="8659">
<ORIGINAL_TEXT>Maybe too those past events also mean that politicians are more willing to be "strict" sooner and less worried about staying "popular".</ORIGINAL_TEXT>
<TOKEN end_char="8663" id="token-90-0" morph="none" pos="word" start_char="8659">Maybe</TOKEN>
<TOKEN end_char="8667" id="token-90-1" morph="none" pos="word" start_char="8665">too</TOKEN>
<TOKEN end_char="8673" id="token-90-2" morph="none" pos="word" start_char="8669">those</TOKEN>
<TOKEN end_char="8678" id="token-90-3" morph="none" pos="word" start_char="8675">past</TOKEN>
<TOKEN end_char="8685" id="token-90-4" morph="none" pos="word" start_char="8680">events</TOKEN>
<TOKEN end_char="8690" id="token-90-5" morph="none" pos="word" start_char="8687">also</TOKEN>
<TOKEN end_char="8695" id="token-90-6" morph="none" pos="word" start_char="8692">mean</TOKEN>
<TOKEN end_char="8700" id="token-90-7" morph="none" pos="word" start_char="8697">that</TOKEN>
<TOKEN end_char="8712" id="token-90-8" morph="none" pos="word" start_char="8702">politicians</TOKEN>
<TOKEN end_char="8716" id="token-90-9" morph="none" pos="word" start_char="8714">are</TOKEN>
<TOKEN end_char="8721" id="token-90-10" morph="none" pos="word" start_char="8718">more</TOKEN>
<TOKEN end_char="8729" id="token-90-11" morph="none" pos="word" start_char="8723">willing</TOKEN>
<TOKEN end_char="8732" id="token-90-12" morph="none" pos="word" start_char="8731">to</TOKEN>
<TOKEN end_char="8735" id="token-90-13" morph="none" pos="word" start_char="8734">be</TOKEN>
<TOKEN end_char="8737" id="token-90-14" morph="none" pos="punct" start_char="8737">"</TOKEN>
<TOKEN end_char="8743" id="token-90-15" morph="none" pos="word" start_char="8738">strict</TOKEN>
<TOKEN end_char="8744" id="token-90-16" morph="none" pos="punct" start_char="8744">"</TOKEN>
<TOKEN end_char="8751" id="token-90-17" morph="none" pos="word" start_char="8746">sooner</TOKEN>
<TOKEN end_char="8755" id="token-90-18" morph="none" pos="word" start_char="8753">and</TOKEN>
<TOKEN end_char="8760" id="token-90-19" morph="none" pos="word" start_char="8757">less</TOKEN>
<TOKEN end_char="8768" id="token-90-20" morph="none" pos="word" start_char="8762">worried</TOKEN>
<TOKEN end_char="8774" id="token-90-21" morph="none" pos="word" start_char="8770">about</TOKEN>
<TOKEN end_char="8782" id="token-90-22" morph="none" pos="word" start_char="8776">staying</TOKEN>
<TOKEN end_char="8784" id="token-90-23" morph="none" pos="punct" start_char="8784">"</TOKEN>
<TOKEN end_char="8791" id="token-90-24" morph="none" pos="word" start_char="8785">popular</TOKEN>
<TOKEN end_char="8793" id="token-90-25" morph="none" pos="punct" start_char="8792">".</TOKEN>
</SEG>
<SEG end_char="9091" id="segment-91" start_char="8795">
<ORIGINAL_TEXT>And whilst it is a sweeping generalisation many parts of the Far East are fairly monocultural with more of a tradition of "compliance"...and some of them not slow to send out law enforcement who do not "engage with people" or "advise" them to "go home" but show them the sharp end of bamboo canes.</ORIGINAL_TEXT>
<TOKEN end_char="8797" id="token-91-0" morph="none" pos="word" start_char="8795">And</TOKEN>
<TOKEN end_char="8804" id="token-91-1" morph="none" pos="word" start_char="8799">whilst</TOKEN>
<TOKEN end_char="8807" id="token-91-2" morph="none" pos="word" start_char="8806">it</TOKEN>
<TOKEN end_char="8810" id="token-91-3" morph="none" pos="word" start_char="8809">is</TOKEN>
<TOKEN end_char="8812" id="token-91-4" morph="none" pos="word" start_char="8812">a</TOKEN>
<TOKEN end_char="8821" id="token-91-5" morph="none" pos="word" start_char="8814">sweeping</TOKEN>
<TOKEN end_char="8836" id="token-91-6" morph="none" pos="word" start_char="8823">generalisation</TOKEN>
<TOKEN end_char="8841" id="token-91-7" morph="none" pos="word" start_char="8838">many</TOKEN>
<TOKEN end_char="8847" id="token-91-8" morph="none" pos="word" start_char="8843">parts</TOKEN>
<TOKEN end_char="8850" id="token-91-9" morph="none" pos="word" start_char="8849">of</TOKEN>
<TOKEN end_char="8854" id="token-91-10" morph="none" pos="word" start_char="8852">the</TOKEN>
<TOKEN end_char="8858" id="token-91-11" morph="none" pos="word" start_char="8856">Far</TOKEN>
<TOKEN end_char="8863" id="token-91-12" morph="none" pos="word" start_char="8860">East</TOKEN>
<TOKEN end_char="8867" id="token-91-13" morph="none" pos="word" start_char="8865">are</TOKEN>
<TOKEN end_char="8874" id="token-91-14" morph="none" pos="word" start_char="8869">fairly</TOKEN>
<TOKEN end_char="8887" id="token-91-15" morph="none" pos="word" start_char="8876">monocultural</TOKEN>
<TOKEN end_char="8892" id="token-91-16" morph="none" pos="word" start_char="8889">with</TOKEN>
<TOKEN end_char="8897" id="token-91-17" morph="none" pos="word" start_char="8894">more</TOKEN>
<TOKEN end_char="8900" id="token-91-18" morph="none" pos="word" start_char="8899">of</TOKEN>
<TOKEN end_char="8902" id="token-91-19" morph="none" pos="word" start_char="8902">a</TOKEN>
<TOKEN end_char="8912" id="token-91-20" morph="none" pos="word" start_char="8904">tradition</TOKEN>
<TOKEN end_char="8915" id="token-91-21" morph="none" pos="word" start_char="8914">of</TOKEN>
<TOKEN end_char="8917" id="token-91-22" morph="none" pos="punct" start_char="8917">"</TOKEN>
<TOKEN end_char="8934" id="token-91-23" morph="none" pos="unknown" start_char="8918">compliance"...and</TOKEN>
<TOKEN end_char="8939" id="token-91-24" morph="none" pos="word" start_char="8936">some</TOKEN>
<TOKEN end_char="8942" id="token-91-25" morph="none" pos="word" start_char="8941">of</TOKEN>
<TOKEN end_char="8947" id="token-91-26" morph="none" pos="word" start_char="8944">them</TOKEN>
<TOKEN end_char="8951" id="token-91-27" morph="none" pos="word" start_char="8949">not</TOKEN>
<TOKEN end_char="8956" id="token-91-28" morph="none" pos="word" start_char="8953">slow</TOKEN>
<TOKEN end_char="8959" id="token-91-29" morph="none" pos="word" start_char="8958">to</TOKEN>
<TOKEN end_char="8964" id="token-91-30" morph="none" pos="word" start_char="8961">send</TOKEN>
<TOKEN end_char="8968" id="token-91-31" morph="none" pos="word" start_char="8966">out</TOKEN>
<TOKEN end_char="8972" id="token-91-32" morph="none" pos="word" start_char="8970">law</TOKEN>
<TOKEN end_char="8984" id="token-91-33" morph="none" pos="word" start_char="8974">enforcement</TOKEN>
<TOKEN end_char="8988" id="token-91-34" morph="none" pos="word" start_char="8986">who</TOKEN>
<TOKEN end_char="8991" id="token-91-35" morph="none" pos="word" start_char="8990">do</TOKEN>
<TOKEN end_char="8995" id="token-91-36" morph="none" pos="word" start_char="8993">not</TOKEN>
<TOKEN end_char="8997" id="token-91-37" morph="none" pos="punct" start_char="8997">"</TOKEN>
<TOKEN end_char="9003" id="token-91-38" morph="none" pos="word" start_char="8998">engage</TOKEN>
<TOKEN end_char="9008" id="token-91-39" morph="none" pos="word" start_char="9005">with</TOKEN>
<TOKEN end_char="9015" id="token-91-40" morph="none" pos="word" start_char="9010">people</TOKEN>
<TOKEN end_char="9016" id="token-91-41" morph="none" pos="punct" start_char="9016">"</TOKEN>
<TOKEN end_char="9019" id="token-91-42" morph="none" pos="word" start_char="9018">or</TOKEN>
<TOKEN end_char="9021" id="token-91-43" morph="none" pos="punct" start_char="9021">"</TOKEN>
<TOKEN end_char="9027" id="token-91-44" morph="none" pos="word" start_char="9022">advise</TOKEN>
<TOKEN end_char="9028" id="token-91-45" morph="none" pos="punct" start_char="9028">"</TOKEN>
<TOKEN end_char="9033" id="token-91-46" morph="none" pos="word" start_char="9030">them</TOKEN>
<TOKEN end_char="9036" id="token-91-47" morph="none" pos="word" start_char="9035">to</TOKEN>
<TOKEN end_char="9038" id="token-91-48" morph="none" pos="punct" start_char="9038">"</TOKEN>
<TOKEN end_char="9040" id="token-91-49" morph="none" pos="word" start_char="9039">go</TOKEN>
<TOKEN end_char="9045" id="token-91-50" morph="none" pos="word" start_char="9042">home</TOKEN>
<TOKEN end_char="9046" id="token-91-51" morph="none" pos="punct" start_char="9046">"</TOKEN>
<TOKEN end_char="9050" id="token-91-52" morph="none" pos="word" start_char="9048">but</TOKEN>
<TOKEN end_char="9055" id="token-91-53" morph="none" pos="word" start_char="9052">show</TOKEN>
<TOKEN end_char="9060" id="token-91-54" morph="none" pos="word" start_char="9057">them</TOKEN>
<TOKEN end_char="9064" id="token-91-55" morph="none" pos="word" start_char="9062">the</TOKEN>
<TOKEN end_char="9070" id="token-91-56" morph="none" pos="word" start_char="9066">sharp</TOKEN>
<TOKEN end_char="9074" id="token-91-57" morph="none" pos="word" start_char="9072">end</TOKEN>
<TOKEN end_char="9077" id="token-91-58" morph="none" pos="word" start_char="9076">of</TOKEN>
<TOKEN end_char="9084" id="token-91-59" morph="none" pos="word" start_char="9079">bamboo</TOKEN>
<TOKEN end_char="9090" id="token-91-60" morph="none" pos="word" start_char="9086">canes</TOKEN>
<TOKEN end_char="9091" id="token-91-61" morph="none" pos="punct" start_char="9091">.</TOKEN>
</SEG>
<SEG end_char="9110" id="segment-92" start_char="9093">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="9097" id="token-92-0" morph="none" pos="word" start_char="9093">Click</TOKEN>
<TOKEN end_char="9100" id="token-92-1" morph="none" pos="word" start_char="9099">to</TOKEN>
<TOKEN end_char="9107" id="token-92-2" morph="none" pos="word" start_char="9102">expand</TOKEN>
<TOKEN end_char="9110" id="token-92-3" morph="none" pos="punct" start_char="9108">...</TOKEN>
</SEG>
<SEG end_char="9192" id="segment-93" start_char="9113">
<ORIGINAL_TEXT>This may be true for individual countries, but not for every single one of them.</ORIGINAL_TEXT>
<TOKEN end_char="9116" id="token-93-0" morph="none" pos="word" start_char="9113">This</TOKEN>
<TOKEN end_char="9120" id="token-93-1" morph="none" pos="word" start_char="9118">may</TOKEN>
<TOKEN end_char="9123" id="token-93-2" morph="none" pos="word" start_char="9122">be</TOKEN>
<TOKEN end_char="9128" id="token-93-3" morph="none" pos="word" start_char="9125">true</TOKEN>
<TOKEN end_char="9132" id="token-93-4" morph="none" pos="word" start_char="9130">for</TOKEN>
<TOKEN end_char="9143" id="token-93-5" morph="none" pos="word" start_char="9134">individual</TOKEN>
<TOKEN end_char="9153" id="token-93-6" morph="none" pos="word" start_char="9145">countries</TOKEN>
<TOKEN end_char="9154" id="token-93-7" morph="none" pos="punct" start_char="9154">,</TOKEN>
<TOKEN end_char="9158" id="token-93-8" morph="none" pos="word" start_char="9156">but</TOKEN>
<TOKEN end_char="9162" id="token-93-9" morph="none" pos="word" start_char="9160">not</TOKEN>
<TOKEN end_char="9166" id="token-93-10" morph="none" pos="word" start_char="9164">for</TOKEN>
<TOKEN end_char="9172" id="token-93-11" morph="none" pos="word" start_char="9168">every</TOKEN>
<TOKEN end_char="9179" id="token-93-12" morph="none" pos="word" start_char="9174">single</TOKEN>
<TOKEN end_char="9183" id="token-93-13" morph="none" pos="word" start_char="9181">one</TOKEN>
<TOKEN end_char="9186" id="token-93-14" morph="none" pos="word" start_char="9185">of</TOKEN>
<TOKEN end_char="9191" id="token-93-15" morph="none" pos="word" start_char="9188">them</TOKEN>
<TOKEN end_char="9192" id="token-93-16" morph="none" pos="punct" start_char="9192">.</TOKEN>
</SEG>
<SEG end_char="9333" id="segment-94" start_char="9194">
<ORIGINAL_TEXT>Some of them have barely got a sanitation system let alone a comprehensive test and trace system or the ability to enforce strict lockdowns.</ORIGINAL_TEXT>
<TOKEN end_char="9197" id="token-94-0" morph="none" pos="word" start_char="9194">Some</TOKEN>
<TOKEN end_char="9200" id="token-94-1" morph="none" pos="word" start_char="9199">of</TOKEN>
<TOKEN end_char="9205" id="token-94-2" morph="none" pos="word" start_char="9202">them</TOKEN>
<TOKEN end_char="9210" id="token-94-3" morph="none" pos="word" start_char="9207">have</TOKEN>
<TOKEN end_char="9217" id="token-94-4" morph="none" pos="word" start_char="9212">barely</TOKEN>
<TOKEN end_char="9221" id="token-94-5" morph="none" pos="word" start_char="9219">got</TOKEN>
<TOKEN end_char="9223" id="token-94-6" morph="none" pos="word" start_char="9223">a</TOKEN>
<TOKEN end_char="9234" id="token-94-7" morph="none" pos="word" start_char="9225">sanitation</TOKEN>
<TOKEN end_char="9241" id="token-94-8" morph="none" pos="word" start_char="9236">system</TOKEN>
<TOKEN end_char="9245" id="token-94-9" morph="none" pos="word" start_char="9243">let</TOKEN>
<TOKEN end_char="9251" id="token-94-10" morph="none" pos="word" start_char="9247">alone</TOKEN>
<TOKEN end_char="9253" id="token-94-11" morph="none" pos="word" start_char="9253">a</TOKEN>
<TOKEN end_char="9267" id="token-94-12" morph="none" pos="word" start_char="9255">comprehensive</TOKEN>
<TOKEN end_char="9272" id="token-94-13" morph="none" pos="word" start_char="9269">test</TOKEN>
<TOKEN end_char="9276" id="token-94-14" morph="none" pos="word" start_char="9274">and</TOKEN>
<TOKEN end_char="9282" id="token-94-15" morph="none" pos="word" start_char="9278">trace</TOKEN>
<TOKEN end_char="9289" id="token-94-16" morph="none" pos="word" start_char="9284">system</TOKEN>
<TOKEN end_char="9292" id="token-94-17" morph="none" pos="word" start_char="9291">or</TOKEN>
<TOKEN end_char="9296" id="token-94-18" morph="none" pos="word" start_char="9294">the</TOKEN>
<TOKEN end_char="9304" id="token-94-19" morph="none" pos="word" start_char="9298">ability</TOKEN>
<TOKEN end_char="9307" id="token-94-20" morph="none" pos="word" start_char="9306">to</TOKEN>
<TOKEN end_char="9315" id="token-94-21" morph="none" pos="word" start_char="9309">enforce</TOKEN>
<TOKEN end_char="9322" id="token-94-22" morph="none" pos="word" start_char="9317">strict</TOKEN>
<TOKEN end_char="9332" id="token-94-23" morph="none" pos="word" start_char="9324">lockdowns</TOKEN>
<TOKEN end_char="9333" id="token-94-24" morph="none" pos="punct" start_char="9333">.</TOKEN>
</SEG>
<SEG end_char="9381" id="segment-95" start_char="9336">
<ORIGINAL_TEXT>There must be something else that is going on.</ORIGINAL_TEXT>
<TOKEN end_char="9340" id="token-95-0" morph="none" pos="word" start_char="9336">There</TOKEN>
<TOKEN end_char="9345" id="token-95-1" morph="none" pos="word" start_char="9342">must</TOKEN>
<TOKEN end_char="9348" id="token-95-2" morph="none" pos="word" start_char="9347">be</TOKEN>
<TOKEN end_char="9358" id="token-95-3" morph="none" pos="word" start_char="9350">something</TOKEN>
<TOKEN end_char="9363" id="token-95-4" morph="none" pos="word" start_char="9360">else</TOKEN>
<TOKEN end_char="9368" id="token-95-5" morph="none" pos="word" start_char="9365">that</TOKEN>
<TOKEN end_char="9371" id="token-95-6" morph="none" pos="word" start_char="9370">is</TOKEN>
<TOKEN end_char="9377" id="token-95-7" morph="none" pos="word" start_char="9373">going</TOKEN>
<TOKEN end_char="9380" id="token-95-8" morph="none" pos="word" start_char="9379">on</TOKEN>
<TOKEN end_char="9381" id="token-95-9" morph="none" pos="punct" start_char="9381">.</TOKEN>
</SEG>
<SEG end_char="9439" id="segment-96" start_char="9386">
<ORIGINAL_TEXT>realfrankturner said: Also look at places like NZ too.</ORIGINAL_TEXT>
<TOKEN end_char="9400" id="token-96-0" morph="none" pos="word" start_char="9386">realfrankturner</TOKEN>
<TOKEN end_char="9405" id="token-96-1" morph="none" pos="word" start_char="9402">said</TOKEN>
<TOKEN end_char="9406" id="token-96-2" morph="none" pos="punct" start_char="9406">:</TOKEN>
<TOKEN end_char="9411" id="token-96-3" morph="none" pos="word" start_char="9408">Also</TOKEN>
<TOKEN end_char="9416" id="token-96-4" morph="none" pos="word" start_char="9413">look</TOKEN>
<TOKEN end_char="9419" id="token-96-5" morph="none" pos="word" start_char="9418">at</TOKEN>
<TOKEN end_char="9426" id="token-96-6" morph="none" pos="word" start_char="9421">places</TOKEN>
<TOKEN end_char="9431" id="token-96-7" morph="none" pos="word" start_char="9428">like</TOKEN>
<TOKEN end_char="9434" id="token-96-8" morph="none" pos="word" start_char="9433">NZ</TOKEN>
<TOKEN end_char="9438" id="token-96-9" morph="none" pos="word" start_char="9436">too</TOKEN>
<TOKEN end_char="9439" id="token-96-10" morph="none" pos="punct" start_char="9439">.</TOKEN>
</SEG>
<SEG end_char="9560" id="segment-97" start_char="9443">
<ORIGINAL_TEXT>NZ is interesting because the Maori population is much more susceptible to Flu outbreaks that the European population.</ORIGINAL_TEXT>
<TOKEN end_char="9444" id="token-97-0" morph="none" pos="word" start_char="9443">NZ</TOKEN>
<TOKEN end_char="9447" id="token-97-1" morph="none" pos="word" start_char="9446">is</TOKEN>
<TOKEN end_char="9459" id="token-97-2" morph="none" pos="word" start_char="9449">interesting</TOKEN>
<TOKEN end_char="9467" id="token-97-3" morph="none" pos="word" start_char="9461">because</TOKEN>
<TOKEN end_char="9471" id="token-97-4" morph="none" pos="word" start_char="9469">the</TOKEN>
<TOKEN end_char="9477" id="token-97-5" morph="none" pos="word" start_char="9473">Maori</TOKEN>
<TOKEN end_char="9488" id="token-97-6" morph="none" pos="word" start_char="9479">population</TOKEN>
<TOKEN end_char="9491" id="token-97-7" morph="none" pos="word" start_char="9490">is</TOKEN>
<TOKEN end_char="9496" id="token-97-8" morph="none" pos="word" start_char="9493">much</TOKEN>
<TOKEN end_char="9501" id="token-97-9" morph="none" pos="word" start_char="9498">more</TOKEN>
<TOKEN end_char="9513" id="token-97-10" morph="none" pos="word" start_char="9503">susceptible</TOKEN>
<TOKEN end_char="9516" id="token-97-11" morph="none" pos="word" start_char="9515">to</TOKEN>
<TOKEN end_char="9520" id="token-97-12" morph="none" pos="word" start_char="9518">Flu</TOKEN>
<TOKEN end_char="9530" id="token-97-13" morph="none" pos="word" start_char="9522">outbreaks</TOKEN>
<TOKEN end_char="9535" id="token-97-14" morph="none" pos="word" start_char="9532">that</TOKEN>
<TOKEN end_char="9539" id="token-97-15" morph="none" pos="word" start_char="9537">the</TOKEN>
<TOKEN end_char="9548" id="token-97-16" morph="none" pos="word" start_char="9541">European</TOKEN>
<TOKEN end_char="9559" id="token-97-17" morph="none" pos="word" start_char="9550">population</TOKEN>
<TOKEN end_char="9560" id="token-97-18" morph="none" pos="punct" start_char="9560">.</TOKEN>
</SEG>
<SEG end_char="9760" id="segment-98" start_char="9563">
<ORIGINAL_TEXT>If you talk to people, they will claim its deprivation that is the cause but if you look at the NZ government pandemic response document it does actually say there may be genetic makeup differences.</ORIGINAL_TEXT>
<TOKEN end_char="9564" id="token-98-0" morph="none" pos="word" start_char="9563">If</TOKEN>
<TOKEN end_char="9568" id="token-98-1" morph="none" pos="word" start_char="9566">you</TOKEN>
<TOKEN end_char="9573" id="token-98-2" morph="none" pos="word" start_char="9570">talk</TOKEN>
<TOKEN end_char="9576" id="token-98-3" morph="none" pos="word" start_char="9575">to</TOKEN>
<TOKEN end_char="9583" id="token-98-4" morph="none" pos="word" start_char="9578">people</TOKEN>
<TOKEN end_char="9584" id="token-98-5" morph="none" pos="punct" start_char="9584">,</TOKEN>
<TOKEN end_char="9589" id="token-98-6" morph="none" pos="word" start_char="9586">they</TOKEN>
<TOKEN end_char="9594" id="token-98-7" morph="none" pos="word" start_char="9591">will</TOKEN>
<TOKEN end_char="9600" id="token-98-8" morph="none" pos="word" start_char="9596">claim</TOKEN>
<TOKEN end_char="9604" id="token-98-9" morph="none" pos="word" start_char="9602">its</TOKEN>
<TOKEN end_char="9616" id="token-98-10" morph="none" pos="word" start_char="9606">deprivation</TOKEN>
<TOKEN end_char="9621" id="token-98-11" morph="none" pos="word" start_char="9618">that</TOKEN>
<TOKEN end_char="9624" id="token-98-12" morph="none" pos="word" start_char="9623">is</TOKEN>
<TOKEN end_char="9628" id="token-98-13" morph="none" pos="word" start_char="9626">the</TOKEN>
<TOKEN end_char="9634" id="token-98-14" morph="none" pos="word" start_char="9630">cause</TOKEN>
<TOKEN end_char="9638" id="token-98-15" morph="none" pos="word" start_char="9636">but</TOKEN>
<TOKEN end_char="9641" id="token-98-16" morph="none" pos="word" start_char="9640">if</TOKEN>
<TOKEN end_char="9645" id="token-98-17" morph="none" pos="word" start_char="9643">you</TOKEN>
<TOKEN end_char="9650" id="token-98-18" morph="none" pos="word" start_char="9647">look</TOKEN>
<TOKEN end_char="9653" id="token-98-19" morph="none" pos="word" start_char="9652">at</TOKEN>
<TOKEN end_char="9657" id="token-98-20" morph="none" pos="word" start_char="9655">the</TOKEN>
<TOKEN end_char="9660" id="token-98-21" morph="none" pos="word" start_char="9659">NZ</TOKEN>
<TOKEN end_char="9671" id="token-98-22" morph="none" pos="word" start_char="9662">government</TOKEN>
<TOKEN end_char="9680" id="token-98-23" morph="none" pos="word" start_char="9673">pandemic</TOKEN>
<TOKEN end_char="9689" id="token-98-24" morph="none" pos="word" start_char="9682">response</TOKEN>
<TOKEN end_char="9698" id="token-98-25" morph="none" pos="word" start_char="9691">document</TOKEN>
<TOKEN end_char="9701" id="token-98-26" morph="none" pos="word" start_char="9700">it</TOKEN>
<TOKEN end_char="9706" id="token-98-27" morph="none" pos="word" start_char="9703">does</TOKEN>
<TOKEN end_char="9715" id="token-98-28" morph="none" pos="word" start_char="9708">actually</TOKEN>
<TOKEN end_char="9719" id="token-98-29" morph="none" pos="word" start_char="9717">say</TOKEN>
<TOKEN end_char="9725" id="token-98-30" morph="none" pos="word" start_char="9721">there</TOKEN>
<TOKEN end_char="9729" id="token-98-31" morph="none" pos="word" start_char="9727">may</TOKEN>
<TOKEN end_char="9732" id="token-98-32" morph="none" pos="word" start_char="9731">be</TOKEN>
<TOKEN end_char="9740" id="token-98-33" morph="none" pos="word" start_char="9734">genetic</TOKEN>
<TOKEN end_char="9747" id="token-98-34" morph="none" pos="word" start_char="9742">makeup</TOKEN>
<TOKEN end_char="9759" id="token-98-35" morph="none" pos="word" start_char="9749">differences</TOKEN>
<TOKEN end_char="9760" id="token-98-36" morph="none" pos="punct" start_char="9760">.</TOKEN>
</SEG>
<SEG end_char="9798" id="segment-99" start_char="9763">
<ORIGINAL_TEXT>Maybe it goes the other way for flu.</ORIGINAL_TEXT>
<TOKEN end_char="9767" id="token-99-0" morph="none" pos="word" start_char="9763">Maybe</TOKEN>
<TOKEN end_char="9770" id="token-99-1" morph="none" pos="word" start_char="9769">it</TOKEN>
<TOKEN end_char="9775" id="token-99-2" morph="none" pos="word" start_char="9772">goes</TOKEN>
<TOKEN end_char="9779" id="token-99-3" morph="none" pos="word" start_char="9777">the</TOKEN>
<TOKEN end_char="9785" id="token-99-4" morph="none" pos="word" start_char="9781">other</TOKEN>
<TOKEN end_char="9789" id="token-99-5" morph="none" pos="word" start_char="9787">way</TOKEN>
<TOKEN end_char="9793" id="token-99-6" morph="none" pos="word" start_char="9791">for</TOKEN>
<TOKEN end_char="9797" id="token-99-7" morph="none" pos="word" start_char="9795">flu</TOKEN>
<TOKEN end_char="9798" id="token-99-8" morph="none" pos="punct" start_char="9798">.</TOKEN>
</SEG>
<SEG end_char="9907" id="segment-100" start_char="9803">
<ORIGINAL_TEXT>BorkenArrow said: I don't believe every single country in Asia did that or has the capability to do that.</ORIGINAL_TEXT>
<TOKEN end_char="9813" id="token-100-0" morph="none" pos="word" start_char="9803">BorkenArrow</TOKEN>
<TOKEN end_char="9818" id="token-100-1" morph="none" pos="word" start_char="9815">said</TOKEN>
<TOKEN end_char="9819" id="token-100-2" morph="none" pos="punct" start_char="9819">:</TOKEN>
<TOKEN end_char="9821" id="token-100-3" morph="none" pos="word" start_char="9821">I</TOKEN>
<TOKEN end_char="9827" id="token-100-4" morph="none" pos="word" start_char="9823">don't</TOKEN>
<TOKEN end_char="9835" id="token-100-5" morph="none" pos="word" start_char="9829">believe</TOKEN>
<TOKEN end_char="9841" id="token-100-6" morph="none" pos="word" start_char="9837">every</TOKEN>
<TOKEN end_char="9848" id="token-100-7" morph="none" pos="word" start_char="9843">single</TOKEN>
<TOKEN end_char="9856" id="token-100-8" morph="none" pos="word" start_char="9850">country</TOKEN>
<TOKEN end_char="9859" id="token-100-9" morph="none" pos="word" start_char="9858">in</TOKEN>
<TOKEN end_char="9864" id="token-100-10" morph="none" pos="word" start_char="9861">Asia</TOKEN>
<TOKEN end_char="9868" id="token-100-11" morph="none" pos="word" start_char="9866">did</TOKEN>
<TOKEN end_char="9873" id="token-100-12" morph="none" pos="word" start_char="9870">that</TOKEN>
<TOKEN end_char="9876" id="token-100-13" morph="none" pos="word" start_char="9875">or</TOKEN>
<TOKEN end_char="9880" id="token-100-14" morph="none" pos="word" start_char="9878">has</TOKEN>
<TOKEN end_char="9884" id="token-100-15" morph="none" pos="word" start_char="9882">the</TOKEN>
<TOKEN end_char="9895" id="token-100-16" morph="none" pos="word" start_char="9886">capability</TOKEN>
<TOKEN end_char="9898" id="token-100-17" morph="none" pos="word" start_char="9897">to</TOKEN>
<TOKEN end_char="9901" id="token-100-18" morph="none" pos="word" start_char="9900">do</TOKEN>
<TOKEN end_char="9906" id="token-100-19" morph="none" pos="word" start_char="9903">that</TOKEN>
<TOKEN end_char="9907" id="token-100-20" morph="none" pos="punct" start_char="9907">.</TOKEN>
</SEG>
<SEG end_char="9947" id="segment-101" start_char="9909">
<ORIGINAL_TEXT>Good article though so thanks for that.</ORIGINAL_TEXT>
<TOKEN end_char="9912" id="token-101-0" morph="none" pos="word" start_char="9909">Good</TOKEN>
<TOKEN end_char="9920" id="token-101-1" morph="none" pos="word" start_char="9914">article</TOKEN>
<TOKEN end_char="9927" id="token-101-2" morph="none" pos="word" start_char="9922">though</TOKEN>
<TOKEN end_char="9930" id="token-101-3" morph="none" pos="word" start_char="9929">so</TOKEN>
<TOKEN end_char="9937" id="token-101-4" morph="none" pos="word" start_char="9932">thanks</TOKEN>
<TOKEN end_char="9941" id="token-101-5" morph="none" pos="word" start_char="9939">for</TOKEN>
<TOKEN end_char="9946" id="token-101-6" morph="none" pos="word" start_char="9943">that</TOKEN>
<TOKEN end_char="9947" id="token-101-7" morph="none" pos="punct" start_char="9947">.</TOKEN>
</SEG>
<SEG end_char="9995" id="segment-102" start_char="9949">
<ORIGINAL_TEXT>So it looks like asians are genetically immune.</ORIGINAL_TEXT>
<TOKEN end_char="9950" id="token-102-0" morph="none" pos="word" start_char="9949">So</TOKEN>
<TOKEN end_char="9953" id="token-102-1" morph="none" pos="word" start_char="9952">it</TOKEN>
<TOKEN end_char="9959" id="token-102-2" morph="none" pos="word" start_char="9955">looks</TOKEN>
<TOKEN end_char="9964" id="token-102-3" morph="none" pos="word" start_char="9961">like</TOKEN>
<TOKEN end_char="9971" id="token-102-4" morph="none" pos="word" start_char="9966">asians</TOKEN>
<TOKEN end_char="9975" id="token-102-5" morph="none" pos="word" start_char="9973">are</TOKEN>
<TOKEN end_char="9987" id="token-102-6" morph="none" pos="word" start_char="9977">genetically</TOKEN>
<TOKEN end_char="9994" id="token-102-7" morph="none" pos="word" start_char="9989">immune</TOKEN>
<TOKEN end_char="9995" id="token-102-8" morph="none" pos="punct" start_char="9995">.</TOKEN>
</SEG>
<SEG end_char="10065" id="segment-103" start_char="9997">
<ORIGINAL_TEXT>and yes had a little trouble spelling the user name when registering.</ORIGINAL_TEXT>
<TOKEN end_char="9999" id="token-103-0" morph="none" pos="word" start_char="9997">and</TOKEN>
<TOKEN end_char="10003" id="token-103-1" morph="none" pos="word" start_char="10001">yes</TOKEN>
<TOKEN end_char="10007" id="token-103-2" morph="none" pos="word" start_char="10005">had</TOKEN>
<TOKEN end_char="10009" id="token-103-3" morph="none" pos="word" start_char="10009">a</TOKEN>
<TOKEN end_char="10016" id="token-103-4" morph="none" pos="word" start_char="10011">little</TOKEN>
<TOKEN end_char="10024" id="token-103-5" morph="none" pos="word" start_char="10018">trouble</TOKEN>
<TOKEN end_char="10033" id="token-103-6" morph="none" pos="word" start_char="10026">spelling</TOKEN>
<TOKEN end_char="10037" id="token-103-7" morph="none" pos="word" start_char="10035">the</TOKEN>
<TOKEN end_char="10042" id="token-103-8" morph="none" pos="word" start_char="10039">user</TOKEN>
<TOKEN end_char="10047" id="token-103-9" morph="none" pos="word" start_char="10044">name</TOKEN>
<TOKEN end_char="10052" id="token-103-10" morph="none" pos="word" start_char="10049">when</TOKEN>
<TOKEN end_char="10064" id="token-103-11" morph="none" pos="word" start_char="10054">registering</TOKEN>
<TOKEN end_char="10065" id="token-103-12" morph="none" pos="punct" start_char="10065">.</TOKEN>
</SEG>
<SEG end_char="10099" id="segment-104" start_char="10067">
<ORIGINAL_TEXT>It caused much amusement to some.</ORIGINAL_TEXT>
<TOKEN end_char="10068" id="token-104-0" morph="none" pos="word" start_char="10067">It</TOKEN>
<TOKEN end_char="10075" id="token-104-1" morph="none" pos="word" start_char="10070">caused</TOKEN>
<TOKEN end_char="10080" id="token-104-2" morph="none" pos="word" start_char="10077">much</TOKEN>
<TOKEN end_char="10090" id="token-104-3" morph="none" pos="word" start_char="10082">amusement</TOKEN>
<TOKEN end_char="10093" id="token-104-4" morph="none" pos="word" start_char="10092">to</TOKEN>
<TOKEN end_char="10098" id="token-104-5" morph="none" pos="word" start_char="10095">some</TOKEN>
<TOKEN end_char="10099" id="token-104-6" morph="none" pos="punct" start_char="10099">.</TOKEN>
</SEG>
<SEG end_char="10118" id="segment-105" start_char="10101">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="10105" id="token-105-0" morph="none" pos="word" start_char="10101">Click</TOKEN>
<TOKEN end_char="10108" id="token-105-1" morph="none" pos="word" start_char="10107">to</TOKEN>
<TOKEN end_char="10115" id="token-105-2" morph="none" pos="word" start_char="10110">expand</TOKEN>
<TOKEN end_char="10118" id="token-105-3" morph="none" pos="punct" start_char="10116">...</TOKEN>
</SEG>
<SEG end_char="10244" id="segment-106" start_char="10121">
<ORIGINAL_TEXT>From the incidence figures it also appears that the 200K western population in Thailand have acquired this genetic immunity.</ORIGINAL_TEXT>
<TOKEN end_char="10124" id="token-106-0" morph="none" pos="word" start_char="10121">From</TOKEN>
<TOKEN end_char="10128" id="token-106-1" morph="none" pos="word" start_char="10126">the</TOKEN>
<TOKEN end_char="10138" id="token-106-2" morph="none" pos="word" start_char="10130">incidence</TOKEN>
<TOKEN end_char="10146" id="token-106-3" morph="none" pos="word" start_char="10140">figures</TOKEN>
<TOKEN end_char="10149" id="token-106-4" morph="none" pos="word" start_char="10148">it</TOKEN>
<TOKEN end_char="10154" id="token-106-5" morph="none" pos="word" start_char="10151">also</TOKEN>
<TOKEN end_char="10162" id="token-106-6" morph="none" pos="word" start_char="10156">appears</TOKEN>
<TOKEN end_char="10167" id="token-106-7" morph="none" pos="word" start_char="10164">that</TOKEN>
<TOKEN end_char="10171" id="token-106-8" morph="none" pos="word" start_char="10169">the</TOKEN>
<TOKEN end_char="10176" id="token-106-9" morph="none" pos="word" start_char="10173">200K</TOKEN>
<TOKEN end_char="10184" id="token-106-10" morph="none" pos="word" start_char="10178">western</TOKEN>
<TOKEN end_char="10195" id="token-106-11" morph="none" pos="word" start_char="10186">population</TOKEN>
<TOKEN end_char="10198" id="token-106-12" morph="none" pos="word" start_char="10197">in</TOKEN>
<TOKEN end_char="10207" id="token-106-13" morph="none" pos="word" start_char="10200">Thailand</TOKEN>
<TOKEN end_char="10212" id="token-106-14" morph="none" pos="word" start_char="10209">have</TOKEN>
<TOKEN end_char="10221" id="token-106-15" morph="none" pos="word" start_char="10214">acquired</TOKEN>
<TOKEN end_char="10226" id="token-106-16" morph="none" pos="word" start_char="10223">this</TOKEN>
<TOKEN end_char="10234" id="token-106-17" morph="none" pos="word" start_char="10228">genetic</TOKEN>
<TOKEN end_char="10243" id="token-106-18" morph="none" pos="word" start_char="10236">immunity</TOKEN>
<TOKEN end_char="10244" id="token-106-19" morph="none" pos="punct" start_char="10244">.</TOKEN>
</SEG>
<SEG end_char="10334" id="segment-107" start_char="10246">
<ORIGINAL_TEXT>As have the western origin population of Hong Kong, Macao, Cambodia, Vietnam and Laos....</ORIGINAL_TEXT>
<TOKEN end_char="10247" id="token-107-0" morph="none" pos="word" start_char="10246">As</TOKEN>
<TOKEN end_char="10252" id="token-107-1" morph="none" pos="word" start_char="10249">have</TOKEN>
<TOKEN end_char="10256" id="token-107-2" morph="none" pos="word" start_char="10254">the</TOKEN>
<TOKEN end_char="10264" id="token-107-3" morph="none" pos="word" start_char="10258">western</TOKEN>
<TOKEN end_char="10271" id="token-107-4" morph="none" pos="word" start_char="10266">origin</TOKEN>
<TOKEN end_char="10282" id="token-107-5" morph="none" pos="word" start_char="10273">population</TOKEN>
<TOKEN end_char="10285" id="token-107-6" morph="none" pos="word" start_char="10284">of</TOKEN>
<TOKEN end_char="10290" id="token-107-7" morph="none" pos="word" start_char="10287">Hong</TOKEN>
<TOKEN end_char="10295" id="token-107-8" morph="none" pos="word" start_char="10292">Kong</TOKEN>
<TOKEN end_char="10296" id="token-107-9" morph="none" pos="punct" start_char="10296">,</TOKEN>
<TOKEN end_char="10302" id="token-107-10" morph="none" pos="word" start_char="10298">Macao</TOKEN>
<TOKEN end_char="10303" id="token-107-11" morph="none" pos="punct" start_char="10303">,</TOKEN>
<TOKEN end_char="10312" id="token-107-12" morph="none" pos="word" start_char="10305">Cambodia</TOKEN>
<TOKEN end_char="10313" id="token-107-13" morph="none" pos="punct" start_char="10313">,</TOKEN>
<TOKEN end_char="10321" id="token-107-14" morph="none" pos="word" start_char="10315">Vietnam</TOKEN>
<TOKEN end_char="10325" id="token-107-15" morph="none" pos="word" start_char="10323">and</TOKEN>
<TOKEN end_char="10330" id="token-107-16" morph="none" pos="word" start_char="10327">Laos</TOKEN>
<TOKEN end_char="10334" id="token-107-17" morph="none" pos="punct" start_char="10331">....</TOKEN>
</SEG>
<SEG end_char="10537" id="segment-108" start_char="10337">
<ORIGINAL_TEXT>As the genetics of this population will not have changed to when they were born (in the west) it looks more likely that the measures taken in these countries to stop the spread of covid were effective.</ORIGINAL_TEXT>
<TOKEN end_char="10338" id="token-108-0" morph="none" pos="word" start_char="10337">As</TOKEN>
<TOKEN end_char="10342" id="token-108-1" morph="none" pos="word" start_char="10340">the</TOKEN>
<TOKEN end_char="10351" id="token-108-2" morph="none" pos="word" start_char="10344">genetics</TOKEN>
<TOKEN end_char="10354" id="token-108-3" morph="none" pos="word" start_char="10353">of</TOKEN>
<TOKEN end_char="10359" id="token-108-4" morph="none" pos="word" start_char="10356">this</TOKEN>
<TOKEN end_char="10370" id="token-108-5" morph="none" pos="word" start_char="10361">population</TOKEN>
<TOKEN end_char="10375" id="token-108-6" morph="none" pos="word" start_char="10372">will</TOKEN>
<TOKEN end_char="10379" id="token-108-7" morph="none" pos="word" start_char="10377">not</TOKEN>
<TOKEN end_char="10384" id="token-108-8" morph="none" pos="word" start_char="10381">have</TOKEN>
<TOKEN end_char="10392" id="token-108-9" morph="none" pos="word" start_char="10386">changed</TOKEN>
<TOKEN end_char="10395" id="token-108-10" morph="none" pos="word" start_char="10394">to</TOKEN>
<TOKEN end_char="10400" id="token-108-11" morph="none" pos="word" start_char="10397">when</TOKEN>
<TOKEN end_char="10405" id="token-108-12" morph="none" pos="word" start_char="10402">they</TOKEN>
<TOKEN end_char="10410" id="token-108-13" morph="none" pos="word" start_char="10407">were</TOKEN>
<TOKEN end_char="10415" id="token-108-14" morph="none" pos="word" start_char="10412">born</TOKEN>
<TOKEN end_char="10417" id="token-108-15" morph="none" pos="punct" start_char="10417">(</TOKEN>
<TOKEN end_char="10419" id="token-108-16" morph="none" pos="word" start_char="10418">in</TOKEN>
<TOKEN end_char="10423" id="token-108-17" morph="none" pos="word" start_char="10421">the</TOKEN>
<TOKEN end_char="10428" id="token-108-18" morph="none" pos="word" start_char="10425">west</TOKEN>
<TOKEN end_char="10429" id="token-108-19" morph="none" pos="punct" start_char="10429">)</TOKEN>
<TOKEN end_char="10432" id="token-108-20" morph="none" pos="word" start_char="10431">it</TOKEN>
<TOKEN end_char="10438" id="token-108-21" morph="none" pos="word" start_char="10434">looks</TOKEN>
<TOKEN end_char="10443" id="token-108-22" morph="none" pos="word" start_char="10440">more</TOKEN>
<TOKEN end_char="10450" id="token-108-23" morph="none" pos="word" start_char="10445">likely</TOKEN>
<TOKEN end_char="10455" id="token-108-24" morph="none" pos="word" start_char="10452">that</TOKEN>
<TOKEN end_char="10459" id="token-108-25" morph="none" pos="word" start_char="10457">the</TOKEN>
<TOKEN end_char="10468" id="token-108-26" morph="none" pos="word" start_char="10461">measures</TOKEN>
<TOKEN end_char="10474" id="token-108-27" morph="none" pos="word" start_char="10470">taken</TOKEN>
<TOKEN end_char="10477" id="token-108-28" morph="none" pos="word" start_char="10476">in</TOKEN>
<TOKEN end_char="10483" id="token-108-29" morph="none" pos="word" start_char="10479">these</TOKEN>
<TOKEN end_char="10493" id="token-108-30" morph="none" pos="word" start_char="10485">countries</TOKEN>
<TOKEN end_char="10496" id="token-108-31" morph="none" pos="word" start_char="10495">to</TOKEN>
<TOKEN end_char="10501" id="token-108-32" morph="none" pos="word" start_char="10498">stop</TOKEN>
<TOKEN end_char="10505" id="token-108-33" morph="none" pos="word" start_char="10503">the</TOKEN>
<TOKEN end_char="10512" id="token-108-34" morph="none" pos="word" start_char="10507">spread</TOKEN>
<TOKEN end_char="10515" id="token-108-35" morph="none" pos="word" start_char="10514">of</TOKEN>
<TOKEN end_char="10521" id="token-108-36" morph="none" pos="word" start_char="10517">covid</TOKEN>
<TOKEN end_char="10526" id="token-108-37" morph="none" pos="word" start_char="10523">were</TOKEN>
<TOKEN end_char="10536" id="token-108-38" morph="none" pos="word" start_char="10528">effective</TOKEN>
<TOKEN end_char="10537" id="token-108-39" morph="none" pos="punct" start_char="10537">.</TOKEN>
</SEG>
<SEG end_char="10730" id="segment-109" start_char="10540">
<ORIGINAL_TEXT>The main difference in the genetics would be that many Asians lack the ability to produce the enzyme (lactase) to digest milk when adult vs those mostly of northern european ancestry who can.</ORIGINAL_TEXT>
<TOKEN end_char="10542" id="token-109-0" morph="none" pos="word" start_char="10540">The</TOKEN>
<TOKEN end_char="10547" id="token-109-1" morph="none" pos="word" start_char="10544">main</TOKEN>
<TOKEN end_char="10558" id="token-109-2" morph="none" pos="word" start_char="10549">difference</TOKEN>
<TOKEN end_char="10561" id="token-109-3" morph="none" pos="word" start_char="10560">in</TOKEN>
<TOKEN end_char="10565" id="token-109-4" morph="none" pos="word" start_char="10563">the</TOKEN>
<TOKEN end_char="10574" id="token-109-5" morph="none" pos="word" start_char="10567">genetics</TOKEN>
<TOKEN end_char="10580" id="token-109-6" morph="none" pos="word" start_char="10576">would</TOKEN>
<TOKEN end_char="10583" id="token-109-7" morph="none" pos="word" start_char="10582">be</TOKEN>
<TOKEN end_char="10588" id="token-109-8" morph="none" pos="word" start_char="10585">that</TOKEN>
<TOKEN end_char="10593" id="token-109-9" morph="none" pos="word" start_char="10590">many</TOKEN>
<TOKEN end_char="10600" id="token-109-10" morph="none" pos="word" start_char="10595">Asians</TOKEN>
<TOKEN end_char="10605" id="token-109-11" morph="none" pos="word" start_char="10602">lack</TOKEN>
<TOKEN end_char="10609" id="token-109-12" morph="none" pos="word" start_char="10607">the</TOKEN>
<TOKEN end_char="10617" id="token-109-13" morph="none" pos="word" start_char="10611">ability</TOKEN>
<TOKEN end_char="10620" id="token-109-14" morph="none" pos="word" start_char="10619">to</TOKEN>
<TOKEN end_char="10628" id="token-109-15" morph="none" pos="word" start_char="10622">produce</TOKEN>
<TOKEN end_char="10632" id="token-109-16" morph="none" pos="word" start_char="10630">the</TOKEN>
<TOKEN end_char="10639" id="token-109-17" morph="none" pos="word" start_char="10634">enzyme</TOKEN>
<TOKEN end_char="10641" id="token-109-18" morph="none" pos="punct" start_char="10641">(</TOKEN>
<TOKEN end_char="10648" id="token-109-19" morph="none" pos="word" start_char="10642">lactase</TOKEN>
<TOKEN end_char="10649" id="token-109-20" morph="none" pos="punct" start_char="10649">)</TOKEN>
<TOKEN end_char="10652" id="token-109-21" morph="none" pos="word" start_char="10651">to</TOKEN>
<TOKEN end_char="10659" id="token-109-22" morph="none" pos="word" start_char="10654">digest</TOKEN>
<TOKEN end_char="10664" id="token-109-23" morph="none" pos="word" start_char="10661">milk</TOKEN>
<TOKEN end_char="10669" id="token-109-24" morph="none" pos="word" start_char="10666">when</TOKEN>
<TOKEN end_char="10675" id="token-109-25" morph="none" pos="word" start_char="10671">adult</TOKEN>
<TOKEN end_char="10678" id="token-109-26" morph="none" pos="word" start_char="10677">vs</TOKEN>
<TOKEN end_char="10684" id="token-109-27" morph="none" pos="word" start_char="10680">those</TOKEN>
<TOKEN end_char="10691" id="token-109-28" morph="none" pos="word" start_char="10686">mostly</TOKEN>
<TOKEN end_char="10694" id="token-109-29" morph="none" pos="word" start_char="10693">of</TOKEN>
<TOKEN end_char="10703" id="token-109-30" morph="none" pos="word" start_char="10696">northern</TOKEN>
<TOKEN end_char="10712" id="token-109-31" morph="none" pos="word" start_char="10705">european</TOKEN>
<TOKEN end_char="10721" id="token-109-32" morph="none" pos="word" start_char="10714">ancestry</TOKEN>
<TOKEN end_char="10725" id="token-109-33" morph="none" pos="word" start_char="10723">who</TOKEN>
<TOKEN end_char="10729" id="token-109-34" morph="none" pos="word" start_char="10727">can</TOKEN>
<TOKEN end_char="10730" id="token-109-35" morph="none" pos="punct" start_char="10730">.</TOKEN>
</SEG>
<SEG end_char="10831" id="segment-110" start_char="10732">
<ORIGINAL_TEXT>So one group has a diet which will use coconut milk and the other group animal based dairy products.</ORIGINAL_TEXT>
<TOKEN end_char="10733" id="token-110-0" morph="none" pos="word" start_char="10732">So</TOKEN>
<TOKEN end_char="10737" id="token-110-1" morph="none" pos="word" start_char="10735">one</TOKEN>
<TOKEN end_char="10743" id="token-110-2" morph="none" pos="word" start_char="10739">group</TOKEN>
<TOKEN end_char="10747" id="token-110-3" morph="none" pos="word" start_char="10745">has</TOKEN>
<TOKEN end_char="10749" id="token-110-4" morph="none" pos="word" start_char="10749">a</TOKEN>
<TOKEN end_char="10754" id="token-110-5" morph="none" pos="word" start_char="10751">diet</TOKEN>
<TOKEN end_char="10760" id="token-110-6" morph="none" pos="word" start_char="10756">which</TOKEN>
<TOKEN end_char="10765" id="token-110-7" morph="none" pos="word" start_char="10762">will</TOKEN>
<TOKEN end_char="10769" id="token-110-8" morph="none" pos="word" start_char="10767">use</TOKEN>
<TOKEN end_char="10777" id="token-110-9" morph="none" pos="word" start_char="10771">coconut</TOKEN>
<TOKEN end_char="10782" id="token-110-10" morph="none" pos="word" start_char="10779">milk</TOKEN>
<TOKEN end_char="10786" id="token-110-11" morph="none" pos="word" start_char="10784">and</TOKEN>
<TOKEN end_char="10790" id="token-110-12" morph="none" pos="word" start_char="10788">the</TOKEN>
<TOKEN end_char="10796" id="token-110-13" morph="none" pos="word" start_char="10792">other</TOKEN>
<TOKEN end_char="10802" id="token-110-14" morph="none" pos="word" start_char="10798">group</TOKEN>
<TOKEN end_char="10809" id="token-110-15" morph="none" pos="word" start_char="10804">animal</TOKEN>
<TOKEN end_char="10815" id="token-110-16" morph="none" pos="word" start_char="10811">based</TOKEN>
<TOKEN end_char="10821" id="token-110-17" morph="none" pos="word" start_char="10817">dairy</TOKEN>
<TOKEN end_char="10830" id="token-110-18" morph="none" pos="word" start_char="10823">products</TOKEN>
<TOKEN end_char="10831" id="token-110-19" morph="none" pos="punct" start_char="10831">.</TOKEN>
</SEG>
<SEG end_char="10861" id="segment-111" start_char="10833">
<ORIGINAL_TEXT>So diet may also be a factor.</ORIGINAL_TEXT>
<TOKEN end_char="10834" id="token-111-0" morph="none" pos="word" start_char="10833">So</TOKEN>
<TOKEN end_char="10839" id="token-111-1" morph="none" pos="word" start_char="10836">diet</TOKEN>
<TOKEN end_char="10843" id="token-111-2" morph="none" pos="word" start_char="10841">may</TOKEN>
<TOKEN end_char="10848" id="token-111-3" morph="none" pos="word" start_char="10845">also</TOKEN>
<TOKEN end_char="10851" id="token-111-4" morph="none" pos="word" start_char="10850">be</TOKEN>
<TOKEN end_char="10853" id="token-111-5" morph="none" pos="word" start_char="10853">a</TOKEN>
<TOKEN end_char="10860" id="token-111-6" morph="none" pos="word" start_char="10855">factor</TOKEN>
<TOKEN end_char="10861" id="token-111-7" morph="none" pos="punct" start_char="10861">.</TOKEN>
</SEG>
<SEG end_char="11009" id="segment-112" start_char="10867">
<ORIGINAL_TEXT>goingoingong said: From the incidence figures it also appears that the 200K western population in Thailand have acquired this genetic immunity.</ORIGINAL_TEXT>
<TOKEN end_char="10878" id="token-112-0" morph="none" pos="word" start_char="10867">goingoingong</TOKEN>
<TOKEN end_char="10883" id="token-112-1" morph="none" pos="word" start_char="10880">said</TOKEN>
<TOKEN end_char="10884" id="token-112-2" morph="none" pos="punct" start_char="10884">:</TOKEN>
<TOKEN end_char="10889" id="token-112-3" morph="none" pos="word" start_char="10886">From</TOKEN>
<TOKEN end_char="10893" id="token-112-4" morph="none" pos="word" start_char="10891">the</TOKEN>
<TOKEN end_char="10903" id="token-112-5" morph="none" pos="word" start_char="10895">incidence</TOKEN>
<TOKEN end_char="10911" id="token-112-6" morph="none" pos="word" start_char="10905">figures</TOKEN>
<TOKEN end_char="10914" id="token-112-7" morph="none" pos="word" start_char="10913">it</TOKEN>
<TOKEN end_char="10919" id="token-112-8" morph="none" pos="word" start_char="10916">also</TOKEN>
<TOKEN end_char="10927" id="token-112-9" morph="none" pos="word" start_char="10921">appears</TOKEN>
<TOKEN end_char="10932" id="token-112-10" morph="none" pos="word" start_char="10929">that</TOKEN>
<TOKEN end_char="10936" id="token-112-11" morph="none" pos="word" start_char="10934">the</TOKEN>
<TOKEN end_char="10941" id="token-112-12" morph="none" pos="word" start_char="10938">200K</TOKEN>
<TOKEN end_char="10949" id="token-112-13" morph="none" pos="word" start_char="10943">western</TOKEN>
<TOKEN end_char="10960" id="token-112-14" morph="none" pos="word" start_char="10951">population</TOKEN>
<TOKEN end_char="10963" id="token-112-15" morph="none" pos="word" start_char="10962">in</TOKEN>
<TOKEN end_char="10972" id="token-112-16" morph="none" pos="word" start_char="10965">Thailand</TOKEN>
<TOKEN end_char="10977" id="token-112-17" morph="none" pos="word" start_char="10974">have</TOKEN>
<TOKEN end_char="10986" id="token-112-18" morph="none" pos="word" start_char="10979">acquired</TOKEN>
<TOKEN end_char="10991" id="token-112-19" morph="none" pos="word" start_char="10988">this</TOKEN>
<TOKEN end_char="10999" id="token-112-20" morph="none" pos="word" start_char="10993">genetic</TOKEN>
<TOKEN end_char="11008" id="token-112-21" morph="none" pos="word" start_char="11001">immunity</TOKEN>
<TOKEN end_char="11009" id="token-112-22" morph="none" pos="punct" start_char="11009">.</TOKEN>
</SEG>
<SEG end_char="11301" id="segment-113" start_char="11011">
<ORIGINAL_TEXT>As have the western origin population of Hong Kong, Macao, Cambodia, Vietnam and Laos.... As the genetics of this population will not have changed to when they were born (in the west) it looks more likely that the measures taken in these countries to stop the spread of covid were effective.</ORIGINAL_TEXT>
<TOKEN end_char="11012" id="token-113-0" morph="none" pos="word" start_char="11011">As</TOKEN>
<TOKEN end_char="11017" id="token-113-1" morph="none" pos="word" start_char="11014">have</TOKEN>
<TOKEN end_char="11021" id="token-113-2" morph="none" pos="word" start_char="11019">the</TOKEN>
<TOKEN end_char="11029" id="token-113-3" morph="none" pos="word" start_char="11023">western</TOKEN>
<TOKEN end_char="11036" id="token-113-4" morph="none" pos="word" start_char="11031">origin</TOKEN>
<TOKEN end_char="11047" id="token-113-5" morph="none" pos="word" start_char="11038">population</TOKEN>
<TOKEN end_char="11050" id="token-113-6" morph="none" pos="word" start_char="11049">of</TOKEN>
<TOKEN end_char="11055" id="token-113-7" morph="none" pos="word" start_char="11052">Hong</TOKEN>
<TOKEN end_char="11060" id="token-113-8" morph="none" pos="word" start_char="11057">Kong</TOKEN>
<TOKEN end_char="11061" id="token-113-9" morph="none" pos="punct" start_char="11061">,</TOKEN>
<TOKEN end_char="11067" id="token-113-10" morph="none" pos="word" start_char="11063">Macao</TOKEN>
<TOKEN end_char="11068" id="token-113-11" morph="none" pos="punct" start_char="11068">,</TOKEN>
<TOKEN end_char="11077" id="token-113-12" morph="none" pos="word" start_char="11070">Cambodia</TOKEN>
<TOKEN end_char="11078" id="token-113-13" morph="none" pos="punct" start_char="11078">,</TOKEN>
<TOKEN end_char="11086" id="token-113-14" morph="none" pos="word" start_char="11080">Vietnam</TOKEN>
<TOKEN end_char="11090" id="token-113-15" morph="none" pos="word" start_char="11088">and</TOKEN>
<TOKEN end_char="11095" id="token-113-16" morph="none" pos="word" start_char="11092">Laos</TOKEN>
<TOKEN end_char="11099" id="token-113-17" morph="none" pos="punct" start_char="11096">....</TOKEN>
<TOKEN end_char="11102" id="token-113-18" morph="none" pos="word" start_char="11101">As</TOKEN>
<TOKEN end_char="11106" id="token-113-19" morph="none" pos="word" start_char="11104">the</TOKEN>
<TOKEN end_char="11115" id="token-113-20" morph="none" pos="word" start_char="11108">genetics</TOKEN>
<TOKEN end_char="11118" id="token-113-21" morph="none" pos="word" start_char="11117">of</TOKEN>
<TOKEN end_char="11123" id="token-113-22" morph="none" pos="word" start_char="11120">this</TOKEN>
<TOKEN end_char="11134" id="token-113-23" morph="none" pos="word" start_char="11125">population</TOKEN>
<TOKEN end_char="11139" id="token-113-24" morph="none" pos="word" start_char="11136">will</TOKEN>
<TOKEN end_char="11143" id="token-113-25" morph="none" pos="word" start_char="11141">not</TOKEN>
<TOKEN end_char="11148" id="token-113-26" morph="none" pos="word" start_char="11145">have</TOKEN>
<TOKEN end_char="11156" id="token-113-27" morph="none" pos="word" start_char="11150">changed</TOKEN>
<TOKEN end_char="11159" id="token-113-28" morph="none" pos="word" start_char="11158">to</TOKEN>
<TOKEN end_char="11164" id="token-113-29" morph="none" pos="word" start_char="11161">when</TOKEN>
<TOKEN end_char="11169" id="token-113-30" morph="none" pos="word" start_char="11166">they</TOKEN>
<TOKEN end_char="11174" id="token-113-31" morph="none" pos="word" start_char="11171">were</TOKEN>
<TOKEN end_char="11179" id="token-113-32" morph="none" pos="word" start_char="11176">born</TOKEN>
<TOKEN end_char="11181" id="token-113-33" morph="none" pos="punct" start_char="11181">(</TOKEN>
<TOKEN end_char="11183" id="token-113-34" morph="none" pos="word" start_char="11182">in</TOKEN>
<TOKEN end_char="11187" id="token-113-35" morph="none" pos="word" start_char="11185">the</TOKEN>
<TOKEN end_char="11192" id="token-113-36" morph="none" pos="word" start_char="11189">west</TOKEN>
<TOKEN end_char="11193" id="token-113-37" morph="none" pos="punct" start_char="11193">)</TOKEN>
<TOKEN end_char="11196" id="token-113-38" morph="none" pos="word" start_char="11195">it</TOKEN>
<TOKEN end_char="11202" id="token-113-39" morph="none" pos="word" start_char="11198">looks</TOKEN>
<TOKEN end_char="11207" id="token-113-40" morph="none" pos="word" start_char="11204">more</TOKEN>
<TOKEN end_char="11214" id="token-113-41" morph="none" pos="word" start_char="11209">likely</TOKEN>
<TOKEN end_char="11219" id="token-113-42" morph="none" pos="word" start_char="11216">that</TOKEN>
<TOKEN end_char="11223" id="token-113-43" morph="none" pos="word" start_char="11221">the</TOKEN>
<TOKEN end_char="11232" id="token-113-44" morph="none" pos="word" start_char="11225">measures</TOKEN>
<TOKEN end_char="11238" id="token-113-45" morph="none" pos="word" start_char="11234">taken</TOKEN>
<TOKEN end_char="11241" id="token-113-46" morph="none" pos="word" start_char="11240">in</TOKEN>
<TOKEN end_char="11247" id="token-113-47" morph="none" pos="word" start_char="11243">these</TOKEN>
<TOKEN end_char="11257" id="token-113-48" morph="none" pos="word" start_char="11249">countries</TOKEN>
<TOKEN end_char="11260" id="token-113-49" morph="none" pos="word" start_char="11259">to</TOKEN>
<TOKEN end_char="11265" id="token-113-50" morph="none" pos="word" start_char="11262">stop</TOKEN>
<TOKEN end_char="11269" id="token-113-51" morph="none" pos="word" start_char="11267">the</TOKEN>
<TOKEN end_char="11276" id="token-113-52" morph="none" pos="word" start_char="11271">spread</TOKEN>
<TOKEN end_char="11279" id="token-113-53" morph="none" pos="word" start_char="11278">of</TOKEN>
<TOKEN end_char="11285" id="token-113-54" morph="none" pos="word" start_char="11281">covid</TOKEN>
<TOKEN end_char="11290" id="token-113-55" morph="none" pos="word" start_char="11287">were</TOKEN>
<TOKEN end_char="11300" id="token-113-56" morph="none" pos="word" start_char="11292">effective</TOKEN>
<TOKEN end_char="11301" id="token-113-57" morph="none" pos="punct" start_char="11301">.</TOKEN>
</SEG>
<SEG end_char="11493" id="segment-114" start_char="11303">
<ORIGINAL_TEXT>The main difference in the genetics would be that many Asians lack the ability to produce the enzyme (lactase) to digest milk when adult vs those mostly of northern european ancestry who can.</ORIGINAL_TEXT>
<TOKEN end_char="11305" id="token-114-0" morph="none" pos="word" start_char="11303">The</TOKEN>
<TOKEN end_char="11310" id="token-114-1" morph="none" pos="word" start_char="11307">main</TOKEN>
<TOKEN end_char="11321" id="token-114-2" morph="none" pos="word" start_char="11312">difference</TOKEN>
<TOKEN end_char="11324" id="token-114-3" morph="none" pos="word" start_char="11323">in</TOKEN>
<TOKEN end_char="11328" id="token-114-4" morph="none" pos="word" start_char="11326">the</TOKEN>
<TOKEN end_char="11337" id="token-114-5" morph="none" pos="word" start_char="11330">genetics</TOKEN>
<TOKEN end_char="11343" id="token-114-6" morph="none" pos="word" start_char="11339">would</TOKEN>
<TOKEN end_char="11346" id="token-114-7" morph="none" pos="word" start_char="11345">be</TOKEN>
<TOKEN end_char="11351" id="token-114-8" morph="none" pos="word" start_char="11348">that</TOKEN>
<TOKEN end_char="11356" id="token-114-9" morph="none" pos="word" start_char="11353">many</TOKEN>
<TOKEN end_char="11363" id="token-114-10" morph="none" pos="word" start_char="11358">Asians</TOKEN>
<TOKEN end_char="11368" id="token-114-11" morph="none" pos="word" start_char="11365">lack</TOKEN>
<TOKEN end_char="11372" id="token-114-12" morph="none" pos="word" start_char="11370">the</TOKEN>
<TOKEN end_char="11380" id="token-114-13" morph="none" pos="word" start_char="11374">ability</TOKEN>
<TOKEN end_char="11383" id="token-114-14" morph="none" pos="word" start_char="11382">to</TOKEN>
<TOKEN end_char="11391" id="token-114-15" morph="none" pos="word" start_char="11385">produce</TOKEN>
<TOKEN end_char="11395" id="token-114-16" morph="none" pos="word" start_char="11393">the</TOKEN>
<TOKEN end_char="11402" id="token-114-17" morph="none" pos="word" start_char="11397">enzyme</TOKEN>
<TOKEN end_char="11404" id="token-114-18" morph="none" pos="punct" start_char="11404">(</TOKEN>
<TOKEN end_char="11411" id="token-114-19" morph="none" pos="word" start_char="11405">lactase</TOKEN>
<TOKEN end_char="11412" id="token-114-20" morph="none" pos="punct" start_char="11412">)</TOKEN>
<TOKEN end_char="11415" id="token-114-21" morph="none" pos="word" start_char="11414">to</TOKEN>
<TOKEN end_char="11422" id="token-114-22" morph="none" pos="word" start_char="11417">digest</TOKEN>
<TOKEN end_char="11427" id="token-114-23" morph="none" pos="word" start_char="11424">milk</TOKEN>
<TOKEN end_char="11432" id="token-114-24" morph="none" pos="word" start_char="11429">when</TOKEN>
<TOKEN end_char="11438" id="token-114-25" morph="none" pos="word" start_char="11434">adult</TOKEN>
<TOKEN end_char="11441" id="token-114-26" morph="none" pos="word" start_char="11440">vs</TOKEN>
<TOKEN end_char="11447" id="token-114-27" morph="none" pos="word" start_char="11443">those</TOKEN>
<TOKEN end_char="11454" id="token-114-28" morph="none" pos="word" start_char="11449">mostly</TOKEN>
<TOKEN end_char="11457" id="token-114-29" morph="none" pos="word" start_char="11456">of</TOKEN>
<TOKEN end_char="11466" id="token-114-30" morph="none" pos="word" start_char="11459">northern</TOKEN>
<TOKEN end_char="11475" id="token-114-31" morph="none" pos="word" start_char="11468">european</TOKEN>
<TOKEN end_char="11484" id="token-114-32" morph="none" pos="word" start_char="11477">ancestry</TOKEN>
<TOKEN end_char="11488" id="token-114-33" morph="none" pos="word" start_char="11486">who</TOKEN>
<TOKEN end_char="11492" id="token-114-34" morph="none" pos="word" start_char="11490">can</TOKEN>
<TOKEN end_char="11493" id="token-114-35" morph="none" pos="punct" start_char="11493">.</TOKEN>
</SEG>
<SEG end_char="11594" id="segment-115" start_char="11495">
<ORIGINAL_TEXT>So one group has a diet which will use coconut milk and the other group animal based dairy products.</ORIGINAL_TEXT>
<TOKEN end_char="11496" id="token-115-0" morph="none" pos="word" start_char="11495">So</TOKEN>
<TOKEN end_char="11500" id="token-115-1" morph="none" pos="word" start_char="11498">one</TOKEN>
<TOKEN end_char="11506" id="token-115-2" morph="none" pos="word" start_char="11502">group</TOKEN>
<TOKEN end_char="11510" id="token-115-3" morph="none" pos="word" start_char="11508">has</TOKEN>
<TOKEN end_char="11512" id="token-115-4" morph="none" pos="word" start_char="11512">a</TOKEN>
<TOKEN end_char="11517" id="token-115-5" morph="none" pos="word" start_char="11514">diet</TOKEN>
<TOKEN end_char="11523" id="token-115-6" morph="none" pos="word" start_char="11519">which</TOKEN>
<TOKEN end_char="11528" id="token-115-7" morph="none" pos="word" start_char="11525">will</TOKEN>
<TOKEN end_char="11532" id="token-115-8" morph="none" pos="word" start_char="11530">use</TOKEN>
<TOKEN end_char="11540" id="token-115-9" morph="none" pos="word" start_char="11534">coconut</TOKEN>
<TOKEN end_char="11545" id="token-115-10" morph="none" pos="word" start_char="11542">milk</TOKEN>
<TOKEN end_char="11549" id="token-115-11" morph="none" pos="word" start_char="11547">and</TOKEN>
<TOKEN end_char="11553" id="token-115-12" morph="none" pos="word" start_char="11551">the</TOKEN>
<TOKEN end_char="11559" id="token-115-13" morph="none" pos="word" start_char="11555">other</TOKEN>
<TOKEN end_char="11565" id="token-115-14" morph="none" pos="word" start_char="11561">group</TOKEN>
<TOKEN end_char="11572" id="token-115-15" morph="none" pos="word" start_char="11567">animal</TOKEN>
<TOKEN end_char="11578" id="token-115-16" morph="none" pos="word" start_char="11574">based</TOKEN>
<TOKEN end_char="11584" id="token-115-17" morph="none" pos="word" start_char="11580">dairy</TOKEN>
<TOKEN end_char="11593" id="token-115-18" morph="none" pos="word" start_char="11586">products</TOKEN>
<TOKEN end_char="11594" id="token-115-19" morph="none" pos="punct" start_char="11594">.</TOKEN>
</SEG>
<SEG end_char="11624" id="segment-116" start_char="11596">
<ORIGINAL_TEXT>So diet may also be a factor.</ORIGINAL_TEXT>
<TOKEN end_char="11597" id="token-116-0" morph="none" pos="word" start_char="11596">So</TOKEN>
<TOKEN end_char="11602" id="token-116-1" morph="none" pos="word" start_char="11599">diet</TOKEN>
<TOKEN end_char="11606" id="token-116-2" morph="none" pos="word" start_char="11604">may</TOKEN>
<TOKEN end_char="11611" id="token-116-3" morph="none" pos="word" start_char="11608">also</TOKEN>
<TOKEN end_char="11614" id="token-116-4" morph="none" pos="word" start_char="11613">be</TOKEN>
<TOKEN end_char="11616" id="token-116-5" morph="none" pos="word" start_char="11616">a</TOKEN>
<TOKEN end_char="11623" id="token-116-6" morph="none" pos="word" start_char="11618">factor</TOKEN>
<TOKEN end_char="11624" id="token-116-7" morph="none" pos="punct" start_char="11624">.</TOKEN>
</SEG>
<SEG end_char="11643" id="segment-117" start_char="11626">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="11630" id="token-117-0" morph="none" pos="word" start_char="11626">Click</TOKEN>
<TOKEN end_char="11633" id="token-117-1" morph="none" pos="word" start_char="11632">to</TOKEN>
<TOKEN end_char="11640" id="token-117-2" morph="none" pos="word" start_char="11635">expand</TOKEN>
<TOKEN end_char="11643" id="token-117-3" morph="none" pos="punct" start_char="11641">...</TOKEN>
</SEG>
<SEG end_char="11689" id="segment-118" start_char="11646">
<ORIGINAL_TEXT>Yup it's a myriad of factors all kicking in.</ORIGINAL_TEXT>
<TOKEN end_char="11648" id="token-118-0" morph="none" pos="word" start_char="11646">Yup</TOKEN>
<TOKEN end_char="11653" id="token-118-1" morph="none" pos="word" start_char="11650">it's</TOKEN>
<TOKEN end_char="11655" id="token-118-2" morph="none" pos="word" start_char="11655">a</TOKEN>
<TOKEN end_char="11662" id="token-118-3" morph="none" pos="word" start_char="11657">myriad</TOKEN>
<TOKEN end_char="11665" id="token-118-4" morph="none" pos="word" start_char="11664">of</TOKEN>
<TOKEN end_char="11673" id="token-118-5" morph="none" pos="word" start_char="11667">factors</TOKEN>
<TOKEN end_char="11677" id="token-118-6" morph="none" pos="word" start_char="11675">all</TOKEN>
<TOKEN end_char="11685" id="token-118-7" morph="none" pos="word" start_char="11679">kicking</TOKEN>
<TOKEN end_char="11688" id="token-118-8" morph="none" pos="word" start_char="11687">in</TOKEN>
<TOKEN end_char="11689" id="token-118-9" morph="none" pos="punct" start_char="11689">.</TOKEN>
</SEG>
<SEG end_char="11898" id="segment-119" start_char="11692">
<ORIGINAL_TEXT>I'm definitely not a fan of people trying to boil it down to, effectively, 'it was Boris that done it' without digging into some of the reasons that governments across the globe made the decisions they have.</ORIGINAL_TEXT>
<TOKEN end_char="11694" id="token-119-0" morph="none" pos="word" start_char="11692">I'm</TOKEN>
<TOKEN end_char="11705" id="token-119-1" morph="none" pos="word" start_char="11696">definitely</TOKEN>
<TOKEN end_char="11709" id="token-119-2" morph="none" pos="word" start_char="11707">not</TOKEN>
<TOKEN end_char="11711" id="token-119-3" morph="none" pos="word" start_char="11711">a</TOKEN>
<TOKEN end_char="11715" id="token-119-4" morph="none" pos="word" start_char="11713">fan</TOKEN>
<TOKEN end_char="11718" id="token-119-5" morph="none" pos="word" start_char="11717">of</TOKEN>
<TOKEN end_char="11725" id="token-119-6" morph="none" pos="word" start_char="11720">people</TOKEN>
<TOKEN end_char="11732" id="token-119-7" morph="none" pos="word" start_char="11727">trying</TOKEN>
<TOKEN end_char="11735" id="token-119-8" morph="none" pos="word" start_char="11734">to</TOKEN>
<TOKEN end_char="11740" id="token-119-9" morph="none" pos="word" start_char="11737">boil</TOKEN>
<TOKEN end_char="11743" id="token-119-10" morph="none" pos="word" start_char="11742">it</TOKEN>
<TOKEN end_char="11748" id="token-119-11" morph="none" pos="word" start_char="11745">down</TOKEN>
<TOKEN end_char="11751" id="token-119-12" morph="none" pos="word" start_char="11750">to</TOKEN>
<TOKEN end_char="11752" id="token-119-13" morph="none" pos="punct" start_char="11752">,</TOKEN>
<TOKEN end_char="11764" id="token-119-14" morph="none" pos="word" start_char="11754">effectively</TOKEN>
<TOKEN end_char="11765" id="token-119-15" morph="none" pos="punct" start_char="11765">,</TOKEN>
<TOKEN end_char="11767" id="token-119-16" morph="none" pos="punct" start_char="11767">'</TOKEN>
<TOKEN end_char="11769" id="token-119-17" morph="none" pos="word" start_char="11768">it</TOKEN>
<TOKEN end_char="11773" id="token-119-18" morph="none" pos="word" start_char="11771">was</TOKEN>
<TOKEN end_char="11779" id="token-119-19" morph="none" pos="word" start_char="11775">Boris</TOKEN>
<TOKEN end_char="11784" id="token-119-20" morph="none" pos="word" start_char="11781">that</TOKEN>
<TOKEN end_char="11789" id="token-119-21" morph="none" pos="word" start_char="11786">done</TOKEN>
<TOKEN end_char="11792" id="token-119-22" morph="none" pos="word" start_char="11791">it</TOKEN>
<TOKEN end_char="11793" id="token-119-23" morph="none" pos="punct" start_char="11793">'</TOKEN>
<TOKEN end_char="11801" id="token-119-24" morph="none" pos="word" start_char="11795">without</TOKEN>
<TOKEN end_char="11809" id="token-119-25" morph="none" pos="word" start_char="11803">digging</TOKEN>
<TOKEN end_char="11814" id="token-119-26" morph="none" pos="word" start_char="11811">into</TOKEN>
<TOKEN end_char="11819" id="token-119-27" morph="none" pos="word" start_char="11816">some</TOKEN>
<TOKEN end_char="11822" id="token-119-28" morph="none" pos="word" start_char="11821">of</TOKEN>
<TOKEN end_char="11826" id="token-119-29" morph="none" pos="word" start_char="11824">the</TOKEN>
<TOKEN end_char="11834" id="token-119-30" morph="none" pos="word" start_char="11828">reasons</TOKEN>
<TOKEN end_char="11839" id="token-119-31" morph="none" pos="word" start_char="11836">that</TOKEN>
<TOKEN end_char="11851" id="token-119-32" morph="none" pos="word" start_char="11841">governments</TOKEN>
<TOKEN end_char="11858" id="token-119-33" morph="none" pos="word" start_char="11853">across</TOKEN>
<TOKEN end_char="11862" id="token-119-34" morph="none" pos="word" start_char="11860">the</TOKEN>
<TOKEN end_char="11868" id="token-119-35" morph="none" pos="word" start_char="11864">globe</TOKEN>
<TOKEN end_char="11873" id="token-119-36" morph="none" pos="word" start_char="11870">made</TOKEN>
<TOKEN end_char="11877" id="token-119-37" morph="none" pos="word" start_char="11875">the</TOKEN>
<TOKEN end_char="11887" id="token-119-38" morph="none" pos="word" start_char="11879">decisions</TOKEN>
<TOKEN end_char="11892" id="token-119-39" morph="none" pos="word" start_char="11889">they</TOKEN>
<TOKEN end_char="11897" id="token-119-40" morph="none" pos="word" start_char="11894">have</TOKEN>
<TOKEN end_char="11898" id="token-119-41" morph="none" pos="punct" start_char="11898">.</TOKEN>
</SEG>
<SEG end_char="11986" id="segment-120" start_char="11900">
<ORIGINAL_TEXT>Absolutely the government could have acted sooner, but would people have accepted that?</ORIGINAL_TEXT>
<TOKEN end_char="11909" id="token-120-0" morph="none" pos="word" start_char="11900">Absolutely</TOKEN>
<TOKEN end_char="11913" id="token-120-1" morph="none" pos="word" start_char="11911">the</TOKEN>
<TOKEN end_char="11924" id="token-120-2" morph="none" pos="word" start_char="11915">government</TOKEN>
<TOKEN end_char="11930" id="token-120-3" morph="none" pos="word" start_char="11926">could</TOKEN>
<TOKEN end_char="11935" id="token-120-4" morph="none" pos="word" start_char="11932">have</TOKEN>
<TOKEN end_char="11941" id="token-120-5" morph="none" pos="word" start_char="11937">acted</TOKEN>
<TOKEN end_char="11948" id="token-120-6" morph="none" pos="word" start_char="11943">sooner</TOKEN>
<TOKEN end_char="11949" id="token-120-7" morph="none" pos="punct" start_char="11949">,</TOKEN>
<TOKEN end_char="11953" id="token-120-8" morph="none" pos="word" start_char="11951">but</TOKEN>
<TOKEN end_char="11959" id="token-120-9" morph="none" pos="word" start_char="11955">would</TOKEN>
<TOKEN end_char="11966" id="token-120-10" morph="none" pos="word" start_char="11961">people</TOKEN>
<TOKEN end_char="11971" id="token-120-11" morph="none" pos="word" start_char="11968">have</TOKEN>
<TOKEN end_char="11980" id="token-120-12" morph="none" pos="word" start_char="11973">accepted</TOKEN>
<TOKEN end_char="11985" id="token-120-13" morph="none" pos="word" start_char="11982">that</TOKEN>
<TOKEN end_char="11986" id="token-120-14" morph="none" pos="punct" start_char="11986">?</TOKEN>
</SEG>
<SEG end_char="12219" id="segment-121" start_char="11989">
<ORIGINAL_TEXT>I like the quote from the WHO doctor in the Panorama documentary last night that went along the lines of 'Europeans nations seemed to need to have experienced the deaths themselves before accepting the seriousness of the pandemic'.</ORIGINAL_TEXT>
<TOKEN end_char="11989" id="token-121-0" morph="none" pos="word" start_char="11989">I</TOKEN>
<TOKEN end_char="11994" id="token-121-1" morph="none" pos="word" start_char="11991">like</TOKEN>
<TOKEN end_char="11998" id="token-121-2" morph="none" pos="word" start_char="11996">the</TOKEN>
<TOKEN end_char="12004" id="token-121-3" morph="none" pos="word" start_char="12000">quote</TOKEN>
<TOKEN end_char="12009" id="token-121-4" morph="none" pos="word" start_char="12006">from</TOKEN>
<TOKEN end_char="12013" id="token-121-5" morph="none" pos="word" start_char="12011">the</TOKEN>
<TOKEN end_char="12017" id="token-121-6" morph="none" pos="word" start_char="12015">WHO</TOKEN>
<TOKEN end_char="12024" id="token-121-7" morph="none" pos="word" start_char="12019">doctor</TOKEN>
<TOKEN end_char="12027" id="token-121-8" morph="none" pos="word" start_char="12026">in</TOKEN>
<TOKEN end_char="12031" id="token-121-9" morph="none" pos="word" start_char="12029">the</TOKEN>
<TOKEN end_char="12040" id="token-121-10" morph="none" pos="word" start_char="12033">Panorama</TOKEN>
<TOKEN end_char="12052" id="token-121-11" morph="none" pos="word" start_char="12042">documentary</TOKEN>
<TOKEN end_char="12057" id="token-121-12" morph="none" pos="word" start_char="12054">last</TOKEN>
<TOKEN end_char="12063" id="token-121-13" morph="none" pos="word" start_char="12059">night</TOKEN>
<TOKEN end_char="12068" id="token-121-14" morph="none" pos="word" start_char="12065">that</TOKEN>
<TOKEN end_char="12073" id="token-121-15" morph="none" pos="word" start_char="12070">went</TOKEN>
<TOKEN end_char="12079" id="token-121-16" morph="none" pos="word" start_char="12075">along</TOKEN>
<TOKEN end_char="12083" id="token-121-17" morph="none" pos="word" start_char="12081">the</TOKEN>
<TOKEN end_char="12089" id="token-121-18" morph="none" pos="word" start_char="12085">lines</TOKEN>
<TOKEN end_char="12092" id="token-121-19" morph="none" pos="word" start_char="12091">of</TOKEN>
<TOKEN end_char="12094" id="token-121-20" morph="none" pos="punct" start_char="12094">'</TOKEN>
<TOKEN end_char="12103" id="token-121-21" morph="none" pos="word" start_char="12095">Europeans</TOKEN>
<TOKEN end_char="12111" id="token-121-22" morph="none" pos="word" start_char="12105">nations</TOKEN>
<TOKEN end_char="12118" id="token-121-23" morph="none" pos="word" start_char="12113">seemed</TOKEN>
<TOKEN end_char="12121" id="token-121-24" morph="none" pos="word" start_char="12120">to</TOKEN>
<TOKEN end_char="12126" id="token-121-25" morph="none" pos="word" start_char="12123">need</TOKEN>
<TOKEN end_char="12129" id="token-121-26" morph="none" pos="word" start_char="12128">to</TOKEN>
<TOKEN end_char="12134" id="token-121-27" morph="none" pos="word" start_char="12131">have</TOKEN>
<TOKEN end_char="12146" id="token-121-28" morph="none" pos="word" start_char="12136">experienced</TOKEN>
<TOKEN end_char="12150" id="token-121-29" morph="none" pos="word" start_char="12148">the</TOKEN>
<TOKEN end_char="12157" id="token-121-30" morph="none" pos="word" start_char="12152">deaths</TOKEN>
<TOKEN end_char="12168" id="token-121-31" morph="none" pos="word" start_char="12159">themselves</TOKEN>
<TOKEN end_char="12175" id="token-121-32" morph="none" pos="word" start_char="12170">before</TOKEN>
<TOKEN end_char="12185" id="token-121-33" morph="none" pos="word" start_char="12177">accepting</TOKEN>
<TOKEN end_char="12189" id="token-121-34" morph="none" pos="word" start_char="12187">the</TOKEN>
<TOKEN end_char="12201" id="token-121-35" morph="none" pos="word" start_char="12191">seriousness</TOKEN>
<TOKEN end_char="12204" id="token-121-36" morph="none" pos="word" start_char="12203">of</TOKEN>
<TOKEN end_char="12208" id="token-121-37" morph="none" pos="word" start_char="12206">the</TOKEN>
<TOKEN end_char="12217" id="token-121-38" morph="none" pos="word" start_char="12210">pandemic</TOKEN>
<TOKEN end_char="12219" id="token-121-39" morph="none" pos="punct" start_char="12218">'.</TOKEN>
</SEG>
<SEG end_char="12467" id="segment-122" start_char="12222">
<ORIGINAL_TEXT>For some of the richer Asian countries like Japan and South Korea, where it's typical to see people in masks in the decades before Covid hit, the 'been there, done that' plays a massive factor in the populations willingness to do the right thing.</ORIGINAL_TEXT>
<TOKEN end_char="12224" id="token-122-0" morph="none" pos="word" start_char="12222">For</TOKEN>
<TOKEN end_char="12229" id="token-122-1" morph="none" pos="word" start_char="12226">some</TOKEN>
<TOKEN end_char="12232" id="token-122-2" morph="none" pos="word" start_char="12231">of</TOKEN>
<TOKEN end_char="12236" id="token-122-3" morph="none" pos="word" start_char="12234">the</TOKEN>
<TOKEN end_char="12243" id="token-122-4" morph="none" pos="word" start_char="12238">richer</TOKEN>
<TOKEN end_char="12249" id="token-122-5" morph="none" pos="word" start_char="12245">Asian</TOKEN>
<TOKEN end_char="12259" id="token-122-6" morph="none" pos="word" start_char="12251">countries</TOKEN>
<TOKEN end_char="12264" id="token-122-7" morph="none" pos="word" start_char="12261">like</TOKEN>
<TOKEN end_char="12270" id="token-122-8" morph="none" pos="word" start_char="12266">Japan</TOKEN>
<TOKEN end_char="12274" id="token-122-9" morph="none" pos="word" start_char="12272">and</TOKEN>
<TOKEN end_char="12280" id="token-122-10" morph="none" pos="word" start_char="12276">South</TOKEN>
<TOKEN end_char="12286" id="token-122-11" morph="none" pos="word" start_char="12282">Korea</TOKEN>
<TOKEN end_char="12287" id="token-122-12" morph="none" pos="punct" start_char="12287">,</TOKEN>
<TOKEN end_char="12293" id="token-122-13" morph="none" pos="word" start_char="12289">where</TOKEN>
<TOKEN end_char="12298" id="token-122-14" morph="none" pos="word" start_char="12295">it's</TOKEN>
<TOKEN end_char="12306" id="token-122-15" morph="none" pos="word" start_char="12300">typical</TOKEN>
<TOKEN end_char="12309" id="token-122-16" morph="none" pos="word" start_char="12308">to</TOKEN>
<TOKEN end_char="12313" id="token-122-17" morph="none" pos="word" start_char="12311">see</TOKEN>
<TOKEN end_char="12320" id="token-122-18" morph="none" pos="word" start_char="12315">people</TOKEN>
<TOKEN end_char="12323" id="token-122-19" morph="none" pos="word" start_char="12322">in</TOKEN>
<TOKEN end_char="12329" id="token-122-20" morph="none" pos="word" start_char="12325">masks</TOKEN>
<TOKEN end_char="12332" id="token-122-21" morph="none" pos="word" start_char="12331">in</TOKEN>
<TOKEN end_char="12336" id="token-122-22" morph="none" pos="word" start_char="12334">the</TOKEN>
<TOKEN end_char="12344" id="token-122-23" morph="none" pos="word" start_char="12338">decades</TOKEN>
<TOKEN end_char="12351" id="token-122-24" morph="none" pos="word" start_char="12346">before</TOKEN>
<TOKEN end_char="12357" id="token-122-25" morph="none" pos="word" start_char="12353">Covid</TOKEN>
<TOKEN end_char="12361" id="token-122-26" morph="none" pos="word" start_char="12359">hit</TOKEN>
<TOKEN end_char="12362" id="token-122-27" morph="none" pos="punct" start_char="12362">,</TOKEN>
<TOKEN end_char="12366" id="token-122-28" morph="none" pos="word" start_char="12364">the</TOKEN>
<TOKEN end_char="12368" id="token-122-29" morph="none" pos="punct" start_char="12368">'</TOKEN>
<TOKEN end_char="12372" id="token-122-30" morph="none" pos="word" start_char="12369">been</TOKEN>
<TOKEN end_char="12378" id="token-122-31" morph="none" pos="word" start_char="12374">there</TOKEN>
<TOKEN end_char="12379" id="token-122-32" morph="none" pos="punct" start_char="12379">,</TOKEN>
<TOKEN end_char="12384" id="token-122-33" morph="none" pos="word" start_char="12381">done</TOKEN>
<TOKEN end_char="12389" id="token-122-34" morph="none" pos="word" start_char="12386">that</TOKEN>
<TOKEN end_char="12390" id="token-122-35" morph="none" pos="punct" start_char="12390">'</TOKEN>
<TOKEN end_char="12396" id="token-122-36" morph="none" pos="word" start_char="12392">plays</TOKEN>
<TOKEN end_char="12398" id="token-122-37" morph="none" pos="word" start_char="12398">a</TOKEN>
<TOKEN end_char="12406" id="token-122-38" morph="none" pos="word" start_char="12400">massive</TOKEN>
<TOKEN end_char="12413" id="token-122-39" morph="none" pos="word" start_char="12408">factor</TOKEN>
<TOKEN end_char="12416" id="token-122-40" morph="none" pos="word" start_char="12415">in</TOKEN>
<TOKEN end_char="12420" id="token-122-41" morph="none" pos="word" start_char="12418">the</TOKEN>
<TOKEN end_char="12432" id="token-122-42" morph="none" pos="word" start_char="12422">populations</TOKEN>
<TOKEN end_char="12444" id="token-122-43" morph="none" pos="word" start_char="12434">willingness</TOKEN>
<TOKEN end_char="12447" id="token-122-44" morph="none" pos="word" start_char="12446">to</TOKEN>
<TOKEN end_char="12450" id="token-122-45" morph="none" pos="word" start_char="12449">do</TOKEN>
<TOKEN end_char="12454" id="token-122-46" morph="none" pos="word" start_char="12452">the</TOKEN>
<TOKEN end_char="12460" id="token-122-47" morph="none" pos="word" start_char="12456">right</TOKEN>
<TOKEN end_char="12466" id="token-122-48" morph="none" pos="word" start_char="12462">thing</TOKEN>
<TOKEN end_char="12467" id="token-122-49" morph="none" pos="punct" start_char="12467">.</TOKEN>
</SEG>
<SEG end_char="12580" id="segment-123" start_char="12470">
<ORIGINAL_TEXT>An ageing population, which is common in Europe, is clearly one of the biggest indicators of a high death toll.</ORIGINAL_TEXT>
<TOKEN end_char="12471" id="token-123-0" morph="none" pos="word" start_char="12470">An</TOKEN>
<TOKEN end_char="12478" id="token-123-1" morph="none" pos="word" start_char="12473">ageing</TOKEN>
<TOKEN end_char="12489" id="token-123-2" morph="none" pos="word" start_char="12480">population</TOKEN>
<TOKEN end_char="12490" id="token-123-3" morph="none" pos="punct" start_char="12490">,</TOKEN>
<TOKEN end_char="12496" id="token-123-4" morph="none" pos="word" start_char="12492">which</TOKEN>
<TOKEN end_char="12499" id="token-123-5" morph="none" pos="word" start_char="12498">is</TOKEN>
<TOKEN end_char="12506" id="token-123-6" morph="none" pos="word" start_char="12501">common</TOKEN>
<TOKEN end_char="12509" id="token-123-7" morph="none" pos="word" start_char="12508">in</TOKEN>
<TOKEN end_char="12516" id="token-123-8" morph="none" pos="word" start_char="12511">Europe</TOKEN>
<TOKEN end_char="12517" id="token-123-9" morph="none" pos="punct" start_char="12517">,</TOKEN>
<TOKEN end_char="12520" id="token-123-10" morph="none" pos="word" start_char="12519">is</TOKEN>
<TOKEN end_char="12528" id="token-123-11" morph="none" pos="word" start_char="12522">clearly</TOKEN>
<TOKEN end_char="12532" id="token-123-12" morph="none" pos="word" start_char="12530">one</TOKEN>
<TOKEN end_char="12535" id="token-123-13" morph="none" pos="word" start_char="12534">of</TOKEN>
<TOKEN end_char="12539" id="token-123-14" morph="none" pos="word" start_char="12537">the</TOKEN>
<TOKEN end_char="12547" id="token-123-15" morph="none" pos="word" start_char="12541">biggest</TOKEN>
<TOKEN end_char="12558" id="token-123-16" morph="none" pos="word" start_char="12549">indicators</TOKEN>
<TOKEN end_char="12561" id="token-123-17" morph="none" pos="word" start_char="12560">of</TOKEN>
<TOKEN end_char="12563" id="token-123-18" morph="none" pos="word" start_char="12563">a</TOKEN>
<TOKEN end_char="12568" id="token-123-19" morph="none" pos="word" start_char="12565">high</TOKEN>
<TOKEN end_char="12574" id="token-123-20" morph="none" pos="word" start_char="12570">death</TOKEN>
<TOKEN end_char="12579" id="token-123-21" morph="none" pos="word" start_char="12576">toll</TOKEN>
<TOKEN end_char="12580" id="token-123-22" morph="none" pos="punct" start_char="12580">.</TOKEN>
</SEG>
<SEG end_char="12765" id="segment-124" start_char="12582">
<ORIGINAL_TEXT>We have an average age of 40 years ish and if you look at Africa or South America, and a lot of the poorer Asian countries like India their population is 10-15 years younger than ours.</ORIGINAL_TEXT>
<TOKEN end_char="12583" id="token-124-0" morph="none" pos="word" start_char="12582">We</TOKEN>
<TOKEN end_char="12588" id="token-124-1" morph="none" pos="word" start_char="12585">have</TOKEN>
<TOKEN end_char="12591" id="token-124-2" morph="none" pos="word" start_char="12590">an</TOKEN>
<TOKEN end_char="12599" id="token-124-3" morph="none" pos="word" start_char="12593">average</TOKEN>
<TOKEN end_char="12603" id="token-124-4" morph="none" pos="word" start_char="12601">age</TOKEN>
<TOKEN end_char="12606" id="token-124-5" morph="none" pos="word" start_char="12605">of</TOKEN>
<TOKEN end_char="12609" id="token-124-6" morph="none" pos="word" start_char="12608">40</TOKEN>
<TOKEN end_char="12615" id="token-124-7" morph="none" pos="word" start_char="12611">years</TOKEN>
<TOKEN end_char="12619" id="token-124-8" morph="none" pos="word" start_char="12617">ish</TOKEN>
<TOKEN end_char="12623" id="token-124-9" morph="none" pos="word" start_char="12621">and</TOKEN>
<TOKEN end_char="12626" id="token-124-10" morph="none" pos="word" start_char="12625">if</TOKEN>
<TOKEN end_char="12630" id="token-124-11" morph="none" pos="word" start_char="12628">you</TOKEN>
<TOKEN end_char="12635" id="token-124-12" morph="none" pos="word" start_char="12632">look</TOKEN>
<TOKEN end_char="12638" id="token-124-13" morph="none" pos="word" start_char="12637">at</TOKEN>
<TOKEN end_char="12645" id="token-124-14" morph="none" pos="word" start_char="12640">Africa</TOKEN>
<TOKEN end_char="12648" id="token-124-15" morph="none" pos="word" start_char="12647">or</TOKEN>
<TOKEN end_char="12654" id="token-124-16" morph="none" pos="word" start_char="12650">South</TOKEN>
<TOKEN end_char="12662" id="token-124-17" morph="none" pos="word" start_char="12656">America</TOKEN>
<TOKEN end_char="12663" id="token-124-18" morph="none" pos="punct" start_char="12663">,</TOKEN>
<TOKEN end_char="12667" id="token-124-19" morph="none" pos="word" start_char="12665">and</TOKEN>
<TOKEN end_char="12669" id="token-124-20" morph="none" pos="word" start_char="12669">a</TOKEN>
<TOKEN end_char="12673" id="token-124-21" morph="none" pos="word" start_char="12671">lot</TOKEN>
<TOKEN end_char="12676" id="token-124-22" morph="none" pos="word" start_char="12675">of</TOKEN>
<TOKEN end_char="12680" id="token-124-23" morph="none" pos="word" start_char="12678">the</TOKEN>
<TOKEN end_char="12687" id="token-124-24" morph="none" pos="word" start_char="12682">poorer</TOKEN>
<TOKEN end_char="12693" id="token-124-25" morph="none" pos="word" start_char="12689">Asian</TOKEN>
<TOKEN end_char="12703" id="token-124-26" morph="none" pos="word" start_char="12695">countries</TOKEN>
<TOKEN end_char="12708" id="token-124-27" morph="none" pos="word" start_char="12705">like</TOKEN>
<TOKEN end_char="12714" id="token-124-28" morph="none" pos="word" start_char="12710">India</TOKEN>
<TOKEN end_char="12720" id="token-124-29" morph="none" pos="word" start_char="12716">their</TOKEN>
<TOKEN end_char="12731" id="token-124-30" morph="none" pos="word" start_char="12722">population</TOKEN>
<TOKEN end_char="12734" id="token-124-31" morph="none" pos="word" start_char="12733">is</TOKEN>
<TOKEN end_char="12740" id="token-124-32" morph="none" pos="unknown" start_char="12736">10-15</TOKEN>
<TOKEN end_char="12746" id="token-124-33" morph="none" pos="word" start_char="12742">years</TOKEN>
<TOKEN end_char="12754" id="token-124-34" morph="none" pos="word" start_char="12748">younger</TOKEN>
<TOKEN end_char="12759" id="token-124-35" morph="none" pos="word" start_char="12756">than</TOKEN>
<TOKEN end_char="12764" id="token-124-36" morph="none" pos="word" start_char="12761">ours</TOKEN>
<TOKEN end_char="12765" id="token-124-37" morph="none" pos="punct" start_char="12765">.</TOKEN>
</SEG>
<SEG end_char="12893" id="segment-125" start_char="12770">
<ORIGINAL_TEXT>Belzok said: An ageing population, which is common in Europe, is clearly one of the biggest indicators of a high death toll.</ORIGINAL_TEXT>
<TOKEN end_char="12775" id="token-125-0" morph="none" pos="word" start_char="12770">Belzok</TOKEN>
<TOKEN end_char="12780" id="token-125-1" morph="none" pos="word" start_char="12777">said</TOKEN>
<TOKEN end_char="12781" id="token-125-2" morph="none" pos="punct" start_char="12781">:</TOKEN>
<TOKEN end_char="12784" id="token-125-3" morph="none" pos="word" start_char="12783">An</TOKEN>
<TOKEN end_char="12791" id="token-125-4" morph="none" pos="word" start_char="12786">ageing</TOKEN>
<TOKEN end_char="12802" id="token-125-5" morph="none" pos="word" start_char="12793">population</TOKEN>
<TOKEN end_char="12803" id="token-125-6" morph="none" pos="punct" start_char="12803">,</TOKEN>
<TOKEN end_char="12809" id="token-125-7" morph="none" pos="word" start_char="12805">which</TOKEN>
<TOKEN end_char="12812" id="token-125-8" morph="none" pos="word" start_char="12811">is</TOKEN>
<TOKEN end_char="12819" id="token-125-9" morph="none" pos="word" start_char="12814">common</TOKEN>
<TOKEN end_char="12822" id="token-125-10" morph="none" pos="word" start_char="12821">in</TOKEN>
<TOKEN end_char="12829" id="token-125-11" morph="none" pos="word" start_char="12824">Europe</TOKEN>
<TOKEN end_char="12830" id="token-125-12" morph="none" pos="punct" start_char="12830">,</TOKEN>
<TOKEN end_char="12833" id="token-125-13" morph="none" pos="word" start_char="12832">is</TOKEN>
<TOKEN end_char="12841" id="token-125-14" morph="none" pos="word" start_char="12835">clearly</TOKEN>
<TOKEN end_char="12845" id="token-125-15" morph="none" pos="word" start_char="12843">one</TOKEN>
<TOKEN end_char="12848" id="token-125-16" morph="none" pos="word" start_char="12847">of</TOKEN>
<TOKEN end_char="12852" id="token-125-17" morph="none" pos="word" start_char="12850">the</TOKEN>
<TOKEN end_char="12860" id="token-125-18" morph="none" pos="word" start_char="12854">biggest</TOKEN>
<TOKEN end_char="12871" id="token-125-19" morph="none" pos="word" start_char="12862">indicators</TOKEN>
<TOKEN end_char="12874" id="token-125-20" morph="none" pos="word" start_char="12873">of</TOKEN>
<TOKEN end_char="12876" id="token-125-21" morph="none" pos="word" start_char="12876">a</TOKEN>
<TOKEN end_char="12881" id="token-125-22" morph="none" pos="word" start_char="12878">high</TOKEN>
<TOKEN end_char="12887" id="token-125-23" morph="none" pos="word" start_char="12883">death</TOKEN>
<TOKEN end_char="12892" id="token-125-24" morph="none" pos="word" start_char="12889">toll</TOKEN>
<TOKEN end_char="12893" id="token-125-25" morph="none" pos="punct" start_char="12893">.</TOKEN>
</SEG>
<SEG end_char="13078" id="segment-126" start_char="12895">
<ORIGINAL_TEXT>We have an average age of 40 years ish and if you look at Africa or South America, and a lot of the poorer Asian countries like India their population is 10-15 years younger than ours.</ORIGINAL_TEXT>
<TOKEN end_char="12896" id="token-126-0" morph="none" pos="word" start_char="12895">We</TOKEN>
<TOKEN end_char="12901" id="token-126-1" morph="none" pos="word" start_char="12898">have</TOKEN>
<TOKEN end_char="12904" id="token-126-2" morph="none" pos="word" start_char="12903">an</TOKEN>
<TOKEN end_char="12912" id="token-126-3" morph="none" pos="word" start_char="12906">average</TOKEN>
<TOKEN end_char="12916" id="token-126-4" morph="none" pos="word" start_char="12914">age</TOKEN>
<TOKEN end_char="12919" id="token-126-5" morph="none" pos="word" start_char="12918">of</TOKEN>
<TOKEN end_char="12922" id="token-126-6" morph="none" pos="word" start_char="12921">40</TOKEN>
<TOKEN end_char="12928" id="token-126-7" morph="none" pos="word" start_char="12924">years</TOKEN>
<TOKEN end_char="12932" id="token-126-8" morph="none" pos="word" start_char="12930">ish</TOKEN>
<TOKEN end_char="12936" id="token-126-9" morph="none" pos="word" start_char="12934">and</TOKEN>
<TOKEN end_char="12939" id="token-126-10" morph="none" pos="word" start_char="12938">if</TOKEN>
<TOKEN end_char="12943" id="token-126-11" morph="none" pos="word" start_char="12941">you</TOKEN>
<TOKEN end_char="12948" id="token-126-12" morph="none" pos="word" start_char="12945">look</TOKEN>
<TOKEN end_char="12951" id="token-126-13" morph="none" pos="word" start_char="12950">at</TOKEN>
<TOKEN end_char="12958" id="token-126-14" morph="none" pos="word" start_char="12953">Africa</TOKEN>
<TOKEN end_char="12961" id="token-126-15" morph="none" pos="word" start_char="12960">or</TOKEN>
<TOKEN end_char="12967" id="token-126-16" morph="none" pos="word" start_char="12963">South</TOKEN>
<TOKEN end_char="12975" id="token-126-17" morph="none" pos="word" start_char="12969">America</TOKEN>
<TOKEN end_char="12976" id="token-126-18" morph="none" pos="punct" start_char="12976">,</TOKEN>
<TOKEN end_char="12980" id="token-126-19" morph="none" pos="word" start_char="12978">and</TOKEN>
<TOKEN end_char="12982" id="token-126-20" morph="none" pos="word" start_char="12982">a</TOKEN>
<TOKEN end_char="12986" id="token-126-21" morph="none" pos="word" start_char="12984">lot</TOKEN>
<TOKEN end_char="12989" id="token-126-22" morph="none" pos="word" start_char="12988">of</TOKEN>
<TOKEN end_char="12993" id="token-126-23" morph="none" pos="word" start_char="12991">the</TOKEN>
<TOKEN end_char="13000" id="token-126-24" morph="none" pos="word" start_char="12995">poorer</TOKEN>
<TOKEN end_char="13006" id="token-126-25" morph="none" pos="word" start_char="13002">Asian</TOKEN>
<TOKEN end_char="13016" id="token-126-26" morph="none" pos="word" start_char="13008">countries</TOKEN>
<TOKEN end_char="13021" id="token-126-27" morph="none" pos="word" start_char="13018">like</TOKEN>
<TOKEN end_char="13027" id="token-126-28" morph="none" pos="word" start_char="13023">India</TOKEN>
<TOKEN end_char="13033" id="token-126-29" morph="none" pos="word" start_char="13029">their</TOKEN>
<TOKEN end_char="13044" id="token-126-30" morph="none" pos="word" start_char="13035">population</TOKEN>
<TOKEN end_char="13047" id="token-126-31" morph="none" pos="word" start_char="13046">is</TOKEN>
<TOKEN end_char="13053" id="token-126-32" morph="none" pos="unknown" start_char="13049">10-15</TOKEN>
<TOKEN end_char="13059" id="token-126-33" morph="none" pos="word" start_char="13055">years</TOKEN>
<TOKEN end_char="13067" id="token-126-34" morph="none" pos="word" start_char="13061">younger</TOKEN>
<TOKEN end_char="13072" id="token-126-35" morph="none" pos="word" start_char="13069">than</TOKEN>
<TOKEN end_char="13077" id="token-126-36" morph="none" pos="word" start_char="13074">ours</TOKEN>
<TOKEN end_char="13078" id="token-126-37" morph="none" pos="punct" start_char="13078">.</TOKEN>
</SEG>
<SEG end_char="13325" id="segment-127" start_char="13082">
<ORIGINAL_TEXT>Although a quick google produces median age Japan: 48.6 (70 deaths per million) Hong Kong: 45.6 (27) South Korea: 43.2 (33) Taiwan: 42.3 (0.4) Macau: 40.8 (0) UK 40.6 (1,852 deaths per million) (DPM 4,630 * Taiwan, 56 * South Korea, 26 * Japan)</ORIGINAL_TEXT>
<TOKEN end_char="13089" id="token-127-0" morph="none" pos="word" start_char="13082">Although</TOKEN>
<TOKEN end_char="13091" id="token-127-1" morph="none" pos="word" start_char="13091">a</TOKEN>
<TOKEN end_char="13097" id="token-127-2" morph="none" pos="word" start_char="13093">quick</TOKEN>
<TOKEN end_char="13104" id="token-127-3" morph="none" pos="word" start_char="13099">google</TOKEN>
<TOKEN end_char="13113" id="token-127-4" morph="none" pos="word" start_char="13106">produces</TOKEN>
<TOKEN end_char="13120" id="token-127-5" morph="none" pos="word" start_char="13115">median</TOKEN>
<TOKEN end_char="13124" id="token-127-6" morph="none" pos="word" start_char="13122">age</TOKEN>
<TOKEN end_char="13130" id="token-127-7" morph="none" pos="word" start_char="13126">Japan</TOKEN>
<TOKEN end_char="13131" id="token-127-8" morph="none" pos="punct" start_char="13131">:</TOKEN>
<TOKEN end_char="13136" id="token-127-9" morph="none" pos="word" start_char="13133">48.6</TOKEN>
<TOKEN end_char="13138" id="token-127-10" morph="none" pos="punct" start_char="13138">(</TOKEN>
<TOKEN end_char="13140" id="token-127-11" morph="none" pos="word" start_char="13139">70</TOKEN>
<TOKEN end_char="13147" id="token-127-12" morph="none" pos="word" start_char="13142">deaths</TOKEN>
<TOKEN end_char="13151" id="token-127-13" morph="none" pos="word" start_char="13149">per</TOKEN>
<TOKEN end_char="13159" id="token-127-14" morph="none" pos="word" start_char="13153">million</TOKEN>
<TOKEN end_char="13160" id="token-127-15" morph="none" pos="punct" start_char="13160">)</TOKEN>
<TOKEN end_char="13165" id="token-127-16" morph="none" pos="word" start_char="13162">Hong</TOKEN>
<TOKEN end_char="13170" id="token-127-17" morph="none" pos="word" start_char="13167">Kong</TOKEN>
<TOKEN end_char="13171" id="token-127-18" morph="none" pos="punct" start_char="13171">:</TOKEN>
<TOKEN end_char="13176" id="token-127-19" morph="none" pos="word" start_char="13173">45.6</TOKEN>
<TOKEN end_char="13178" id="token-127-20" morph="none" pos="punct" start_char="13178">(</TOKEN>
<TOKEN end_char="13180" id="token-127-21" morph="none" pos="word" start_char="13179">27</TOKEN>
<TOKEN end_char="13181" id="token-127-22" morph="none" pos="punct" start_char="13181">)</TOKEN>
<TOKEN end_char="13187" id="token-127-23" morph="none" pos="word" start_char="13183">South</TOKEN>
<TOKEN end_char="13193" id="token-127-24" morph="none" pos="word" start_char="13189">Korea</TOKEN>
<TOKEN end_char="13194" id="token-127-25" morph="none" pos="punct" start_char="13194">:</TOKEN>
<TOKEN end_char="13199" id="token-127-26" morph="none" pos="word" start_char="13196">43.2</TOKEN>
<TOKEN end_char="13201" id="token-127-27" morph="none" pos="punct" start_char="13201">(</TOKEN>
<TOKEN end_char="13203" id="token-127-28" morph="none" pos="word" start_char="13202">33</TOKEN>
<TOKEN end_char="13204" id="token-127-29" morph="none" pos="punct" start_char="13204">)</TOKEN>
<TOKEN end_char="13211" id="token-127-30" morph="none" pos="word" start_char="13206">Taiwan</TOKEN>
<TOKEN end_char="13212" id="token-127-31" morph="none" pos="punct" start_char="13212">:</TOKEN>
<TOKEN end_char="13217" id="token-127-32" morph="none" pos="word" start_char="13214">42.3</TOKEN>
<TOKEN end_char="13219" id="token-127-33" morph="none" pos="punct" start_char="13219">(</TOKEN>
<TOKEN end_char="13222" id="token-127-34" morph="none" pos="unknown" start_char="13220">0.4</TOKEN>
<TOKEN end_char="13223" id="token-127-35" morph="none" pos="punct" start_char="13223">)</TOKEN>
<TOKEN end_char="13229" id="token-127-36" morph="none" pos="word" start_char="13225">Macau</TOKEN>
<TOKEN end_char="13230" id="token-127-37" morph="none" pos="punct" start_char="13230">:</TOKEN>
<TOKEN end_char="13235" id="token-127-38" morph="none" pos="word" start_char="13232">40.8</TOKEN>
<TOKEN end_char="13237" id="token-127-39" morph="none" pos="punct" start_char="13237">(</TOKEN>
<TOKEN end_char="13238" id="token-127-40" morph="none" pos="word" start_char="13238">0</TOKEN>
<TOKEN end_char="13239" id="token-127-41" morph="none" pos="punct" start_char="13239">)</TOKEN>
<TOKEN end_char="13242" id="token-127-42" morph="none" pos="word" start_char="13241">UK</TOKEN>
<TOKEN end_char="13247" id="token-127-43" morph="none" pos="word" start_char="13244">40.6</TOKEN>
<TOKEN end_char="13249" id="token-127-44" morph="none" pos="punct" start_char="13249">(</TOKEN>
<TOKEN end_char="13254" id="token-127-45" morph="none" pos="unknown" start_char="13250">1,852</TOKEN>
<TOKEN end_char="13261" id="token-127-46" morph="none" pos="word" start_char="13256">deaths</TOKEN>
<TOKEN end_char="13265" id="token-127-47" morph="none" pos="word" start_char="13263">per</TOKEN>
<TOKEN end_char="13273" id="token-127-48" morph="none" pos="word" start_char="13267">million</TOKEN>
<TOKEN end_char="13274" id="token-127-49" morph="none" pos="punct" start_char="13274">)</TOKEN>
<TOKEN end_char="13276" id="token-127-50" morph="none" pos="punct" start_char="13276">(</TOKEN>
<TOKEN end_char="13279" id="token-127-51" morph="none" pos="word" start_char="13277">DPM</TOKEN>
<TOKEN end_char="13285" id="token-127-52" morph="none" pos="unknown" start_char="13281">4,630</TOKEN>
<TOKEN end_char="13287" id="token-127-53" morph="none" pos="punct" start_char="13287">*</TOKEN>
<TOKEN end_char="13294" id="token-127-54" morph="none" pos="word" start_char="13289">Taiwan</TOKEN>
<TOKEN end_char="13295" id="token-127-55" morph="none" pos="punct" start_char="13295">,</TOKEN>
<TOKEN end_char="13298" id="token-127-56" morph="none" pos="word" start_char="13297">56</TOKEN>
<TOKEN end_char="13300" id="token-127-57" morph="none" pos="punct" start_char="13300">*</TOKEN>
<TOKEN end_char="13306" id="token-127-58" morph="none" pos="word" start_char="13302">South</TOKEN>
<TOKEN end_char="13312" id="token-127-59" morph="none" pos="word" start_char="13308">Korea</TOKEN>
<TOKEN end_char="13313" id="token-127-60" morph="none" pos="punct" start_char="13313">,</TOKEN>
<TOKEN end_char="13316" id="token-127-61" morph="none" pos="word" start_char="13315">26</TOKEN>
<TOKEN end_char="13318" id="token-127-62" morph="none" pos="punct" start_char="13318">*</TOKEN>
<TOKEN end_char="13324" id="token-127-63" morph="none" pos="word" start_char="13320">Japan</TOKEN>
<TOKEN end_char="13325" id="token-127-64" morph="none" pos="punct" start_char="13325">)</TOKEN>
</SEG>
<SEG end_char="13411" id="segment-128" start_char="13328">
<ORIGINAL_TEXT>Five countries with higher median age have much, much lower death per million rates.</ORIGINAL_TEXT>
<TOKEN end_char="13331" id="token-128-0" morph="none" pos="word" start_char="13328">Five</TOKEN>
<TOKEN end_char="13341" id="token-128-1" morph="none" pos="word" start_char="13333">countries</TOKEN>
<TOKEN end_char="13346" id="token-128-2" morph="none" pos="word" start_char="13343">with</TOKEN>
<TOKEN end_char="13353" id="token-128-3" morph="none" pos="word" start_char="13348">higher</TOKEN>
<TOKEN end_char="13360" id="token-128-4" morph="none" pos="word" start_char="13355">median</TOKEN>
<TOKEN end_char="13364" id="token-128-5" morph="none" pos="word" start_char="13362">age</TOKEN>
<TOKEN end_char="13369" id="token-128-6" morph="none" pos="word" start_char="13366">have</TOKEN>
<TOKEN end_char="13374" id="token-128-7" morph="none" pos="word" start_char="13371">much</TOKEN>
<TOKEN end_char="13375" id="token-128-8" morph="none" pos="punct" start_char="13375">,</TOKEN>
<TOKEN end_char="13380" id="token-128-9" morph="none" pos="word" start_char="13377">much</TOKEN>
<TOKEN end_char="13386" id="token-128-10" morph="none" pos="word" start_char="13382">lower</TOKEN>
<TOKEN end_char="13392" id="token-128-11" morph="none" pos="word" start_char="13388">death</TOKEN>
<TOKEN end_char="13396" id="token-128-12" morph="none" pos="word" start_char="13394">per</TOKEN>
<TOKEN end_char="13404" id="token-128-13" morph="none" pos="word" start_char="13398">million</TOKEN>
<TOKEN end_char="13410" id="token-128-14" morph="none" pos="word" start_char="13406">rates</TOKEN>
<TOKEN end_char="13411" id="token-128-15" morph="none" pos="punct" start_char="13411">.</TOKEN>
</SEG>
<SEG end_char="13495" id="segment-129" start_char="13413">
<ORIGINAL_TEXT>What they have in common is that they locked down earlier and/or masked up earlier.</ORIGINAL_TEXT>
<TOKEN end_char="13416" id="token-129-0" morph="none" pos="word" start_char="13413">What</TOKEN>
<TOKEN end_char="13421" id="token-129-1" morph="none" pos="word" start_char="13418">they</TOKEN>
<TOKEN end_char="13426" id="token-129-2" morph="none" pos="word" start_char="13423">have</TOKEN>
<TOKEN end_char="13429" id="token-129-3" morph="none" pos="word" start_char="13428">in</TOKEN>
<TOKEN end_char="13436" id="token-129-4" morph="none" pos="word" start_char="13431">common</TOKEN>
<TOKEN end_char="13439" id="token-129-5" morph="none" pos="word" start_char="13438">is</TOKEN>
<TOKEN end_char="13444" id="token-129-6" morph="none" pos="word" start_char="13441">that</TOKEN>
<TOKEN end_char="13449" id="token-129-7" morph="none" pos="word" start_char="13446">they</TOKEN>
<TOKEN end_char="13456" id="token-129-8" morph="none" pos="word" start_char="13451">locked</TOKEN>
<TOKEN end_char="13461" id="token-129-9" morph="none" pos="word" start_char="13458">down</TOKEN>
<TOKEN end_char="13469" id="token-129-10" morph="none" pos="word" start_char="13463">earlier</TOKEN>
<TOKEN end_char="13476" id="token-129-11" morph="none" pos="unknown" start_char="13471">and/or</TOKEN>
<TOKEN end_char="13483" id="token-129-12" morph="none" pos="word" start_char="13478">masked</TOKEN>
<TOKEN end_char="13486" id="token-129-13" morph="none" pos="word" start_char="13485">up</TOKEN>
<TOKEN end_char="13494" id="token-129-14" morph="none" pos="word" start_char="13488">earlier</TOKEN>
<TOKEN end_char="13495" id="token-129-15" morph="none" pos="punct" start_char="13495">.</TOKEN>
</SEG>
<SEG end_char="13642" id="segment-130" start_char="13500">
<ORIGINAL_TEXT>goingoingong said: From the incidence figures it also appears that the 200K western population in Thailand have acquired this genetic immunity.</ORIGINAL_TEXT>
<TOKEN end_char="13511" id="token-130-0" morph="none" pos="word" start_char="13500">goingoingong</TOKEN>
<TOKEN end_char="13516" id="token-130-1" morph="none" pos="word" start_char="13513">said</TOKEN>
<TOKEN end_char="13517" id="token-130-2" morph="none" pos="punct" start_char="13517">:</TOKEN>
<TOKEN end_char="13522" id="token-130-3" morph="none" pos="word" start_char="13519">From</TOKEN>
<TOKEN end_char="13526" id="token-130-4" morph="none" pos="word" start_char="13524">the</TOKEN>
<TOKEN end_char="13536" id="token-130-5" morph="none" pos="word" start_char="13528">incidence</TOKEN>
<TOKEN end_char="13544" id="token-130-6" morph="none" pos="word" start_char="13538">figures</TOKEN>
<TOKEN end_char="13547" id="token-130-7" morph="none" pos="word" start_char="13546">it</TOKEN>
<TOKEN end_char="13552" id="token-130-8" morph="none" pos="word" start_char="13549">also</TOKEN>
<TOKEN end_char="13560" id="token-130-9" morph="none" pos="word" start_char="13554">appears</TOKEN>
<TOKEN end_char="13565" id="token-130-10" morph="none" pos="word" start_char="13562">that</TOKEN>
<TOKEN end_char="13569" id="token-130-11" morph="none" pos="word" start_char="13567">the</TOKEN>
<TOKEN end_char="13574" id="token-130-12" morph="none" pos="word" start_char="13571">200K</TOKEN>
<TOKEN end_char="13582" id="token-130-13" morph="none" pos="word" start_char="13576">western</TOKEN>
<TOKEN end_char="13593" id="token-130-14" morph="none" pos="word" start_char="13584">population</TOKEN>
<TOKEN end_char="13596" id="token-130-15" morph="none" pos="word" start_char="13595">in</TOKEN>
<TOKEN end_char="13605" id="token-130-16" morph="none" pos="word" start_char="13598">Thailand</TOKEN>
<TOKEN end_char="13610" id="token-130-17" morph="none" pos="word" start_char="13607">have</TOKEN>
<TOKEN end_char="13619" id="token-130-18" morph="none" pos="word" start_char="13612">acquired</TOKEN>
<TOKEN end_char="13624" id="token-130-19" morph="none" pos="word" start_char="13621">this</TOKEN>
<TOKEN end_char="13632" id="token-130-20" morph="none" pos="word" start_char="13626">genetic</TOKEN>
<TOKEN end_char="13641" id="token-130-21" morph="none" pos="word" start_char="13634">immunity</TOKEN>
<TOKEN end_char="13642" id="token-130-22" morph="none" pos="punct" start_char="13642">.</TOKEN>
</SEG>
<SEG end_char="13934" id="segment-131" start_char="13644">
<ORIGINAL_TEXT>As have the western origin population of Hong Kong, Macao, Cambodia, Vietnam and Laos.... As the genetics of this population will not have changed to when they were born (in the west) it looks more likely that the measures taken in these countries to stop the spread of covid were effective.</ORIGINAL_TEXT>
<TOKEN end_char="13645" id="token-131-0" morph="none" pos="word" start_char="13644">As</TOKEN>
<TOKEN end_char="13650" id="token-131-1" morph="none" pos="word" start_char="13647">have</TOKEN>
<TOKEN end_char="13654" id="token-131-2" morph="none" pos="word" start_char="13652">the</TOKEN>
<TOKEN end_char="13662" id="token-131-3" morph="none" pos="word" start_char="13656">western</TOKEN>
<TOKEN end_char="13669" id="token-131-4" morph="none" pos="word" start_char="13664">origin</TOKEN>
<TOKEN end_char="13680" id="token-131-5" morph="none" pos="word" start_char="13671">population</TOKEN>
<TOKEN end_char="13683" id="token-131-6" morph="none" pos="word" start_char="13682">of</TOKEN>
<TOKEN end_char="13688" id="token-131-7" morph="none" pos="word" start_char="13685">Hong</TOKEN>
<TOKEN end_char="13693" id="token-131-8" morph="none" pos="word" start_char="13690">Kong</TOKEN>
<TOKEN end_char="13694" id="token-131-9" morph="none" pos="punct" start_char="13694">,</TOKEN>
<TOKEN end_char="13700" id="token-131-10" morph="none" pos="word" start_char="13696">Macao</TOKEN>
<TOKEN end_char="13701" id="token-131-11" morph="none" pos="punct" start_char="13701">,</TOKEN>
<TOKEN end_char="13710" id="token-131-12" morph="none" pos="word" start_char="13703">Cambodia</TOKEN>
<TOKEN end_char="13711" id="token-131-13" morph="none" pos="punct" start_char="13711">,</TOKEN>
<TOKEN end_char="13719" id="token-131-14" morph="none" pos="word" start_char="13713">Vietnam</TOKEN>
<TOKEN end_char="13723" id="token-131-15" morph="none" pos="word" start_char="13721">and</TOKEN>
<TOKEN end_char="13728" id="token-131-16" morph="none" pos="word" start_char="13725">Laos</TOKEN>
<TOKEN end_char="13732" id="token-131-17" morph="none" pos="punct" start_char="13729">....</TOKEN>
<TOKEN end_char="13735" id="token-131-18" morph="none" pos="word" start_char="13734">As</TOKEN>
<TOKEN end_char="13739" id="token-131-19" morph="none" pos="word" start_char="13737">the</TOKEN>
<TOKEN end_char="13748" id="token-131-20" morph="none" pos="word" start_char="13741">genetics</TOKEN>
<TOKEN end_char="13751" id="token-131-21" morph="none" pos="word" start_char="13750">of</TOKEN>
<TOKEN end_char="13756" id="token-131-22" morph="none" pos="word" start_char="13753">this</TOKEN>
<TOKEN end_char="13767" id="token-131-23" morph="none" pos="word" start_char="13758">population</TOKEN>
<TOKEN end_char="13772" id="token-131-24" morph="none" pos="word" start_char="13769">will</TOKEN>
<TOKEN end_char="13776" id="token-131-25" morph="none" pos="word" start_char="13774">not</TOKEN>
<TOKEN end_char="13781" id="token-131-26" morph="none" pos="word" start_char="13778">have</TOKEN>
<TOKEN end_char="13789" id="token-131-27" morph="none" pos="word" start_char="13783">changed</TOKEN>
<TOKEN end_char="13792" id="token-131-28" morph="none" pos="word" start_char="13791">to</TOKEN>
<TOKEN end_char="13797" id="token-131-29" morph="none" pos="word" start_char="13794">when</TOKEN>
<TOKEN end_char="13802" id="token-131-30" morph="none" pos="word" start_char="13799">they</TOKEN>
<TOKEN end_char="13807" id="token-131-31" morph="none" pos="word" start_char="13804">were</TOKEN>
<TOKEN end_char="13812" id="token-131-32" morph="none" pos="word" start_char="13809">born</TOKEN>
<TOKEN end_char="13814" id="token-131-33" morph="none" pos="punct" start_char="13814">(</TOKEN>
<TOKEN end_char="13816" id="token-131-34" morph="none" pos="word" start_char="13815">in</TOKEN>
<TOKEN end_char="13820" id="token-131-35" morph="none" pos="word" start_char="13818">the</TOKEN>
<TOKEN end_char="13825" id="token-131-36" morph="none" pos="word" start_char="13822">west</TOKEN>
<TOKEN end_char="13826" id="token-131-37" morph="none" pos="punct" start_char="13826">)</TOKEN>
<TOKEN end_char="13829" id="token-131-38" morph="none" pos="word" start_char="13828">it</TOKEN>
<TOKEN end_char="13835" id="token-131-39" morph="none" pos="word" start_char="13831">looks</TOKEN>
<TOKEN end_char="13840" id="token-131-40" morph="none" pos="word" start_char="13837">more</TOKEN>
<TOKEN end_char="13847" id="token-131-41" morph="none" pos="word" start_char="13842">likely</TOKEN>
<TOKEN end_char="13852" id="token-131-42" morph="none" pos="word" start_char="13849">that</TOKEN>
<TOKEN end_char="13856" id="token-131-43" morph="none" pos="word" start_char="13854">the</TOKEN>
<TOKEN end_char="13865" id="token-131-44" morph="none" pos="word" start_char="13858">measures</TOKEN>
<TOKEN end_char="13871" id="token-131-45" morph="none" pos="word" start_char="13867">taken</TOKEN>
<TOKEN end_char="13874" id="token-131-46" morph="none" pos="word" start_char="13873">in</TOKEN>
<TOKEN end_char="13880" id="token-131-47" morph="none" pos="word" start_char="13876">these</TOKEN>
<TOKEN end_char="13890" id="token-131-48" morph="none" pos="word" start_char="13882">countries</TOKEN>
<TOKEN end_char="13893" id="token-131-49" morph="none" pos="word" start_char="13892">to</TOKEN>
<TOKEN end_char="13898" id="token-131-50" morph="none" pos="word" start_char="13895">stop</TOKEN>
<TOKEN end_char="13902" id="token-131-51" morph="none" pos="word" start_char="13900">the</TOKEN>
<TOKEN end_char="13909" id="token-131-52" morph="none" pos="word" start_char="13904">spread</TOKEN>
<TOKEN end_char="13912" id="token-131-53" morph="none" pos="word" start_char="13911">of</TOKEN>
<TOKEN end_char="13918" id="token-131-54" morph="none" pos="word" start_char="13914">covid</TOKEN>
<TOKEN end_char="13923" id="token-131-55" morph="none" pos="word" start_char="13920">were</TOKEN>
<TOKEN end_char="13933" id="token-131-56" morph="none" pos="word" start_char="13925">effective</TOKEN>
<TOKEN end_char="13934" id="token-131-57" morph="none" pos="punct" start_char="13934">.</TOKEN>
</SEG>
<SEG end_char="14126" id="segment-132" start_char="13936">
<ORIGINAL_TEXT>The main difference in the genetics would be that many Asians lack the ability to produce the enzyme (lactase) to digest milk when adult vs those mostly of northern european ancestry who can.</ORIGINAL_TEXT>
<TOKEN end_char="13938" id="token-132-0" morph="none" pos="word" start_char="13936">The</TOKEN>
<TOKEN end_char="13943" id="token-132-1" morph="none" pos="word" start_char="13940">main</TOKEN>
<TOKEN end_char="13954" id="token-132-2" morph="none" pos="word" start_char="13945">difference</TOKEN>
<TOKEN end_char="13957" id="token-132-3" morph="none" pos="word" start_char="13956">in</TOKEN>
<TOKEN end_char="13961" id="token-132-4" morph="none" pos="word" start_char="13959">the</TOKEN>
<TOKEN end_char="13970" id="token-132-5" morph="none" pos="word" start_char="13963">genetics</TOKEN>
<TOKEN end_char="13976" id="token-132-6" morph="none" pos="word" start_char="13972">would</TOKEN>
<TOKEN end_char="13979" id="token-132-7" morph="none" pos="word" start_char="13978">be</TOKEN>
<TOKEN end_char="13984" id="token-132-8" morph="none" pos="word" start_char="13981">that</TOKEN>
<TOKEN end_char="13989" id="token-132-9" morph="none" pos="word" start_char="13986">many</TOKEN>
<TOKEN end_char="13996" id="token-132-10" morph="none" pos="word" start_char="13991">Asians</TOKEN>
<TOKEN end_char="14001" id="token-132-11" morph="none" pos="word" start_char="13998">lack</TOKEN>
<TOKEN end_char="14005" id="token-132-12" morph="none" pos="word" start_char="14003">the</TOKEN>
<TOKEN end_char="14013" id="token-132-13" morph="none" pos="word" start_char="14007">ability</TOKEN>
<TOKEN end_char="14016" id="token-132-14" morph="none" pos="word" start_char="14015">to</TOKEN>
<TOKEN end_char="14024" id="token-132-15" morph="none" pos="word" start_char="14018">produce</TOKEN>
<TOKEN end_char="14028" id="token-132-16" morph="none" pos="word" start_char="14026">the</TOKEN>
<TOKEN end_char="14035" id="token-132-17" morph="none" pos="word" start_char="14030">enzyme</TOKEN>
<TOKEN end_char="14037" id="token-132-18" morph="none" pos="punct" start_char="14037">(</TOKEN>
<TOKEN end_char="14044" id="token-132-19" morph="none" pos="word" start_char="14038">lactase</TOKEN>
<TOKEN end_char="14045" id="token-132-20" morph="none" pos="punct" start_char="14045">)</TOKEN>
<TOKEN end_char="14048" id="token-132-21" morph="none" pos="word" start_char="14047">to</TOKEN>
<TOKEN end_char="14055" id="token-132-22" morph="none" pos="word" start_char="14050">digest</TOKEN>
<TOKEN end_char="14060" id="token-132-23" morph="none" pos="word" start_char="14057">milk</TOKEN>
<TOKEN end_char="14065" id="token-132-24" morph="none" pos="word" start_char="14062">when</TOKEN>
<TOKEN end_char="14071" id="token-132-25" morph="none" pos="word" start_char="14067">adult</TOKEN>
<TOKEN end_char="14074" id="token-132-26" morph="none" pos="word" start_char="14073">vs</TOKEN>
<TOKEN end_char="14080" id="token-132-27" morph="none" pos="word" start_char="14076">those</TOKEN>
<TOKEN end_char="14087" id="token-132-28" morph="none" pos="word" start_char="14082">mostly</TOKEN>
<TOKEN end_char="14090" id="token-132-29" morph="none" pos="word" start_char="14089">of</TOKEN>
<TOKEN end_char="14099" id="token-132-30" morph="none" pos="word" start_char="14092">northern</TOKEN>
<TOKEN end_char="14108" id="token-132-31" morph="none" pos="word" start_char="14101">european</TOKEN>
<TOKEN end_char="14117" id="token-132-32" morph="none" pos="word" start_char="14110">ancestry</TOKEN>
<TOKEN end_char="14121" id="token-132-33" morph="none" pos="word" start_char="14119">who</TOKEN>
<TOKEN end_char="14125" id="token-132-34" morph="none" pos="word" start_char="14123">can</TOKEN>
<TOKEN end_char="14126" id="token-132-35" morph="none" pos="punct" start_char="14126">.</TOKEN>
</SEG>
<SEG end_char="14227" id="segment-133" start_char="14128">
<ORIGINAL_TEXT>So one group has a diet which will use coconut milk and the other group animal based dairy products.</ORIGINAL_TEXT>
<TOKEN end_char="14129" id="token-133-0" morph="none" pos="word" start_char="14128">So</TOKEN>
<TOKEN end_char="14133" id="token-133-1" morph="none" pos="word" start_char="14131">one</TOKEN>
<TOKEN end_char="14139" id="token-133-2" morph="none" pos="word" start_char="14135">group</TOKEN>
<TOKEN end_char="14143" id="token-133-3" morph="none" pos="word" start_char="14141">has</TOKEN>
<TOKEN end_char="14145" id="token-133-4" morph="none" pos="word" start_char="14145">a</TOKEN>
<TOKEN end_char="14150" id="token-133-5" morph="none" pos="word" start_char="14147">diet</TOKEN>
<TOKEN end_char="14156" id="token-133-6" morph="none" pos="word" start_char="14152">which</TOKEN>
<TOKEN end_char="14161" id="token-133-7" morph="none" pos="word" start_char="14158">will</TOKEN>
<TOKEN end_char="14165" id="token-133-8" morph="none" pos="word" start_char="14163">use</TOKEN>
<TOKEN end_char="14173" id="token-133-9" morph="none" pos="word" start_char="14167">coconut</TOKEN>
<TOKEN end_char="14178" id="token-133-10" morph="none" pos="word" start_char="14175">milk</TOKEN>
<TOKEN end_char="14182" id="token-133-11" morph="none" pos="word" start_char="14180">and</TOKEN>
<TOKEN end_char="14186" id="token-133-12" morph="none" pos="word" start_char="14184">the</TOKEN>
<TOKEN end_char="14192" id="token-133-13" morph="none" pos="word" start_char="14188">other</TOKEN>
<TOKEN end_char="14198" id="token-133-14" morph="none" pos="word" start_char="14194">group</TOKEN>
<TOKEN end_char="14205" id="token-133-15" morph="none" pos="word" start_char="14200">animal</TOKEN>
<TOKEN end_char="14211" id="token-133-16" morph="none" pos="word" start_char="14207">based</TOKEN>
<TOKEN end_char="14217" id="token-133-17" morph="none" pos="word" start_char="14213">dairy</TOKEN>
<TOKEN end_char="14226" id="token-133-18" morph="none" pos="word" start_char="14219">products</TOKEN>
<TOKEN end_char="14227" id="token-133-19" morph="none" pos="punct" start_char="14227">.</TOKEN>
</SEG>
<SEG end_char="14257" id="segment-134" start_char="14229">
<ORIGINAL_TEXT>So diet may also be a factor.</ORIGINAL_TEXT>
<TOKEN end_char="14230" id="token-134-0" morph="none" pos="word" start_char="14229">So</TOKEN>
<TOKEN end_char="14235" id="token-134-1" morph="none" pos="word" start_char="14232">diet</TOKEN>
<TOKEN end_char="14239" id="token-134-2" morph="none" pos="word" start_char="14237">may</TOKEN>
<TOKEN end_char="14244" id="token-134-3" morph="none" pos="word" start_char="14241">also</TOKEN>
<TOKEN end_char="14247" id="token-134-4" morph="none" pos="word" start_char="14246">be</TOKEN>
<TOKEN end_char="14249" id="token-134-5" morph="none" pos="word" start_char="14249">a</TOKEN>
<TOKEN end_char="14256" id="token-134-6" morph="none" pos="word" start_char="14251">factor</TOKEN>
<TOKEN end_char="14257" id="token-134-7" morph="none" pos="punct" start_char="14257">.</TOKEN>
</SEG>
<SEG end_char="14276" id="segment-135" start_char="14259">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="14263" id="token-135-0" morph="none" pos="word" start_char="14259">Click</TOKEN>
<TOKEN end_char="14266" id="token-135-1" morph="none" pos="word" start_char="14265">to</TOKEN>
<TOKEN end_char="14273" id="token-135-2" morph="none" pos="word" start_char="14268">expand</TOKEN>
<TOKEN end_char="14276" id="token-135-3" morph="none" pos="punct" start_char="14274">...</TOKEN>
</SEG>
<SEG end_char="14408" id="segment-136" start_char="14279">
<ORIGINAL_TEXT>Well if a population is not getting infected then a minority won't have anybody to catch it from....thats how herd immunity works.</ORIGINAL_TEXT>
<TOKEN end_char="14282" id="token-136-0" morph="none" pos="word" start_char="14279">Well</TOKEN>
<TOKEN end_char="14285" id="token-136-1" morph="none" pos="word" start_char="14284">if</TOKEN>
<TOKEN end_char="14287" id="token-136-2" morph="none" pos="word" start_char="14287">a</TOKEN>
<TOKEN end_char="14298" id="token-136-3" morph="none" pos="word" start_char="14289">population</TOKEN>
<TOKEN end_char="14301" id="token-136-4" morph="none" pos="word" start_char="14300">is</TOKEN>
<TOKEN end_char="14305" id="token-136-5" morph="none" pos="word" start_char="14303">not</TOKEN>
<TOKEN end_char="14313" id="token-136-6" morph="none" pos="word" start_char="14307">getting</TOKEN>
<TOKEN end_char="14322" id="token-136-7" morph="none" pos="word" start_char="14315">infected</TOKEN>
<TOKEN end_char="14327" id="token-136-8" morph="none" pos="word" start_char="14324">then</TOKEN>
<TOKEN end_char="14329" id="token-136-9" morph="none" pos="word" start_char="14329">a</TOKEN>
<TOKEN end_char="14338" id="token-136-10" morph="none" pos="word" start_char="14331">minority</TOKEN>
<TOKEN end_char="14344" id="token-136-11" morph="none" pos="word" start_char="14340">won't</TOKEN>
<TOKEN end_char="14349" id="token-136-12" morph="none" pos="word" start_char="14346">have</TOKEN>
<TOKEN end_char="14357" id="token-136-13" morph="none" pos="word" start_char="14351">anybody</TOKEN>
<TOKEN end_char="14360" id="token-136-14" morph="none" pos="word" start_char="14359">to</TOKEN>
<TOKEN end_char="14366" id="token-136-15" morph="none" pos="word" start_char="14362">catch</TOKEN>
<TOKEN end_char="14369" id="token-136-16" morph="none" pos="word" start_char="14368">it</TOKEN>
<TOKEN end_char="14383" id="token-136-17" morph="none" pos="unknown" start_char="14371">from....thats</TOKEN>
<TOKEN end_char="14387" id="token-136-18" morph="none" pos="word" start_char="14385">how</TOKEN>
<TOKEN end_char="14392" id="token-136-19" morph="none" pos="word" start_char="14389">herd</TOKEN>
<TOKEN end_char="14401" id="token-136-20" morph="none" pos="word" start_char="14394">immunity</TOKEN>
<TOKEN end_char="14407" id="token-136-21" morph="none" pos="word" start_char="14403">works</TOKEN>
<TOKEN end_char="14408" id="token-136-22" morph="none" pos="punct" start_char="14408">.</TOKEN>
</SEG>
<SEG end_char="14437" id="segment-137" start_char="14411">
<ORIGINAL_TEXT>and then we have this......</ORIGINAL_TEXT>
<TOKEN end_char="14413" id="token-137-0" morph="none" pos="word" start_char="14411">and</TOKEN>
<TOKEN end_char="14418" id="token-137-1" morph="none" pos="word" start_char="14415">then</TOKEN>
<TOKEN end_char="14421" id="token-137-2" morph="none" pos="word" start_char="14420">we</TOKEN>
<TOKEN end_char="14426" id="token-137-3" morph="none" pos="word" start_char="14423">have</TOKEN>
<TOKEN end_char="14431" id="token-137-4" morph="none" pos="word" start_char="14428">this</TOKEN>
<TOKEN end_char="14437" id="token-137-5" morph="none" pos="punct" start_char="14432">......</TOKEN>
</SEG>
<SEG end_char="14496" id="segment-138" start_char="14440">
<ORIGINAL_TEXT>So even in western countries Asians have much lower risk.</ORIGINAL_TEXT>
<TOKEN end_char="14441" id="token-138-0" morph="none" pos="word" start_char="14440">So</TOKEN>
<TOKEN end_char="14446" id="token-138-1" morph="none" pos="word" start_char="14443">even</TOKEN>
<TOKEN end_char="14449" id="token-138-2" morph="none" pos="word" start_char="14448">in</TOKEN>
<TOKEN end_char="14457" id="token-138-3" morph="none" pos="word" start_char="14451">western</TOKEN>
<TOKEN end_char="14467" id="token-138-4" morph="none" pos="word" start_char="14459">countries</TOKEN>
<TOKEN end_char="14474" id="token-138-5" morph="none" pos="word" start_char="14469">Asians</TOKEN>
<TOKEN end_char="14479" id="token-138-6" morph="none" pos="word" start_char="14476">have</TOKEN>
<TOKEN end_char="14484" id="token-138-7" morph="none" pos="word" start_char="14481">much</TOKEN>
<TOKEN end_char="14490" id="token-138-8" morph="none" pos="word" start_char="14486">lower</TOKEN>
<TOKEN end_char="14495" id="token-138-9" morph="none" pos="word" start_char="14492">risk</TOKEN>
<TOKEN end_char="14496" id="token-138-10" morph="none" pos="punct" start_char="14496">.</TOKEN>
</SEG>
<SEG end_char="14849" id="segment-139" start_char="14502">
<ORIGINAL_TEXT>goingoingong said: Although a quick google produces median age Japan: 48.6 (70 deaths per million) Hong Kong: 45.6 (27) South Korea: 43.2 (33) Taiwan: 42.3 (0.4) Macau: 40.8 (0) UK 40.6 (1,852 deaths per million) (DPM 4,630 * Taiwan, 56 * South Korea, 26 * Japan) Five countries with higher median age have much, much lower death per million rates.</ORIGINAL_TEXT>
<TOKEN end_char="14513" id="token-139-0" morph="none" pos="word" start_char="14502">goingoingong</TOKEN>
<TOKEN end_char="14518" id="token-139-1" morph="none" pos="word" start_char="14515">said</TOKEN>
<TOKEN end_char="14519" id="token-139-2" morph="none" pos="punct" start_char="14519">:</TOKEN>
<TOKEN end_char="14528" id="token-139-3" morph="none" pos="word" start_char="14521">Although</TOKEN>
<TOKEN end_char="14530" id="token-139-4" morph="none" pos="word" start_char="14530">a</TOKEN>
<TOKEN end_char="14536" id="token-139-5" morph="none" pos="word" start_char="14532">quick</TOKEN>
<TOKEN end_char="14543" id="token-139-6" morph="none" pos="word" start_char="14538">google</TOKEN>
<TOKEN end_char="14552" id="token-139-7" morph="none" pos="word" start_char="14545">produces</TOKEN>
<TOKEN end_char="14559" id="token-139-8" morph="none" pos="word" start_char="14554">median</TOKEN>
<TOKEN end_char="14563" id="token-139-9" morph="none" pos="word" start_char="14561">age</TOKEN>
<TOKEN end_char="14569" id="token-139-10" morph="none" pos="word" start_char="14565">Japan</TOKEN>
<TOKEN end_char="14570" id="token-139-11" morph="none" pos="punct" start_char="14570">:</TOKEN>
<TOKEN end_char="14575" id="token-139-12" morph="none" pos="word" start_char="14572">48.6</TOKEN>
<TOKEN end_char="14577" id="token-139-13" morph="none" pos="punct" start_char="14577">(</TOKEN>
<TOKEN end_char="14579" id="token-139-14" morph="none" pos="word" start_char="14578">70</TOKEN>
<TOKEN end_char="14586" id="token-139-15" morph="none" pos="word" start_char="14581">deaths</TOKEN>
<TOKEN end_char="14590" id="token-139-16" morph="none" pos="word" start_char="14588">per</TOKEN>
<TOKEN end_char="14598" id="token-139-17" morph="none" pos="word" start_char="14592">million</TOKEN>
<TOKEN end_char="14599" id="token-139-18" morph="none" pos="punct" start_char="14599">)</TOKEN>
<TOKEN end_char="14604" id="token-139-19" morph="none" pos="word" start_char="14601">Hong</TOKEN>
<TOKEN end_char="14609" id="token-139-20" morph="none" pos="word" start_char="14606">Kong</TOKEN>
<TOKEN end_char="14610" id="token-139-21" morph="none" pos="punct" start_char="14610">:</TOKEN>
<TOKEN end_char="14615" id="token-139-22" morph="none" pos="word" start_char="14612">45.6</TOKEN>
<TOKEN end_char="14617" id="token-139-23" morph="none" pos="punct" start_char="14617">(</TOKEN>
<TOKEN end_char="14619" id="token-139-24" morph="none" pos="word" start_char="14618">27</TOKEN>
<TOKEN end_char="14620" id="token-139-25" morph="none" pos="punct" start_char="14620">)</TOKEN>
<TOKEN end_char="14626" id="token-139-26" morph="none" pos="word" start_char="14622">South</TOKEN>
<TOKEN end_char="14632" id="token-139-27" morph="none" pos="word" start_char="14628">Korea</TOKEN>
<TOKEN end_char="14633" id="token-139-28" morph="none" pos="punct" start_char="14633">:</TOKEN>
<TOKEN end_char="14638" id="token-139-29" morph="none" pos="word" start_char="14635">43.2</TOKEN>
<TOKEN end_char="14640" id="token-139-30" morph="none" pos="punct" start_char="14640">(</TOKEN>
<TOKEN end_char="14642" id="token-139-31" morph="none" pos="word" start_char="14641">33</TOKEN>
<TOKEN end_char="14643" id="token-139-32" morph="none" pos="punct" start_char="14643">)</TOKEN>
<TOKEN end_char="14650" id="token-139-33" morph="none" pos="word" start_char="14645">Taiwan</TOKEN>
<TOKEN end_char="14651" id="token-139-34" morph="none" pos="punct" start_char="14651">:</TOKEN>
<TOKEN end_char="14656" id="token-139-35" morph="none" pos="word" start_char="14653">42.3</TOKEN>
<TOKEN end_char="14658" id="token-139-36" morph="none" pos="punct" start_char="14658">(</TOKEN>
<TOKEN end_char="14661" id="token-139-37" morph="none" pos="unknown" start_char="14659">0.4</TOKEN>
<TOKEN end_char="14662" id="token-139-38" morph="none" pos="punct" start_char="14662">)</TOKEN>
<TOKEN end_char="14668" id="token-139-39" morph="none" pos="word" start_char="14664">Macau</TOKEN>
<TOKEN end_char="14669" id="token-139-40" morph="none" pos="punct" start_char="14669">:</TOKEN>
<TOKEN end_char="14674" id="token-139-41" morph="none" pos="word" start_char="14671">40.8</TOKEN>
<TOKEN end_char="14676" id="token-139-42" morph="none" pos="punct" start_char="14676">(</TOKEN>
<TOKEN end_char="14677" id="token-139-43" morph="none" pos="word" start_char="14677">0</TOKEN>
<TOKEN end_char="14678" id="token-139-44" morph="none" pos="punct" start_char="14678">)</TOKEN>
<TOKEN end_char="14681" id="token-139-45" morph="none" pos="word" start_char="14680">UK</TOKEN>
<TOKEN end_char="14686" id="token-139-46" morph="none" pos="word" start_char="14683">40.6</TOKEN>
<TOKEN end_char="14688" id="token-139-47" morph="none" pos="punct" start_char="14688">(</TOKEN>
<TOKEN end_char="14693" id="token-139-48" morph="none" pos="unknown" start_char="14689">1,852</TOKEN>
<TOKEN end_char="14700" id="token-139-49" morph="none" pos="word" start_char="14695">deaths</TOKEN>
<TOKEN end_char="14704" id="token-139-50" morph="none" pos="word" start_char="14702">per</TOKEN>
<TOKEN end_char="14712" id="token-139-51" morph="none" pos="word" start_char="14706">million</TOKEN>
<TOKEN end_char="14713" id="token-139-52" morph="none" pos="punct" start_char="14713">)</TOKEN>
<TOKEN end_char="14715" id="token-139-53" morph="none" pos="punct" start_char="14715">(</TOKEN>
<TOKEN end_char="14718" id="token-139-54" morph="none" pos="word" start_char="14716">DPM</TOKEN>
<TOKEN end_char="14724" id="token-139-55" morph="none" pos="unknown" start_char="14720">4,630</TOKEN>
<TOKEN end_char="14726" id="token-139-56" morph="none" pos="punct" start_char="14726">*</TOKEN>
<TOKEN end_char="14733" id="token-139-57" morph="none" pos="word" start_char="14728">Taiwan</TOKEN>
<TOKEN end_char="14734" id="token-139-58" morph="none" pos="punct" start_char="14734">,</TOKEN>
<TOKEN end_char="14737" id="token-139-59" morph="none" pos="word" start_char="14736">56</TOKEN>
<TOKEN end_char="14739" id="token-139-60" morph="none" pos="punct" start_char="14739">*</TOKEN>
<TOKEN end_char="14745" id="token-139-61" morph="none" pos="word" start_char="14741">South</TOKEN>
<TOKEN end_char="14751" id="token-139-62" morph="none" pos="word" start_char="14747">Korea</TOKEN>
<TOKEN end_char="14752" id="token-139-63" morph="none" pos="punct" start_char="14752">,</TOKEN>
<TOKEN end_char="14755" id="token-139-64" morph="none" pos="word" start_char="14754">26</TOKEN>
<TOKEN end_char="14757" id="token-139-65" morph="none" pos="punct" start_char="14757">*</TOKEN>
<TOKEN end_char="14763" id="token-139-66" morph="none" pos="word" start_char="14759">Japan</TOKEN>
<TOKEN end_char="14764" id="token-139-67" morph="none" pos="punct" start_char="14764">)</TOKEN>
<TOKEN end_char="14769" id="token-139-68" morph="none" pos="word" start_char="14766">Five</TOKEN>
<TOKEN end_char="14779" id="token-139-69" morph="none" pos="word" start_char="14771">countries</TOKEN>
<TOKEN end_char="14784" id="token-139-70" morph="none" pos="word" start_char="14781">with</TOKEN>
<TOKEN end_char="14791" id="token-139-71" morph="none" pos="word" start_char="14786">higher</TOKEN>
<TOKEN end_char="14798" id="token-139-72" morph="none" pos="word" start_char="14793">median</TOKEN>
<TOKEN end_char="14802" id="token-139-73" morph="none" pos="word" start_char="14800">age</TOKEN>
<TOKEN end_char="14807" id="token-139-74" morph="none" pos="word" start_char="14804">have</TOKEN>
<TOKEN end_char="14812" id="token-139-75" morph="none" pos="word" start_char="14809">much</TOKEN>
<TOKEN end_char="14813" id="token-139-76" morph="none" pos="punct" start_char="14813">,</TOKEN>
<TOKEN end_char="14818" id="token-139-77" morph="none" pos="word" start_char="14815">much</TOKEN>
<TOKEN end_char="14824" id="token-139-78" morph="none" pos="word" start_char="14820">lower</TOKEN>
<TOKEN end_char="14830" id="token-139-79" morph="none" pos="word" start_char="14826">death</TOKEN>
<TOKEN end_char="14834" id="token-139-80" morph="none" pos="word" start_char="14832">per</TOKEN>
<TOKEN end_char="14842" id="token-139-81" morph="none" pos="word" start_char="14836">million</TOKEN>
<TOKEN end_char="14848" id="token-139-82" morph="none" pos="word" start_char="14844">rates</TOKEN>
<TOKEN end_char="14849" id="token-139-83" morph="none" pos="punct" start_char="14849">.</TOKEN>
</SEG>
<SEG end_char="14933" id="segment-140" start_char="14851">
<ORIGINAL_TEXT>What they have in common is that they locked down earlier and/or masked up earlier.</ORIGINAL_TEXT>
<TOKEN end_char="14854" id="token-140-0" morph="none" pos="word" start_char="14851">What</TOKEN>
<TOKEN end_char="14859" id="token-140-1" morph="none" pos="word" start_char="14856">they</TOKEN>
<TOKEN end_char="14864" id="token-140-2" morph="none" pos="word" start_char="14861">have</TOKEN>
<TOKEN end_char="14867" id="token-140-3" morph="none" pos="word" start_char="14866">in</TOKEN>
<TOKEN end_char="14874" id="token-140-4" morph="none" pos="word" start_char="14869">common</TOKEN>
<TOKEN end_char="14877" id="token-140-5" morph="none" pos="word" start_char="14876">is</TOKEN>
<TOKEN end_char="14882" id="token-140-6" morph="none" pos="word" start_char="14879">that</TOKEN>
<TOKEN end_char="14887" id="token-140-7" morph="none" pos="word" start_char="14884">they</TOKEN>
<TOKEN end_char="14894" id="token-140-8" morph="none" pos="word" start_char="14889">locked</TOKEN>
<TOKEN end_char="14899" id="token-140-9" morph="none" pos="word" start_char="14896">down</TOKEN>
<TOKEN end_char="14907" id="token-140-10" morph="none" pos="word" start_char="14901">earlier</TOKEN>
<TOKEN end_char="14914" id="token-140-11" morph="none" pos="unknown" start_char="14909">and/or</TOKEN>
<TOKEN end_char="14921" id="token-140-12" morph="none" pos="word" start_char="14916">masked</TOKEN>
<TOKEN end_char="14924" id="token-140-13" morph="none" pos="word" start_char="14923">up</TOKEN>
<TOKEN end_char="14932" id="token-140-14" morph="none" pos="word" start_char="14926">earlier</TOKEN>
<TOKEN end_char="14933" id="token-140-15" morph="none" pos="punct" start_char="14933">.</TOKEN>
</SEG>
<SEG end_char="14952" id="segment-141" start_char="14935">
<ORIGINAL_TEXT>Click to expand...</ORIGINAL_TEXT>
<TOKEN end_char="14939" id="token-141-0" morph="none" pos="word" start_char="14935">Click</TOKEN>
<TOKEN end_char="14942" id="token-141-1" morph="none" pos="word" start_char="14941">to</TOKEN>
<TOKEN end_char="14949" id="token-141-2" morph="none" pos="word" start_char="14944">expand</TOKEN>
<TOKEN end_char="14952" id="token-141-3" morph="none" pos="punct" start_char="14950">...</TOKEN>
</SEG>
<SEG end_char="15110" id="segment-142" start_char="14955">
<ORIGINAL_TEXT>Yeah, I was fairly clear about the relatively rich nations bit and also having experienced SARS and MERS so wearing masks in public was already commonplace.</ORIGINAL_TEXT>
<TOKEN end_char="14958" id="token-142-0" morph="none" pos="word" start_char="14955">Yeah</TOKEN>
<TOKEN end_char="14959" id="token-142-1" morph="none" pos="punct" start_char="14959">,</TOKEN>
<TOKEN end_char="14961" id="token-142-2" morph="none" pos="word" start_char="14961">I</TOKEN>
<TOKEN end_char="14965" id="token-142-3" morph="none" pos="word" start_char="14963">was</TOKEN>
<TOKEN end_char="14972" id="token-142-4" morph="none" pos="word" start_char="14967">fairly</TOKEN>
<TOKEN end_char="14978" id="token-142-5" morph="none" pos="word" start_char="14974">clear</TOKEN>
<TOKEN end_char="14984" id="token-142-6" morph="none" pos="word" start_char="14980">about</TOKEN>
<TOKEN end_char="14988" id="token-142-7" morph="none" pos="word" start_char="14986">the</TOKEN>
<TOKEN end_char="14999" id="token-142-8" morph="none" pos="word" start_char="14990">relatively</TOKEN>
<TOKEN end_char="15004" id="token-142-9" morph="none" pos="word" start_char="15001">rich</TOKEN>
<TOKEN end_char="15012" id="token-142-10" morph="none" pos="word" start_char="15006">nations</TOKEN>
<TOKEN end_char="15016" id="token-142-11" morph="none" pos="word" start_char="15014">bit</TOKEN>
<TOKEN end_char="15020" id="token-142-12" morph="none" pos="word" start_char="15018">and</TOKEN>
<TOKEN end_char="15025" id="token-142-13" morph="none" pos="word" start_char="15022">also</TOKEN>
<TOKEN end_char="15032" id="token-142-14" morph="none" pos="word" start_char="15027">having</TOKEN>
<TOKEN end_char="15044" id="token-142-15" morph="none" pos="word" start_char="15034">experienced</TOKEN>
<TOKEN end_char="15049" id="token-142-16" morph="none" pos="word" start_char="15046">SARS</TOKEN>
<TOKEN end_char="15053" id="token-142-17" morph="none" pos="word" start_char="15051">and</TOKEN>
<TOKEN end_char="15058" id="token-142-18" morph="none" pos="word" start_char="15055">MERS</TOKEN>
<TOKEN end_char="15061" id="token-142-19" morph="none" pos="word" start_char="15060">so</TOKEN>
<TOKEN end_char="15069" id="token-142-20" morph="none" pos="word" start_char="15063">wearing</TOKEN>
<TOKEN end_char="15075" id="token-142-21" morph="none" pos="word" start_char="15071">masks</TOKEN>
<TOKEN end_char="15078" id="token-142-22" morph="none" pos="word" start_char="15077">in</TOKEN>
<TOKEN end_char="15085" id="token-142-23" morph="none" pos="word" start_char="15080">public</TOKEN>
<TOKEN end_char="15089" id="token-142-24" morph="none" pos="word" start_char="15087">was</TOKEN>
<TOKEN end_char="15097" id="token-142-25" morph="none" pos="word" start_char="15091">already</TOKEN>
<TOKEN end_char="15109" id="token-142-26" morph="none" pos="word" start_char="15099">commonplace</TOKEN>
<TOKEN end_char="15110" id="token-142-27" morph="none" pos="punct" start_char="15110">.</TOKEN>
</SEG>
<SEG end_char="15168" id="segment-143" start_char="15113">
<ORIGINAL_TEXT>For poor nations the low median age helps reduce deaths.</ORIGINAL_TEXT>
<TOKEN end_char="15115" id="token-143-0" morph="none" pos="word" start_char="15113">For</TOKEN>
<TOKEN end_char="15120" id="token-143-1" morph="none" pos="word" start_char="15117">poor</TOKEN>
<TOKEN end_char="15128" id="token-143-2" morph="none" pos="word" start_char="15122">nations</TOKEN>
<TOKEN end_char="15132" id="token-143-3" morph="none" pos="word" start_char="15130">the</TOKEN>
<TOKEN end_char="15136" id="token-143-4" morph="none" pos="word" start_char="15134">low</TOKEN>
<TOKEN end_char="15143" id="token-143-5" morph="none" pos="word" start_char="15138">median</TOKEN>
<TOKEN end_char="15147" id="token-143-6" morph="none" pos="word" start_char="15145">age</TOKEN>
<TOKEN end_char="15153" id="token-143-7" morph="none" pos="word" start_char="15149">helps</TOKEN>
<TOKEN end_char="15160" id="token-143-8" morph="none" pos="word" start_char="15155">reduce</TOKEN>
<TOKEN end_char="15167" id="token-143-9" morph="none" pos="word" start_char="15162">deaths</TOKEN>
<TOKEN end_char="15168" id="token-143-10" morph="none" pos="punct" start_char="15168">.</TOKEN>
</SEG>
<SEG end_char="15266" id="segment-144" start_char="15174">
<ORIGINAL_TEXT>Belzok said: Compliance is a massive consideration in government's response around the world.</ORIGINAL_TEXT>
<TOKEN end_char="15179" id="token-144-0" morph="none" pos="word" start_char="15174">Belzok</TOKEN>
<TOKEN end_char="15184" id="token-144-1" morph="none" pos="word" start_char="15181">said</TOKEN>
<TOKEN end_char="15185" id="token-144-2" morph="none" pos="punct" start_char="15185">:</TOKEN>
<TOKEN end_char="15196" id="token-144-3" morph="none" pos="word" start_char="15187">Compliance</TOKEN>
<TOKEN end_char="15199" id="token-144-4" morph="none" pos="word" start_char="15198">is</TOKEN>
<TOKEN end_char="15201" id="token-144-5" morph="none" pos="word" start_char="15201">a</TOKEN>
<TOKEN end_char="15209" id="token-144-6" morph="none" pos="word" start_char="15203">massive</TOKEN>
<TOKEN end_char="15223" id="token-144-7" morph="none" pos="word" start_char="15211">consideration</TOKEN>
<TOKEN end_char="15226" id="token-144-8" morph="none" pos="word" start_char="15225">in</TOKEN>
<TOKEN end_char="15239" id="token-144-9" morph="none" pos="word" start_char="15228">government's</TOKEN>
<TOKEN end_char="15248" id="token-144-10" morph="none" pos="word" start_char="15241">response</TOKEN>
<TOKEN end_char="15255" id="token-144-11" morph="none" pos="word" start_char="15250">around</TOKEN>
<TOKEN end_char="15259" id="token-144-12" morph="none" pos="word" start_char="15257">the</TOKEN>
<TOKEN end_char="15265" id="token-144-13" morph="none" pos="word" start_char="15261">world</TOKEN>
<TOKEN end_char="15266" id="token-144-14" morph="none" pos="punct" start_char="15266">.</TOKEN>
</SEG>
<SEG end_char="15401" id="segment-145" start_char="15268">
<ORIGINAL_TEXT>You can see in Germany, fairly will known to be rule followers, have done well whereas the likes of us and the US haven't faired well.</ORIGINAL_TEXT>
<TOKEN end_char="15270" id="token-145-0" morph="none" pos="word" start_char="15268">You</TOKEN>
<TOKEN end_char="15274" id="token-145-1" morph="none" pos="word" start_char="15272">can</TOKEN>
<TOKEN end_char="15278" id="token-145-2" morph="none" pos="word" start_char="15276">see</TOKEN>
<TOKEN end_char="15281" id="token-145-3" morph="none" pos="word" start_char="15280">in</TOKEN>
<TOKEN end_char="15289" id="token-145-4" morph="none" pos="word" start_char="15283">Germany</TOKEN>
<TOKEN end_char="15290" id="token-145-5" morph="none" pos="punct" start_char="15290">,</TOKEN>
<TOKEN end_char="15297" id="token-145-6" morph="none" pos="word" start_char="15292">fairly</TOKEN>
<TOKEN end_char="15302" id="token-145-7" morph="none" pos="word" start_char="15299">will</TOKEN>
<TOKEN end_char="15308" id="token-145-8" morph="none" pos="word" start_char="15304">known</TOKEN>
<TOKEN end_char="15311" id="token-145-9" morph="none" pos="word" start_char="15310">to</TOKEN>
<TOKEN end_char="15314" id="token-145-10" morph="none" pos="word" start_char="15313">be</TOKEN>
<TOKEN end_char="15319" id="token-145-11" morph="none" pos="word" start_char="15316">rule</TOKEN>
<TOKEN end_char="15329" id="token-145-12" morph="none" pos="word" start_char="15321">followers</TOKEN>
<TOKEN end_char="15330" id="token-145-13" morph="none" pos="punct" start_char="15330">,</TOKEN>
<TOKEN end_char="15335" id="token-145-14" morph="none" pos="word" start_char="15332">have</TOKEN>
<TOKEN end_char="15340" id="token-145-15" morph="none" pos="word" start_char="15337">done</TOKEN>
<TOKEN end_char="15345" id="token-145-16" morph="none" pos="word" start_char="15342">well</TOKEN>
<TOKEN end_char="15353" id="token-145-17" morph="none" pos="word" start_char="15347">whereas</TOKEN>
<TOKEN end_char="15357" id="token-145-18" morph="none" pos="word" start_char="15355">the</TOKEN>
<TOKEN end_char="15363" id="token-145-19" morph="none" pos="word" start_char="15359">likes</TOKEN>
<TOKEN end_char="15366" id="token-145-20" morph="none" pos="word" start_char="15365">of</TOKEN>
<TOKEN end_char="15369" id="token-145-21" morph="none" pos="word" start_char="15368">us</TOKEN>
<TOKEN end_char="15373" id="token-145-22" morph="none" pos="word" start_char="15371">and</TOKEN>
<TOKEN end_char="15377" id="token-145-23" morph="none" pos="word" start_char="15375">the</TOKEN>
<TOKEN end_char="15380" id="token-145-24" morph="none" pos="word" start_char="15379">US</TOKEN>
<TOKEN end_char="15388" id="token-145-25" morph="none" pos="word" start_char="15382">haven't</TOKEN>
<TOKEN end_char="15395" id="token-145-26" morph="none" pos="word" start_char="15390">faired</TOKEN>
<TOKEN end_char="15400" id="token-145-27" morph="none" pos="word" start_char="15397">well</TOKEN>
<TOKEN end_char="15401" id="token-145-28" morph="none" pos="punct" start_char="15401">.</TOKEN>
</SEG>
<SEG end_char="15610" id="segment-146" start_char="15403">
<ORIGINAL_TEXT>We've sadly not got enough jail cells to house those who flout the law so dramatically, such as the organisers of parties, weddings and so on where there have been hundreds in attendance during the lockdowns.</ORIGINAL_TEXT>
<TOKEN end_char="15407" id="token-146-0" morph="none" pos="word" start_char="15403">We've</TOKEN>
<TOKEN end_char="15413" id="token-146-1" morph="none" pos="word" start_char="15409">sadly</TOKEN>
<TOKEN end_char="15417" id="token-146-2" morph="none" pos="word" start_char="15415">not</TOKEN>
<TOKEN end_char="15421" id="token-146-3" morph="none" pos="word" start_char="15419">got</TOKEN>
<TOKEN end_char="15428" id="token-146-4" morph="none" pos="word" start_char="15423">enough</TOKEN>
<TOKEN end_char="15433" id="token-146-5" morph="none" pos="word" start_char="15430">jail</TOKEN>
<TOKEN end_char="15439" id="token-146-6" morph="none" pos="word" start_char="15435">cells</TOKEN>
<TOKEN end_char="15442" id="token-146-7" morph="none" pos="word" start_char="15441">to</TOKEN>
<TOKEN end_char="15448" id="token-146-8" morph="none" pos="word" start_char="15444">house</TOKEN>
<TOKEN end_char="15454" id="token-146-9" morph="none" pos="word" start_char="15450">those</TOKEN>
<TOKEN end_char="15458" id="token-146-10" morph="none" pos="word" start_char="15456">who</TOKEN>
<TOKEN end_char="15464" id="token-146-11" morph="none" pos="word" start_char="15460">flout</TOKEN>
<TOKEN end_char="15468" id="token-146-12" morph="none" pos="word" start_char="15466">the</TOKEN>
<TOKEN end_char="15472" id="token-146-13" morph="none" pos="word" start_char="15470">law</TOKEN>
<TOKEN end_char="15475" id="token-146-14" morph="none" pos="word" start_char="15474">so</TOKEN>
<TOKEN end_char="15488" id="token-146-15" morph="none" pos="word" start_char="15477">dramatically</TOKEN>
<TOKEN end_char="15489" id="token-146-16" morph="none" pos="punct" start_char="15489">,</TOKEN>
<TOKEN end_char="15494" id="token-146-17" morph="none" pos="word" start_char="15491">such</TOKEN>
<TOKEN end_char="15497" id="token-146-18" morph="none" pos="word" start_char="15496">as</TOKEN>
<TOKEN end_char="15501" id="token-146-19" morph="none" pos="word" start_char="15499">the</TOKEN>
<TOKEN end_char="15512" id="token-146-20" morph="none" pos="word" start_char="15503">organisers</TOKEN>
<TOKEN end_char="15515" id="token-146-21" morph="none" pos="word" start_char="15514">of</TOKEN>
<TOKEN end_char="15523" id="token-146-22" morph="none" pos="word" start_char="15517">parties</TOKEN>
<TOKEN end_char="15524" id="token-146-23" morph="none" pos="punct" start_char="15524">,</TOKEN>
<TOKEN end_char="15533" id="token-146-24" morph="none" pos="word" start_char="15526">weddings</TOKEN>
<TOKEN end_char="15537" id="token-146-25" morph="none" pos="word" start_char="15535">and</TOKEN>
<TOKEN end_char="15540" id="token-146-26" morph="none" pos="word" start_char="15539">so</TOKEN>
<TOKEN end_char="15543" id="token-146-27" morph="none" pos="word" start_char="15542">on</TOKEN>
<TOKEN end_char="15549" id="token-146-28" morph="none" pos="word" start_char="15545">where</TOKEN>
<TOKEN end_char="15555" id="token-146-29" morph="none" pos="word" start_char="15551">there</TOKEN>
<TOKEN end_char="15560" id="token-146-30" morph="none" pos="word" start_char="15557">have</TOKEN>
<TOKEN end_char="15565" id="token-146-31" morph="none" pos="word" start_char="15562">been</TOKEN>
<TOKEN end_char="15574" id="token-146-32" morph="none" pos="word" start_char="15567">hundreds</TOKEN>
<TOKEN end_char="15577" id="token-146-33" morph="none" pos="word" start_char="15576">in</TOKEN>
<TOKEN end_char="15588" id="token-146-34" morph="none" pos="word" start_char="15579">attendance</TOKEN>
<TOKEN end_char="15595" id="token-146-35" morph="none" pos="word" start_char="15590">during</TOKEN>
<TOKEN end_char="15599" id="token-146-36" morph="none" pos="word" start_char="15597">the</TOKEN>
<TOKEN end_char="15609" id="token-146-37" morph="none" pos="word" start_char="15601">lockdowns</TOKEN>
<TOKEN end_char="15610" id="token-146-38" morph="none" pos="punct" start_char="15610">.</TOKEN>
</SEG>
<SEG end_char="15688" id="segment-147" start_char="15613">
<ORIGINAL_TEXT>Germany had a good first wave, but they had a bad second and now third wave.</ORIGINAL_TEXT>
<TOKEN end_char="15619" id="token-147-0" morph="none" pos="word" start_char="15613">Germany</TOKEN>
<TOKEN end_char="15623" id="token-147-1" morph="none" pos="word" start_char="15621">had</TOKEN>
<TOKEN end_char="15625" id="token-147-2" morph="none" pos="word" start_char="15625">a</TOKEN>
<TOKEN end_char="15630" id="token-147-3" morph="none" pos="word" start_char="15627">good</TOKEN>
<TOKEN end_char="15636" id="token-147-4" morph="none" pos="word" start_char="15632">first</TOKEN>
<TOKEN end_char="15641" id="token-147-5" morph="none" pos="word" start_char="15638">wave</TOKEN>
<TOKEN end_char="15642" id="token-147-6" morph="none" pos="punct" start_char="15642">,</TOKEN>
<TOKEN end_char="15646" id="token-147-7" morph="none" pos="word" start_char="15644">but</TOKEN>
<TOKEN end_char="15651" id="token-147-8" morph="none" pos="word" start_char="15648">they</TOKEN>
<TOKEN end_char="15655" id="token-147-9" morph="none" pos="word" start_char="15653">had</TOKEN>
<TOKEN end_char="15657" id="token-147-10" morph="none" pos="word" start_char="15657">a</TOKEN>
<TOKEN end_char="15661" id="token-147-11" morph="none" pos="word" start_char="15659">bad</TOKEN>
<TOKEN end_char="15668" id="token-147-12" morph="none" pos="word" start_char="15663">second</TOKEN>
<TOKEN end_char="15672" id="token-147-13" morph="none" pos="word" start_char="15670">and</TOKEN>
<TOKEN end_char="15676" id="token-147-14" morph="none" pos="word" start_char="15674">now</TOKEN>
<TOKEN end_char="15682" id="token-147-15" morph="none" pos="word" start_char="15678">third</TOKEN>
<TOKEN end_char="15687" id="token-147-16" morph="none" pos="word" start_char="15684">wave</TOKEN>
<TOKEN end_char="15688" id="token-147-17" morph="none" pos="punct" start_char="15688">.</TOKEN>
</SEG>
<SEG end_char="15810" id="segment-148" start_char="15691">
<ORIGINAL_TEXT>They had just 10,000 deaths up to October, but they're now past 75,000 and have the fourth highest death toll in Europe.</ORIGINAL_TEXT>
<TOKEN end_char="15694" id="token-148-0" morph="none" pos="word" start_char="15691">They</TOKEN>
<TOKEN end_char="15698" id="token-148-1" morph="none" pos="word" start_char="15696">had</TOKEN>
<TOKEN end_char="15703" id="token-148-2" morph="none" pos="word" start_char="15700">just</TOKEN>
<TOKEN end_char="15710" id="token-148-3" morph="none" pos="unknown" start_char="15705">10,000</TOKEN>
<TOKEN end_char="15717" id="token-148-4" morph="none" pos="word" start_char="15712">deaths</TOKEN>
<TOKEN end_char="15720" id="token-148-5" morph="none" pos="word" start_char="15719">up</TOKEN>
<TOKEN end_char="15723" id="token-148-6" morph="none" pos="word" start_char="15722">to</TOKEN>
<TOKEN end_char="15731" id="token-148-7" morph="none" pos="word" start_char="15725">October</TOKEN>
<TOKEN end_char="15732" id="token-148-8" morph="none" pos="punct" start_char="15732">,</TOKEN>
<TOKEN end_char="15736" id="token-148-9" morph="none" pos="word" start_char="15734">but</TOKEN>
<TOKEN end_char="15744" id="token-148-10" morph="none" pos="word" start_char="15738">they're</TOKEN>
<TOKEN end_char="15748" id="token-148-11" morph="none" pos="word" start_char="15746">now</TOKEN>
<TOKEN end_char="15753" id="token-148-12" morph="none" pos="word" start_char="15750">past</TOKEN>
<TOKEN end_char="15760" id="token-148-13" morph="none" pos="unknown" start_char="15755">75,000</TOKEN>
<TOKEN end_char="15764" id="token-148-14" morph="none" pos="word" start_char="15762">and</TOKEN>
<TOKEN end_char="15769" id="token-148-15" morph="none" pos="word" start_char="15766">have</TOKEN>
<TOKEN end_char="15773" id="token-148-16" morph="none" pos="word" start_char="15771">the</TOKEN>
<TOKEN end_char="15780" id="token-148-17" morph="none" pos="word" start_char="15775">fourth</TOKEN>
<TOKEN end_char="15788" id="token-148-18" morph="none" pos="word" start_char="15782">highest</TOKEN>
<TOKEN end_char="15794" id="token-148-19" morph="none" pos="word" start_char="15790">death</TOKEN>
<TOKEN end_char="15799" id="token-148-20" morph="none" pos="word" start_char="15796">toll</TOKEN>
<TOKEN end_char="15802" id="token-148-21" morph="none" pos="word" start_char="15801">in</TOKEN>
<TOKEN end_char="15809" id="token-148-22" morph="none" pos="word" start_char="15804">Europe</TOKEN>
<TOKEN end_char="15810" id="token-148-23" morph="none" pos="punct" start_char="15810">.</TOKEN>
</SEG>
<SEG end_char="15963" id="segment-149" start_char="15816">
<ORIGINAL_TEXT>BorkenArrow said: Well if a population is not getting infected then a minority won't have anybody to catch it from....thats how herd immunity works.</ORIGINAL_TEXT>
<TOKEN end_char="15826" id="token-149-0" morph="none" pos="word" start_char="15816">BorkenArrow</TOKEN>
<TOKEN end_char="15831" id="token-149-1" morph="none" pos="word" start_char="15828">said</TOKEN>
<TOKEN end_char="15832" id="token-149-2" morph="none" pos="punct" start_char="15832">:</TOKEN>
<TOKEN end_char="15837" id="token-149-3" morph="none" pos="word" start_char="15834">Well</TOKEN>
<TOKEN end_char="15840" id="token-149-4" morph="none" pos="word" start_char="15839">if</TOKEN>
<TOKEN end_char="15842" id="token-149-5" morph="none" pos="word" start_char="15842">a</TOKEN>
<TOKEN end_char="15853" id="token-149-6" morph="none" pos="word" start_char="15844">population</TOKEN>
<TOKEN end_char="15856" id="token-149-7" morph="none" pos="word" start_char="15855">is</TOKEN>
<TOKEN end_char="15860" id="token-149-8" morph="none" pos="word" start_char="15858">not</TOKEN>
<TOKEN end_char="15868" id="token-149-9" morph="none" pos="word" start_char="15862">getting</TOKEN>
<TOKEN end_char="15877" id="token-149-10" morph="none" pos="word" start_char="15870">infected</TOKEN>
<TOKEN end_char="15882" id="token-149-11" morph="none" pos="word" start_char="15879">then</TOKEN>
<TOKEN end_char="15884" id="token-149-12" morph="none" pos="word" start_char="15884">a</TOKEN>
<TOKEN end_char="15893" id="token-149-13" morph="none" pos="word" start_char="15886">minority</TOKEN>
<TOKEN end_char="15899" id="token-149-14" morph="none" pos="word" start_char="15895">won't</TOKEN>
<TOKEN end_char="15904" id="token-149-15" morph="none" pos="word" start_char="15901">have</TOKEN>
<TOKEN end_char="15912" id="token-149-16" morph="none" pos="word" start_char="15906">anybody</TOKEN>
<TOKEN end_char="15915" id="token-149-17" morph="none" pos="word" start_char="15914">to</TOKEN>
<TOKEN end_char="15921" id="token-149-18" morph="none" pos="word" start_char="15917">catch</TOKEN>
<TOKEN end_char="15924" id="token-149-19" morph="none" pos="word" start_char="15923">it</TOKEN>
<TOKEN end_char="15938" id="token-149-20" morph="none" pos="unknown" start_char="15926">from....thats</TOKEN>
<TOKEN end_char="15942" id="token-149-21" morph="none" pos="word" start_char="15940">how</TOKEN>
<TOKEN end_char="15947" id="token-149-22" morph="none" pos="word" start_char="15944">herd</TOKEN>
<TOKEN end_char="15956" id="token-149-23" morph="none" pos="word" start_char="15949">immunity</TOKEN>
<TOKEN end_char="15962" id="token-149-24" morph="none" pos="word" start_char="15958">works</TOKEN>
<TOKEN end_char="15963" id="token-149-25" morph="none" pos="punct" start_char="15963">.</TOKEN>
</SEG>
<SEG end_char="15991" id="segment-150" start_char="15965">
<ORIGINAL_TEXT>and then we have this......</ORIGINAL_TEXT>
<TOKEN end_char="15967" id="token-150-0" morph="none" pos="word" start_char="15965">and</TOKEN>
<TOKEN end_char="15972" id="token-150-1" morph="none" pos="word" start_char="15969">then</TOKEN>
<TOKEN end_char="15975" id="token-150-2" morph="none" pos="word" start_char="15974">we</TOKEN>
<TOKEN end_char="15980" id="token-150-3" morph="none" pos="word" start_char="15977">have</TOKEN>
<TOKEN end_char="15985" id="token-150-4" morph="none" pos="word" start_char="15982">this</TOKEN>
<TOKEN end_char="15991" id="token-150-5" morph="none" pos="punct" start_char="15986">......</TOKEN>
</SEG>
<SEG end_char="16049" id="segment-151" start_char="15993">
<ORIGINAL_TEXT>So even in western countries Asians have much lower risk.</ORIGINAL_TEXT>
<TOKEN end_char="15994" id="token-151-0" morph="none" pos="word" start_char="15993">So</TOKEN>
<TOKEN end_char="15999" id="token-151-1" morph="none" pos="word" start_char="15996">even</TOKEN>
<TOKEN end_char="16002" id="token-151-2" morph="none" pos="word" start_char="16001">in</TOKEN>
<TOKEN end_char="16010" id="token-151-3" morph="none" pos="word" start_char="16004">western</TOKEN>
<TOKEN end_char="16020" id="token-151-4" morph="none" pos="word" start_char="16012">countries</TOKEN>
<TOKEN end_char="16027" id="token-151-5" morph="none" pos="word" start_char="16022">Asians</TOKEN>
<TOKEN end_char="16032" id="token-151-6" morph="none" pos="word" start_char="16029">have</TOKEN>
<TOKEN end_char="16037" id="token-151-7" morph="none" pos="word" start_char="16034">much</TOKEN>
<TOKEN end_char="16043" id="token-151-8" morph="none" pos="word" start_char="16039">lower</TOKEN>
<TOKEN end_char="16048" id="token-151-9" morph="none" pos="word" start_char="16045">risk</TOKEN>
<TOKEN end_char="16049" id="token-151-10" morph="none" pos="punct" start_char="16049">.</TOKEN>
</SEG>
<SEG end_char="16073" id="segment-152" start_char="16052">
<ORIGINAL_TEXT>That graph is the USA.</ORIGINAL_TEXT>
<TOKEN end_char="16055" id="token-152-0" morph="none" pos="word" start_char="16052">That</TOKEN>
<TOKEN end_char="16061" id="token-152-1" morph="none" pos="word" start_char="16057">graph</TOKEN>
<TOKEN end_char="16064" id="token-152-2" morph="none" pos="word" start_char="16063">is</TOKEN>
<TOKEN end_char="16068" id="token-152-3" morph="none" pos="word" start_char="16066">the</TOKEN>
<TOKEN end_char="16072" id="token-152-4" morph="none" pos="word" start_char="16070">USA</TOKEN>
<TOKEN end_char="16073" id="token-152-5" morph="none" pos="punct" start_char="16073">.</TOKEN>
</SEG>
<SEG end_char="16114" id="segment-153" start_char="16075">
<ORIGINAL_TEXT>UK stats show BAME figures to be higher.</ORIGINAL_TEXT>
<TOKEN end_char="16076" id="token-153-0" morph="none" pos="word" start_char="16075">UK</TOKEN>
<TOKEN end_char="16082" id="token-153-1" morph="none" pos="word" start_char="16078">stats</TOKEN>
<TOKEN end_char="16087" id="token-153-2" morph="none" pos="word" start_char="16084">show</TOKEN>
<TOKEN end_char="16092" id="token-153-3" morph="none" pos="word" start_char="16089">BAME</TOKEN>
<TOKEN end_char="16100" id="token-153-4" morph="none" pos="word" start_char="16094">figures</TOKEN>
<TOKEN end_char="16103" id="token-153-5" morph="none" pos="word" start_char="16102">to</TOKEN>
<TOKEN end_char="16106" id="token-153-6" morph="none" pos="word" start_char="16105">be</TOKEN>
<TOKEN end_char="16113" id="token-153-7" morph="none" pos="word" start_char="16108">higher</TOKEN>
<TOKEN end_char="16114" id="token-153-8" morph="none" pos="punct" start_char="16114">.</TOKEN>
</SEG>
<SEG end_char="16343" id="segment-154" start_char="16117">
<ORIGINAL_TEXT>Considering deaths up to 28 July 2020, males and females of Black and South Asian ethnic background were shown to have increased risks of death involving the coronavirus (COVID-19) compared with those of White ethnic background</ORIGINAL_TEXT>
<TOKEN end_char="16127" id="token-154-0" morph="none" pos="word" start_char="16117">Considering</TOKEN>
<TOKEN end_char="16134" id="token-154-1" morph="none" pos="word" start_char="16129">deaths</TOKEN>
<TOKEN end_char="16137" id="token-154-2" morph="none" pos="word" start_char="16136">up</TOKEN>
<TOKEN end_char="16140" id="token-154-3" morph="none" pos="word" start_char="16139">to</TOKEN>
<TOKEN end_char="16143" id="token-154-4" morph="none" pos="word" start_char="16142">28</TOKEN>
<TOKEN end_char="16148" id="token-154-5" morph="none" pos="word" start_char="16145">July</TOKEN>
<TOKEN end_char="16153" id="token-154-6" morph="none" pos="word" start_char="16150">2020</TOKEN>
<TOKEN end_char="16154" id="token-154-7" morph="none" pos="punct" start_char="16154">,</TOKEN>
<TOKEN end_char="16160" id="token-154-8" morph="none" pos="word" start_char="16156">males</TOKEN>
<TOKEN end_char="16164" id="token-154-9" morph="none" pos="word" start_char="16162">and</TOKEN>
<TOKEN end_char="16172" id="token-154-10" morph="none" pos="word" start_char="16166">females</TOKEN>
<TOKEN end_char="16175" id="token-154-11" morph="none" pos="word" start_char="16174">of</TOKEN>
<TOKEN end_char="16181" id="token-154-12" morph="none" pos="word" start_char="16177">Black</TOKEN>
<TOKEN end_char="16185" id="token-154-13" morph="none" pos="word" start_char="16183">and</TOKEN>
<TOKEN end_char="16191" id="token-154-14" morph="none" pos="word" start_char="16187">South</TOKEN>
<TOKEN end_char="16197" id="token-154-15" morph="none" pos="word" start_char="16193">Asian</TOKEN>
<TOKEN end_char="16204" id="token-154-16" morph="none" pos="word" start_char="16199">ethnic</TOKEN>
<TOKEN end_char="16215" id="token-154-17" morph="none" pos="word" start_char="16206">background</TOKEN>
<TOKEN end_char="16220" id="token-154-18" morph="none" pos="word" start_char="16217">were</TOKEN>
<TOKEN end_char="16226" id="token-154-19" morph="none" pos="word" start_char="16222">shown</TOKEN>
<TOKEN end_char="16229" id="token-154-20" morph="none" pos="word" start_char="16228">to</TOKEN>
<TOKEN end_char="16234" id="token-154-21" morph="none" pos="word" start_char="16231">have</TOKEN>
<TOKEN end_char="16244" id="token-154-22" morph="none" pos="word" start_char="16236">increased</TOKEN>
<TOKEN end_char="16250" id="token-154-23" morph="none" pos="word" start_char="16246">risks</TOKEN>
<TOKEN end_char="16253" id="token-154-24" morph="none" pos="word" start_char="16252">of</TOKEN>
<TOKEN end_char="16259" id="token-154-25" morph="none" pos="word" start_char="16255">death</TOKEN>
<TOKEN end_char="16269" id="token-154-26" morph="none" pos="word" start_char="16261">involving</TOKEN>
<TOKEN end_char="16273" id="token-154-27" morph="none" pos="word" start_char="16271">the</TOKEN>
<TOKEN end_char="16285" id="token-154-28" morph="none" pos="word" start_char="16275">coronavirus</TOKEN>
<TOKEN end_char="16287" id="token-154-29" morph="none" pos="punct" start_char="16287">(</TOKEN>
<TOKEN end_char="16295" id="token-154-30" morph="none" pos="unknown" start_char="16288">COVID-19</TOKEN>
<TOKEN end_char="16296" id="token-154-31" morph="none" pos="punct" start_char="16296">)</TOKEN>
<TOKEN end_char="16305" id="token-154-32" morph="none" pos="word" start_char="16298">compared</TOKEN>
<TOKEN end_char="16310" id="token-154-33" morph="none" pos="word" start_char="16307">with</TOKEN>
<TOKEN end_char="16316" id="token-154-34" morph="none" pos="word" start_char="16312">those</TOKEN>
<TOKEN end_char="16319" id="token-154-35" morph="none" pos="word" start_char="16318">of</TOKEN>
<TOKEN end_char="16325" id="token-154-36" morph="none" pos="word" start_char="16321">White</TOKEN>
<TOKEN end_char="16332" id="token-154-37" morph="none" pos="word" start_char="16327">ethnic</TOKEN>
<TOKEN end_char="16343" id="token-154-38" morph="none" pos="word" start_char="16334">background</TOKEN>
</SEG>
<SEG end_char="16469" id="segment-155" start_char="16346">
<ORIGINAL_TEXT>Updating ethnic contrasts in deaths involving the coronavirus (COVID-19), England and Wales - Office for National Statistics</ORIGINAL_TEXT>
<TOKEN end_char="16353" id="token-155-0" morph="none" pos="word" start_char="16346">Updating</TOKEN>
<TOKEN end_char="16360" id="token-155-1" morph="none" pos="word" start_char="16355">ethnic</TOKEN>
<TOKEN end_char="16370" id="token-155-2" morph="none" pos="word" start_char="16362">contrasts</TOKEN>
<TOKEN end_char="16373" id="token-155-3" morph="none" pos="word" start_char="16372">in</TOKEN>
<TOKEN end_char="16380" id="token-155-4" morph="none" pos="word" start_char="16375">deaths</TOKEN>
<TOKEN end_char="16390" id="token-155-5" morph="none" pos="word" start_char="16382">involving</TOKEN>
<TOKEN end_char="16394" id="token-155-6" morph="none" pos="word" start_char="16392">the</TOKEN>
<TOKEN end_char="16406" id="token-155-7" morph="none" pos="word" start_char="16396">coronavirus</TOKEN>
<TOKEN end_char="16408" id="token-155-8" morph="none" pos="punct" start_char="16408">(</TOKEN>
<TOKEN end_char="16416" id="token-155-9" morph="none" pos="unknown" start_char="16409">COVID-19</TOKEN>
<TOKEN end_char="16418" id="token-155-10" morph="none" pos="punct" start_char="16417">),</TOKEN>
<TOKEN end_char="16426" id="token-155-11" morph="none" pos="word" start_char="16420">England</TOKEN>
<TOKEN end_char="16430" id="token-155-12" morph="none" pos="word" start_char="16428">and</TOKEN>
<TOKEN end_char="16436" id="token-155-13" morph="none" pos="word" start_char="16432">Wales</TOKEN>
<TOKEN end_char="16438" id="token-155-14" morph="none" pos="punct" start_char="16438">-</TOKEN>
<TOKEN end_char="16445" id="token-155-15" morph="none" pos="word" start_char="16440">Office</TOKEN>
<TOKEN end_char="16449" id="token-155-16" morph="none" pos="word" start_char="16447">for</TOKEN>
<TOKEN end_char="16458" id="token-155-17" morph="none" pos="word" start_char="16451">National</TOKEN>
<TOKEN end_char="16469" id="token-155-18" morph="none" pos="word" start_char="16460">Statistics</TOKEN>
</SEG>
<SEG end_char="16565" id="segment-156" start_char="16475">
<ORIGINAL_TEXT>derek500 said: Germany had a good first wave, but they had a bad second and now third wave.</ORIGINAL_TEXT>
<TOKEN end_char="16482" id="token-156-0" morph="none" pos="word" start_char="16475">derek500</TOKEN>
<TOKEN end_char="16487" id="token-156-1" morph="none" pos="word" start_char="16484">said</TOKEN>
<TOKEN end_char="16488" id="token-156-2" morph="none" pos="punct" start_char="16488">:</TOKEN>
<TOKEN end_char="16496" id="token-156-3" morph="none" pos="word" start_char="16490">Germany</TOKEN>
<TOKEN end_char="16500" id="token-156-4" morph="none" pos="word" start_char="16498">had</TOKEN>
<TOKEN end_char="16502" id="token-156-5" morph="none" pos="word" start_char="16502">a</TOKEN>
<TOKEN end_char="16507" id="token-156-6" morph="none" pos="word" start_char="16504">good</TOKEN>
<TOKEN end_char="16513" id="token-156-7" morph="none" pos="word" start_char="16509">first</TOKEN>
<TOKEN end_char="16518" id="token-156-8" morph="none" pos="word" start_char="16515">wave</TOKEN>
<TOKEN end_char="16519" id="token-156-9" morph="none" pos="punct" start_char="16519">,</TOKEN>
<TOKEN end_char="16523" id="token-156-10" morph="none" pos="word" start_char="16521">but</TOKEN>
<TOKEN end_char="16528" id="token-156-11" morph="none" pos="word" start_char="16525">they</TOKEN>
<TOKEN end_char="16532" id="token-156-12" morph="none" pos="word" start_char="16530">had</TOKEN>
<TOKEN end_char="16534" id="token-156-13" morph="none" pos="word" start_char="16534">a</TOKEN>
<TOKEN end_char="16538" id="token-156-14" morph="none" pos="word" start_char="16536">bad</TOKEN>
<TOKEN end_char="16545" id="token-156-15" morph="none" pos="word" start_char="16540">second</TOKEN>
<TOKEN end_char="16549" id="token-156-16" morph="none" pos="word" start_char="16547">and</TOKEN>
<TOKEN end_char="16553" id="token-156-17" morph="none" pos="word" start_char="16551">now</TOKEN>
<TOKEN end_char="16559" id="token-156-18" morph="none" pos="word" start_char="16555">third</TOKEN>
<TOKEN end_char="16564" id="token-156-19" morph="none" pos="word" start_char="16561">wave</TOKEN>
<TOKEN end_char="16565" id="token-156-20" morph="none" pos="punct" start_char="16565">.</TOKEN>
</SEG>
<SEG end_char="16686" id="segment-157" start_char="16567">
<ORIGINAL_TEXT>They had just 10,000 deaths up to October, but they're now past 75,000 and have the fourth highest death toll in Europe.</ORIGINAL_TEXT>
<TOKEN end_char="16570" id="token-157-0" morph="none" pos="word" start_char="16567">They</TOKEN>
<TOKEN end_char="16574" id="token-157-1" morph="none" pos="word" start_char="16572">had</TOKEN>
<TOKEN end_char="16579" id="token-157-2" morph="none" pos="word" start_char="16576">just</TOKEN>
<TOKEN end_char="16586" id="token-157-3" morph="none" pos="unknown" start_char="16581">10,000</TOKEN>
<TOKEN end_char="16593" id="token-157-4" morph="none" pos="word" start_char="16588">deaths</TOKEN>
<TOKEN end_char="16596" id="token-157-5" morph="none" pos="word" start_char="16595">up</TOKEN>
<TOKEN end_char="16599" id="token-157-6" morph="none" pos="word" start_char="16598">to</TOKEN>
<TOKEN end_char="16607" id="token-157-7" morph="none" pos="word" start_char="16601">October</TOKEN>
<TOKEN end_char="16608" id="token-157-8" morph="none" pos="punct" start_char="16608">,</TOKEN>
<TOKEN end_char="16612" id="token-157-9" morph="none" pos="word" start_char="16610">but</TOKEN>
<TOKEN end_char="16620" id="token-157-10" morph="none" pos="word" start_char="16614">they're</TOKEN>
<TOKEN end_char="16624" id="token-157-11" morph="none" pos="word" start_char="16622">now</TOKEN>
<TOKEN end_char="16629" id="token-157-12" morph="none" pos="word" start_char="16626">past</TOKEN>
<TOKEN end_char="16636" id="token-157-13" morph="none" pos="unknown" start_char="16631">75,000</TOKEN>
<TOKEN end_char="16640" id="token-157-14" morph="none" pos="word" start_char="16638">and</TOKEN>
<TOKEN end_char="16645" id="token-157-15" morph="none" pos="word" start_char="16642">have</TOKEN>
<TOKEN end_char="16649" id="token-157-16" morph="none" pos="word" start_char="16647">the</TOKEN>
<TOKEN end_char="16656" id="token-157-17" morph="none" pos="word" start_char="16651">fourth</TOKEN>
<TOKEN end_char="16664" id="token-157-18" morph="none" pos="word" start_char="16658">highest</TOKEN>
<TOKEN end_char="16670" id="token-157-19" morph="none" pos="word" start_char="16666">death</TOKEN>
<TOKEN end_char="16675" id="token-157-20" morph="none" pos="word" start_char="16672">toll</TOKEN>
<TOKEN end_char="16678" id="token-157-21" morph="none" pos="word" start_char="16677">in</TOKEN>
<TOKEN end_char="16685" id="token-157-22" morph="none" pos="word" start_char="16680">Europe</TOKEN>
<TOKEN end_char="16686" id="token-157-23" morph="none" pos="punct" start_char="16686">.</TOKEN>
</SEG>
<SEG end_char="16792" id="segment-158" start_char="16689">
<ORIGINAL_TEXT>They are definitely being hit hard and now not benefitting from the most at risk having been vaccinated.</ORIGINAL_TEXT>
<TOKEN end_char="16692" id="token-158-0" morph="none" pos="word" start_char="16689">They</TOKEN>
<TOKEN end_char="16696" id="token-158-1" morph="none" pos="word" start_char="16694">are</TOKEN>
<TOKEN end_char="16707" id="token-158-2" morph="none" pos="word" start_char="16698">definitely</TOKEN>
<TOKEN end_char="16713" id="token-158-3" morph="none" pos="word" start_char="16709">being</TOKEN>
<TOKEN end_char="16717" id="token-158-4" morph="none" pos="word" start_char="16715">hit</TOKEN>
<TOKEN end_char="16722" id="token-158-5" morph="none" pos="word" start_char="16719">hard</TOKEN>
<TOKEN end_char="16726" id="token-158-6" morph="none" pos="word" start_char="16724">and</TOKEN>
<TOKEN end_char="16730" id="token-158-7" morph="none" pos="word" start_char="16728">now</TOKEN>
<TOKEN end_char="16734" id="token-158-8" morph="none" pos="word" start_char="16732">not</TOKEN>
<TOKEN end_char="16746" id="token-158-9" morph="none" pos="word" start_char="16736">benefitting</TOKEN>
<TOKEN end_char="16751" id="token-158-10" morph="none" pos="word" start_char="16748">from</TOKEN>
<TOKEN end_char="16755" id="token-158-11" morph="none" pos="word" start_char="16753">the</TOKEN>
<TOKEN end_char="16760" id="token-158-12" morph="none" pos="word" start_char="16757">most</TOKEN>
<TOKEN end_char="16763" id="token-158-13" morph="none" pos="word" start_char="16762">at</TOKEN>
<TOKEN end_char="16768" id="token-158-14" morph="none" pos="word" start_char="16765">risk</TOKEN>
<TOKEN end_char="16775" id="token-158-15" morph="none" pos="word" start_char="16770">having</TOKEN>
<TOKEN end_char="16780" id="token-158-16" morph="none" pos="word" start_char="16777">been</TOKEN>
<TOKEN end_char="16791" id="token-158-17" morph="none" pos="word" start_char="16782">vaccinated</TOKEN>
<TOKEN end_char="16792" id="token-158-18" morph="none" pos="punct" start_char="16792">.</TOKEN>
</SEG>
<SEG end_char="16924" id="segment-159" start_char="16795">
<ORIGINAL_TEXT>Not seen any news about how the Germans are dealing with the new lockdowns, they could be getting fed up and complacency kicks in.</ORIGINAL_TEXT>
<TOKEN end_char="16797" id="token-159-0" morph="none" pos="word" start_char="16795">Not</TOKEN>
<TOKEN end_char="16802" id="token-159-1" morph="none" pos="word" start_char="16799">seen</TOKEN>
<TOKEN end_char="16806" id="token-159-2" morph="none" pos="word" start_char="16804">any</TOKEN>
<TOKEN end_char="16811" id="token-159-3" morph="none" pos="word" start_char="16808">news</TOKEN>
<TOKEN end_char="16817" id="token-159-4" morph="none" pos="word" start_char="16813">about</TOKEN>
<TOKEN end_char="16821" id="token-159-5" morph="none" pos="word" start_char="16819">how</TOKEN>
<TOKEN end_char="16825" id="token-159-6" morph="none" pos="word" start_char="16823">the</TOKEN>
<TOKEN end_char="16833" id="token-159-7" morph="none" pos="word" start_char="16827">Germans</TOKEN>
<TOKEN end_char="16837" id="token-159-8" morph="none" pos="word" start_char="16835">are</TOKEN>
<TOKEN end_char="16845" id="token-159-9" morph="none" pos="word" start_char="16839">dealing</TOKEN>
<TOKEN end_char="16850" id="token-159-10" morph="none" pos="word" start_char="16847">with</TOKEN>
<TOKEN end_char="16854" id="token-159-11" morph="none" pos="word" start_char="16852">the</TOKEN>
<TOKEN end_char="16858" id="token-159-12" morph="none" pos="word" start_char="16856">new</TOKEN>
<TOKEN end_char="16868" id="token-159-13" morph="none" pos="word" start_char="16860">lockdowns</TOKEN>
<TOKEN end_char="16869" id="token-159-14" morph="none" pos="punct" start_char="16869">,</TOKEN>
<TOKEN end_char="16874" id="token-159-15" morph="none" pos="word" start_char="16871">they</TOKEN>
<TOKEN end_char="16880" id="token-159-16" morph="none" pos="word" start_char="16876">could</TOKEN>
<TOKEN end_char="16883" id="token-159-17" morph="none" pos="word" start_char="16882">be</TOKEN>
<TOKEN end_char="16891" id="token-159-18" morph="none" pos="word" start_char="16885">getting</TOKEN>
<TOKEN end_char="16895" id="token-159-19" morph="none" pos="word" start_char="16893">fed</TOKEN>
<TOKEN end_char="16898" id="token-159-20" morph="none" pos="word" start_char="16897">up</TOKEN>
<TOKEN end_char="16902" id="token-159-21" morph="none" pos="word" start_char="16900">and</TOKEN>
<TOKEN end_char="16914" id="token-159-22" morph="none" pos="word" start_char="16904">complacency</TOKEN>
<TOKEN end_char="16920" id="token-159-23" morph="none" pos="word" start_char="16916">kicks</TOKEN>
<TOKEN end_char="16923" id="token-159-24" morph="none" pos="word" start_char="16922">in</TOKEN>
<TOKEN end_char="16924" id="token-159-25" morph="none" pos="punct" start_char="16924">.</TOKEN>
</SEG>
<SEG end_char="17032" id="segment-160" start_char="16927">
<ORIGINAL_TEXT>Sadly for the rest of Europe I suspect the UK is going to move down the deaths per capita table in Europe.</ORIGINAL_TEXT>
<TOKEN end_char="16931" id="token-160-0" morph="none" pos="word" start_char="16927">Sadly</TOKEN>
<TOKEN end_char="16935" id="token-160-1" morph="none" pos="word" start_char="16933">for</TOKEN>
<TOKEN end_char="16939" id="token-160-2" morph="none" pos="word" start_char="16937">the</TOKEN>
<TOKEN end_char="16944" id="token-160-3" morph="none" pos="word" start_char="16941">rest</TOKEN>
<TOKEN end_char="16947" id="token-160-4" morph="none" pos="word" start_char="16946">of</TOKEN>
<TOKEN end_char="16954" id="token-160-5" morph="none" pos="word" start_char="16949">Europe</TOKEN>
<TOKEN end_char="16956" id="token-160-6" morph="none" pos="word" start_char="16956">I</TOKEN>
<TOKEN end_char="16964" id="token-160-7" morph="none" pos="word" start_char="16958">suspect</TOKEN>
<TOKEN end_char="16968" id="token-160-8" morph="none" pos="word" start_char="16966">the</TOKEN>
<TOKEN end_char="16971" id="token-160-9" morph="none" pos="word" start_char="16970">UK</TOKEN>
<TOKEN end_char="16974" id="token-160-10" morph="none" pos="word" start_char="16973">is</TOKEN>
<TOKEN end_char="16980" id="token-160-11" morph="none" pos="word" start_char="16976">going</TOKEN>
<TOKEN end_char="16983" id="token-160-12" morph="none" pos="word" start_char="16982">to</TOKEN>
<TOKEN end_char="16988" id="token-160-13" morph="none" pos="word" start_char="16985">move</TOKEN>
<TOKEN end_char="16993" id="token-160-14" morph="none" pos="word" start_char="16990">down</TOKEN>
<TOKEN end_char="16997" id="token-160-15" morph="none" pos="word" start_char="16995">the</TOKEN>
<TOKEN end_char="17004" id="token-160-16" morph="none" pos="word" start_char="16999">deaths</TOKEN>
<TOKEN end_char="17008" id="token-160-17" morph="none" pos="word" start_char="17006">per</TOKEN>
<TOKEN end_char="17015" id="token-160-18" morph="none" pos="word" start_char="17010">capita</TOKEN>
<TOKEN end_char="17021" id="token-160-19" morph="none" pos="word" start_char="17017">table</TOKEN>
<TOKEN end_char="17024" id="token-160-20" morph="none" pos="word" start_char="17023">in</TOKEN>
<TOKEN end_char="17031" id="token-160-21" morph="none" pos="word" start_char="17026">Europe</TOKEN>
<TOKEN end_char="17032" id="token-160-22" morph="none" pos="punct" start_char="17032">.</TOKEN>
</SEG>
<SEG end_char="17078" id="segment-161" start_char="17038">
<ORIGINAL_TEXT>goingoingong said: That graph is the USA.</ORIGINAL_TEXT>
<TOKEN end_char="17049" id="token-161-0" morph="none" pos="word" start_char="17038">goingoingong</TOKEN>
<TOKEN end_char="17054" id="token-161-1" morph="none" pos="word" start_char="17051">said</TOKEN>
<TOKEN end_char="17055" id="token-161-2" morph="none" pos="punct" start_char="17055">:</TOKEN>
<TOKEN end_char="17060" id="token-161-3" morph="none" pos="word" start_char="17057">That</TOKEN>
<TOKEN end_char="17066" id="token-161-4" morph="none" pos="word" start_char="17062">graph</TOKEN>
<TOKEN end_char="17069" id="token-161-5" morph="none" pos="word" start_char="17068">is</TOKEN>
<TOKEN end_char="17073" id="token-161-6" morph="none" pos="word" start_char="17071">the</TOKEN>
<TOKEN end_char="17077" id="token-161-7" morph="none" pos="word" start_char="17075">USA</TOKEN>
<TOKEN end_char="17078" id="token-161-8" morph="none" pos="punct" start_char="17078">.</TOKEN>
</SEG>
<SEG end_char="17119" id="segment-162" start_char="17080">
<ORIGINAL_TEXT>UK stats show BAME figures to be higher.</ORIGINAL_TEXT>
<TOKEN end_char="17081" id="token-162-0" morph="none" pos="word" start_char="17080">UK</TOKEN>
<TOKEN end_char="17087" id="token-162-1" morph="none" pos="word" start_char="17083">stats</TOKEN>
<TOKEN end_char="17092" id="token-162-2" morph="none" pos="word" start_char="17089">show</TOKEN>
<TOKEN end_char="17097" id="token-162-3" morph="none" pos="word" start_char="17094">BAME</TOKEN>
<TOKEN end_char="17105" id="token-162-4" morph="none" pos="word" start_char="17099">figures</TOKEN>
<TOKEN end_char="17108" id="token-162-5" morph="none" pos="word" start_char="17107">to</TOKEN>
<TOKEN end_char="17111" id="token-162-6" morph="none" pos="word" start_char="17110">be</TOKEN>
<TOKEN end_char="17118" id="token-162-7" morph="none" pos="word" start_char="17113">higher</TOKEN>
<TOKEN end_char="17119" id="token-162-8" morph="none" pos="punct" start_char="17119">.</TOKEN>
</SEG>
<SEG end_char="17348" id="segment-163" start_char="17122">
<ORIGINAL_TEXT>Considering deaths up to 28 July 2020, males and females of Black and South Asian ethnic background were shown to have increased risks of death involving the coronavirus (COVID-19) compared with those of White ethnic background</ORIGINAL_TEXT>
<TOKEN end_char="17132" id="token-163-0" morph="none" pos="word" start_char="17122">Considering</TOKEN>
<TOKEN end_char="17139" id="token-163-1" morph="none" pos="word" start_char="17134">deaths</TOKEN>
<TOKEN end_char="17142" id="token-163-2" morph="none" pos="word" start_char="17141">up</TOKEN>
<TOKEN end_char="17145" id="token-163-3" morph="none" pos="word" start_char="17144">to</TOKEN>
<TOKEN end_char="17148" id="token-163-4" morph="none" pos="word" start_char="17147">28</TOKEN>
<TOKEN end_char="17153" id="token-163-5" morph="none" pos="word" start_char="17150">July</TOKEN>
<TOKEN end_char="17158" id="token-163-6" morph="none" pos="word" start_char="17155">2020</TOKEN>
<TOKEN end_char="17159" id="token-163-7" morph="none" pos="punct" start_char="17159">,</TOKEN>
<TOKEN end_char="17165" id="token-163-8" morph="none" pos="word" start_char="17161">males</TOKEN>
<TOKEN end_char="17169" id="token-163-9" morph="none" pos="word" start_char="17167">and</TOKEN>
<TOKEN end_char="17177" id="token-163-10" morph="none" pos="word" start_char="17171">females</TOKEN>
<TOKEN end_char="17180" id="token-163-11" morph="none" pos="word" start_char="17179">of</TOKEN>
<TOKEN end_char="17186" id="token-163-12" morph="none" pos="word" start_char="17182">Black</TOKEN>
<TOKEN end_char="17190" id="token-163-13" morph="none" pos="word" start_char="17188">and</TOKEN>
<TOKEN end_char="17196" id="token-163-14" morph="none" pos="word" start_char="17192">South</TOKEN>
<TOKEN end_char="17202" id="token-163-15" morph="none" pos="word" start_char="17198">Asian</TOKEN>
<TOKEN end_char="17209" id="token-163-16" morph="none" pos="word" start_char="17204">ethnic</TOKEN>
<TOKEN end_char="17220" id="token-163-17" morph="none" pos="word" start_char="17211">background</TOKEN>
<TOKEN end_char="17225" id="token-163-18" morph="none" pos="word" start_char="17222">were</TOKEN>
<TOKEN end_char="17231" id="token-163-19" morph="none" pos="word" start_char="17227">shown</TOKEN>
<TOKEN end_char="17234" id="token-163-20" morph="none" pos="word" start_char="17233">to</TOKEN>
<TOKEN end_char="17239" id="token-163-21" morph="none" pos="word" start_char="17236">have</TOKEN>
<TOKEN end_char="17249" id="token-163-22" morph="none" pos="word" start_char="17241">increased</TOKEN>
<TOKEN end_char="17255" id="token-163-23" morph="none" pos="word" start_char="17251">risks</TOKEN>
<TOKEN end_char="17258" id="token-163-24" morph="none" pos="word" start_char="17257">of</TOKEN>
<TOKEN end_char="17264" id="token-163-25" morph="none" pos="word" start_char="17260">death</TOKEN>
<TOKEN end_char="17274" id="token-163-26" morph="none" pos="word" start_char="17266">involving</TOKEN>
<TOKEN end_char="17278" id="token-163-27" morph="none" pos="word" start_char="17276">the</TOKEN>
<TOKEN end_char="17290" id="token-163-28" morph="none" pos="word" start_char="17280">coronavirus</TOKEN>
<TOKEN end_char="17292" id="token-163-29" morph="none" pos="punct" start_char="17292">(</TOKEN>
<TOKEN end_char="17300" id="token-163-30" morph="none" pos="unknown" start_char="17293">COVID-19</TOKEN>
<TOKEN end_char="17301" id="token-163-31" morph="none" pos="punct" start_char="17301">)</TOKEN>
<TOKEN end_char="17310" id="token-163-32" morph="none" pos="word" start_char="17303">compared</TOKEN>
<TOKEN end_char="17315" id="token-163-33" morph="none" pos="word" start_char="17312">with</TOKEN>
<TOKEN end_char="17321" id="token-163-34" morph="none" pos="word" start_char="17317">those</TOKEN>
<TOKEN end_char="17324" id="token-163-35" morph="none" pos="word" start_char="17323">of</TOKEN>
<TOKEN end_char="17330" id="token-163-36" morph="none" pos="word" start_char="17326">White</TOKEN>
<TOKEN end_char="17337" id="token-163-37" morph="none" pos="word" start_char="17332">ethnic</TOKEN>
<TOKEN end_char="17348" id="token-163-38" morph="none" pos="word" start_char="17339">background</TOKEN>
</SEG>
<SEG end_char="17474" id="segment-164" start_char="17351">
<ORIGINAL_TEXT>Updating ethnic contrasts in deaths involving the coronavirus (COVID-19), England and Wales - Office for National Statistics</ORIGINAL_TEXT>
<TOKEN end_char="17358" id="token-164-0" morph="none" pos="word" start_char="17351">Updating</TOKEN>
<TOKEN end_char="17365" id="token-164-1" morph="none" pos="word" start_char="17360">ethnic</TOKEN>
<TOKEN end_char="17375" id="token-164-2" morph="none" pos="word" start_char="17367">contrasts</TOKEN>
<TOKEN end_char="17378" id="token-164-3" morph="none" pos="word" start_char="17377">in</TOKEN>
<TOKEN end_char="17385" id="token-164-4" morph="none" pos="word" start_char="17380">deaths</TOKEN>
<TOKEN end_char="17395" id="token-164-5" morph="none" pos="word" start_char="17387">involving</TOKEN>
<TOKEN end_char="17399" id="token-164-6" morph="none" pos="word" start_char="17397">the</TOKEN>
<TOKEN end_char="17411" id="token-164-7" morph="none" pos="word" start_char="17401">coronavirus</TOKEN>
<TOKEN end_char="17413" id="token-164-8" morph="none" pos="punct" start_char="17413">(</TOKEN>
<TOKEN end_char="17421" id="token-164-9" morph="none" pos="unknown" start_char="17414">COVID-19</TOKEN>
<TOKEN end_char="17423" id="token-164-10" morph="none" pos="punct" start_char="17422">),</TOKEN>
<TOKEN end_char="17431" id="token-164-11" morph="none" pos="word" start_char="17425">England</TOKEN>
<TOKEN end_char="17435" id="token-164-12" morph="none" pos="word" start_char="17433">and</TOKEN>
<TOKEN end_char="17441" id="token-164-13" morph="none" pos="word" start_char="17437">Wales</TOKEN>
<TOKEN end_char="17443" id="token-164-14" morph="none" pos="punct" start_char="17443">-</TOKEN>
<TOKEN end_char="17450" id="token-164-15" morph="none" pos="word" start_char="17445">Office</TOKEN>
<TOKEN end_char="17454" id="token-164-16" morph="none" pos="word" start_char="17452">for</TOKEN>
<TOKEN end_char="17463" id="token-164-17" morph="none" pos="word" start_char="17456">National</TOKEN>
<TOKEN end_char="17474" id="token-164-18" morph="none" pos="word" start_char="17465">Statistics</TOKEN>
</SEG>
<SEG end_char="17594" id="segment-165" start_char="17477">
<ORIGINAL_TEXT>Yes if you add the Black and other susceptible minorities to the asian figures, you will get a higher number for sure.</ORIGINAL_TEXT>
<TOKEN end_char="17479" id="token-165-0" morph="none" pos="word" start_char="17477">Yes</TOKEN>
<TOKEN end_char="17482" id="token-165-1" morph="none" pos="word" start_char="17481">if</TOKEN>
<TOKEN end_char="17486" id="token-165-2" morph="none" pos="word" start_char="17484">you</TOKEN>
<TOKEN end_char="17490" id="token-165-3" morph="none" pos="word" start_char="17488">add</TOKEN>
<TOKEN end_char="17494" id="token-165-4" morph="none" pos="word" start_char="17492">the</TOKEN>
<TOKEN end_char="17500" id="token-165-5" morph="none" pos="word" start_char="17496">Black</TOKEN>
<TOKEN end_char="17504" id="token-165-6" morph="none" pos="word" start_char="17502">and</TOKEN>
<TOKEN end_char="17510" id="token-165-7" morph="none" pos="word" start_char="17506">other</TOKEN>
<TOKEN end_char="17522" id="token-165-8" morph="none" pos="word" start_char="17512">susceptible</TOKEN>
<TOKEN end_char="17533" id="token-165-9" morph="none" pos="word" start_char="17524">minorities</TOKEN>
<TOKEN end_char="17536" id="token-165-10" morph="none" pos="word" start_char="17535">to</TOKEN>
<TOKEN end_char="17540" id="token-165-11" morph="none" pos="word" start_char="17538">the</TOKEN>
<TOKEN end_char="17546" id="token-165-12" morph="none" pos="word" start_char="17542">asian</TOKEN>
<TOKEN end_char="17554" id="token-165-13" morph="none" pos="word" start_char="17548">figures</TOKEN>
<TOKEN end_char="17555" id="token-165-14" morph="none" pos="punct" start_char="17555">,</TOKEN>
<TOKEN end_char="17559" id="token-165-15" morph="none" pos="word" start_char="17557">you</TOKEN>
<TOKEN end_char="17564" id="token-165-16" morph="none" pos="word" start_char="17561">will</TOKEN>
<TOKEN end_char="17568" id="token-165-17" morph="none" pos="word" start_char="17566">get</TOKEN>
<TOKEN end_char="17570" id="token-165-18" morph="none" pos="word" start_char="17570">a</TOKEN>
<TOKEN end_char="17577" id="token-165-19" morph="none" pos="word" start_char="17572">higher</TOKEN>
<TOKEN end_char="17584" id="token-165-20" morph="none" pos="word" start_char="17579">number</TOKEN>
<TOKEN end_char="17588" id="token-165-21" morph="none" pos="word" start_char="17586">for</TOKEN>
<TOKEN end_char="17593" id="token-165-22" morph="none" pos="word" start_char="17590">sure</TOKEN>
<TOKEN end_char="17594" id="token-165-23" morph="none" pos="punct" start_char="17594">.</TOKEN>
</SEG>
<SEG end_char="17734" id="segment-166" start_char="17599">
<ORIGINAL_TEXT>BorkenArrow said: Yes if you add the Black and other susceptible minorities to the asian figures, you will get a higher number for sure.</ORIGINAL_TEXT>
<TOKEN end_char="17609" id="token-166-0" morph="none" pos="word" start_char="17599">BorkenArrow</TOKEN>
<TOKEN end_char="17614" id="token-166-1" morph="none" pos="word" start_char="17611">said</TOKEN>
<TOKEN end_char="17615" id="token-166-2" morph="none" pos="punct" start_char="17615">:</TOKEN>
<TOKEN end_char="17619" id="token-166-3" morph="none" pos="word" start_char="17617">Yes</TOKEN>
<TOKEN end_char="17622" id="token-166-4" morph="none" pos="word" start_char="17621">if</TOKEN>
<TOKEN end_char="17626" id="token-166-5" morph="none" pos="word" start_char="17624">you</TOKEN>
<TOKEN end_char="17630" id="token-166-6" morph="none" pos="word" start_char="17628">add</TOKEN>
<TOKEN end_char="17634" id="token-166-7" morph="none" pos="word" start_char="17632">the</TOKEN>
<TOKEN end_char="17640" id="token-166-8" morph="none" pos="word" start_char="17636">Black</TOKEN>
<TOKEN end_char="17644" id="token-166-9" morph="none" pos="word" start_char="17642">and</TOKEN>
<TOKEN end_char="17650" id="token-166-10" morph="none" pos="word" start_char="17646">other</TOKEN>
<TOKEN end_char="17662" id="token-166-11" morph="none" pos="word" start_char="17652">susceptible</TOKEN>
<TOKEN end_char="17673" id="token-166-12" morph="none" pos="word" start_char="17664">minorities</TOKEN>
<TOKEN end_char="17676" id="token-166-13" morph="none" pos="word" start_char="17675">to</TOKEN>
<TOKEN end_char="17680" id="token-166-14" morph="none" pos="word" start_char="17678">the</TOKEN>
<TOKEN end_char="17686" id="token-166-15" morph="none" pos="word" start_char="17682">asian</TOKEN>
<TOKEN end_char="17694" id="token-166-16" morph="none" pos="word" start_char="17688">figures</TOKEN>
<TOKEN end_char="17695" id="token-166-17" morph="none" pos="punct" start_char="17695">,</TOKEN>
<TOKEN end_char="17699" id="token-166-18" morph="none" pos="word" start_char="17697">you</TOKEN>
<TOKEN end_char="17704" id="token-166-19" morph="none" pos="word" start_char="17701">will</TOKEN>
<TOKEN end_char="17708" id="token-166-20" morph="none" pos="word" start_char="17706">get</TOKEN>
<TOKEN end_char="17710" id="token-166-21" morph="none" pos="word" start_char="17710">a</TOKEN>
<TOKEN end_char="17717" id="token-166-22" morph="none" pos="word" start_char="17712">higher</TOKEN>
<TOKEN end_char="17724" id="token-166-23" morph="none" pos="word" start_char="17719">number</TOKEN>
<TOKEN end_char="17728" id="token-166-24" morph="none" pos="word" start_char="17726">for</TOKEN>
<TOKEN end_char="17733" id="token-166-25" morph="none" pos="word" start_char="17730">sure</TOKEN>
<TOKEN end_char="17734" id="token-166-26" morph="none" pos="punct" start_char="17734">.</TOKEN>
</SEG>
<SEG end_char="17813" id="segment-167" start_char="17738">
<ORIGINAL_TEXT>Try looking at the graphic for figure 1 which show by seperate ethnic group.</ORIGINAL_TEXT>
<TOKEN end_char="17740" id="token-167-0" morph="none" pos="word" start_char="17738">Try</TOKEN>
<TOKEN end_char="17748" id="token-167-1" morph="none" pos="word" start_char="17742">looking</TOKEN>
<TOKEN end_char="17751" id="token-167-2" morph="none" pos="word" start_char="17750">at</TOKEN>
<TOKEN end_char="17755" id="token-167-3" morph="none" pos="word" start_char="17753">the</TOKEN>
<TOKEN end_char="17763" id="token-167-4" morph="none" pos="word" start_char="17757">graphic</TOKEN>
<TOKEN end_char="17767" id="token-167-5" morph="none" pos="word" start_char="17765">for</TOKEN>
<TOKEN end_char="17774" id="token-167-6" morph="none" pos="word" start_char="17769">figure</TOKEN>
<TOKEN end_char="17776" id="token-167-7" morph="none" pos="word" start_char="17776">1</TOKEN>
<TOKEN end_char="17782" id="token-167-8" morph="none" pos="word" start_char="17778">which</TOKEN>
<TOKEN end_char="17787" id="token-167-9" morph="none" pos="word" start_char="17784">show</TOKEN>
<TOKEN end_char="17790" id="token-167-10" morph="none" pos="word" start_char="17789">by</TOKEN>
<TOKEN end_char="17799" id="token-167-11" morph="none" pos="word" start_char="17792">seperate</TOKEN>
<TOKEN end_char="17806" id="token-167-12" morph="none" pos="word" start_char="17801">ethnic</TOKEN>
<TOKEN end_char="17812" id="token-167-13" morph="none" pos="word" start_char="17808">group</TOKEN>
<TOKEN end_char="17813" id="token-167-14" morph="none" pos="punct" start_char="17813">.</TOKEN>
</SEG>
<SEG end_char="17873" id="segment-168" start_char="17815">
<ORIGINAL_TEXT>Again white per 100,000 rate is lower than any other group.</ORIGINAL_TEXT>
<TOKEN end_char="17819" id="token-168-0" morph="none" pos="word" start_char="17815">Again</TOKEN>
<TOKEN end_char="17825" id="token-168-1" morph="none" pos="word" start_char="17821">white</TOKEN>
<TOKEN end_char="17829" id="token-168-2" morph="none" pos="word" start_char="17827">per</TOKEN>
<TOKEN end_char="17837" id="token-168-3" morph="none" pos="unknown" start_char="17831">100,000</TOKEN>
<TOKEN end_char="17842" id="token-168-4" morph="none" pos="word" start_char="17839">rate</TOKEN>
<TOKEN end_char="17845" id="token-168-5" morph="none" pos="word" start_char="17844">is</TOKEN>
<TOKEN end_char="17851" id="token-168-6" morph="none" pos="word" start_char="17847">lower</TOKEN>
<TOKEN end_char="17856" id="token-168-7" morph="none" pos="word" start_char="17853">than</TOKEN>
<TOKEN end_char="17860" id="token-168-8" morph="none" pos="word" start_char="17858">any</TOKEN>
<TOKEN end_char="17866" id="token-168-9" morph="none" pos="word" start_char="17862">other</TOKEN>
<TOKEN end_char="17872" id="token-168-10" morph="none" pos="word" start_char="17868">group</TOKEN>
<TOKEN end_char="17873" id="token-168-11" morph="none" pos="punct" start_char="17873">.</TOKEN>
</SEG>
<SEG end_char="18073" id="segment-169" start_char="17877">
<ORIGINAL_TEXT>I can only presume you are pushing this genetics line for an agenda that it wasn't the early lockdown, test and trace, masking that lead to the lower deaths rates seen in many Asian countries but "</ORIGINAL_TEXT>
<TOKEN end_char="17877" id="token-169-0" morph="none" pos="word" start_char="17877">I</TOKEN>
<TOKEN end_char="17881" id="token-169-1" morph="none" pos="word" start_char="17879">can</TOKEN>
<TOKEN end_char="17886" id="token-169-2" morph="none" pos="word" start_char="17883">only</TOKEN>
<TOKEN end_char="17894" id="token-169-3" morph="none" pos="word" start_char="17888">presume</TOKEN>
<TOKEN end_char="17898" id="token-169-4" morph="none" pos="word" start_char="17896">you</TOKEN>
<TOKEN end_char="17902" id="token-169-5" morph="none" pos="word" start_char="17900">are</TOKEN>
<TOKEN end_char="17910" id="token-169-6" morph="none" pos="word" start_char="17904">pushing</TOKEN>
<TOKEN end_char="17915" id="token-169-7" morph="none" pos="word" start_char="17912">this</TOKEN>
<TOKEN end_char="17924" id="token-169-8" morph="none" pos="word" start_char="17917">genetics</TOKEN>
<TOKEN end_char="17929" id="token-169-9" morph="none" pos="word" start_char="17926">line</TOKEN>
<TOKEN end_char="17933" id="token-169-10" morph="none" pos="word" start_char="17931">for</TOKEN>
<TOKEN end_char="17936" id="token-169-11" morph="none" pos="word" start_char="17935">an</TOKEN>
<TOKEN end_char="17943" id="token-169-12" morph="none" pos="word" start_char="17938">agenda</TOKEN>
<TOKEN end_char="17948" id="token-169-13" morph="none" pos="word" start_char="17945">that</TOKEN>
<TOKEN end_char="17951" id="token-169-14" morph="none" pos="word" start_char="17950">it</TOKEN>
<TOKEN end_char="17958" id="token-169-15" morph="none" pos="word" start_char="17953">wasn't</TOKEN>
<TOKEN end_char="17962" id="token-169-16" morph="none" pos="word" start_char="17960">the</TOKEN>
<TOKEN end_char="17968" id="token-169-17" morph="none" pos="word" start_char="17964">early</TOKEN>
<TOKEN end_char="17977" id="token-169-18" morph="none" pos="word" start_char="17970">lockdown</TOKEN>
<TOKEN end_char="17978" id="token-169-19" morph="none" pos="punct" start_char="17978">,</TOKEN>
<TOKEN end_char="17983" id="token-169-20" morph="none" pos="word" start_char="17980">test</TOKEN>
<TOKEN end_char="17987" id="token-169-21" morph="none" pos="word" start_char="17985">and</TOKEN>
<TOKEN end_char="17993" id="token-169-22" morph="none" pos="word" start_char="17989">trace</TOKEN>
<TOKEN end_char="17994" id="token-169-23" morph="none" pos="punct" start_char="17994">,</TOKEN>
<TOKEN end_char="18002" id="token-169-24" morph="none" pos="word" start_char="17996">masking</TOKEN>
<TOKEN end_char="18007" id="token-169-25" morph="none" pos="word" start_char="18004">that</TOKEN>
<TOKEN end_char="18012" id="token-169-26" morph="none" pos="word" start_char="18009">lead</TOKEN>
<TOKEN end_char="18015" id="token-169-27" morph="none" pos="word" start_char="18014">to</TOKEN>
<TOKEN end_char="18019" id="token-169-28" morph="none" pos="word" start_char="18017">the</TOKEN>
<TOKEN end_char="18025" id="token-169-29" morph="none" pos="word" start_char="18021">lower</TOKEN>
<TOKEN end_char="18032" id="token-169-30" morph="none" pos="word" start_char="18027">deaths</TOKEN>
<TOKEN end_char="18038" id="token-169-31" morph="none" pos="word" start_char="18034">rates</TOKEN>
<TOKEN end_char="18043" id="token-169-32" morph="none" pos="word" start_char="18040">seen</TOKEN>
<TOKEN end_char="18046" id="token-169-33" morph="none" pos="word" start_char="18045">in</TOKEN>
<TOKEN end_char="18051" id="token-169-34" morph="none" pos="word" start_char="18048">many</TOKEN>
<TOKEN end_char="18057" id="token-169-35" morph="none" pos="word" start_char="18053">Asian</TOKEN>
<TOKEN end_char="18067" id="token-169-36" morph="none" pos="word" start_char="18059">countries</TOKEN>
<TOKEN end_char="18071" id="token-169-37" morph="none" pos="word" start_char="18069">but</TOKEN>
<TOKEN end_char="18073" id="token-169-38" morph="none" pos="punct" start_char="18073">"</TOKEN>
</SEG>
<SEG end_char="18104" id="segment-170" start_char="18076">
<ORIGINAL_TEXT>asians are genetically immune</ORIGINAL_TEXT>
<TOKEN end_char="18081" id="token-170-0" morph="none" pos="word" start_char="18076">asians</TOKEN>
<TOKEN end_char="18085" id="token-170-1" morph="none" pos="word" start_char="18083">are</TOKEN>
<TOKEN end_char="18097" id="token-170-2" morph="none" pos="word" start_char="18087">genetically</TOKEN>
<TOKEN end_char="18104" id="token-170-3" morph="none" pos="word" start_char="18099">immune</TOKEN>
</SEG>
<SEG end_char="18108" id="segment-171" start_char="18107">
<ORIGINAL_TEXT>".</ORIGINAL_TEXT>
<TOKEN end_char="18108" id="token-171-0" morph="none" pos="punct" start_char="18107">".</TOKEN>
</SEG>
<SEG end_char="18277" id="segment-172" start_char="18111">
<ORIGINAL_TEXT>As such it tries to takes any blame away from the UK government for being late to the party and failing to act as quickly as some other, more sucessful, countries did.</ORIGINAL_TEXT>
<TOKEN end_char="18112" id="token-172-0" morph="none" pos="word" start_char="18111">As</TOKEN>
<TOKEN end_char="18117" id="token-172-1" morph="none" pos="word" start_char="18114">such</TOKEN>
<TOKEN end_char="18120" id="token-172-2" morph="none" pos="word" start_char="18119">it</TOKEN>
<TOKEN end_char="18126" id="token-172-3" morph="none" pos="word" start_char="18122">tries</TOKEN>
<TOKEN end_char="18129" id="token-172-4" morph="none" pos="word" start_char="18128">to</TOKEN>
<TOKEN end_char="18135" id="token-172-5" morph="none" pos="word" start_char="18131">takes</TOKEN>
<TOKEN end_char="18139" id="token-172-6" morph="none" pos="word" start_char="18137">any</TOKEN>
<TOKEN end_char="18145" id="token-172-7" morph="none" pos="word" start_char="18141">blame</TOKEN>
<TOKEN end_char="18150" id="token-172-8" morph="none" pos="word" start_char="18147">away</TOKEN>
<TOKEN end_char="18155" id="token-172-9" morph="none" pos="word" start_char="18152">from</TOKEN>
<TOKEN end_char="18159" id="token-172-10" morph="none" pos="word" start_char="18157">the</TOKEN>
<TOKEN end_char="18162" id="token-172-11" morph="none" pos="word" start_char="18161">UK</TOKEN>
<TOKEN end_char="18173" id="token-172-12" morph="none" pos="word" start_char="18164">government</TOKEN>
<TOKEN end_char="18177" id="token-172-13" morph="none" pos="word" start_char="18175">for</TOKEN>
<TOKEN end_char="18183" id="token-172-14" morph="none" pos="word" start_char="18179">being</TOKEN>
<TOKEN end_char="18188" id="token-172-15" morph="none" pos="word" start_char="18185">late</TOKEN>
<TOKEN end_char="18191" id="token-172-16" morph="none" pos="word" start_char="18190">to</TOKEN>
<TOKEN end_char="18195" id="token-172-17" morph="none" pos="word" start_char="18193">the</TOKEN>
<TOKEN end_char="18201" id="token-172-18" morph="none" pos="word" start_char="18197">party</TOKEN>
<TOKEN end_char="18205" id="token-172-19" morph="none" pos="word" start_char="18203">and</TOKEN>
<TOKEN end_char="18213" id="token-172-20" morph="none" pos="word" start_char="18207">failing</TOKEN>
<TOKEN end_char="18216" id="token-172-21" morph="none" pos="word" start_char="18215">to</TOKEN>
<TOKEN end_char="18220" id="token-172-22" morph="none" pos="word" start_char="18218">act</TOKEN>
<TOKEN end_char="18223" id="token-172-23" morph="none" pos="word" start_char="18222">as</TOKEN>
<TOKEN end_char="18231" id="token-172-24" morph="none" pos="word" start_char="18225">quickly</TOKEN>
<TOKEN end_char="18234" id="token-172-25" morph="none" pos="word" start_char="18233">as</TOKEN>
<TOKEN end_char="18239" id="token-172-26" morph="none" pos="word" start_char="18236">some</TOKEN>
<TOKEN end_char="18245" id="token-172-27" morph="none" pos="word" start_char="18241">other</TOKEN>
<TOKEN end_char="18246" id="token-172-28" morph="none" pos="punct" start_char="18246">,</TOKEN>
<TOKEN end_char="18251" id="token-172-29" morph="none" pos="word" start_char="18248">more</TOKEN>
<TOKEN end_char="18261" id="token-172-30" morph="none" pos="word" start_char="18253">sucessful</TOKEN>
<TOKEN end_char="18262" id="token-172-31" morph="none" pos="punct" start_char="18262">,</TOKEN>
<TOKEN end_char="18272" id="token-172-32" morph="none" pos="word" start_char="18264">countries</TOKEN>
<TOKEN end_char="18276" id="token-172-33" morph="none" pos="word" start_char="18274">did</TOKEN>
<TOKEN end_char="18277" id="token-172-34" morph="none" pos="punct" start_char="18277">.</TOKEN>
</SEG>
<SEG end_char="18400" id="segment-173" start_char="18279">
<ORIGINAL_TEXT>UK figures show that Asians are not genetically immune when the mortality rates are compared together in the same country.</ORIGINAL_TEXT>
<TOKEN end_char="18280" id="token-173-0" morph="none" pos="word" start_char="18279">UK</TOKEN>
<TOKEN end_char="18288" id="token-173-1" morph="none" pos="word" start_char="18282">figures</TOKEN>
<TOKEN end_char="18293" id="token-173-2" morph="none" pos="word" start_char="18290">show</TOKEN>
<TOKEN end_char="18298" id="token-173-3" morph="none" pos="word" start_char="18295">that</TOKEN>
<TOKEN end_char="18305" id="token-173-4" morph="none" pos="word" start_char="18300">Asians</TOKEN>
<TOKEN end_char="18309" id="token-173-5" morph="none" pos="word" start_char="18307">are</TOKEN>
<TOKEN end_char="18313" id="token-173-6" morph="none" pos="word" start_char="18311">not</TOKEN>
<TOKEN end_char="18325" id="token-173-7" morph="none" pos="word" start_char="18315">genetically</TOKEN>
<TOKEN end_char="18332" id="token-173-8" morph="none" pos="word" start_char="18327">immune</TOKEN>
<TOKEN end_char="18337" id="token-173-9" morph="none" pos="word" start_char="18334">when</TOKEN>
<TOKEN end_char="18341" id="token-173-10" morph="none" pos="word" start_char="18339">the</TOKEN>
<TOKEN end_char="18351" id="token-173-11" morph="none" pos="word" start_char="18343">mortality</TOKEN>
<TOKEN end_char="18357" id="token-173-12" morph="none" pos="word" start_char="18353">rates</TOKEN>
<TOKEN end_char="18361" id="token-173-13" morph="none" pos="word" start_char="18359">are</TOKEN>
<TOKEN end_char="18370" id="token-173-14" morph="none" pos="word" start_char="18363">compared</TOKEN>
<TOKEN end_char="18379" id="token-173-15" morph="none" pos="word" start_char="18372">together</TOKEN>
<TOKEN end_char="18382" id="token-173-16" morph="none" pos="word" start_char="18381">in</TOKEN>
<TOKEN end_char="18386" id="token-173-17" morph="none" pos="word" start_char="18384">the</TOKEN>
<TOKEN end_char="18391" id="token-173-18" morph="none" pos="word" start_char="18388">same</TOKEN>
<TOKEN end_char="18399" id="token-173-19" morph="none" pos="word" start_char="18393">country</TOKEN>
<TOKEN end_char="18400" id="token-173-20" morph="none" pos="punct" start_char="18400">.</TOKEN>
</SEG>
<SEG end_char="18497" id="segment-174" start_char="18404">
<ORIGINAL_TEXT>It would be interesting, from a scientific perspective to see how many have been asymptomatic.</ORIGINAL_TEXT>
<TOKEN end_char="18405" id="token-174-0" morph="none" pos="word" start_char="18404">It</TOKEN>
<TOKEN end_char="18411" id="token-174-1" morph="none" pos="word" start_char="18407">would</TOKEN>
<TOKEN end_char="18414" id="token-174-2" morph="none" pos="word" start_char="18413">be</TOKEN>
<TOKEN end_char="18426" id="token-174-3" morph="none" pos="word" start_char="18416">interesting</TOKEN>
<TOKEN end_char="18427" id="token-174-4" morph="none" pos="punct" start_char="18427">,</TOKEN>
<TOKEN end_char="18432" id="token-174-5" morph="none" pos="word" start_char="18429">from</TOKEN>
<TOKEN end_char="18434" id="token-174-6" morph="none" pos="word" start_char="18434">a</TOKEN>
<TOKEN end_char="18445" id="token-174-7" morph="none" pos="word" start_char="18436">scientific</TOKEN>
<TOKEN end_char="18457" id="token-174-8" morph="none" pos="word" start_char="18447">perspective</TOKEN>
<TOKEN end_char="18460" id="token-174-9" morph="none" pos="word" start_char="18459">to</TOKEN>
<TOKEN end_char="18464" id="token-174-10" morph="none" pos="word" start_char="18462">see</TOKEN>
<TOKEN end_char="18468" id="token-174-11" morph="none" pos="word" start_char="18466">how</TOKEN>
<TOKEN end_char="18473" id="token-174-12" morph="none" pos="word" start_char="18470">many</TOKEN>
<TOKEN end_char="18478" id="token-174-13" morph="none" pos="word" start_char="18475">have</TOKEN>
<TOKEN end_char="18483" id="token-174-14" morph="none" pos="word" start_char="18480">been</TOKEN>
<TOKEN end_char="18496" id="token-174-15" morph="none" pos="word" start_char="18485">asymptomatic</TOKEN>
<TOKEN end_char="18497" id="token-174-16" morph="none" pos="punct" start_char="18497">.</TOKEN>
</SEG>
<SEG end_char="18715" id="segment-175" start_char="18499">
<ORIGINAL_TEXT>Some countries in SE asia, such as Cambodia came through relatively unscathed, but had an outbreak recently in the capital city which has now started to spread - and these are areas with little/ low level health care.</ORIGINAL_TEXT>
<TOKEN end_char="18502" id="token-175-0" morph="none" pos="word" start_char="18499">Some</TOKEN>
<TOKEN end_char="18512" id="token-175-1" morph="none" pos="word" start_char="18504">countries</TOKEN>
<TOKEN end_char="18515" id="token-175-2" morph="none" pos="word" start_char="18514">in</TOKEN>
<TOKEN end_char="18518" id="token-175-3" morph="none" pos="word" start_char="18517">SE</TOKEN>
<TOKEN end_char="18523" id="token-175-4" morph="none" pos="word" start_char="18520">asia</TOKEN>
<TOKEN end_char="18524" id="token-175-5" morph="none" pos="punct" start_char="18524">,</TOKEN>
<TOKEN end_char="18529" id="token-175-6" morph="none" pos="word" start_char="18526">such</TOKEN>
<TOKEN end_char="18532" id="token-175-7" morph="none" pos="word" start_char="18531">as</TOKEN>
<TOKEN end_char="18541" id="token-175-8" morph="none" pos="word" start_char="18534">Cambodia</TOKEN>
<TOKEN end_char="18546" id="token-175-9" morph="none" pos="word" start_char="18543">came</TOKEN>
<TOKEN end_char="18554" id="token-175-10" morph="none" pos="word" start_char="18548">through</TOKEN>
<TOKEN end_char="18565" id="token-175-11" morph="none" pos="word" start_char="18556">relatively</TOKEN>
<TOKEN end_char="18575" id="token-175-12" morph="none" pos="word" start_char="18567">unscathed</TOKEN>
<TOKEN end_char="18576" id="token-175-13" morph="none" pos="punct" start_char="18576">,</TOKEN>
<TOKEN end_char="18580" id="token-175-14" morph="none" pos="word" start_char="18578">but</TOKEN>
<TOKEN end_char="18584" id="token-175-15" morph="none" pos="word" start_char="18582">had</TOKEN>
<TOKEN end_char="18587" id="token-175-16" morph="none" pos="word" start_char="18586">an</TOKEN>
<TOKEN end_char="18596" id="token-175-17" morph="none" pos="word" start_char="18589">outbreak</TOKEN>
<TOKEN end_char="18605" id="token-175-18" morph="none" pos="word" start_char="18598">recently</TOKEN>
<TOKEN end_char="18608" id="token-175-19" morph="none" pos="word" start_char="18607">in</TOKEN>
<TOKEN end_char="18612" id="token-175-20" morph="none" pos="word" start_char="18610">the</TOKEN>
<TOKEN end_char="18620" id="token-175-21" morph="none" pos="word" start_char="18614">capital</TOKEN>
<TOKEN end_char="18625" id="token-175-22" morph="none" pos="word" start_char="18622">city</TOKEN>
<TOKEN end_char="18631" id="token-175-23" morph="none" pos="word" start_char="18627">which</TOKEN>
<TOKEN end_char="18635" id="token-175-24" morph="none" pos="word" start_char="18633">has</TOKEN>
<TOKEN end_char="18639" id="token-175-25" morph="none" pos="word" start_char="18637">now</TOKEN>
<TOKEN end_char="18647" id="token-175-26" morph="none" pos="word" start_char="18641">started</TOKEN>
<TOKEN end_char="18650" id="token-175-27" morph="none" pos="word" start_char="18649">to</TOKEN>
<TOKEN end_char="18657" id="token-175-28" morph="none" pos="word" start_char="18652">spread</TOKEN>
<TOKEN end_char="18659" id="token-175-29" morph="none" pos="punct" start_char="18659">-</TOKEN>
<TOKEN end_char="18663" id="token-175-30" morph="none" pos="word" start_char="18661">and</TOKEN>
<TOKEN end_char="18669" id="token-175-31" morph="none" pos="word" start_char="18665">these</TOKEN>
<TOKEN end_char="18673" id="token-175-32" morph="none" pos="word" start_char="18671">are</TOKEN>
<TOKEN end_char="18679" id="token-175-33" morph="none" pos="word" start_char="18675">areas</TOKEN>
<TOKEN end_char="18684" id="token-175-34" morph="none" pos="word" start_char="18681">with</TOKEN>
<TOKEN end_char="18691" id="token-175-35" morph="none" pos="word" start_char="18686">little</TOKEN>
<TOKEN end_char="18692" id="token-175-36" morph="none" pos="punct" start_char="18692">/</TOKEN>
<TOKEN end_char="18696" id="token-175-37" morph="none" pos="word" start_char="18694">low</TOKEN>
<TOKEN end_char="18702" id="token-175-38" morph="none" pos="word" start_char="18698">level</TOKEN>
<TOKEN end_char="18709" id="token-175-39" morph="none" pos="word" start_char="18704">health</TOKEN>
<TOKEN end_char="18714" id="token-175-40" morph="none" pos="word" start_char="18711">care</TOKEN>
<TOKEN end_char="18715" id="token-175-41" morph="none" pos="punct" start_char="18715">.</TOKEN>
</SEG>
<SEG end_char="18842" id="segment-176" start_char="18717">
<ORIGINAL_TEXT>There is likely a multitude of factors though, including levels obesity being significantly lower, more time spent outside etc</ORIGINAL_TEXT>
<TOKEN end_char="18721" id="token-176-0" morph="none" pos="word" start_char="18717">There</TOKEN>
<TOKEN end_char="18724" id="token-176-1" morph="none" pos="word" start_char="18723">is</TOKEN>
<TOKEN end_char="18731" id="token-176-2" morph="none" pos="word" start_char="18726">likely</TOKEN>
<TOKEN end_char="18733" id="token-176-3" morph="none" pos="word" start_char="18733">a</TOKEN>
<TOKEN end_char="18743" id="token-176-4" morph="none" pos="word" start_char="18735">multitude</TOKEN>
<TOKEN end_char="18746" id="token-176-5" morph="none" pos="word" start_char="18745">of</TOKEN>
<TOKEN end_char="18754" id="token-176-6" morph="none" pos="word" start_char="18748">factors</TOKEN>
<TOKEN end_char="18761" id="token-176-7" morph="none" pos="word" start_char="18756">though</TOKEN>
<TOKEN end_char="18762" id="token-176-8" morph="none" pos="punct" start_char="18762">,</TOKEN>
<TOKEN end_char="18772" id="token-176-9" morph="none" pos="word" start_char="18764">including</TOKEN>
<TOKEN end_char="18779" id="token-176-10" morph="none" pos="word" start_char="18774">levels</TOKEN>
<TOKEN end_char="18787" id="token-176-11" morph="none" pos="word" start_char="18781">obesity</TOKEN>
<TOKEN end_char="18793" id="token-176-12" morph="none" pos="word" start_char="18789">being</TOKEN>
<TOKEN end_char="18807" id="token-176-13" morph="none" pos="word" start_char="18795">significantly</TOKEN>
<TOKEN end_char="18813" id="token-176-14" morph="none" pos="word" start_char="18809">lower</TOKEN>
<TOKEN end_char="18814" id="token-176-15" morph="none" pos="punct" start_char="18814">,</TOKEN>
<TOKEN end_char="18819" id="token-176-16" morph="none" pos="word" start_char="18816">more</TOKEN>
<TOKEN end_char="18824" id="token-176-17" morph="none" pos="word" start_char="18821">time</TOKEN>
<TOKEN end_char="18830" id="token-176-18" morph="none" pos="word" start_char="18826">spent</TOKEN>
<TOKEN end_char="18838" id="token-176-19" morph="none" pos="word" start_char="18832">outside</TOKEN>
<TOKEN end_char="18842" id="token-176-20" morph="none" pos="word" start_char="18840">etc</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>