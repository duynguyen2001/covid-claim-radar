<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CA2J" lang="spa" raw_text_char_length="4313" raw_text_md5="ec6657d26bcba451156489bd8172980e" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="92" id="segment-0" start_char="1">
<ORIGINAL_TEXT>No, this 2015 Italian TV news does not prove that the Covid-19 was created in the laboratory</ORIGINAL_TEXT>
<TOKEN end_char="2" id="token-0-0" morph="none" pos="word" start_char="1">No</TOKEN>
<TOKEN end_char="3" id="token-0-1" morph="none" pos="punct" start_char="3">,</TOKEN>
<TOKEN end_char="8" id="token-0-2" morph="none" pos="word" start_char="5">this</TOKEN>
<TOKEN end_char="13" id="token-0-3" morph="none" pos="word" start_char="10">2015</TOKEN>
<TOKEN end_char="21" id="token-0-4" morph="none" pos="word" start_char="15">Italian</TOKEN>
<TOKEN end_char="24" id="token-0-5" morph="none" pos="word" start_char="23">TV</TOKEN>
<TOKEN end_char="29" id="token-0-6" morph="none" pos="word" start_char="26">news</TOKEN>
<TOKEN end_char="34" id="token-0-7" morph="none" pos="word" start_char="31">does</TOKEN>
<TOKEN end_char="38" id="token-0-8" morph="none" pos="word" start_char="36">not</TOKEN>
<TOKEN end_char="44" id="token-0-9" morph="none" pos="word" start_char="40">prove</TOKEN>
<TOKEN end_char="49" id="token-0-10" morph="none" pos="word" start_char="46">that</TOKEN>
<TOKEN end_char="53" id="token-0-11" morph="none" pos="word" start_char="51">the</TOKEN>
<TOKEN end_char="62" id="token-0-12" morph="none" pos="unknown" start_char="55">Covid-19</TOKEN>
<TOKEN end_char="66" id="token-0-13" morph="none" pos="word" start_char="64">was</TOKEN>
<TOKEN end_char="74" id="token-0-14" morph="none" pos="word" start_char="68">created</TOKEN>
<TOKEN end_char="77" id="token-0-15" morph="none" pos="word" start_char="76">in</TOKEN>
<TOKEN end_char="81" id="token-0-16" morph="none" pos="word" start_char="79">the</TOKEN>
<TOKEN end_char="92" id="token-0-17" morph="none" pos="word" start_char="83">laboratory</TOKEN>
</SEG>
<SEG end_char="143" id="segment-1" start_char="97">
<ORIGINAL_TEXT>A sample for a screening test in Nice, April 2.</ORIGINAL_TEXT>
<TOKEN end_char="97" id="token-1-0" morph="none" pos="word" start_char="97">A</TOKEN>
<TOKEN end_char="104" id="token-1-1" morph="none" pos="word" start_char="99">sample</TOKEN>
<TOKEN end_char="108" id="token-1-2" morph="none" pos="word" start_char="106">for</TOKEN>
<TOKEN end_char="110" id="token-1-3" morph="none" pos="word" start_char="110">a</TOKEN>
<TOKEN end_char="120" id="token-1-4" morph="none" pos="word" start_char="112">screening</TOKEN>
<TOKEN end_char="125" id="token-1-5" morph="none" pos="word" start_char="122">test</TOKEN>
<TOKEN end_char="128" id="token-1-6" morph="none" pos="word" start_char="127">in</TOKEN>
<TOKEN end_char="133" id="token-1-7" morph="none" pos="word" start_char="130">Nice</TOKEN>
<TOKEN end_char="134" id="token-1-8" morph="none" pos="punct" start_char="134">,</TOKEN>
<TOKEN end_char="140" id="token-1-9" morph="none" pos="word" start_char="136">April</TOKEN>
<TOKEN end_char="142" id="token-1-10" morph="none" pos="word" start_char="142">2</TOKEN>
<TOKEN end_char="143" id="token-1-11" morph="none" pos="punct" start_char="143">.</TOKEN>
</SEG>
<SEG end_char="145" id="segment-2" start_char="145">
<ORIGINAL_TEXT>–</ORIGINAL_TEXT>
<TOKEN end_char="145" id="token-2-0" morph="none" pos="punct" start_char="145">–</TOKEN>
</SEG>
<SEG end_char="160" id="segment-3" start_char="148">
<ORIGINAL_TEXT>SYSPEO / SIPA</ORIGINAL_TEXT>
<TOKEN end_char="153" id="token-3-0" morph="none" pos="word" start_char="148">SYSPEO</TOKEN>
<TOKEN end_char="155" id="token-3-1" morph="none" pos="punct" start_char="155">/</TOKEN>
<TOKEN end_char="160" id="token-3-2" morph="none" pos="word" start_char="157">SIPA</TOKEN>
<TRANSLATED_TEXT>SPAIN / SPAIN</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="276" id="segment-4" start_char="166">
<ORIGINAL_TEXT>In 2015, the Leonardo program, broadcast on RAI, devoted a report to an experiment carried out in a laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="167" id="token-4-0" morph="none" pos="word" start_char="166">In</TOKEN>
<TOKEN end_char="172" id="token-4-1" morph="none" pos="word" start_char="169">2015</TOKEN>
<TOKEN end_char="173" id="token-4-2" morph="none" pos="punct" start_char="173">,</TOKEN>
<TOKEN end_char="177" id="token-4-3" morph="none" pos="word" start_char="175">the</TOKEN>
<TOKEN end_char="186" id="token-4-4" morph="none" pos="word" start_char="179">Leonardo</TOKEN>
<TOKEN end_char="194" id="token-4-5" morph="none" pos="word" start_char="188">program</TOKEN>
<TOKEN end_char="195" id="token-4-6" morph="none" pos="punct" start_char="195">,</TOKEN>
<TOKEN end_char="205" id="token-4-7" morph="none" pos="word" start_char="197">broadcast</TOKEN>
<TOKEN end_char="208" id="token-4-8" morph="none" pos="word" start_char="207">on</TOKEN>
<TOKEN end_char="212" id="token-4-9" morph="none" pos="word" start_char="210">RAI</TOKEN>
<TOKEN end_char="213" id="token-4-10" morph="none" pos="punct" start_char="213">,</TOKEN>
<TOKEN end_char="221" id="token-4-11" morph="none" pos="word" start_char="215">devoted</TOKEN>
<TOKEN end_char="223" id="token-4-12" morph="none" pos="word" start_char="223">a</TOKEN>
<TOKEN end_char="230" id="token-4-13" morph="none" pos="word" start_char="225">report</TOKEN>
<TOKEN end_char="233" id="token-4-14" morph="none" pos="word" start_char="232">to</TOKEN>
<TOKEN end_char="236" id="token-4-15" morph="none" pos="word" start_char="235">an</TOKEN>
<TOKEN end_char="247" id="token-4-16" morph="none" pos="word" start_char="238">experiment</TOKEN>
<TOKEN end_char="255" id="token-4-17" morph="none" pos="word" start_char="249">carried</TOKEN>
<TOKEN end_char="259" id="token-4-18" morph="none" pos="word" start_char="257">out</TOKEN>
<TOKEN end_char="262" id="token-4-19" morph="none" pos="word" start_char="261">in</TOKEN>
<TOKEN end_char="264" id="token-4-20" morph="none" pos="word" start_char="264">a</TOKEN>
<TOKEN end_char="275" id="token-4-21" morph="none" pos="word" start_char="266">laboratory</TOKEN>
<TOKEN end_char="276" id="token-4-22" morph="none" pos="punct" start_char="276">.</TOKEN>
</SEG>
<SEG end_char="335" id="segment-5" start_char="280">
<ORIGINAL_TEXT>The experiment involved a virus different from Covid-19.</ORIGINAL_TEXT>
<TOKEN end_char="282" id="token-5-0" morph="none" pos="word" start_char="280">The</TOKEN>
<TOKEN end_char="293" id="token-5-1" morph="none" pos="word" start_char="284">experiment</TOKEN>
<TOKEN end_char="302" id="token-5-2" morph="none" pos="word" start_char="295">involved</TOKEN>
<TOKEN end_char="304" id="token-5-3" morph="none" pos="word" start_char="304">a</TOKEN>
<TOKEN end_char="310" id="token-5-4" morph="none" pos="word" start_char="306">virus</TOKEN>
<TOKEN end_char="320" id="token-5-5" morph="none" pos="word" start_char="312">different</TOKEN>
<TOKEN end_char="325" id="token-5-6" morph="none" pos="word" start_char="322">from</TOKEN>
<TOKEN end_char="334" id="token-5-7" morph="none" pos="unknown" start_char="327">Covid-19</TOKEN>
<TOKEN end_char="335" id="token-5-8" morph="none" pos="punct" start_char="335">.</TOKEN>
</SEG>
<SEG end_char="391" id="segment-6" start_char="339">
<ORIGINAL_TEXT>This verification request was sent to us by a reader.</ORIGINAL_TEXT>
<TOKEN end_char="342" id="token-6-0" morph="none" pos="word" start_char="339">This</TOKEN>
<TOKEN end_char="355" id="token-6-1" morph="none" pos="word" start_char="344">verification</TOKEN>
<TOKEN end_char="363" id="token-6-2" morph="none" pos="word" start_char="357">request</TOKEN>
<TOKEN end_char="367" id="token-6-3" morph="none" pos="word" start_char="365">was</TOKEN>
<TOKEN end_char="372" id="token-6-4" morph="none" pos="word" start_char="369">sent</TOKEN>
<TOKEN end_char="375" id="token-6-5" morph="none" pos="word" start_char="374">to</TOKEN>
<TOKEN end_char="378" id="token-6-6" morph="none" pos="word" start_char="377">us</TOKEN>
<TOKEN end_char="381" id="token-6-7" morph="none" pos="word" start_char="380">by</TOKEN>
<TOKEN end_char="383" id="token-6-8" morph="none" pos="word" start_char="383">a</TOKEN>
<TOKEN end_char="390" id="token-6-9" morph="none" pos="word" start_char="385">reader</TOKEN>
<TOKEN end_char="391" id="token-6-10" morph="none" pos="punct" start_char="391">.</TOKEN>
</SEG>
<SEG end_char="474" id="segment-7" start_char="396">
<ORIGINAL_TEXT>Why does a report broadcast in 2015 on Italian television resurface on YouTube?</ORIGINAL_TEXT>
<TOKEN end_char="398" id="token-7-0" morph="none" pos="word" start_char="396">Why</TOKEN>
<TOKEN end_char="403" id="token-7-1" morph="none" pos="word" start_char="400">does</TOKEN>
<TOKEN end_char="405" id="token-7-2" morph="none" pos="word" start_char="405">a</TOKEN>
<TOKEN end_char="412" id="token-7-3" morph="none" pos="word" start_char="407">report</TOKEN>
<TOKEN end_char="422" id="token-7-4" morph="none" pos="word" start_char="414">broadcast</TOKEN>
<TOKEN end_char="425" id="token-7-5" morph="none" pos="word" start_char="424">in</TOKEN>
<TOKEN end_char="430" id="token-7-6" morph="none" pos="word" start_char="427">2015</TOKEN>
<TOKEN end_char="433" id="token-7-7" morph="none" pos="word" start_char="432">on</TOKEN>
<TOKEN end_char="441" id="token-7-8" morph="none" pos="word" start_char="435">Italian</TOKEN>
<TOKEN end_char="452" id="token-7-9" morph="none" pos="word" start_char="443">television</TOKEN>
<TOKEN end_char="462" id="token-7-10" morph="none" pos="word" start_char="454">resurface</TOKEN>
<TOKEN end_char="465" id="token-7-11" morph="none" pos="word" start_char="464">on</TOKEN>
<TOKEN end_char="473" id="token-7-12" morph="none" pos="word" start_char="467">YouTube</TOKEN>
<TOKEN end_char="474" id="token-7-13" morph="none" pos="punct" start_char="474">?</TOKEN>
</SEG>
<SEG end_char="627" id="segment-8" start_char="476">
<ORIGINAL_TEXT>Posted on March 27 on a French-language channel with the title "A coronavirus created in the laboratory", it has accumulated nearly 110,000 views since.</ORIGINAL_TEXT>
<TOKEN end_char="481" id="token-8-0" morph="none" pos="word" start_char="476">Posted</TOKEN>
<TOKEN end_char="484" id="token-8-1" morph="none" pos="word" start_char="483">on</TOKEN>
<TOKEN end_char="490" id="token-8-2" morph="none" pos="word" start_char="486">March</TOKEN>
<TOKEN end_char="493" id="token-8-3" morph="none" pos="word" start_char="492">27</TOKEN>
<TOKEN end_char="496" id="token-8-4" morph="none" pos="word" start_char="495">on</TOKEN>
<TOKEN end_char="498" id="token-8-5" morph="none" pos="word" start_char="498">a</TOKEN>
<TOKEN end_char="514" id="token-8-6" morph="none" pos="unknown" start_char="500">French-language</TOKEN>
<TOKEN end_char="522" id="token-8-7" morph="none" pos="word" start_char="516">channel</TOKEN>
<TOKEN end_char="527" id="token-8-8" morph="none" pos="word" start_char="524">with</TOKEN>
<TOKEN end_char="531" id="token-8-9" morph="none" pos="word" start_char="529">the</TOKEN>
<TOKEN end_char="537" id="token-8-10" morph="none" pos="word" start_char="533">title</TOKEN>
<TOKEN end_char="539" id="token-8-11" morph="none" pos="punct" start_char="539">"</TOKEN>
<TOKEN end_char="540" id="token-8-12" morph="none" pos="word" start_char="540">A</TOKEN>
<TOKEN end_char="552" id="token-8-13" morph="none" pos="word" start_char="542">coronavirus</TOKEN>
<TOKEN end_char="560" id="token-8-14" morph="none" pos="word" start_char="554">created</TOKEN>
<TOKEN end_char="563" id="token-8-15" morph="none" pos="word" start_char="562">in</TOKEN>
<TOKEN end_char="567" id="token-8-16" morph="none" pos="word" start_char="565">the</TOKEN>
<TOKEN end_char="578" id="token-8-17" morph="none" pos="word" start_char="569">laboratory</TOKEN>
<TOKEN end_char="580" id="token-8-18" morph="none" pos="punct" start_char="579">",</TOKEN>
<TOKEN end_char="583" id="token-8-19" morph="none" pos="word" start_char="582">it</TOKEN>
<TOKEN end_char="587" id="token-8-20" morph="none" pos="word" start_char="585">has</TOKEN>
<TOKEN end_char="599" id="token-8-21" morph="none" pos="word" start_char="589">accumulated</TOKEN>
<TOKEN end_char="606" id="token-8-22" morph="none" pos="word" start_char="601">nearly</TOKEN>
<TOKEN end_char="614" id="token-8-23" morph="none" pos="unknown" start_char="608">110,000</TOKEN>
<TOKEN end_char="620" id="token-8-24" morph="none" pos="word" start_char="616">views</TOKEN>
<TOKEN end_char="626" id="token-8-25" morph="none" pos="word" start_char="622">since</TOKEN>
<TOKEN end_char="627" id="token-8-26" morph="none" pos="punct" start_char="627">.</TOKEN>
</SEG>
<SEG end_char="780" id="segment-9" start_char="630">
<ORIGINAL_TEXT>The report deals with an experiment carried out on the SARS virus, which raged in 2003 and which is part of the family of coronaviruses, like Covid-19.</ORIGINAL_TEXT>
<TOKEN end_char="632" id="token-9-0" morph="none" pos="word" start_char="630">The</TOKEN>
<TOKEN end_char="639" id="token-9-1" morph="none" pos="word" start_char="634">report</TOKEN>
<TOKEN end_char="645" id="token-9-2" morph="none" pos="word" start_char="641">deals</TOKEN>
<TOKEN end_char="650" id="token-9-3" morph="none" pos="word" start_char="647">with</TOKEN>
<TOKEN end_char="653" id="token-9-4" morph="none" pos="word" start_char="652">an</TOKEN>
<TOKEN end_char="664" id="token-9-5" morph="none" pos="word" start_char="655">experiment</TOKEN>
<TOKEN end_char="672" id="token-9-6" morph="none" pos="word" start_char="666">carried</TOKEN>
<TOKEN end_char="676" id="token-9-7" morph="none" pos="word" start_char="674">out</TOKEN>
<TOKEN end_char="679" id="token-9-8" morph="none" pos="word" start_char="678">on</TOKEN>
<TOKEN end_char="683" id="token-9-9" morph="none" pos="word" start_char="681">the</TOKEN>
<TOKEN end_char="688" id="token-9-10" morph="none" pos="word" start_char="685">SARS</TOKEN>
<TOKEN end_char="694" id="token-9-11" morph="none" pos="word" start_char="690">virus</TOKEN>
<TOKEN end_char="695" id="token-9-12" morph="none" pos="punct" start_char="695">,</TOKEN>
<TOKEN end_char="701" id="token-9-13" morph="none" pos="word" start_char="697">which</TOKEN>
<TOKEN end_char="707" id="token-9-14" morph="none" pos="word" start_char="703">raged</TOKEN>
<TOKEN end_char="710" id="token-9-15" morph="none" pos="word" start_char="709">in</TOKEN>
<TOKEN end_char="715" id="token-9-16" morph="none" pos="word" start_char="712">2003</TOKEN>
<TOKEN end_char="719" id="token-9-17" morph="none" pos="word" start_char="717">and</TOKEN>
<TOKEN end_char="725" id="token-9-18" morph="none" pos="word" start_char="721">which</TOKEN>
<TOKEN end_char="728" id="token-9-19" morph="none" pos="word" start_char="727">is</TOKEN>
<TOKEN end_char="733" id="token-9-20" morph="none" pos="word" start_char="730">part</TOKEN>
<TOKEN end_char="736" id="token-9-21" morph="none" pos="word" start_char="735">of</TOKEN>
<TOKEN end_char="740" id="token-9-22" morph="none" pos="word" start_char="738">the</TOKEN>
<TOKEN end_char="747" id="token-9-23" morph="none" pos="word" start_char="742">family</TOKEN>
<TOKEN end_char="750" id="token-9-24" morph="none" pos="word" start_char="749">of</TOKEN>
<TOKEN end_char="764" id="token-9-25" morph="none" pos="word" start_char="752">coronaviruses</TOKEN>
<TOKEN end_char="765" id="token-9-26" morph="none" pos="punct" start_char="765">,</TOKEN>
<TOKEN end_char="770" id="token-9-27" morph="none" pos="word" start_char="767">like</TOKEN>
<TOKEN end_char="779" id="token-9-28" morph="none" pos="unknown" start_char="772">Covid-19</TOKEN>
<TOKEN end_char="780" id="token-9-29" morph="none" pos="punct" start_char="780">.</TOKEN>
</SEG>
<SEG end_char="945" id="segment-10" start_char="782">
<ORIGINAL_TEXT>"It’s just an experiment, but it raises a lot of concerns," said the presenter of Leonardo, a science show broadcast on RAI, the Italian public broadcasting system.</ORIGINAL_TEXT>
<TOKEN end_char="782" id="token-10-0" morph="none" pos="punct" start_char="782">"</TOKEN>
<TOKEN end_char="786" id="token-10-1" morph="none" pos="word" start_char="783">It’s</TOKEN>
<TOKEN end_char="791" id="token-10-2" morph="none" pos="word" start_char="788">just</TOKEN>
<TOKEN end_char="794" id="token-10-3" morph="none" pos="word" start_char="793">an</TOKEN>
<TOKEN end_char="805" id="token-10-4" morph="none" pos="word" start_char="796">experiment</TOKEN>
<TOKEN end_char="806" id="token-10-5" morph="none" pos="punct" start_char="806">,</TOKEN>
<TOKEN end_char="810" id="token-10-6" morph="none" pos="word" start_char="808">but</TOKEN>
<TOKEN end_char="813" id="token-10-7" morph="none" pos="word" start_char="812">it</TOKEN>
<TOKEN end_char="820" id="token-10-8" morph="none" pos="word" start_char="815">raises</TOKEN>
<TOKEN end_char="822" id="token-10-9" morph="none" pos="word" start_char="822">a</TOKEN>
<TOKEN end_char="826" id="token-10-10" morph="none" pos="word" start_char="824">lot</TOKEN>
<TOKEN end_char="829" id="token-10-11" morph="none" pos="word" start_char="828">of</TOKEN>
<TOKEN end_char="838" id="token-10-12" morph="none" pos="word" start_char="831">concerns</TOKEN>
<TOKEN end_char="840" id="token-10-13" morph="none" pos="punct" start_char="839">,"</TOKEN>
<TOKEN end_char="845" id="token-10-14" morph="none" pos="word" start_char="842">said</TOKEN>
<TOKEN end_char="849" id="token-10-15" morph="none" pos="word" start_char="847">the</TOKEN>
<TOKEN end_char="859" id="token-10-16" morph="none" pos="word" start_char="851">presenter</TOKEN>
<TOKEN end_char="862" id="token-10-17" morph="none" pos="word" start_char="861">of</TOKEN>
<TOKEN end_char="871" id="token-10-18" morph="none" pos="word" start_char="864">Leonardo</TOKEN>
<TOKEN end_char="872" id="token-10-19" morph="none" pos="punct" start_char="872">,</TOKEN>
<TOKEN end_char="874" id="token-10-20" morph="none" pos="word" start_char="874">a</TOKEN>
<TOKEN end_char="882" id="token-10-21" morph="none" pos="word" start_char="876">science</TOKEN>
<TOKEN end_char="887" id="token-10-22" morph="none" pos="word" start_char="884">show</TOKEN>
<TOKEN end_char="897" id="token-10-23" morph="none" pos="word" start_char="889">broadcast</TOKEN>
<TOKEN end_char="900" id="token-10-24" morph="none" pos="word" start_char="899">on</TOKEN>
<TOKEN end_char="904" id="token-10-25" morph="none" pos="word" start_char="902">RAI</TOKEN>
<TOKEN end_char="905" id="token-10-26" morph="none" pos="punct" start_char="905">,</TOKEN>
<TOKEN end_char="909" id="token-10-27" morph="none" pos="word" start_char="907">the</TOKEN>
<TOKEN end_char="917" id="token-10-28" morph="none" pos="word" start_char="911">Italian</TOKEN>
<TOKEN end_char="924" id="token-10-29" morph="none" pos="word" start_char="919">public</TOKEN>
<TOKEN end_char="937" id="token-10-30" morph="none" pos="word" start_char="926">broadcasting</TOKEN>
<TOKEN end_char="944" id="token-10-31" morph="none" pos="word" start_char="939">system</TOKEN>
<TOKEN end_char="945" id="token-10-32" morph="none" pos="punct" start_char="945">.</TOKEN>
</SEG>
<SEG end_char="1065" id="segment-11" start_char="947">
<ORIGINAL_TEXT>A group of Chinese researchers has grafted a protein from bats into the Sars virus – acute pneumonia – taken from mice.</ORIGINAL_TEXT>
<TOKEN end_char="947" id="token-11-0" morph="none" pos="word" start_char="947">A</TOKEN>
<TOKEN end_char="953" id="token-11-1" morph="none" pos="word" start_char="949">group</TOKEN>
<TOKEN end_char="956" id="token-11-2" morph="none" pos="word" start_char="955">of</TOKEN>
<TOKEN end_char="964" id="token-11-3" morph="none" pos="word" start_char="958">Chinese</TOKEN>
<TOKEN end_char="976" id="token-11-4" morph="none" pos="word" start_char="966">researchers</TOKEN>
<TOKEN end_char="980" id="token-11-5" morph="none" pos="word" start_char="978">has</TOKEN>
<TOKEN end_char="988" id="token-11-6" morph="none" pos="word" start_char="982">grafted</TOKEN>
<TOKEN end_char="990" id="token-11-7" morph="none" pos="word" start_char="990">a</TOKEN>
<TOKEN end_char="998" id="token-11-8" morph="none" pos="word" start_char="992">protein</TOKEN>
<TOKEN end_char="1003" id="token-11-9" morph="none" pos="word" start_char="1000">from</TOKEN>
<TOKEN end_char="1008" id="token-11-10" morph="none" pos="word" start_char="1005">bats</TOKEN>
<TOKEN end_char="1013" id="token-11-11" morph="none" pos="word" start_char="1010">into</TOKEN>
<TOKEN end_char="1017" id="token-11-12" morph="none" pos="word" start_char="1015">the</TOKEN>
<TOKEN end_char="1022" id="token-11-13" morph="none" pos="word" start_char="1019">Sars</TOKEN>
<TOKEN end_char="1028" id="token-11-14" morph="none" pos="word" start_char="1024">virus</TOKEN>
<TOKEN end_char="1030" id="token-11-15" morph="none" pos="punct" start_char="1030">–</TOKEN>
<TOKEN end_char="1036" id="token-11-16" morph="none" pos="word" start_char="1032">acute</TOKEN>
<TOKEN end_char="1046" id="token-11-17" morph="none" pos="word" start_char="1038">pneumonia</TOKEN>
<TOKEN end_char="1048" id="token-11-18" morph="none" pos="punct" start_char="1048">–</TOKEN>
<TOKEN end_char="1054" id="token-11-19" morph="none" pos="word" start_char="1050">taken</TOKEN>
<TOKEN end_char="1059" id="token-11-20" morph="none" pos="word" start_char="1056">from</TOKEN>
<TOKEN end_char="1064" id="token-11-21" morph="none" pos="word" start_char="1061">mice</TOKEN>
<TOKEN end_char="1065" id="token-11-22" morph="none" pos="punct" start_char="1065">.</TOKEN>
</SEG>
<SEG end_char="1119" id="segment-12" start_char="1067">
<ORIGINAL_TEXT>The result is a super virus that could strike humans.</ORIGINAL_TEXT>
<TOKEN end_char="1069" id="token-12-0" morph="none" pos="word" start_char="1067">The</TOKEN>
<TOKEN end_char="1076" id="token-12-1" morph="none" pos="word" start_char="1071">result</TOKEN>
<TOKEN end_char="1079" id="token-12-2" morph="none" pos="word" start_char="1078">is</TOKEN>
<TOKEN end_char="1081" id="token-12-3" morph="none" pos="word" start_char="1081">a</TOKEN>
<TOKEN end_char="1087" id="token-12-4" morph="none" pos="word" start_char="1083">super</TOKEN>
<TOKEN end_char="1093" id="token-12-5" morph="none" pos="word" start_char="1089">virus</TOKEN>
<TOKEN end_char="1098" id="token-12-6" morph="none" pos="word" start_char="1095">that</TOKEN>
<TOKEN end_char="1104" id="token-12-7" morph="none" pos="word" start_char="1100">could</TOKEN>
<TOKEN end_char="1111" id="token-12-8" morph="none" pos="word" start_char="1106">strike</TOKEN>
<TOKEN end_char="1118" id="token-12-9" morph="none" pos="word" start_char="1113">humans</TOKEN>
<TOKEN end_char="1119" id="token-12-10" morph="none" pos="punct" start_char="1119">.</TOKEN>
</SEG>
<SEG end_char="1197" id="segment-13" start_char="1121">
<ORIGINAL_TEXT>The presenter immediately added that the virus "is confined in a laboratory".</ORIGINAL_TEXT>
<TOKEN end_char="1123" id="token-13-0" morph="none" pos="word" start_char="1121">The</TOKEN>
<TOKEN end_char="1133" id="token-13-1" morph="none" pos="word" start_char="1125">presenter</TOKEN>
<TOKEN end_char="1145" id="token-13-2" morph="none" pos="word" start_char="1135">immediately</TOKEN>
<TOKEN end_char="1151" id="token-13-3" morph="none" pos="word" start_char="1147">added</TOKEN>
<TOKEN end_char="1156" id="token-13-4" morph="none" pos="word" start_char="1153">that</TOKEN>
<TOKEN end_char="1160" id="token-13-5" morph="none" pos="word" start_char="1158">the</TOKEN>
<TOKEN end_char="1166" id="token-13-6" morph="none" pos="word" start_char="1162">virus</TOKEN>
<TOKEN end_char="1168" id="token-13-7" morph="none" pos="punct" start_char="1168">"</TOKEN>
<TOKEN end_char="1170" id="token-13-8" morph="none" pos="word" start_char="1169">is</TOKEN>
<TOKEN end_char="1179" id="token-13-9" morph="none" pos="word" start_char="1172">confined</TOKEN>
<TOKEN end_char="1182" id="token-13-10" morph="none" pos="word" start_char="1181">in</TOKEN>
<TOKEN end_char="1184" id="token-13-11" morph="none" pos="word" start_char="1184">a</TOKEN>
<TOKEN end_char="1195" id="token-13-12" morph="none" pos="word" start_char="1186">laboratory</TOKEN>
<TOKEN end_char="1197" id="token-13-13" morph="none" pos="punct" start_char="1196">".</TOKEN>
</SEG>
<SEG end_char="1304" id="segment-14" start_char="1200">
<ORIGINAL_TEXT>The report was relayed by Matteo Salvini, former interior minister and member of the League, on March 25.</ORIGINAL_TEXT>
<TOKEN end_char="1202" id="token-14-0" morph="none" pos="word" start_char="1200">The</TOKEN>
<TOKEN end_char="1209" id="token-14-1" morph="none" pos="word" start_char="1204">report</TOKEN>
<TOKEN end_char="1213" id="token-14-2" morph="none" pos="word" start_char="1211">was</TOKEN>
<TOKEN end_char="1221" id="token-14-3" morph="none" pos="word" start_char="1215">relayed</TOKEN>
<TOKEN end_char="1224" id="token-14-4" morph="none" pos="word" start_char="1223">by</TOKEN>
<TOKEN end_char="1231" id="token-14-5" morph="none" pos="word" start_char="1226">Matteo</TOKEN>
<TOKEN end_char="1239" id="token-14-6" morph="none" pos="word" start_char="1233">Salvini</TOKEN>
<TOKEN end_char="1240" id="token-14-7" morph="none" pos="punct" start_char="1240">,</TOKEN>
<TOKEN end_char="1247" id="token-14-8" morph="none" pos="word" start_char="1242">former</TOKEN>
<TOKEN end_char="1256" id="token-14-9" morph="none" pos="word" start_char="1249">interior</TOKEN>
<TOKEN end_char="1265" id="token-14-10" morph="none" pos="word" start_char="1258">minister</TOKEN>
<TOKEN end_char="1269" id="token-14-11" morph="none" pos="word" start_char="1267">and</TOKEN>
<TOKEN end_char="1276" id="token-14-12" morph="none" pos="word" start_char="1271">member</TOKEN>
<TOKEN end_char="1279" id="token-14-13" morph="none" pos="word" start_char="1278">of</TOKEN>
<TOKEN end_char="1283" id="token-14-14" morph="none" pos="word" start_char="1281">the</TOKEN>
<TOKEN end_char="1290" id="token-14-15" morph="none" pos="word" start_char="1285">League</TOKEN>
<TOKEN end_char="1291" id="token-14-16" morph="none" pos="punct" start_char="1291">,</TOKEN>
<TOKEN end_char="1294" id="token-14-17" morph="none" pos="word" start_char="1293">on</TOKEN>
<TOKEN end_char="1300" id="token-14-18" morph="none" pos="word" start_char="1296">March</TOKEN>
<TOKEN end_char="1303" id="token-14-19" morph="none" pos="word" start_char="1302">25</TOKEN>
<TOKEN end_char="1304" id="token-14-20" morph="none" pos="punct" start_char="1304">.</TOKEN>
</SEG>
<SEG end_char="1310" id="segment-15" start_char="1307">
<ORIGINAL_TEXT>????</ORIGINAL_TEXT>
<TOKEN end_char="1310" id="token-15-0" morph="none" pos="punct" start_char="1307">????</TOKEN>
</SEG>
<SEG end_char="1330" id="segment-16" start_char="1312">
<ORIGINAL_TEXT>INCREDIBILE !!! ???</ORIGINAL_TEXT>
<TOKEN end_char="1322" id="token-16-0" morph="none" pos="word" start_char="1312">INCREDIBILE</TOKEN>
<TOKEN end_char="1326" id="token-16-1" morph="none" pos="punct" start_char="1324">!!!</TOKEN>
<TOKEN end_char="1330" id="token-16-2" morph="none" pos="punct" start_char="1328">???</TOKEN>
<TRANSLATED_TEXT>INCREDIBLE!???</TRANSLATED_TEXT><DETECTED_LANGUAGE>uk</DETECTED_LANGUAGE></SEG>
<SEG end_char="1519" id="segment-17" start_char="1332">
<ORIGINAL_TEXT>?Da Tgr Leonardo (Rai Tre) del 16.11.2015 servizio su un supervirus polmonare Coronavirus creato dai cinesi con pipistrelli e topi, pericolosissimo per l’uomo (con annesse preoccupazioni).</ORIGINAL_TEXT>
<TOKEN end_char="1332" id="token-17-0" morph="none" pos="punct" start_char="1332">?</TOKEN>
<TOKEN end_char="1334" id="token-17-1" morph="none" pos="word" start_char="1333">Da</TOKEN>
<TOKEN end_char="1338" id="token-17-2" morph="none" pos="word" start_char="1336">Tgr</TOKEN>
<TOKEN end_char="1347" id="token-17-3" morph="none" pos="word" start_char="1340">Leonardo</TOKEN>
<TOKEN end_char="1349" id="token-17-4" morph="none" pos="punct" start_char="1349">(</TOKEN>
<TOKEN end_char="1352" id="token-17-5" morph="none" pos="word" start_char="1350">Rai</TOKEN>
<TOKEN end_char="1356" id="token-17-6" morph="none" pos="word" start_char="1354">Tre</TOKEN>
<TOKEN end_char="1357" id="token-17-7" morph="none" pos="punct" start_char="1357">)</TOKEN>
<TOKEN end_char="1361" id="token-17-8" morph="none" pos="word" start_char="1359">del</TOKEN>
<TOKEN end_char="1372" id="token-17-9" morph="none" pos="unknown" start_char="1363">16.11.2015</TOKEN>
<TOKEN end_char="1381" id="token-17-10" morph="none" pos="word" start_char="1374">servizio</TOKEN>
<TOKEN end_char="1384" id="token-17-11" morph="none" pos="word" start_char="1383">su</TOKEN>
<TOKEN end_char="1387" id="token-17-12" morph="none" pos="word" start_char="1386">un</TOKEN>
<TOKEN end_char="1398" id="token-17-13" morph="none" pos="word" start_char="1389">supervirus</TOKEN>
<TOKEN end_char="1408" id="token-17-14" morph="none" pos="word" start_char="1400">polmonare</TOKEN>
<TOKEN end_char="1420" id="token-17-15" morph="none" pos="word" start_char="1410">Coronavirus</TOKEN>
<TOKEN end_char="1427" id="token-17-16" morph="none" pos="word" start_char="1422">creato</TOKEN>
<TOKEN end_char="1431" id="token-17-17" morph="none" pos="word" start_char="1429">dai</TOKEN>
<TOKEN end_char="1438" id="token-17-18" morph="none" pos="word" start_char="1433">cinesi</TOKEN>
<TOKEN end_char="1442" id="token-17-19" morph="none" pos="word" start_char="1440">con</TOKEN>
<TOKEN end_char="1454" id="token-17-20" morph="none" pos="word" start_char="1444">pipistrelli</TOKEN>
<TOKEN end_char="1456" id="token-17-21" morph="none" pos="word" start_char="1456">e</TOKEN>
<TOKEN end_char="1461" id="token-17-22" morph="none" pos="word" start_char="1458">topi</TOKEN>
<TOKEN end_char="1462" id="token-17-23" morph="none" pos="punct" start_char="1462">,</TOKEN>
<TOKEN end_char="1478" id="token-17-24" morph="none" pos="word" start_char="1464">pericolosissimo</TOKEN>
<TOKEN end_char="1482" id="token-17-25" morph="none" pos="word" start_char="1480">per</TOKEN>
<TOKEN end_char="1489" id="token-17-26" morph="none" pos="word" start_char="1484">l’uomo</TOKEN>
<TOKEN end_char="1491" id="token-17-27" morph="none" pos="punct" start_char="1491">(</TOKEN>
<TOKEN end_char="1494" id="token-17-28" morph="none" pos="word" start_char="1492">con</TOKEN>
<TOKEN end_char="1502" id="token-17-29" morph="none" pos="word" start_char="1496">annesse</TOKEN>
<TOKEN end_char="1517" id="token-17-30" morph="none" pos="word" start_char="1504">preoccupazioni</TOKEN>
<TOKEN end_char="1519" id="token-17-31" morph="none" pos="punct" start_char="1518">).</TOKEN>
<TRANSLATED_TEXT>? From Tgr Leonardo (Rai Tre) of 16.11.2015 service on a lung supervirus Coronavirus created by the Chinese with bats and mice, extremely dangerous for man (with accompanying concerns).</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="1603" id="segment-18" start_char="1521">
<ORIGINAL_TEXT>(1/2) pic.twitter.com/QuLG07XdAt – Matteo Salvini (@matteosalvinimi) March 25, 2020</ORIGINAL_TEXT>
<TOKEN end_char="1521" id="token-18-0" morph="none" pos="punct" start_char="1521">(</TOKEN>
<TOKEN end_char="1524" id="token-18-1" morph="none" pos="unknown" start_char="1522">1/2</TOKEN>
<TOKEN end_char="1525" id="token-18-2" morph="none" pos="punct" start_char="1525">)</TOKEN>
<TOKEN end_char="1552" id="token-18-3" morph="none" pos="unknown" start_char="1527">pic.twitter.com/QuLG07XdAt</TOKEN>
<TOKEN end_char="1554" id="token-18-4" morph="none" pos="punct" start_char="1554">–</TOKEN>
<TOKEN end_char="1561" id="token-18-5" morph="none" pos="word" start_char="1556">Matteo</TOKEN>
<TOKEN end_char="1569" id="token-18-6" morph="none" pos="word" start_char="1563">Salvini</TOKEN>
<TOKEN end_char="1572" id="token-18-7" morph="none" pos="punct" start_char="1571">(@</TOKEN>
<TOKEN end_char="1587" id="token-18-8" morph="none" pos="word" start_char="1573">matteosalvinimi</TOKEN>
<TOKEN end_char="1588" id="token-18-9" morph="none" pos="punct" start_char="1588">)</TOKEN>
<TOKEN end_char="1594" id="token-18-10" morph="none" pos="word" start_char="1590">March</TOKEN>
<TOKEN end_char="1597" id="token-18-11" morph="none" pos="word" start_char="1596">25</TOKEN>
<TOKEN end_char="1598" id="token-18-12" morph="none" pos="punct" start_char="1598">,</TOKEN>
<TOKEN end_char="1603" id="token-18-13" morph="none" pos="word" start_char="1600">2020</TOKEN>
<TRANSLATED_TEXT>(1 / 2) pic.twitter.com / QuLG07XdAt - Matteo Salvini (@matteosalvinmi) March 25, 2020</TRANSLATED_TEXT><DETECTED_LANGUAGE>it</DETECTED_LANGUAGE></SEG>
<SEG end_char="1613" id="segment-19" start_char="1606">
<ORIGINAL_TEXT>FAKE OFF</ORIGINAL_TEXT>
<TOKEN end_char="1609" id="token-19-0" morph="none" pos="word" start_char="1606">FAKE</TOKEN>
<TOKEN end_char="1613" id="token-19-1" morph="none" pos="word" start_char="1611">OFF</TOKEN>
</SEG>
<SEG end_char="1668" id="segment-20" start_char="1616">
<ORIGINAL_TEXT>RAI report alluded to 2015 study published in journal</ORIGINAL_TEXT>
<TOKEN end_char="1618" id="token-20-0" morph="none" pos="word" start_char="1616">RAI</TOKEN>
<TOKEN end_char="1625" id="token-20-1" morph="none" pos="word" start_char="1620">report</TOKEN>
<TOKEN end_char="1633" id="token-20-2" morph="none" pos="word" start_char="1627">alluded</TOKEN>
<TOKEN end_char="1636" id="token-20-3" morph="none" pos="word" start_char="1635">to</TOKEN>
<TOKEN end_char="1641" id="token-20-4" morph="none" pos="word" start_char="1638">2015</TOKEN>
<TOKEN end_char="1647" id="token-20-5" morph="none" pos="word" start_char="1643">study</TOKEN>
<TOKEN end_char="1657" id="token-20-6" morph="none" pos="word" start_char="1649">published</TOKEN>
<TOKEN end_char="1660" id="token-20-7" morph="none" pos="word" start_char="1659">in</TOKEN>
<TOKEN end_char="1668" id="token-20-8" morph="none" pos="word" start_char="1662">journal</TOKEN>
</SEG>
<SEG end_char="1685" id="segment-21" start_char="1671">
<ORIGINAL_TEXT>Nature Medicine</ORIGINAL_TEXT>
<TOKEN end_char="1676" id="token-21-0" morph="none" pos="word" start_char="1671">Nature</TOKEN>
<TOKEN end_char="1685" id="token-21-1" morph="none" pos="word" start_char="1678">Medicine</TOKEN>
<TRANSLATED_TEXT>Naturmedicin</TRANSLATED_TEXT><DETECTED_LANGUAGE>ro</DETECTED_LANGUAGE></SEG>
<SEG end_char="1688" id="segment-22" start_char="1688">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN end_char="1688" id="token-22-0" morph="none" pos="punct" start_char="1688">.</TOKEN>
</SEG>
<SEG end_char="1905" id="segment-23" start_char="1690">
<ORIGINAL_TEXT>Carried out by researchers from the University of North Carolina, the Harvard School of Medicine and the Wuhan Institute of Virology, the study looked well at a virus created using the method described in the report.</ORIGINAL_TEXT>
<TOKEN end_char="1696" id="token-23-0" morph="none" pos="word" start_char="1690">Carried</TOKEN>
<TOKEN end_char="1700" id="token-23-1" morph="none" pos="word" start_char="1698">out</TOKEN>
<TOKEN end_char="1703" id="token-23-2" morph="none" pos="word" start_char="1702">by</TOKEN>
<TOKEN end_char="1715" id="token-23-3" morph="none" pos="word" start_char="1705">researchers</TOKEN>
<TOKEN end_char="1720" id="token-23-4" morph="none" pos="word" start_char="1717">from</TOKEN>
<TOKEN end_char="1724" id="token-23-5" morph="none" pos="word" start_char="1722">the</TOKEN>
<TOKEN end_char="1735" id="token-23-6" morph="none" pos="word" start_char="1726">University</TOKEN>
<TOKEN end_char="1738" id="token-23-7" morph="none" pos="word" start_char="1737">of</TOKEN>
<TOKEN end_char="1744" id="token-23-8" morph="none" pos="word" start_char="1740">North</TOKEN>
<TOKEN end_char="1753" id="token-23-9" morph="none" pos="word" start_char="1746">Carolina</TOKEN>
<TOKEN end_char="1754" id="token-23-10" morph="none" pos="punct" start_char="1754">,</TOKEN>
<TOKEN end_char="1758" id="token-23-11" morph="none" pos="word" start_char="1756">the</TOKEN>
<TOKEN end_char="1766" id="token-23-12" morph="none" pos="word" start_char="1760">Harvard</TOKEN>
<TOKEN end_char="1773" id="token-23-13" morph="none" pos="word" start_char="1768">School</TOKEN>
<TOKEN end_char="1776" id="token-23-14" morph="none" pos="word" start_char="1775">of</TOKEN>
<TOKEN end_char="1785" id="token-23-15" morph="none" pos="word" start_char="1778">Medicine</TOKEN>
<TOKEN end_char="1789" id="token-23-16" morph="none" pos="word" start_char="1787">and</TOKEN>
<TOKEN end_char="1793" id="token-23-17" morph="none" pos="word" start_char="1791">the</TOKEN>
<TOKEN end_char="1799" id="token-23-18" morph="none" pos="word" start_char="1795">Wuhan</TOKEN>
<TOKEN end_char="1809" id="token-23-19" morph="none" pos="word" start_char="1801">Institute</TOKEN>
<TOKEN end_char="1812" id="token-23-20" morph="none" pos="word" start_char="1811">of</TOKEN>
<TOKEN end_char="1821" id="token-23-21" morph="none" pos="word" start_char="1814">Virology</TOKEN>
<TOKEN end_char="1822" id="token-23-22" morph="none" pos="punct" start_char="1822">,</TOKEN>
<TOKEN end_char="1826" id="token-23-23" morph="none" pos="word" start_char="1824">the</TOKEN>
<TOKEN end_char="1832" id="token-23-24" morph="none" pos="word" start_char="1828">study</TOKEN>
<TOKEN end_char="1839" id="token-23-25" morph="none" pos="word" start_char="1834">looked</TOKEN>
<TOKEN end_char="1844" id="token-23-26" morph="none" pos="word" start_char="1841">well</TOKEN>
<TOKEN end_char="1847" id="token-23-27" morph="none" pos="word" start_char="1846">at</TOKEN>
<TOKEN end_char="1849" id="token-23-28" morph="none" pos="word" start_char="1849">a</TOKEN>
<TOKEN end_char="1855" id="token-23-29" morph="none" pos="word" start_char="1851">virus</TOKEN>
<TOKEN end_char="1863" id="token-23-30" morph="none" pos="word" start_char="1857">created</TOKEN>
<TOKEN end_char="1869" id="token-23-31" morph="none" pos="word" start_char="1865">using</TOKEN>
<TOKEN end_char="1873" id="token-23-32" morph="none" pos="word" start_char="1871">the</TOKEN>
<TOKEN end_char="1880" id="token-23-33" morph="none" pos="word" start_char="1875">method</TOKEN>
<TOKEN end_char="1890" id="token-23-34" morph="none" pos="word" start_char="1882">described</TOKEN>
<TOKEN end_char="1893" id="token-23-35" morph="none" pos="word" start_char="1892">in</TOKEN>
<TOKEN end_char="1897" id="token-23-36" morph="none" pos="word" start_char="1895">the</TOKEN>
<TOKEN end_char="1904" id="token-23-37" morph="none" pos="word" start_char="1899">report</TOKEN>
<TOKEN end_char="1905" id="token-23-38" morph="none" pos="punct" start_char="1905">.</TOKEN>
</SEG>
<SEG end_char="2135" id="segment-24" start_char="1908">
<ORIGINAL_TEXT>"The current virus is completely different from the Sars virus [qui a sévi en 2003] and the study virus, "Antonio Lanzavecchia, one of the researchers who participated in the study, said on March 26 in the same Leonardo program.</ORIGINAL_TEXT>
<TOKEN end_char="1908" id="token-24-0" morph="none" pos="punct" start_char="1908">"</TOKEN>
<TOKEN end_char="1911" id="token-24-1" morph="none" pos="word" start_char="1909">The</TOKEN>
<TOKEN end_char="1919" id="token-24-2" morph="none" pos="word" start_char="1913">current</TOKEN>
<TOKEN end_char="1925" id="token-24-3" morph="none" pos="word" start_char="1921">virus</TOKEN>
<TOKEN end_char="1928" id="token-24-4" morph="none" pos="word" start_char="1927">is</TOKEN>
<TOKEN end_char="1939" id="token-24-5" morph="none" pos="word" start_char="1930">completely</TOKEN>
<TOKEN end_char="1949" id="token-24-6" morph="none" pos="word" start_char="1941">different</TOKEN>
<TOKEN end_char="1954" id="token-24-7" morph="none" pos="word" start_char="1951">from</TOKEN>
<TOKEN end_char="1958" id="token-24-8" morph="none" pos="word" start_char="1956">the</TOKEN>
<TOKEN end_char="1963" id="token-24-9" morph="none" pos="word" start_char="1960">Sars</TOKEN>
<TOKEN end_char="1969" id="token-24-10" morph="none" pos="word" start_char="1965">virus</TOKEN>
<TOKEN end_char="1971" id="token-24-11" morph="none" pos="punct" start_char="1971">[</TOKEN>
<TOKEN end_char="1974" id="token-24-12" morph="none" pos="word" start_char="1972">qui</TOKEN>
<TOKEN end_char="1976" id="token-24-13" morph="none" pos="word" start_char="1976">a</TOKEN>
<TOKEN end_char="1981" id="token-24-14" morph="none" pos="word" start_char="1978">sévi</TOKEN>
<TOKEN end_char="1984" id="token-24-15" morph="none" pos="word" start_char="1983">en</TOKEN>
<TOKEN end_char="1989" id="token-24-16" morph="none" pos="word" start_char="1986">2003</TOKEN>
<TOKEN end_char="1990" id="token-24-17" morph="none" pos="punct" start_char="1990">]</TOKEN>
<TOKEN end_char="1994" id="token-24-18" morph="none" pos="word" start_char="1992">and</TOKEN>
<TOKEN end_char="1998" id="token-24-19" morph="none" pos="word" start_char="1996">the</TOKEN>
<TOKEN end_char="2004" id="token-24-20" morph="none" pos="word" start_char="2000">study</TOKEN>
<TOKEN end_char="2010" id="token-24-21" morph="none" pos="word" start_char="2006">virus</TOKEN>
<TOKEN end_char="2011" id="token-24-22" morph="none" pos="punct" start_char="2011">,</TOKEN>
<TOKEN end_char="2013" id="token-24-23" morph="none" pos="punct" start_char="2013">"</TOKEN>
<TOKEN end_char="2020" id="token-24-24" morph="none" pos="word" start_char="2014">Antonio</TOKEN>
<TOKEN end_char="2033" id="token-24-25" morph="none" pos="word" start_char="2022">Lanzavecchia</TOKEN>
<TOKEN end_char="2034" id="token-24-26" morph="none" pos="punct" start_char="2034">,</TOKEN>
<TOKEN end_char="2038" id="token-24-27" morph="none" pos="word" start_char="2036">one</TOKEN>
<TOKEN end_char="2041" id="token-24-28" morph="none" pos="word" start_char="2040">of</TOKEN>
<TOKEN end_char="2045" id="token-24-29" morph="none" pos="word" start_char="2043">the</TOKEN>
<TOKEN end_char="2057" id="token-24-30" morph="none" pos="word" start_char="2047">researchers</TOKEN>
<TOKEN end_char="2061" id="token-24-31" morph="none" pos="word" start_char="2059">who</TOKEN>
<TOKEN end_char="2074" id="token-24-32" morph="none" pos="word" start_char="2063">participated</TOKEN>
<TOKEN end_char="2077" id="token-24-33" morph="none" pos="word" start_char="2076">in</TOKEN>
<TOKEN end_char="2081" id="token-24-34" morph="none" pos="word" start_char="2079">the</TOKEN>
<TOKEN end_char="2087" id="token-24-35" morph="none" pos="word" start_char="2083">study</TOKEN>
<TOKEN end_char="2088" id="token-24-36" morph="none" pos="punct" start_char="2088">,</TOKEN>
<TOKEN end_char="2093" id="token-24-37" morph="none" pos="word" start_char="2090">said</TOKEN>
<TOKEN end_char="2096" id="token-24-38" morph="none" pos="word" start_char="2095">on</TOKEN>
<TOKEN end_char="2102" id="token-24-39" morph="none" pos="word" start_char="2098">March</TOKEN>
<TOKEN end_char="2105" id="token-24-40" morph="none" pos="word" start_char="2104">26</TOKEN>
<TOKEN end_char="2108" id="token-24-41" morph="none" pos="word" start_char="2107">in</TOKEN>
<TOKEN end_char="2112" id="token-24-42" morph="none" pos="word" start_char="2110">the</TOKEN>
<TOKEN end_char="2117" id="token-24-43" morph="none" pos="word" start_char="2114">same</TOKEN>
<TOKEN end_char="2126" id="token-24-44" morph="none" pos="word" start_char="2119">Leonardo</TOKEN>
<TOKEN end_char="2134" id="token-24-45" morph="none" pos="word" start_char="2128">program</TOKEN>
<TOKEN end_char="2135" id="token-24-46" morph="none" pos="punct" start_char="2135">.</TOKEN>
</SEG>
<SEG end_char="2236" id="segment-25" start_char="2137">
<ORIGINAL_TEXT>The Covid-19 and the 2015 virus "are completely different," he says, particularly in their sequence.</ORIGINAL_TEXT>
<TOKEN end_char="2139" id="token-25-0" morph="none" pos="word" start_char="2137">The</TOKEN>
<TOKEN end_char="2148" id="token-25-1" morph="none" pos="unknown" start_char="2141">Covid-19</TOKEN>
<TOKEN end_char="2152" id="token-25-2" morph="none" pos="word" start_char="2150">and</TOKEN>
<TOKEN end_char="2156" id="token-25-3" morph="none" pos="word" start_char="2154">the</TOKEN>
<TOKEN end_char="2161" id="token-25-4" morph="none" pos="word" start_char="2158">2015</TOKEN>
<TOKEN end_char="2167" id="token-25-5" morph="none" pos="word" start_char="2163">virus</TOKEN>
<TOKEN end_char="2169" id="token-25-6" morph="none" pos="punct" start_char="2169">"</TOKEN>
<TOKEN end_char="2172" id="token-25-7" morph="none" pos="word" start_char="2170">are</TOKEN>
<TOKEN end_char="2183" id="token-25-8" morph="none" pos="word" start_char="2174">completely</TOKEN>
<TOKEN end_char="2193" id="token-25-9" morph="none" pos="word" start_char="2185">different</TOKEN>
<TOKEN end_char="2195" id="token-25-10" morph="none" pos="punct" start_char="2194">,"</TOKEN>
<TOKEN end_char="2198" id="token-25-11" morph="none" pos="word" start_char="2197">he</TOKEN>
<TOKEN end_char="2203" id="token-25-12" morph="none" pos="word" start_char="2200">says</TOKEN>
<TOKEN end_char="2204" id="token-25-13" morph="none" pos="punct" start_char="2204">,</TOKEN>
<TOKEN end_char="2217" id="token-25-14" morph="none" pos="word" start_char="2206">particularly</TOKEN>
<TOKEN end_char="2220" id="token-25-15" morph="none" pos="word" start_char="2219">in</TOKEN>
<TOKEN end_char="2226" id="token-25-16" morph="none" pos="word" start_char="2222">their</TOKEN>
<TOKEN end_char="2235" id="token-25-17" morph="none" pos="word" start_char="2228">sequence</TOKEN>
<TOKEN end_char="2236" id="token-25-18" morph="none" pos="punct" start_char="2236">.</TOKEN>
</SEG>
<SEG end_char="2311" id="segment-26" start_char="2239">
<ORIGINAL_TEXT>Covid-19 "is not a laboratory construct or a purposely manipulated virus"</ORIGINAL_TEXT>
<TOKEN end_char="2246" id="token-26-0" morph="none" pos="unknown" start_char="2239">Covid-19</TOKEN>
<TOKEN end_char="2248" id="token-26-1" morph="none" pos="punct" start_char="2248">"</TOKEN>
<TOKEN end_char="2250" id="token-26-2" morph="none" pos="word" start_char="2249">is</TOKEN>
<TOKEN end_char="2254" id="token-26-3" morph="none" pos="word" start_char="2252">not</TOKEN>
<TOKEN end_char="2256" id="token-26-4" morph="none" pos="word" start_char="2256">a</TOKEN>
<TOKEN end_char="2267" id="token-26-5" morph="none" pos="word" start_char="2258">laboratory</TOKEN>
<TOKEN end_char="2277" id="token-26-6" morph="none" pos="word" start_char="2269">construct</TOKEN>
<TOKEN end_char="2280" id="token-26-7" morph="none" pos="word" start_char="2279">or</TOKEN>
<TOKEN end_char="2282" id="token-26-8" morph="none" pos="word" start_char="2282">a</TOKEN>
<TOKEN end_char="2292" id="token-26-9" morph="none" pos="word" start_char="2284">purposely</TOKEN>
<TOKEN end_char="2304" id="token-26-10" morph="none" pos="word" start_char="2294">manipulated</TOKEN>
<TOKEN end_char="2310" id="token-26-11" morph="none" pos="word" start_char="2306">virus</TOKEN>
<TOKEN end_char="2311" id="token-26-12" morph="none" pos="punct" start_char="2311">"</TOKEN>
</SEG>
<SEG end_char="2518" id="segment-27" start_char="2315">
<ORIGINAL_TEXT>In a note added to the 2015 study on March 30, the publisher said, "We are aware that this article is used as the basis for unverified theories that the new coronavirus that causes Covid-19 has been made.</ORIGINAL_TEXT>
<TOKEN end_char="2316" id="token-27-0" morph="none" pos="word" start_char="2315">In</TOKEN>
<TOKEN end_char="2318" id="token-27-1" morph="none" pos="word" start_char="2318">a</TOKEN>
<TOKEN end_char="2323" id="token-27-2" morph="none" pos="word" start_char="2320">note</TOKEN>
<TOKEN end_char="2329" id="token-27-3" morph="none" pos="word" start_char="2325">added</TOKEN>
<TOKEN end_char="2332" id="token-27-4" morph="none" pos="word" start_char="2331">to</TOKEN>
<TOKEN end_char="2336" id="token-27-5" morph="none" pos="word" start_char="2334">the</TOKEN>
<TOKEN end_char="2341" id="token-27-6" morph="none" pos="word" start_char="2338">2015</TOKEN>
<TOKEN end_char="2347" id="token-27-7" morph="none" pos="word" start_char="2343">study</TOKEN>
<TOKEN end_char="2350" id="token-27-8" morph="none" pos="word" start_char="2349">on</TOKEN>
<TOKEN end_char="2356" id="token-27-9" morph="none" pos="word" start_char="2352">March</TOKEN>
<TOKEN end_char="2359" id="token-27-10" morph="none" pos="word" start_char="2358">30</TOKEN>
<TOKEN end_char="2360" id="token-27-11" morph="none" pos="punct" start_char="2360">,</TOKEN>
<TOKEN end_char="2364" id="token-27-12" morph="none" pos="word" start_char="2362">the</TOKEN>
<TOKEN end_char="2374" id="token-27-13" morph="none" pos="word" start_char="2366">publisher</TOKEN>
<TOKEN end_char="2379" id="token-27-14" morph="none" pos="word" start_char="2376">said</TOKEN>
<TOKEN end_char="2380" id="token-27-15" morph="none" pos="punct" start_char="2380">,</TOKEN>
<TOKEN end_char="2382" id="token-27-16" morph="none" pos="punct" start_char="2382">"</TOKEN>
<TOKEN end_char="2384" id="token-27-17" morph="none" pos="word" start_char="2383">We</TOKEN>
<TOKEN end_char="2388" id="token-27-18" morph="none" pos="word" start_char="2386">are</TOKEN>
<TOKEN end_char="2394" id="token-27-19" morph="none" pos="word" start_char="2390">aware</TOKEN>
<TOKEN end_char="2399" id="token-27-20" morph="none" pos="word" start_char="2396">that</TOKEN>
<TOKEN end_char="2404" id="token-27-21" morph="none" pos="word" start_char="2401">this</TOKEN>
<TOKEN end_char="2412" id="token-27-22" morph="none" pos="word" start_char="2406">article</TOKEN>
<TOKEN end_char="2415" id="token-27-23" morph="none" pos="word" start_char="2414">is</TOKEN>
<TOKEN end_char="2420" id="token-27-24" morph="none" pos="word" start_char="2417">used</TOKEN>
<TOKEN end_char="2423" id="token-27-25" morph="none" pos="word" start_char="2422">as</TOKEN>
<TOKEN end_char="2427" id="token-27-26" morph="none" pos="word" start_char="2425">the</TOKEN>
<TOKEN end_char="2433" id="token-27-27" morph="none" pos="word" start_char="2429">basis</TOKEN>
<TOKEN end_char="2437" id="token-27-28" morph="none" pos="word" start_char="2435">for</TOKEN>
<TOKEN end_char="2448" id="token-27-29" morph="none" pos="word" start_char="2439">unverified</TOKEN>
<TOKEN end_char="2457" id="token-27-30" morph="none" pos="word" start_char="2450">theories</TOKEN>
<TOKEN end_char="2462" id="token-27-31" morph="none" pos="word" start_char="2459">that</TOKEN>
<TOKEN end_char="2466" id="token-27-32" morph="none" pos="word" start_char="2464">the</TOKEN>
<TOKEN end_char="2470" id="token-27-33" morph="none" pos="word" start_char="2468">new</TOKEN>
<TOKEN end_char="2482" id="token-27-34" morph="none" pos="word" start_char="2472">coronavirus</TOKEN>
<TOKEN end_char="2487" id="token-27-35" morph="none" pos="word" start_char="2484">that</TOKEN>
<TOKEN end_char="2494" id="token-27-36" morph="none" pos="word" start_char="2489">causes</TOKEN>
<TOKEN end_char="2503" id="token-27-37" morph="none" pos="unknown" start_char="2496">Covid-19</TOKEN>
<TOKEN end_char="2507" id="token-27-38" morph="none" pos="word" start_char="2505">has</TOKEN>
<TOKEN end_char="2512" id="token-27-39" morph="none" pos="word" start_char="2509">been</TOKEN>
<TOKEN end_char="2517" id="token-27-40" morph="none" pos="word" start_char="2514">made</TOKEN>
<TOKEN end_char="2518" id="token-27-41" morph="none" pos="punct" start_char="2518">.</TOKEN>
</SEG>
<SEG end_char="2636" id="segment-28" start_char="2520">
<ORIGINAL_TEXT>There is no evidence that this is true; scientists believe that an animal is the likely source of this coronavirus. "</ORIGINAL_TEXT>
<TOKEN end_char="2524" id="token-28-0" morph="none" pos="word" start_char="2520">There</TOKEN>
<TOKEN end_char="2527" id="token-28-1" morph="none" pos="word" start_char="2526">is</TOKEN>
<TOKEN end_char="2530" id="token-28-2" morph="none" pos="word" start_char="2529">no</TOKEN>
<TOKEN end_char="2539" id="token-28-3" morph="none" pos="word" start_char="2532">evidence</TOKEN>
<TOKEN end_char="2544" id="token-28-4" morph="none" pos="word" start_char="2541">that</TOKEN>
<TOKEN end_char="2549" id="token-28-5" morph="none" pos="word" start_char="2546">this</TOKEN>
<TOKEN end_char="2552" id="token-28-6" morph="none" pos="word" start_char="2551">is</TOKEN>
<TOKEN end_char="2557" id="token-28-7" morph="none" pos="word" start_char="2554">true</TOKEN>
<TOKEN end_char="2558" id="token-28-8" morph="none" pos="punct" start_char="2558">;</TOKEN>
<TOKEN end_char="2569" id="token-28-9" morph="none" pos="word" start_char="2560">scientists</TOKEN>
<TOKEN end_char="2577" id="token-28-10" morph="none" pos="word" start_char="2571">believe</TOKEN>
<TOKEN end_char="2582" id="token-28-11" morph="none" pos="word" start_char="2579">that</TOKEN>
<TOKEN end_char="2585" id="token-28-12" morph="none" pos="word" start_char="2584">an</TOKEN>
<TOKEN end_char="2592" id="token-28-13" morph="none" pos="word" start_char="2587">animal</TOKEN>
<TOKEN end_char="2595" id="token-28-14" morph="none" pos="word" start_char="2594">is</TOKEN>
<TOKEN end_char="2599" id="token-28-15" morph="none" pos="word" start_char="2597">the</TOKEN>
<TOKEN end_char="2606" id="token-28-16" morph="none" pos="word" start_char="2601">likely</TOKEN>
<TOKEN end_char="2613" id="token-28-17" morph="none" pos="word" start_char="2608">source</TOKEN>
<TOKEN end_char="2616" id="token-28-18" morph="none" pos="word" start_char="2615">of</TOKEN>
<TOKEN end_char="2621" id="token-28-19" morph="none" pos="word" start_char="2618">this</TOKEN>
<TOKEN end_char="2633" id="token-28-20" morph="none" pos="word" start_char="2623">coronavirus</TOKEN>
<TOKEN end_char="2634" id="token-28-21" morph="none" pos="punct" start_char="2634">.</TOKEN>
<TOKEN end_char="2636" id="token-28-22" morph="none" pos="punct" start_char="2636">"</TOKEN>
</SEG>
<SEG end_char="2817" id="segment-29" start_char="2639">
<ORIGINAL_TEXT>A study on the origin of Covid-19 published on March 17, 2020 in the same journal concluded that this new virus "is not a laboratory construct or a virus manipulated with design".</ORIGINAL_TEXT>
<TOKEN end_char="2639" id="token-29-0" morph="none" pos="word" start_char="2639">A</TOKEN>
<TOKEN end_char="2645" id="token-29-1" morph="none" pos="word" start_char="2641">study</TOKEN>
<TOKEN end_char="2648" id="token-29-2" morph="none" pos="word" start_char="2647">on</TOKEN>
<TOKEN end_char="2652" id="token-29-3" morph="none" pos="word" start_char="2650">the</TOKEN>
<TOKEN end_char="2659" id="token-29-4" morph="none" pos="word" start_char="2654">origin</TOKEN>
<TOKEN end_char="2662" id="token-29-5" morph="none" pos="word" start_char="2661">of</TOKEN>
<TOKEN end_char="2671" id="token-29-6" morph="none" pos="unknown" start_char="2664">Covid-19</TOKEN>
<TOKEN end_char="2681" id="token-29-7" morph="none" pos="word" start_char="2673">published</TOKEN>
<TOKEN end_char="2684" id="token-29-8" morph="none" pos="word" start_char="2683">on</TOKEN>
<TOKEN end_char="2690" id="token-29-9" morph="none" pos="word" start_char="2686">March</TOKEN>
<TOKEN end_char="2693" id="token-29-10" morph="none" pos="word" start_char="2692">17</TOKEN>
<TOKEN end_char="2694" id="token-29-11" morph="none" pos="punct" start_char="2694">,</TOKEN>
<TOKEN end_char="2699" id="token-29-12" morph="none" pos="word" start_char="2696">2020</TOKEN>
<TOKEN end_char="2702" id="token-29-13" morph="none" pos="word" start_char="2701">in</TOKEN>
<TOKEN end_char="2706" id="token-29-14" morph="none" pos="word" start_char="2704">the</TOKEN>
<TOKEN end_char="2711" id="token-29-15" morph="none" pos="word" start_char="2708">same</TOKEN>
<TOKEN end_char="2719" id="token-29-16" morph="none" pos="word" start_char="2713">journal</TOKEN>
<TOKEN end_char="2729" id="token-29-17" morph="none" pos="word" start_char="2721">concluded</TOKEN>
<TOKEN end_char="2734" id="token-29-18" morph="none" pos="word" start_char="2731">that</TOKEN>
<TOKEN end_char="2739" id="token-29-19" morph="none" pos="word" start_char="2736">this</TOKEN>
<TOKEN end_char="2743" id="token-29-20" morph="none" pos="word" start_char="2741">new</TOKEN>
<TOKEN end_char="2749" id="token-29-21" morph="none" pos="word" start_char="2745">virus</TOKEN>
<TOKEN end_char="2751" id="token-29-22" morph="none" pos="punct" start_char="2751">"</TOKEN>
<TOKEN end_char="2753" id="token-29-23" morph="none" pos="word" start_char="2752">is</TOKEN>
<TOKEN end_char="2757" id="token-29-24" morph="none" pos="word" start_char="2755">not</TOKEN>
<TOKEN end_char="2759" id="token-29-25" morph="none" pos="word" start_char="2759">a</TOKEN>
<TOKEN end_char="2770" id="token-29-26" morph="none" pos="word" start_char="2761">laboratory</TOKEN>
<TOKEN end_char="2780" id="token-29-27" morph="none" pos="word" start_char="2772">construct</TOKEN>
<TOKEN end_char="2783" id="token-29-28" morph="none" pos="word" start_char="2782">or</TOKEN>
<TOKEN end_char="2785" id="token-29-29" morph="none" pos="word" start_char="2785">a</TOKEN>
<TOKEN end_char="2791" id="token-29-30" morph="none" pos="word" start_char="2787">virus</TOKEN>
<TOKEN end_char="2803" id="token-29-31" morph="none" pos="word" start_char="2793">manipulated</TOKEN>
<TOKEN end_char="2808" id="token-29-32" morph="none" pos="word" start_char="2805">with</TOKEN>
<TOKEN end_char="2815" id="token-29-33" morph="none" pos="word" start_char="2810">design</TOKEN>
<TOKEN end_char="2817" id="token-29-34" morph="none" pos="punct" start_char="2816">".</TOKEN>
</SEG>
<SEG end_char="3028" id="segment-30" start_char="2820">
<ORIGINAL_TEXT>Olivier Schwartz, head of the virus and immunity unit at the Institut Pasteur, also recalled that scientists are able to distinguish if a virus has been manipulated in the laboratory by analyzing its sequence.</ORIGINAL_TEXT>
<TOKEN end_char="2826" id="token-30-0" morph="none" pos="word" start_char="2820">Olivier</TOKEN>
<TOKEN end_char="2835" id="token-30-1" morph="none" pos="word" start_char="2828">Schwartz</TOKEN>
<TOKEN end_char="2836" id="token-30-2" morph="none" pos="punct" start_char="2836">,</TOKEN>
<TOKEN end_char="2841" id="token-30-3" morph="none" pos="word" start_char="2838">head</TOKEN>
<TOKEN end_char="2844" id="token-30-4" morph="none" pos="word" start_char="2843">of</TOKEN>
<TOKEN end_char="2848" id="token-30-5" morph="none" pos="word" start_char="2846">the</TOKEN>
<TOKEN end_char="2854" id="token-30-6" morph="none" pos="word" start_char="2850">virus</TOKEN>
<TOKEN end_char="2858" id="token-30-7" morph="none" pos="word" start_char="2856">and</TOKEN>
<TOKEN end_char="2867" id="token-30-8" morph="none" pos="word" start_char="2860">immunity</TOKEN>
<TOKEN end_char="2872" id="token-30-9" morph="none" pos="word" start_char="2869">unit</TOKEN>
<TOKEN end_char="2875" id="token-30-10" morph="none" pos="word" start_char="2874">at</TOKEN>
<TOKEN end_char="2879" id="token-30-11" morph="none" pos="word" start_char="2877">the</TOKEN>
<TOKEN end_char="2888" id="token-30-12" morph="none" pos="word" start_char="2881">Institut</TOKEN>
<TOKEN end_char="2896" id="token-30-13" morph="none" pos="word" start_char="2890">Pasteur</TOKEN>
<TOKEN end_char="2897" id="token-30-14" morph="none" pos="punct" start_char="2897">,</TOKEN>
<TOKEN end_char="2902" id="token-30-15" morph="none" pos="word" start_char="2899">also</TOKEN>
<TOKEN end_char="2911" id="token-30-16" morph="none" pos="word" start_char="2904">recalled</TOKEN>
<TOKEN end_char="2916" id="token-30-17" morph="none" pos="word" start_char="2913">that</TOKEN>
<TOKEN end_char="2927" id="token-30-18" morph="none" pos="word" start_char="2918">scientists</TOKEN>
<TOKEN end_char="2931" id="token-30-19" morph="none" pos="word" start_char="2929">are</TOKEN>
<TOKEN end_char="2936" id="token-30-20" morph="none" pos="word" start_char="2933">able</TOKEN>
<TOKEN end_char="2939" id="token-30-21" morph="none" pos="word" start_char="2938">to</TOKEN>
<TOKEN end_char="2951" id="token-30-22" morph="none" pos="word" start_char="2941">distinguish</TOKEN>
<TOKEN end_char="2954" id="token-30-23" morph="none" pos="word" start_char="2953">if</TOKEN>
<TOKEN end_char="2956" id="token-30-24" morph="none" pos="word" start_char="2956">a</TOKEN>
<TOKEN end_char="2962" id="token-30-25" morph="none" pos="word" start_char="2958">virus</TOKEN>
<TOKEN end_char="2966" id="token-30-26" morph="none" pos="word" start_char="2964">has</TOKEN>
<TOKEN end_char="2971" id="token-30-27" morph="none" pos="word" start_char="2968">been</TOKEN>
<TOKEN end_char="2983" id="token-30-28" morph="none" pos="word" start_char="2973">manipulated</TOKEN>
<TOKEN end_char="2986" id="token-30-29" morph="none" pos="word" start_char="2985">in</TOKEN>
<TOKEN end_char="2990" id="token-30-30" morph="none" pos="word" start_char="2988">the</TOKEN>
<TOKEN end_char="3001" id="token-30-31" morph="none" pos="word" start_char="2992">laboratory</TOKEN>
<TOKEN end_char="3004" id="token-30-32" morph="none" pos="word" start_char="3003">by</TOKEN>
<TOKEN end_char="3014" id="token-30-33" morph="none" pos="word" start_char="3006">analyzing</TOKEN>
<TOKEN end_char="3018" id="token-30-34" morph="none" pos="word" start_char="3016">its</TOKEN>
<TOKEN end_char="3027" id="token-30-35" morph="none" pos="word" start_char="3020">sequence</TOKEN>
<TOKEN end_char="3028" id="token-30-36" morph="none" pos="punct" start_char="3028">.</TOKEN>
</SEG>
<SEG end_char="3163" id="segment-31" start_char="3030">
<ORIGINAL_TEXT>"We now know how to analyze very precisely [les séquences], by creating phylogenetic trees, he developed on March 11 with France Info.</ORIGINAL_TEXT>
<TOKEN end_char="3030" id="token-31-0" morph="none" pos="punct" start_char="3030">"</TOKEN>
<TOKEN end_char="3032" id="token-31-1" morph="none" pos="word" start_char="3031">We</TOKEN>
<TOKEN end_char="3036" id="token-31-2" morph="none" pos="word" start_char="3034">now</TOKEN>
<TOKEN end_char="3041" id="token-31-3" morph="none" pos="word" start_char="3038">know</TOKEN>
<TOKEN end_char="3045" id="token-31-4" morph="none" pos="word" start_char="3043">how</TOKEN>
<TOKEN end_char="3048" id="token-31-5" morph="none" pos="word" start_char="3047">to</TOKEN>
<TOKEN end_char="3056" id="token-31-6" morph="none" pos="word" start_char="3050">analyze</TOKEN>
<TOKEN end_char="3061" id="token-31-7" morph="none" pos="word" start_char="3058">very</TOKEN>
<TOKEN end_char="3071" id="token-31-8" morph="none" pos="word" start_char="3063">precisely</TOKEN>
<TOKEN end_char="3073" id="token-31-9" morph="none" pos="punct" start_char="3073">[</TOKEN>
<TOKEN end_char="3076" id="token-31-10" morph="none" pos="word" start_char="3074">les</TOKEN>
<TOKEN end_char="3086" id="token-31-11" morph="none" pos="word" start_char="3078">séquences</TOKEN>
<TOKEN end_char="3088" id="token-31-12" morph="none" pos="punct" start_char="3087">],</TOKEN>
<TOKEN end_char="3091" id="token-31-13" morph="none" pos="word" start_char="3090">by</TOKEN>
<TOKEN end_char="3100" id="token-31-14" morph="none" pos="word" start_char="3093">creating</TOKEN>
<TOKEN end_char="3113" id="token-31-15" morph="none" pos="word" start_char="3102">phylogenetic</TOKEN>
<TOKEN end_char="3119" id="token-31-16" morph="none" pos="word" start_char="3115">trees</TOKEN>
<TOKEN end_char="3120" id="token-31-17" morph="none" pos="punct" start_char="3120">,</TOKEN>
<TOKEN end_char="3123" id="token-31-18" morph="none" pos="word" start_char="3122">he</TOKEN>
<TOKEN end_char="3133" id="token-31-19" morph="none" pos="word" start_char="3125">developed</TOKEN>
<TOKEN end_char="3136" id="token-31-20" morph="none" pos="word" start_char="3135">on</TOKEN>
<TOKEN end_char="3142" id="token-31-21" morph="none" pos="word" start_char="3138">March</TOKEN>
<TOKEN end_char="3145" id="token-31-22" morph="none" pos="word" start_char="3144">11</TOKEN>
<TOKEN end_char="3150" id="token-31-23" morph="none" pos="word" start_char="3147">with</TOKEN>
<TOKEN end_char="3157" id="token-31-24" morph="none" pos="word" start_char="3152">France</TOKEN>
<TOKEN end_char="3162" id="token-31-25" morph="none" pos="word" start_char="3159">Info</TOKEN>
<TOKEN end_char="3163" id="token-31-26" morph="none" pos="punct" start_char="3163">.</TOKEN>
</SEG>
<SEG end_char="3360" id="segment-32" start_char="3165">
<ORIGINAL_TEXT>We can trace the origin of a virus and we know, in the case of this coronavirus [le Covid-19], that it comes from a virus that occurs naturally in some animals, in some bats, and also in pangolin.</ORIGINAL_TEXT>
<TOKEN end_char="3166" id="token-32-0" morph="none" pos="word" start_char="3165">We</TOKEN>
<TOKEN end_char="3170" id="token-32-1" morph="none" pos="word" start_char="3168">can</TOKEN>
<TOKEN end_char="3176" id="token-32-2" morph="none" pos="word" start_char="3172">trace</TOKEN>
<TOKEN end_char="3180" id="token-32-3" morph="none" pos="word" start_char="3178">the</TOKEN>
<TOKEN end_char="3187" id="token-32-4" morph="none" pos="word" start_char="3182">origin</TOKEN>
<TOKEN end_char="3190" id="token-32-5" morph="none" pos="word" start_char="3189">of</TOKEN>
<TOKEN end_char="3192" id="token-32-6" morph="none" pos="word" start_char="3192">a</TOKEN>
<TOKEN end_char="3198" id="token-32-7" morph="none" pos="word" start_char="3194">virus</TOKEN>
<TOKEN end_char="3202" id="token-32-8" morph="none" pos="word" start_char="3200">and</TOKEN>
<TOKEN end_char="3205" id="token-32-9" morph="none" pos="word" start_char="3204">we</TOKEN>
<TOKEN end_char="3210" id="token-32-10" morph="none" pos="word" start_char="3207">know</TOKEN>
<TOKEN end_char="3211" id="token-32-11" morph="none" pos="punct" start_char="3211">,</TOKEN>
<TOKEN end_char="3214" id="token-32-12" morph="none" pos="word" start_char="3213">in</TOKEN>
<TOKEN end_char="3218" id="token-32-13" morph="none" pos="word" start_char="3216">the</TOKEN>
<TOKEN end_char="3223" id="token-32-14" morph="none" pos="word" start_char="3220">case</TOKEN>
<TOKEN end_char="3226" id="token-32-15" morph="none" pos="word" start_char="3225">of</TOKEN>
<TOKEN end_char="3231" id="token-32-16" morph="none" pos="word" start_char="3228">this</TOKEN>
<TOKEN end_char="3243" id="token-32-17" morph="none" pos="word" start_char="3233">coronavirus</TOKEN>
<TOKEN end_char="3245" id="token-32-18" morph="none" pos="punct" start_char="3245">[</TOKEN>
<TOKEN end_char="3247" id="token-32-19" morph="none" pos="word" start_char="3246">le</TOKEN>
<TOKEN end_char="3256" id="token-32-20" morph="none" pos="unknown" start_char="3249">Covid-19</TOKEN>
<TOKEN end_char="3258" id="token-32-21" morph="none" pos="punct" start_char="3257">],</TOKEN>
<TOKEN end_char="3263" id="token-32-22" morph="none" pos="word" start_char="3260">that</TOKEN>
<TOKEN end_char="3266" id="token-32-23" morph="none" pos="word" start_char="3265">it</TOKEN>
<TOKEN end_char="3272" id="token-32-24" morph="none" pos="word" start_char="3268">comes</TOKEN>
<TOKEN end_char="3277" id="token-32-25" morph="none" pos="word" start_char="3274">from</TOKEN>
<TOKEN end_char="3279" id="token-32-26" morph="none" pos="word" start_char="3279">a</TOKEN>
<TOKEN end_char="3285" id="token-32-27" morph="none" pos="word" start_char="3281">virus</TOKEN>
<TOKEN end_char="3290" id="token-32-28" morph="none" pos="word" start_char="3287">that</TOKEN>
<TOKEN end_char="3297" id="token-32-29" morph="none" pos="word" start_char="3292">occurs</TOKEN>
<TOKEN end_char="3307" id="token-32-30" morph="none" pos="word" start_char="3299">naturally</TOKEN>
<TOKEN end_char="3310" id="token-32-31" morph="none" pos="word" start_char="3309">in</TOKEN>
<TOKEN end_char="3315" id="token-32-32" morph="none" pos="word" start_char="3312">some</TOKEN>
<TOKEN end_char="3323" id="token-32-33" morph="none" pos="word" start_char="3317">animals</TOKEN>
<TOKEN end_char="3324" id="token-32-34" morph="none" pos="punct" start_char="3324">,</TOKEN>
<TOKEN end_char="3327" id="token-32-35" morph="none" pos="word" start_char="3326">in</TOKEN>
<TOKEN end_char="3332" id="token-32-36" morph="none" pos="word" start_char="3329">some</TOKEN>
<TOKEN end_char="3337" id="token-32-37" morph="none" pos="word" start_char="3334">bats</TOKEN>
<TOKEN end_char="3338" id="token-32-38" morph="none" pos="punct" start_char="3338">,</TOKEN>
<TOKEN end_char="3342" id="token-32-39" morph="none" pos="word" start_char="3340">and</TOKEN>
<TOKEN end_char="3347" id="token-32-40" morph="none" pos="word" start_char="3344">also</TOKEN>
<TOKEN end_char="3350" id="token-32-41" morph="none" pos="word" start_char="3349">in</TOKEN>
<TOKEN end_char="3359" id="token-32-42" morph="none" pos="word" start_char="3352">pangolin</TOKEN>
<TOKEN end_char="3360" id="token-32-43" morph="none" pos="punct" start_char="3360">.</TOKEN>
</SEG>
<SEG end_char="3441" id="segment-33" start_char="3362">
<ORIGINAL_TEXT>In these animals, the virus can be found identical to 90 or almost 98% homology.</ORIGINAL_TEXT>
<TOKEN end_char="3363" id="token-33-0" morph="none" pos="word" start_char="3362">In</TOKEN>
<TOKEN end_char="3369" id="token-33-1" morph="none" pos="word" start_char="3365">these</TOKEN>
<TOKEN end_char="3377" id="token-33-2" morph="none" pos="word" start_char="3371">animals</TOKEN>
<TOKEN end_char="3378" id="token-33-3" morph="none" pos="punct" start_char="3378">,</TOKEN>
<TOKEN end_char="3382" id="token-33-4" morph="none" pos="word" start_char="3380">the</TOKEN>
<TOKEN end_char="3388" id="token-33-5" morph="none" pos="word" start_char="3384">virus</TOKEN>
<TOKEN end_char="3392" id="token-33-6" morph="none" pos="word" start_char="3390">can</TOKEN>
<TOKEN end_char="3395" id="token-33-7" morph="none" pos="word" start_char="3394">be</TOKEN>
<TOKEN end_char="3401" id="token-33-8" morph="none" pos="word" start_char="3397">found</TOKEN>
<TOKEN end_char="3411" id="token-33-9" morph="none" pos="word" start_char="3403">identical</TOKEN>
<TOKEN end_char="3414" id="token-33-10" morph="none" pos="word" start_char="3413">to</TOKEN>
<TOKEN end_char="3417" id="token-33-11" morph="none" pos="word" start_char="3416">90</TOKEN>
<TOKEN end_char="3420" id="token-33-12" morph="none" pos="word" start_char="3419">or</TOKEN>
<TOKEN end_char="3427" id="token-33-13" morph="none" pos="word" start_char="3422">almost</TOKEN>
<TOKEN end_char="3430" id="token-33-14" morph="none" pos="word" start_char="3429">98</TOKEN>
<TOKEN end_char="3431" id="token-33-15" morph="none" pos="punct" start_char="3431">%</TOKEN>
<TOKEN end_char="3440" id="token-33-16" morph="none" pos="word" start_char="3433">homology</TOKEN>
<TOKEN end_char="3441" id="token-33-17" morph="none" pos="punct" start_char="3441">.</TOKEN>
</SEG>
<SEG end_char="3529" id="segment-34" start_char="3443">
<ORIGINAL_TEXT>There, we know that the virus has passed directly from the bat or pangolin to humans. "</ORIGINAL_TEXT>
<TOKEN end_char="3447" id="token-34-0" morph="none" pos="word" start_char="3443">There</TOKEN>
<TOKEN end_char="3448" id="token-34-1" morph="none" pos="punct" start_char="3448">,</TOKEN>
<TOKEN end_char="3451" id="token-34-2" morph="none" pos="word" start_char="3450">we</TOKEN>
<TOKEN end_char="3456" id="token-34-3" morph="none" pos="word" start_char="3453">know</TOKEN>
<TOKEN end_char="3461" id="token-34-4" morph="none" pos="word" start_char="3458">that</TOKEN>
<TOKEN end_char="3465" id="token-34-5" morph="none" pos="word" start_char="3463">the</TOKEN>
<TOKEN end_char="3471" id="token-34-6" morph="none" pos="word" start_char="3467">virus</TOKEN>
<TOKEN end_char="3475" id="token-34-7" morph="none" pos="word" start_char="3473">has</TOKEN>
<TOKEN end_char="3482" id="token-34-8" morph="none" pos="word" start_char="3477">passed</TOKEN>
<TOKEN end_char="3491" id="token-34-9" morph="none" pos="word" start_char="3484">directly</TOKEN>
<TOKEN end_char="3496" id="token-34-10" morph="none" pos="word" start_char="3493">from</TOKEN>
<TOKEN end_char="3500" id="token-34-11" morph="none" pos="word" start_char="3498">the</TOKEN>
<TOKEN end_char="3504" id="token-34-12" morph="none" pos="word" start_char="3502">bat</TOKEN>
<TOKEN end_char="3507" id="token-34-13" morph="none" pos="word" start_char="3506">or</TOKEN>
<TOKEN end_char="3516" id="token-34-14" morph="none" pos="word" start_char="3509">pangolin</TOKEN>
<TOKEN end_char="3519" id="token-34-15" morph="none" pos="word" start_char="3518">to</TOKEN>
<TOKEN end_char="3526" id="token-34-16" morph="none" pos="word" start_char="3521">humans</TOKEN>
<TOKEN end_char="3527" id="token-34-17" morph="none" pos="punct" start_char="3527">.</TOKEN>
<TOKEN end_char="3529" id="token-34-18" morph="none" pos="punct" start_char="3529">"</TOKEN>
</SEG>
<SEG end_char="3578" id="segment-35" start_char="3532">
<ORIGINAL_TEXT>This is what was also recalled on a daily basis</ORIGINAL_TEXT>
<TOKEN end_char="3535" id="token-35-0" morph="none" pos="word" start_char="3532">This</TOKEN>
<TOKEN end_char="3538" id="token-35-1" morph="none" pos="word" start_char="3537">is</TOKEN>
<TOKEN end_char="3543" id="token-35-2" morph="none" pos="word" start_char="3540">what</TOKEN>
<TOKEN end_char="3547" id="token-35-3" morph="none" pos="word" start_char="3545">was</TOKEN>
<TOKEN end_char="3552" id="token-35-4" morph="none" pos="word" start_char="3549">also</TOKEN>
<TOKEN end_char="3561" id="token-35-5" morph="none" pos="word" start_char="3554">recalled</TOKEN>
<TOKEN end_char="3564" id="token-35-6" morph="none" pos="word" start_char="3563">on</TOKEN>
<TOKEN end_char="3566" id="token-35-7" morph="none" pos="word" start_char="3566">a</TOKEN>
<TOKEN end_char="3572" id="token-35-8" morph="none" pos="word" start_char="3568">daily</TOKEN>
<TOKEN end_char="3578" id="token-35-9" morph="none" pos="word" start_char="3574">basis</TOKEN>
</SEG>
<SEG end_char="3593" id="segment-36" start_char="3581">
<ORIGINAL_TEXT>La Repubblica</ORIGINAL_TEXT>
<TOKEN end_char="3582" id="token-36-0" morph="none" pos="word" start_char="3581">La</TOKEN>
<TOKEN end_char="3593" id="token-36-1" morph="none" pos="word" start_char="3584">Repubblica</TOKEN>
</SEG>
<SEG end_char="3800" id="segment-37" start_char="3596">
<ORIGINAL_TEXT>Fausto Baldanti, professor at the University of Pavia: "A natural virus and a virus created in the laboratory stand out perfectly", before adding: "The experiment of 2015 took place before everyone’s eyes.</ORIGINAL_TEXT>
<TOKEN end_char="3601" id="token-37-0" morph="none" pos="word" start_char="3596">Fausto</TOKEN>
<TOKEN end_char="3610" id="token-37-1" morph="none" pos="word" start_char="3603">Baldanti</TOKEN>
<TOKEN end_char="3611" id="token-37-2" morph="none" pos="punct" start_char="3611">,</TOKEN>
<TOKEN end_char="3621" id="token-37-3" morph="none" pos="word" start_char="3613">professor</TOKEN>
<TOKEN end_char="3624" id="token-37-4" morph="none" pos="word" start_char="3623">at</TOKEN>
<TOKEN end_char="3628" id="token-37-5" morph="none" pos="word" start_char="3626">the</TOKEN>
<TOKEN end_char="3639" id="token-37-6" morph="none" pos="word" start_char="3630">University</TOKEN>
<TOKEN end_char="3642" id="token-37-7" morph="none" pos="word" start_char="3641">of</TOKEN>
<TOKEN end_char="3648" id="token-37-8" morph="none" pos="word" start_char="3644">Pavia</TOKEN>
<TOKEN end_char="3649" id="token-37-9" morph="none" pos="punct" start_char="3649">:</TOKEN>
<TOKEN end_char="3651" id="token-37-10" morph="none" pos="punct" start_char="3651">"</TOKEN>
<TOKEN end_char="3652" id="token-37-11" morph="none" pos="word" start_char="3652">A</TOKEN>
<TOKEN end_char="3660" id="token-37-12" morph="none" pos="word" start_char="3654">natural</TOKEN>
<TOKEN end_char="3666" id="token-37-13" morph="none" pos="word" start_char="3662">virus</TOKEN>
<TOKEN end_char="3670" id="token-37-14" morph="none" pos="word" start_char="3668">and</TOKEN>
<TOKEN end_char="3672" id="token-37-15" morph="none" pos="word" start_char="3672">a</TOKEN>
<TOKEN end_char="3678" id="token-37-16" morph="none" pos="word" start_char="3674">virus</TOKEN>
<TOKEN end_char="3686" id="token-37-17" morph="none" pos="word" start_char="3680">created</TOKEN>
<TOKEN end_char="3689" id="token-37-18" morph="none" pos="word" start_char="3688">in</TOKEN>
<TOKEN end_char="3693" id="token-37-19" morph="none" pos="word" start_char="3691">the</TOKEN>
<TOKEN end_char="3704" id="token-37-20" morph="none" pos="word" start_char="3695">laboratory</TOKEN>
<TOKEN end_char="3710" id="token-37-21" morph="none" pos="word" start_char="3706">stand</TOKEN>
<TOKEN end_char="3714" id="token-37-22" morph="none" pos="word" start_char="3712">out</TOKEN>
<TOKEN end_char="3724" id="token-37-23" morph="none" pos="word" start_char="3716">perfectly</TOKEN>
<TOKEN end_char="3726" id="token-37-24" morph="none" pos="punct" start_char="3725">",</TOKEN>
<TOKEN end_char="3733" id="token-37-25" morph="none" pos="word" start_char="3728">before</TOKEN>
<TOKEN end_char="3740" id="token-37-26" morph="none" pos="word" start_char="3735">adding</TOKEN>
<TOKEN end_char="3741" id="token-37-27" morph="none" pos="punct" start_char="3741">:</TOKEN>
<TOKEN end_char="3743" id="token-37-28" morph="none" pos="punct" start_char="3743">"</TOKEN>
<TOKEN end_char="3746" id="token-37-29" morph="none" pos="word" start_char="3744">The</TOKEN>
<TOKEN end_char="3757" id="token-37-30" morph="none" pos="word" start_char="3748">experiment</TOKEN>
<TOKEN end_char="3760" id="token-37-31" morph="none" pos="word" start_char="3759">of</TOKEN>
<TOKEN end_char="3765" id="token-37-32" morph="none" pos="word" start_char="3762">2015</TOKEN>
<TOKEN end_char="3770" id="token-37-33" morph="none" pos="word" start_char="3767">took</TOKEN>
<TOKEN end_char="3776" id="token-37-34" morph="none" pos="word" start_char="3772">place</TOKEN>
<TOKEN end_char="3783" id="token-37-35" morph="none" pos="word" start_char="3778">before</TOKEN>
<TOKEN end_char="3794" id="token-37-36" morph="none" pos="word" start_char="3785">everyone’s</TOKEN>
<TOKEN end_char="3799" id="token-37-37" morph="none" pos="word" start_char="3796">eyes</TOKEN>
<TOKEN end_char="3800" id="token-37-38" morph="none" pos="punct" start_char="3800">.</TOKEN>
</SEG>
<SEG end_char="3869" id="segment-38" start_char="3802">
<ORIGINAL_TEXT>The genome of this microorganism has been published in its entirety.</ORIGINAL_TEXT>
<TOKEN end_char="3804" id="token-38-0" morph="none" pos="word" start_char="3802">The</TOKEN>
<TOKEN end_char="3811" id="token-38-1" morph="none" pos="word" start_char="3806">genome</TOKEN>
<TOKEN end_char="3814" id="token-38-2" morph="none" pos="word" start_char="3813">of</TOKEN>
<TOKEN end_char="3819" id="token-38-3" morph="none" pos="word" start_char="3816">this</TOKEN>
<TOKEN end_char="3833" id="token-38-4" morph="none" pos="word" start_char="3821">microorganism</TOKEN>
<TOKEN end_char="3837" id="token-38-5" morph="none" pos="word" start_char="3835">has</TOKEN>
<TOKEN end_char="3842" id="token-38-6" morph="none" pos="word" start_char="3839">been</TOKEN>
<TOKEN end_char="3852" id="token-38-7" morph="none" pos="word" start_char="3844">published</TOKEN>
<TOKEN end_char="3855" id="token-38-8" morph="none" pos="word" start_char="3854">in</TOKEN>
<TOKEN end_char="3859" id="token-38-9" morph="none" pos="word" start_char="3857">its</TOKEN>
<TOKEN end_char="3868" id="token-38-10" morph="none" pos="word" start_char="3861">entirety</TOKEN>
<TOKEN end_char="3869" id="token-38-11" morph="none" pos="punct" start_char="3869">.</TOKEN>
</SEG>
<SEG end_char="3928" id="segment-39" start_char="3871">
<ORIGINAL_TEXT>And this one is not the same as the current coronavirus. "</ORIGINAL_TEXT>
<TOKEN end_char="3873" id="token-39-0" morph="none" pos="word" start_char="3871">And</TOKEN>
<TOKEN end_char="3878" id="token-39-1" morph="none" pos="word" start_char="3875">this</TOKEN>
<TOKEN end_char="3882" id="token-39-2" morph="none" pos="word" start_char="3880">one</TOKEN>
<TOKEN end_char="3885" id="token-39-3" morph="none" pos="word" start_char="3884">is</TOKEN>
<TOKEN end_char="3889" id="token-39-4" morph="none" pos="word" start_char="3887">not</TOKEN>
<TOKEN end_char="3893" id="token-39-5" morph="none" pos="word" start_char="3891">the</TOKEN>
<TOKEN end_char="3898" id="token-39-6" morph="none" pos="word" start_char="3895">same</TOKEN>
<TOKEN end_char="3901" id="token-39-7" morph="none" pos="word" start_char="3900">as</TOKEN>
<TOKEN end_char="3905" id="token-39-8" morph="none" pos="word" start_char="3903">the</TOKEN>
<TOKEN end_char="3913" id="token-39-9" morph="none" pos="word" start_char="3907">current</TOKEN>
<TOKEN end_char="3925" id="token-39-10" morph="none" pos="word" start_char="3915">coronavirus</TOKEN>
<TOKEN end_char="3926" id="token-39-11" morph="none" pos="punct" start_char="3926">.</TOKEN>
<TOKEN end_char="3928" id="token-39-12" morph="none" pos="punct" start_char="3928">"</TOKEN>
</SEG>
<SEG end_char="3987" id="segment-40" start_char="3931">
<ORIGINAL_TEXT>This request for verifications was sent to us by readers.</ORIGINAL_TEXT>
<TOKEN end_char="3934" id="token-40-0" morph="none" pos="word" start_char="3931">This</TOKEN>
<TOKEN end_char="3942" id="token-40-1" morph="none" pos="word" start_char="3936">request</TOKEN>
<TOKEN end_char="3946" id="token-40-2" morph="none" pos="word" start_char="3944">for</TOKEN>
<TOKEN end_char="3960" id="token-40-3" morph="none" pos="word" start_char="3948">verifications</TOKEN>
<TOKEN end_char="3964" id="token-40-4" morph="none" pos="word" start_char="3962">was</TOKEN>
<TOKEN end_char="3969" id="token-40-5" morph="none" pos="word" start_char="3966">sent</TOKEN>
<TOKEN end_char="3972" id="token-40-6" morph="none" pos="word" start_char="3971">to</TOKEN>
<TOKEN end_char="3975" id="token-40-7" morph="none" pos="word" start_char="3974">us</TOKEN>
<TOKEN end_char="3978" id="token-40-8" morph="none" pos="word" start_char="3977">by</TOKEN>
<TOKEN end_char="3986" id="token-40-9" morph="none" pos="word" start_char="3980">readers</TOKEN>
<TOKEN end_char="3987" id="token-40-10" morph="none" pos="punct" start_char="3987">.</TOKEN>
</SEG>
<SEG end_char="4056" id="segment-41" start_char="3989">
<ORIGINAL_TEXT>Do you also want the Fake off team to check an info, photo or video?</ORIGINAL_TEXT>
<TOKEN end_char="3990" id="token-41-0" morph="none" pos="word" start_char="3989">Do</TOKEN>
<TOKEN end_char="3994" id="token-41-1" morph="none" pos="word" start_char="3992">you</TOKEN>
<TOKEN end_char="3999" id="token-41-2" morph="none" pos="word" start_char="3996">also</TOKEN>
<TOKEN end_char="4004" id="token-41-3" morph="none" pos="word" start_char="4001">want</TOKEN>
<TOKEN end_char="4008" id="token-41-4" morph="none" pos="word" start_char="4006">the</TOKEN>
<TOKEN end_char="4013" id="token-41-5" morph="none" pos="word" start_char="4010">Fake</TOKEN>
<TOKEN end_char="4017" id="token-41-6" morph="none" pos="word" start_char="4015">off</TOKEN>
<TOKEN end_char="4022" id="token-41-7" morph="none" pos="word" start_char="4019">team</TOKEN>
<TOKEN end_char="4025" id="token-41-8" morph="none" pos="word" start_char="4024">to</TOKEN>
<TOKEN end_char="4031" id="token-41-9" morph="none" pos="word" start_char="4027">check</TOKEN>
<TOKEN end_char="4034" id="token-41-10" morph="none" pos="word" start_char="4033">an</TOKEN>
<TOKEN end_char="4039" id="token-41-11" morph="none" pos="word" start_char="4036">info</TOKEN>
<TOKEN end_char="4040" id="token-41-12" morph="none" pos="punct" start_char="4040">,</TOKEN>
<TOKEN end_char="4046" id="token-41-13" morph="none" pos="word" start_char="4042">photo</TOKEN>
<TOKEN end_char="4049" id="token-41-14" morph="none" pos="word" start_char="4048">or</TOKEN>
<TOKEN end_char="4055" id="token-41-15" morph="none" pos="word" start_char="4051">video</TOKEN>
<TOKEN end_char="4056" id="token-41-16" morph="none" pos="punct" start_char="4056">?</TOKEN>
</SEG>
<SEG end_char="4140" id="segment-42" start_char="4058">
<ORIGINAL_TEXT>Fill out the form below or write to us on Twitter: https://twitter.com/20minFakeOff</ORIGINAL_TEXT>
<TOKEN end_char="4061" id="token-42-0" morph="none" pos="word" start_char="4058">Fill</TOKEN>
<TOKEN end_char="4065" id="token-42-1" morph="none" pos="word" start_char="4063">out</TOKEN>
<TOKEN end_char="4069" id="token-42-2" morph="none" pos="word" start_char="4067">the</TOKEN>
<TOKEN end_char="4074" id="token-42-3" morph="none" pos="word" start_char="4071">form</TOKEN>
<TOKEN end_char="4080" id="token-42-4" morph="none" pos="word" start_char="4076">below</TOKEN>
<TOKEN end_char="4083" id="token-42-5" morph="none" pos="word" start_char="4082">or</TOKEN>
<TOKEN end_char="4089" id="token-42-6" morph="none" pos="word" start_char="4085">write</TOKEN>
<TOKEN end_char="4092" id="token-42-7" morph="none" pos="word" start_char="4091">to</TOKEN>
<TOKEN end_char="4095" id="token-42-8" morph="none" pos="word" start_char="4094">us</TOKEN>
<TOKEN end_char="4098" id="token-42-9" morph="none" pos="word" start_char="4097">on</TOKEN>
<TOKEN end_char="4106" id="token-42-10" morph="none" pos="word" start_char="4100">Twitter</TOKEN>
<TOKEN end_char="4107" id="token-42-11" morph="none" pos="punct" start_char="4107">:</TOKEN>
<TOKEN end_char="4140" id="token-42-12" morph="none" pos="url" start_char="4109">https://twitter.com/20minFakeOff</TOKEN>
</SEG>
<SEG end_char="4152" id="segment-43" start_char="4143">
<ORIGINAL_TEXT>20 minutes</ORIGINAL_TEXT>
<TOKEN end_char="4144" id="token-43-0" morph="none" pos="word" start_char="4143">20</TOKEN>
<TOKEN end_char="4152" id="token-43-1" morph="none" pos="word" start_char="4146">minutes</TOKEN>
<TRANSLATED_TEXT>20 minutit</TRANSLATED_TEXT><DETECTED_LANGUAGE>et</DETECTED_LANGUAGE></SEG>
<SEG end_char="4204" id="segment-44" start_char="4155">
<ORIGINAL_TEXT>East Facebook partner to fight against false news.</ORIGINAL_TEXT>
<TOKEN end_char="4158" id="token-44-0" morph="none" pos="word" start_char="4155">East</TOKEN>
<TOKEN end_char="4167" id="token-44-1" morph="none" pos="word" start_char="4160">Facebook</TOKEN>
<TOKEN end_char="4175" id="token-44-2" morph="none" pos="word" start_char="4169">partner</TOKEN>
<TOKEN end_char="4178" id="token-44-3" morph="none" pos="word" start_char="4177">to</TOKEN>
<TOKEN end_char="4184" id="token-44-4" morph="none" pos="word" start_char="4180">fight</TOKEN>
<TOKEN end_char="4192" id="token-44-5" morph="none" pos="word" start_char="4186">against</TOKEN>
<TOKEN end_char="4198" id="token-44-6" morph="none" pos="word" start_char="4194">false</TOKEN>
<TOKEN end_char="4203" id="token-44-7" morph="none" pos="word" start_char="4200">news</TOKEN>
<TOKEN end_char="4204" id="token-44-8" morph="none" pos="punct" start_char="4204">.</TOKEN>
</SEG>
<SEG end_char="4309" id="segment-45" start_char="4206">
<ORIGINAL_TEXT>Thanks to this device, users of the social network may report information that they believe to be false.</ORIGINAL_TEXT>
<TOKEN end_char="4211" id="token-45-0" morph="none" pos="word" start_char="4206">Thanks</TOKEN>
<TOKEN end_char="4214" id="token-45-1" morph="none" pos="word" start_char="4213">to</TOKEN>
<TOKEN end_char="4219" id="token-45-2" morph="none" pos="word" start_char="4216">this</TOKEN>
<TOKEN end_char="4226" id="token-45-3" morph="none" pos="word" start_char="4221">device</TOKEN>
<TOKEN end_char="4227" id="token-45-4" morph="none" pos="punct" start_char="4227">,</TOKEN>
<TOKEN end_char="4233" id="token-45-5" morph="none" pos="word" start_char="4229">users</TOKEN>
<TOKEN end_char="4236" id="token-45-6" morph="none" pos="word" start_char="4235">of</TOKEN>
<TOKEN end_char="4240" id="token-45-7" morph="none" pos="word" start_char="4238">the</TOKEN>
<TOKEN end_char="4247" id="token-45-8" morph="none" pos="word" start_char="4242">social</TOKEN>
<TOKEN end_char="4255" id="token-45-9" morph="none" pos="word" start_char="4249">network</TOKEN>
<TOKEN end_char="4259" id="token-45-10" morph="none" pos="word" start_char="4257">may</TOKEN>
<TOKEN end_char="4266" id="token-45-11" morph="none" pos="word" start_char="4261">report</TOKEN>
<TOKEN end_char="4278" id="token-45-12" morph="none" pos="word" start_char="4268">information</TOKEN>
<TOKEN end_char="4283" id="token-45-13" morph="none" pos="word" start_char="4280">that</TOKEN>
<TOKEN end_char="4288" id="token-45-14" morph="none" pos="word" start_char="4285">they</TOKEN>
<TOKEN end_char="4296" id="token-45-15" morph="none" pos="word" start_char="4290">believe</TOKEN>
<TOKEN end_char="4299" id="token-45-16" morph="none" pos="word" start_char="4298">to</TOKEN>
<TOKEN end_char="4302" id="token-45-17" morph="none" pos="word" start_char="4301">be</TOKEN>
<TOKEN end_char="4308" id="token-45-18" morph="none" pos="word" start_char="4304">false</TOKEN>
<TOKEN end_char="4309" id="token-45-19" morph="none" pos="punct" start_char="4309">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>