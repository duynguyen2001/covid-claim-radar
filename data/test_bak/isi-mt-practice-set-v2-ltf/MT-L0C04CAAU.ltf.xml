<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAAU" lang="spa" raw_text_char_length="5655" raw_text_md5="f750e0652f24fa7538dc186eb7cd0046" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="81" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Italian Study Doesn't Contradict Theory That COVID-19 Came From Wuhan: Researcher</ORIGINAL_TEXT>
<TOKEN end_char="7" id="token-0-0" morph="none" pos="word" start_char="1">Italian</TOKEN>
<TOKEN end_char="13" id="token-0-1" morph="none" pos="word" start_char="9">Study</TOKEN>
<TOKEN end_char="21" id="token-0-2" morph="none" pos="word" start_char="15">Doesn't</TOKEN>
<TOKEN end_char="32" id="token-0-3" morph="none" pos="word" start_char="23">Contradict</TOKEN>
<TOKEN end_char="39" id="token-0-4" morph="none" pos="word" start_char="34">Theory</TOKEN>
<TOKEN end_char="44" id="token-0-5" morph="none" pos="word" start_char="41">That</TOKEN>
<TOKEN end_char="53" id="token-0-6" morph="none" pos="unknown" start_char="46">COVID-19</TOKEN>
<TOKEN end_char="58" id="token-0-7" morph="none" pos="word" start_char="55">Came</TOKEN>
<TOKEN end_char="63" id="token-0-8" morph="none" pos="word" start_char="60">From</TOKEN>
<TOKEN end_char="69" id="token-0-9" morph="none" pos="word" start_char="65">Wuhan</TOKEN>
<TOKEN end_char="70" id="token-0-10" morph="none" pos="punct" start_char="70">:</TOKEN>
<TOKEN end_char="81" id="token-0-11" morph="none" pos="word" start_char="72">Researcher</TOKEN>
</SEG>
<SEG end_char="192" id="segment-1" start_char="85">
<ORIGINAL_TEXT>A disembarking airline passenger undergoes a test for COVID-19 at the Fiumicino Airport in Rome, Italy, Dec.</ORIGINAL_TEXT>
<TOKEN end_char="85" id="token-1-0" morph="none" pos="word" start_char="85">A</TOKEN>
<TOKEN end_char="98" id="token-1-1" morph="none" pos="word" start_char="87">disembarking</TOKEN>
<TOKEN end_char="106" id="token-1-2" morph="none" pos="word" start_char="100">airline</TOKEN>
<TOKEN end_char="116" id="token-1-3" morph="none" pos="word" start_char="108">passenger</TOKEN>
<TOKEN end_char="126" id="token-1-4" morph="none" pos="word" start_char="118">undergoes</TOKEN>
<TOKEN end_char="128" id="token-1-5" morph="none" pos="word" start_char="128">a</TOKEN>
<TOKEN end_char="133" id="token-1-6" morph="none" pos="word" start_char="130">test</TOKEN>
<TOKEN end_char="137" id="token-1-7" morph="none" pos="word" start_char="135">for</TOKEN>
<TOKEN end_char="146" id="token-1-8" morph="none" pos="unknown" start_char="139">COVID-19</TOKEN>
<TOKEN end_char="149" id="token-1-9" morph="none" pos="word" start_char="148">at</TOKEN>
<TOKEN end_char="153" id="token-1-10" morph="none" pos="word" start_char="151">the</TOKEN>
<TOKEN end_char="163" id="token-1-11" morph="none" pos="word" start_char="155">Fiumicino</TOKEN>
<TOKEN end_char="171" id="token-1-12" morph="none" pos="word" start_char="165">Airport</TOKEN>
<TOKEN end_char="174" id="token-1-13" morph="none" pos="word" start_char="173">in</TOKEN>
<TOKEN end_char="179" id="token-1-14" morph="none" pos="word" start_char="176">Rome</TOKEN>
<TOKEN end_char="180" id="token-1-15" morph="none" pos="punct" start_char="180">,</TOKEN>
<TOKEN end_char="186" id="token-1-16" morph="none" pos="word" start_char="182">Italy</TOKEN>
<TOKEN end_char="187" id="token-1-17" morph="none" pos="punct" start_char="187">,</TOKEN>
<TOKEN end_char="191" id="token-1-18" morph="none" pos="word" start_char="189">Dec</TOKEN>
<TOKEN end_char="192" id="token-1-19" morph="none" pos="punct" start_char="192">.</TOKEN>
</SEG>
<SEG end_char="201" id="segment-2" start_char="194">
<ORIGINAL_TEXT>9, 2020.</ORIGINAL_TEXT>
<TOKEN end_char="194" id="token-2-0" morph="none" pos="word" start_char="194">9</TOKEN>
<TOKEN end_char="195" id="token-2-1" morph="none" pos="punct" start_char="195">,</TOKEN>
<TOKEN end_char="200" id="token-2-2" morph="none" pos="word" start_char="197">2020</TOKEN>
<TOKEN end_char="201" id="token-2-3" morph="none" pos="punct" start_char="201">.</TOKEN>
</SEG>
<SEG end_char="376" id="segment-3" start_char="205">
<ORIGINAL_TEXT>An Italian medical researcher who reported finding signs of antibodies to COVID-19 in Italian patients before China confirmed its first official case of the disease on Dec.</ORIGINAL_TEXT>
<TOKEN end_char="206" id="token-3-0" morph="none" pos="word" start_char="205">An</TOKEN>
<TOKEN end_char="214" id="token-3-1" morph="none" pos="word" start_char="208">Italian</TOKEN>
<TOKEN end_char="222" id="token-3-2" morph="none" pos="word" start_char="216">medical</TOKEN>
<TOKEN end_char="233" id="token-3-3" morph="none" pos="word" start_char="224">researcher</TOKEN>
<TOKEN end_char="237" id="token-3-4" morph="none" pos="word" start_char="235">who</TOKEN>
<TOKEN end_char="246" id="token-3-5" morph="none" pos="word" start_char="239">reported</TOKEN>
<TOKEN end_char="254" id="token-3-6" morph="none" pos="word" start_char="248">finding</TOKEN>
<TOKEN end_char="260" id="token-3-7" morph="none" pos="word" start_char="256">signs</TOKEN>
<TOKEN end_char="263" id="token-3-8" morph="none" pos="word" start_char="262">of</TOKEN>
<TOKEN end_char="274" id="token-3-9" morph="none" pos="word" start_char="265">antibodies</TOKEN>
<TOKEN end_char="277" id="token-3-10" morph="none" pos="word" start_char="276">to</TOKEN>
<TOKEN end_char="286" id="token-3-11" morph="none" pos="unknown" start_char="279">COVID-19</TOKEN>
<TOKEN end_char="289" id="token-3-12" morph="none" pos="word" start_char="288">in</TOKEN>
<TOKEN end_char="297" id="token-3-13" morph="none" pos="word" start_char="291">Italian</TOKEN>
<TOKEN end_char="306" id="token-3-14" morph="none" pos="word" start_char="299">patients</TOKEN>
<TOKEN end_char="313" id="token-3-15" morph="none" pos="word" start_char="308">before</TOKEN>
<TOKEN end_char="319" id="token-3-16" morph="none" pos="word" start_char="315">China</TOKEN>
<TOKEN end_char="329" id="token-3-17" morph="none" pos="word" start_char="321">confirmed</TOKEN>
<TOKEN end_char="333" id="token-3-18" morph="none" pos="word" start_char="331">its</TOKEN>
<TOKEN end_char="339" id="token-3-19" morph="none" pos="word" start_char="335">first</TOKEN>
<TOKEN end_char="348" id="token-3-20" morph="none" pos="word" start_char="341">official</TOKEN>
<TOKEN end_char="353" id="token-3-21" morph="none" pos="word" start_char="350">case</TOKEN>
<TOKEN end_char="356" id="token-3-22" morph="none" pos="word" start_char="355">of</TOKEN>
<TOKEN end_char="360" id="token-3-23" morph="none" pos="word" start_char="358">the</TOKEN>
<TOKEN end_char="368" id="token-3-24" morph="none" pos="word" start_char="362">disease</TOKEN>
<TOKEN end_char="371" id="token-3-25" morph="none" pos="word" start_char="370">on</TOKEN>
<TOKEN end_char="375" id="token-3-26" morph="none" pos="word" start_char="373">Dec</TOKEN>
<TOKEN end_char="376" id="token-3-27" morph="none" pos="punct" start_char="376">.</TOKEN>
</SEG>
<SEG end_char="474" id="segment-4" start_char="378">
<ORIGINAL_TEXT>31 have said the findings don't undermine the prevailing view that the virus originated in Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="379" id="token-4-0" morph="none" pos="word" start_char="378">31</TOKEN>
<TOKEN end_char="384" id="token-4-1" morph="none" pos="word" start_char="381">have</TOKEN>
<TOKEN end_char="389" id="token-4-2" morph="none" pos="word" start_char="386">said</TOKEN>
<TOKEN end_char="393" id="token-4-3" morph="none" pos="word" start_char="391">the</TOKEN>
<TOKEN end_char="402" id="token-4-4" morph="none" pos="word" start_char="395">findings</TOKEN>
<TOKEN end_char="408" id="token-4-5" morph="none" pos="word" start_char="404">don't</TOKEN>
<TOKEN end_char="418" id="token-4-6" morph="none" pos="word" start_char="410">undermine</TOKEN>
<TOKEN end_char="422" id="token-4-7" morph="none" pos="word" start_char="420">the</TOKEN>
<TOKEN end_char="433" id="token-4-8" morph="none" pos="word" start_char="424">prevailing</TOKEN>
<TOKEN end_char="438" id="token-4-9" morph="none" pos="word" start_char="435">view</TOKEN>
<TOKEN end_char="443" id="token-4-10" morph="none" pos="word" start_char="440">that</TOKEN>
<TOKEN end_char="447" id="token-4-11" morph="none" pos="word" start_char="445">the</TOKEN>
<TOKEN end_char="453" id="token-4-12" morph="none" pos="word" start_char="449">virus</TOKEN>
<TOKEN end_char="464" id="token-4-13" morph="none" pos="word" start_char="455">originated</TOKEN>
<TOKEN end_char="467" id="token-4-14" morph="none" pos="word" start_char="466">in</TOKEN>
<TOKEN end_char="473" id="token-4-15" morph="none" pos="word" start_char="469">Wuhan</TOKEN>
<TOKEN end_char="474" id="token-4-16" morph="none" pos="punct" start_char="474">.</TOKEN>
</SEG>
<SEG end_char="559" id="segment-5" start_char="477">
<ORIGINAL_TEXT>Giovanni Apolone, lead author of a study published in the INT’s scientific magazine</ORIGINAL_TEXT>
<TOKEN end_char="484" id="token-5-0" morph="none" pos="word" start_char="477">Giovanni</TOKEN>
<TOKEN end_char="492" id="token-5-1" morph="none" pos="word" start_char="486">Apolone</TOKEN>
<TOKEN end_char="493" id="token-5-2" morph="none" pos="punct" start_char="493">,</TOKEN>
<TOKEN end_char="498" id="token-5-3" morph="none" pos="word" start_char="495">lead</TOKEN>
<TOKEN end_char="505" id="token-5-4" morph="none" pos="word" start_char="500">author</TOKEN>
<TOKEN end_char="508" id="token-5-5" morph="none" pos="word" start_char="507">of</TOKEN>
<TOKEN end_char="510" id="token-5-6" morph="none" pos="word" start_char="510">a</TOKEN>
<TOKEN end_char="516" id="token-5-7" morph="none" pos="word" start_char="512">study</TOKEN>
<TOKEN end_char="526" id="token-5-8" morph="none" pos="word" start_char="518">published</TOKEN>
<TOKEN end_char="529" id="token-5-9" morph="none" pos="word" start_char="528">in</TOKEN>
<TOKEN end_char="533" id="token-5-10" morph="none" pos="word" start_char="531">the</TOKEN>
<TOKEN end_char="539" id="token-5-11" morph="none" pos="word" start_char="535">INT’s</TOKEN>
<TOKEN end_char="550" id="token-5-12" morph="none" pos="word" start_char="541">scientific</TOKEN>
<TOKEN end_char="559" id="token-5-13" morph="none" pos="word" start_char="552">magazine</TOKEN>
</SEG>
<SEG end_char="575" id="segment-6" start_char="562">
<ORIGINAL_TEXT>Tumori Journal</ORIGINAL_TEXT>
<TOKEN end_char="567" id="token-6-0" morph="none" pos="word" start_char="562">Tumori</TOKEN>
<TOKEN end_char="575" id="token-6-1" morph="none" pos="word" start_char="569">Journal</TOKEN>
</SEG>
<SEG end_char="848" id="segment-7" start_char="578">
<ORIGINAL_TEXT>, said the fact that antibodies to SARS-CoV-2, the virus that causes COVID-19, were circulating in the Italian population prior to the first confirmed Italian case in February didn't challenge the currently prevailing view that it originated in the central city of Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="578" id="token-7-0" morph="none" pos="punct" start_char="578">,</TOKEN>
<TOKEN end_char="583" id="token-7-1" morph="none" pos="word" start_char="580">said</TOKEN>
<TOKEN end_char="587" id="token-7-2" morph="none" pos="word" start_char="585">the</TOKEN>
<TOKEN end_char="592" id="token-7-3" morph="none" pos="word" start_char="589">fact</TOKEN>
<TOKEN end_char="597" id="token-7-4" morph="none" pos="word" start_char="594">that</TOKEN>
<TOKEN end_char="608" id="token-7-5" morph="none" pos="word" start_char="599">antibodies</TOKEN>
<TOKEN end_char="611" id="token-7-6" morph="none" pos="word" start_char="610">to</TOKEN>
<TOKEN end_char="622" id="token-7-7" morph="none" pos="unknown" start_char="613">SARS-CoV-2</TOKEN>
<TOKEN end_char="623" id="token-7-8" morph="none" pos="punct" start_char="623">,</TOKEN>
<TOKEN end_char="627" id="token-7-9" morph="none" pos="word" start_char="625">the</TOKEN>
<TOKEN end_char="633" id="token-7-10" morph="none" pos="word" start_char="629">virus</TOKEN>
<TOKEN end_char="638" id="token-7-11" morph="none" pos="word" start_char="635">that</TOKEN>
<TOKEN end_char="645" id="token-7-12" morph="none" pos="word" start_char="640">causes</TOKEN>
<TOKEN end_char="654" id="token-7-13" morph="none" pos="unknown" start_char="647">COVID-19</TOKEN>
<TOKEN end_char="655" id="token-7-14" morph="none" pos="punct" start_char="655">,</TOKEN>
<TOKEN end_char="660" id="token-7-15" morph="none" pos="word" start_char="657">were</TOKEN>
<TOKEN end_char="672" id="token-7-16" morph="none" pos="word" start_char="662">circulating</TOKEN>
<TOKEN end_char="675" id="token-7-17" morph="none" pos="word" start_char="674">in</TOKEN>
<TOKEN end_char="679" id="token-7-18" morph="none" pos="word" start_char="677">the</TOKEN>
<TOKEN end_char="687" id="token-7-19" morph="none" pos="word" start_char="681">Italian</TOKEN>
<TOKEN end_char="698" id="token-7-20" morph="none" pos="word" start_char="689">population</TOKEN>
<TOKEN end_char="704" id="token-7-21" morph="none" pos="word" start_char="700">prior</TOKEN>
<TOKEN end_char="707" id="token-7-22" morph="none" pos="word" start_char="706">to</TOKEN>
<TOKEN end_char="711" id="token-7-23" morph="none" pos="word" start_char="709">the</TOKEN>
<TOKEN end_char="717" id="token-7-24" morph="none" pos="word" start_char="713">first</TOKEN>
<TOKEN end_char="727" id="token-7-25" morph="none" pos="word" start_char="719">confirmed</TOKEN>
<TOKEN end_char="735" id="token-7-26" morph="none" pos="word" start_char="729">Italian</TOKEN>
<TOKEN end_char="740" id="token-7-27" morph="none" pos="word" start_char="737">case</TOKEN>
<TOKEN end_char="743" id="token-7-28" morph="none" pos="word" start_char="742">in</TOKEN>
<TOKEN end_char="752" id="token-7-29" morph="none" pos="word" start_char="745">February</TOKEN>
<TOKEN end_char="759" id="token-7-30" morph="none" pos="word" start_char="754">didn't</TOKEN>
<TOKEN end_char="769" id="token-7-31" morph="none" pos="word" start_char="761">challenge</TOKEN>
<TOKEN end_char="773" id="token-7-32" morph="none" pos="word" start_char="771">the</TOKEN>
<TOKEN end_char="783" id="token-7-33" morph="none" pos="word" start_char="775">currently</TOKEN>
<TOKEN end_char="794" id="token-7-34" morph="none" pos="word" start_char="785">prevailing</TOKEN>
<TOKEN end_char="799" id="token-7-35" morph="none" pos="word" start_char="796">view</TOKEN>
<TOKEN end_char="804" id="token-7-36" morph="none" pos="word" start_char="801">that</TOKEN>
<TOKEN end_char="807" id="token-7-37" morph="none" pos="word" start_char="806">it</TOKEN>
<TOKEN end_char="818" id="token-7-38" morph="none" pos="word" start_char="809">originated</TOKEN>
<TOKEN end_char="821" id="token-7-39" morph="none" pos="word" start_char="820">in</TOKEN>
<TOKEN end_char="825" id="token-7-40" morph="none" pos="word" start_char="823">the</TOKEN>
<TOKEN end_char="833" id="token-7-41" morph="none" pos="word" start_char="827">central</TOKEN>
<TOKEN end_char="838" id="token-7-42" morph="none" pos="word" start_char="835">city</TOKEN>
<TOKEN end_char="841" id="token-7-43" morph="none" pos="word" start_char="840">of</TOKEN>
<TOKEN end_char="847" id="token-7-44" morph="none" pos="word" start_char="843">Wuhan</TOKEN>
<TOKEN end_char="848" id="token-7-45" morph="none" pos="punct" start_char="848">.</TOKEN>
</SEG>
<SEG end_char="1128" id="segment-8" start_char="851">
<ORIGINAL_TEXT>The Chinese Communist Party (CCP) propaganda machine has kicked into high gear on the subject of the coronavirus' origins in recent weeks, saying the virus could have come to China before tearing through the population of Wuhan and evolving into a global public health disaster.</ORIGINAL_TEXT>
<TOKEN end_char="853" id="token-8-0" morph="none" pos="word" start_char="851">The</TOKEN>
<TOKEN end_char="861" id="token-8-1" morph="none" pos="word" start_char="855">Chinese</TOKEN>
<TOKEN end_char="871" id="token-8-2" morph="none" pos="word" start_char="863">Communist</TOKEN>
<TOKEN end_char="877" id="token-8-3" morph="none" pos="word" start_char="873">Party</TOKEN>
<TOKEN end_char="879" id="token-8-4" morph="none" pos="punct" start_char="879">(</TOKEN>
<TOKEN end_char="882" id="token-8-5" morph="none" pos="word" start_char="880">CCP</TOKEN>
<TOKEN end_char="883" id="token-8-6" morph="none" pos="punct" start_char="883">)</TOKEN>
<TOKEN end_char="894" id="token-8-7" morph="none" pos="word" start_char="885">propaganda</TOKEN>
<TOKEN end_char="902" id="token-8-8" morph="none" pos="word" start_char="896">machine</TOKEN>
<TOKEN end_char="906" id="token-8-9" morph="none" pos="word" start_char="904">has</TOKEN>
<TOKEN end_char="913" id="token-8-10" morph="none" pos="word" start_char="908">kicked</TOKEN>
<TOKEN end_char="918" id="token-8-11" morph="none" pos="word" start_char="915">into</TOKEN>
<TOKEN end_char="923" id="token-8-12" morph="none" pos="word" start_char="920">high</TOKEN>
<TOKEN end_char="928" id="token-8-13" morph="none" pos="word" start_char="925">gear</TOKEN>
<TOKEN end_char="931" id="token-8-14" morph="none" pos="word" start_char="930">on</TOKEN>
<TOKEN end_char="935" id="token-8-15" morph="none" pos="word" start_char="933">the</TOKEN>
<TOKEN end_char="943" id="token-8-16" morph="none" pos="word" start_char="937">subject</TOKEN>
<TOKEN end_char="946" id="token-8-17" morph="none" pos="word" start_char="945">of</TOKEN>
<TOKEN end_char="950" id="token-8-18" morph="none" pos="word" start_char="948">the</TOKEN>
<TOKEN end_char="962" id="token-8-19" morph="none" pos="word" start_char="952">coronavirus</TOKEN>
<TOKEN end_char="963" id="token-8-20" morph="none" pos="punct" start_char="963">'</TOKEN>
<TOKEN end_char="971" id="token-8-21" morph="none" pos="word" start_char="965">origins</TOKEN>
<TOKEN end_char="974" id="token-8-22" morph="none" pos="word" start_char="973">in</TOKEN>
<TOKEN end_char="981" id="token-8-23" morph="none" pos="word" start_char="976">recent</TOKEN>
<TOKEN end_char="987" id="token-8-24" morph="none" pos="word" start_char="983">weeks</TOKEN>
<TOKEN end_char="988" id="token-8-25" morph="none" pos="punct" start_char="988">,</TOKEN>
<TOKEN end_char="995" id="token-8-26" morph="none" pos="word" start_char="990">saying</TOKEN>
<TOKEN end_char="999" id="token-8-27" morph="none" pos="word" start_char="997">the</TOKEN>
<TOKEN end_char="1005" id="token-8-28" morph="none" pos="word" start_char="1001">virus</TOKEN>
<TOKEN end_char="1011" id="token-8-29" morph="none" pos="word" start_char="1007">could</TOKEN>
<TOKEN end_char="1016" id="token-8-30" morph="none" pos="word" start_char="1013">have</TOKEN>
<TOKEN end_char="1021" id="token-8-31" morph="none" pos="word" start_char="1018">come</TOKEN>
<TOKEN end_char="1024" id="token-8-32" morph="none" pos="word" start_char="1023">to</TOKEN>
<TOKEN end_char="1030" id="token-8-33" morph="none" pos="word" start_char="1026">China</TOKEN>
<TOKEN end_char="1037" id="token-8-34" morph="none" pos="word" start_char="1032">before</TOKEN>
<TOKEN end_char="1045" id="token-8-35" morph="none" pos="word" start_char="1039">tearing</TOKEN>
<TOKEN end_char="1053" id="token-8-36" morph="none" pos="word" start_char="1047">through</TOKEN>
<TOKEN end_char="1057" id="token-8-37" morph="none" pos="word" start_char="1055">the</TOKEN>
<TOKEN end_char="1068" id="token-8-38" morph="none" pos="word" start_char="1059">population</TOKEN>
<TOKEN end_char="1071" id="token-8-39" morph="none" pos="word" start_char="1070">of</TOKEN>
<TOKEN end_char="1077" id="token-8-40" morph="none" pos="word" start_char="1073">Wuhan</TOKEN>
<TOKEN end_char="1081" id="token-8-41" morph="none" pos="word" start_char="1079">and</TOKEN>
<TOKEN end_char="1090" id="token-8-42" morph="none" pos="word" start_char="1083">evolving</TOKEN>
<TOKEN end_char="1095" id="token-8-43" morph="none" pos="word" start_char="1092">into</TOKEN>
<TOKEN end_char="1097" id="token-8-44" morph="none" pos="word" start_char="1097">a</TOKEN>
<TOKEN end_char="1104" id="token-8-45" morph="none" pos="word" start_char="1099">global</TOKEN>
<TOKEN end_char="1111" id="token-8-46" morph="none" pos="word" start_char="1106">public</TOKEN>
<TOKEN end_char="1118" id="token-8-47" morph="none" pos="word" start_char="1113">health</TOKEN>
<TOKEN end_char="1127" id="token-8-48" morph="none" pos="word" start_char="1120">disaster</TOKEN>
<TOKEN end_char="1128" id="token-8-49" morph="none" pos="punct" start_char="1128">.</TOKEN>
</SEG>
<SEG end_char="1307" id="segment-9" start_char="1131">
<ORIGINAL_TEXT>SARS-CoV-2 is believed to have originated in bats and mutated via a second animal host -- possibly pangolins -- into a form capable of infecting humans, according to a report in</ORIGINAL_TEXT>
<TOKEN end_char="1140" id="token-9-0" morph="none" pos="unknown" start_char="1131">SARS-CoV-2</TOKEN>
<TOKEN end_char="1143" id="token-9-1" morph="none" pos="word" start_char="1142">is</TOKEN>
<TOKEN end_char="1152" id="token-9-2" morph="none" pos="word" start_char="1145">believed</TOKEN>
<TOKEN end_char="1155" id="token-9-3" morph="none" pos="word" start_char="1154">to</TOKEN>
<TOKEN end_char="1160" id="token-9-4" morph="none" pos="word" start_char="1157">have</TOKEN>
<TOKEN end_char="1171" id="token-9-5" morph="none" pos="word" start_char="1162">originated</TOKEN>
<TOKEN end_char="1174" id="token-9-6" morph="none" pos="word" start_char="1173">in</TOKEN>
<TOKEN end_char="1179" id="token-9-7" morph="none" pos="word" start_char="1176">bats</TOKEN>
<TOKEN end_char="1183" id="token-9-8" morph="none" pos="word" start_char="1181">and</TOKEN>
<TOKEN end_char="1191" id="token-9-9" morph="none" pos="word" start_char="1185">mutated</TOKEN>
<TOKEN end_char="1195" id="token-9-10" morph="none" pos="word" start_char="1193">via</TOKEN>
<TOKEN end_char="1197" id="token-9-11" morph="none" pos="word" start_char="1197">a</TOKEN>
<TOKEN end_char="1204" id="token-9-12" morph="none" pos="word" start_char="1199">second</TOKEN>
<TOKEN end_char="1211" id="token-9-13" morph="none" pos="word" start_char="1206">animal</TOKEN>
<TOKEN end_char="1216" id="token-9-14" morph="none" pos="word" start_char="1213">host</TOKEN>
<TOKEN end_char="1219" id="token-9-15" morph="none" pos="punct" start_char="1218">--</TOKEN>
<TOKEN end_char="1228" id="token-9-16" morph="none" pos="word" start_char="1221">possibly</TOKEN>
<TOKEN end_char="1238" id="token-9-17" morph="none" pos="word" start_char="1230">pangolins</TOKEN>
<TOKEN end_char="1241" id="token-9-18" morph="none" pos="punct" start_char="1240">--</TOKEN>
<TOKEN end_char="1246" id="token-9-19" morph="none" pos="word" start_char="1243">into</TOKEN>
<TOKEN end_char="1248" id="token-9-20" morph="none" pos="word" start_char="1248">a</TOKEN>
<TOKEN end_char="1253" id="token-9-21" morph="none" pos="word" start_char="1250">form</TOKEN>
<TOKEN end_char="1261" id="token-9-22" morph="none" pos="word" start_char="1255">capable</TOKEN>
<TOKEN end_char="1264" id="token-9-23" morph="none" pos="word" start_char="1263">of</TOKEN>
<TOKEN end_char="1274" id="token-9-24" morph="none" pos="word" start_char="1266">infecting</TOKEN>
<TOKEN end_char="1281" id="token-9-25" morph="none" pos="word" start_char="1276">humans</TOKEN>
<TOKEN end_char="1282" id="token-9-26" morph="none" pos="punct" start_char="1282">,</TOKEN>
<TOKEN end_char="1292" id="token-9-27" morph="none" pos="word" start_char="1284">according</TOKEN>
<TOKEN end_char="1295" id="token-9-28" morph="none" pos="word" start_char="1294">to</TOKEN>
<TOKEN end_char="1297" id="token-9-29" morph="none" pos="word" start_char="1297">a</TOKEN>
<TOKEN end_char="1304" id="token-9-30" morph="none" pos="word" start_char="1299">report</TOKEN>
<TOKEN end_char="1307" id="token-9-31" morph="none" pos="word" start_char="1306">in</TOKEN>
</SEG>
<SEG end_char="1319" id="segment-10" start_char="1310">
<ORIGINAL_TEXT>The Lancet</ORIGINAL_TEXT>
<TOKEN end_char="1312" id="token-10-0" morph="none" pos="word" start_char="1310">The</TOKEN>
<TOKEN end_char="1319" id="token-10-1" morph="none" pos="word" start_char="1314">Lancet</TOKEN>
</SEG>
<SEG end_char="1362" id="segment-11" start_char="1322">
<ORIGINAL_TEXT>medical journal's September 2020 edition.</ORIGINAL_TEXT>
<TOKEN end_char="1328" id="token-11-0" morph="none" pos="word" start_char="1322">medical</TOKEN>
<TOKEN end_char="1338" id="token-11-1" morph="none" pos="word" start_char="1330">journal's</TOKEN>
<TOKEN end_char="1348" id="token-11-2" morph="none" pos="word" start_char="1340">September</TOKEN>
<TOKEN end_char="1353" id="token-11-3" morph="none" pos="word" start_char="1350">2020</TOKEN>
<TOKEN end_char="1361" id="token-11-4" morph="none" pos="word" start_char="1355">edition</TOKEN>
<TOKEN end_char="1362" id="token-11-5" morph="none" pos="punct" start_char="1362">.</TOKEN>
</SEG>
<SEG end_char="1576" id="segment-12" start_char="1365">
<ORIGINAL_TEXT>"Everything points to a bat sarbecovirus reservoir; we are very confident about this," David Robertson, head of bioinformatics at the Medical Research Council–University of Glasgow Centre for Virus Research, told</ORIGINAL_TEXT>
<TOKEN end_char="1365" id="token-12-0" morph="none" pos="punct" start_char="1365">"</TOKEN>
<TOKEN end_char="1375" id="token-12-1" morph="none" pos="word" start_char="1366">Everything</TOKEN>
<TOKEN end_char="1382" id="token-12-2" morph="none" pos="word" start_char="1377">points</TOKEN>
<TOKEN end_char="1385" id="token-12-3" morph="none" pos="word" start_char="1384">to</TOKEN>
<TOKEN end_char="1387" id="token-12-4" morph="none" pos="word" start_char="1387">a</TOKEN>
<TOKEN end_char="1391" id="token-12-5" morph="none" pos="word" start_char="1389">bat</TOKEN>
<TOKEN end_char="1404" id="token-12-6" morph="none" pos="word" start_char="1393">sarbecovirus</TOKEN>
<TOKEN end_char="1414" id="token-12-7" morph="none" pos="word" start_char="1406">reservoir</TOKEN>
<TOKEN end_char="1415" id="token-12-8" morph="none" pos="punct" start_char="1415">;</TOKEN>
<TOKEN end_char="1418" id="token-12-9" morph="none" pos="word" start_char="1417">we</TOKEN>
<TOKEN end_char="1422" id="token-12-10" morph="none" pos="word" start_char="1420">are</TOKEN>
<TOKEN end_char="1427" id="token-12-11" morph="none" pos="word" start_char="1424">very</TOKEN>
<TOKEN end_char="1437" id="token-12-12" morph="none" pos="word" start_char="1429">confident</TOKEN>
<TOKEN end_char="1443" id="token-12-13" morph="none" pos="word" start_char="1439">about</TOKEN>
<TOKEN end_char="1448" id="token-12-14" morph="none" pos="word" start_char="1445">this</TOKEN>
<TOKEN end_char="1450" id="token-12-15" morph="none" pos="punct" start_char="1449">,"</TOKEN>
<TOKEN end_char="1456" id="token-12-16" morph="none" pos="word" start_char="1452">David</TOKEN>
<TOKEN end_char="1466" id="token-12-17" morph="none" pos="word" start_char="1458">Robertson</TOKEN>
<TOKEN end_char="1467" id="token-12-18" morph="none" pos="punct" start_char="1467">,</TOKEN>
<TOKEN end_char="1472" id="token-12-19" morph="none" pos="word" start_char="1469">head</TOKEN>
<TOKEN end_char="1475" id="token-12-20" morph="none" pos="word" start_char="1474">of</TOKEN>
<TOKEN end_char="1490" id="token-12-21" morph="none" pos="word" start_char="1477">bioinformatics</TOKEN>
<TOKEN end_char="1493" id="token-12-22" morph="none" pos="word" start_char="1492">at</TOKEN>
<TOKEN end_char="1497" id="token-12-23" morph="none" pos="word" start_char="1495">the</TOKEN>
<TOKEN end_char="1505" id="token-12-24" morph="none" pos="word" start_char="1499">Medical</TOKEN>
<TOKEN end_char="1514" id="token-12-25" morph="none" pos="word" start_char="1507">Research</TOKEN>
<TOKEN end_char="1533" id="token-12-26" morph="none" pos="unknown" start_char="1516">Council–University</TOKEN>
<TOKEN end_char="1536" id="token-12-27" morph="none" pos="word" start_char="1535">of</TOKEN>
<TOKEN end_char="1544" id="token-12-28" morph="none" pos="word" start_char="1538">Glasgow</TOKEN>
<TOKEN end_char="1551" id="token-12-29" morph="none" pos="word" start_char="1546">Centre</TOKEN>
<TOKEN end_char="1555" id="token-12-30" morph="none" pos="word" start_char="1553">for</TOKEN>
<TOKEN end_char="1561" id="token-12-31" morph="none" pos="word" start_char="1557">Virus</TOKEN>
<TOKEN end_char="1570" id="token-12-32" morph="none" pos="word" start_char="1563">Research</TOKEN>
<TOKEN end_char="1571" id="token-12-33" morph="none" pos="punct" start_char="1571">,</TOKEN>
<TOKEN end_char="1576" id="token-12-34" morph="none" pos="word" start_char="1573">told</TOKEN>
</SEG>
<SEG end_char="1588" id="segment-13" start_char="1579">
<ORIGINAL_TEXT>The Lancet</ORIGINAL_TEXT>
<TOKEN end_char="1581" id="token-13-0" morph="none" pos="word" start_char="1579">The</TOKEN>
<TOKEN end_char="1588" id="token-13-1" morph="none" pos="word" start_char="1583">Lancet</TOKEN>
</SEG>
<SEG end_char="1591" id="segment-14" start_char="1591">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN end_char="1591" id="token-14-0" morph="none" pos="punct" start_char="1591">.</TOKEN>
</SEG>
<SEG end_char="1850" id="segment-15" start_char="1594">
<ORIGINAL_TEXT>"The virus would not need to evolve in the pangolin, it would just need to be brought into contact with humans," Robertson said of the theory that pangolins, banned but still traded illegally in huge numbers in China, were an intermediate host for COVID-19.</ORIGINAL_TEXT>
<TOKEN end_char="1594" id="token-15-0" morph="none" pos="punct" start_char="1594">"</TOKEN>
<TOKEN end_char="1597" id="token-15-1" morph="none" pos="word" start_char="1595">The</TOKEN>
<TOKEN end_char="1603" id="token-15-2" morph="none" pos="word" start_char="1599">virus</TOKEN>
<TOKEN end_char="1609" id="token-15-3" morph="none" pos="word" start_char="1605">would</TOKEN>
<TOKEN end_char="1613" id="token-15-4" morph="none" pos="word" start_char="1611">not</TOKEN>
<TOKEN end_char="1618" id="token-15-5" morph="none" pos="word" start_char="1615">need</TOKEN>
<TOKEN end_char="1621" id="token-15-6" morph="none" pos="word" start_char="1620">to</TOKEN>
<TOKEN end_char="1628" id="token-15-7" morph="none" pos="word" start_char="1623">evolve</TOKEN>
<TOKEN end_char="1631" id="token-15-8" morph="none" pos="word" start_char="1630">in</TOKEN>
<TOKEN end_char="1635" id="token-15-9" morph="none" pos="word" start_char="1633">the</TOKEN>
<TOKEN end_char="1644" id="token-15-10" morph="none" pos="word" start_char="1637">pangolin</TOKEN>
<TOKEN end_char="1645" id="token-15-11" morph="none" pos="punct" start_char="1645">,</TOKEN>
<TOKEN end_char="1648" id="token-15-12" morph="none" pos="word" start_char="1647">it</TOKEN>
<TOKEN end_char="1654" id="token-15-13" morph="none" pos="word" start_char="1650">would</TOKEN>
<TOKEN end_char="1659" id="token-15-14" morph="none" pos="word" start_char="1656">just</TOKEN>
<TOKEN end_char="1664" id="token-15-15" morph="none" pos="word" start_char="1661">need</TOKEN>
<TOKEN end_char="1667" id="token-15-16" morph="none" pos="word" start_char="1666">to</TOKEN>
<TOKEN end_char="1670" id="token-15-17" morph="none" pos="word" start_char="1669">be</TOKEN>
<TOKEN end_char="1678" id="token-15-18" morph="none" pos="word" start_char="1672">brought</TOKEN>
<TOKEN end_char="1683" id="token-15-19" morph="none" pos="word" start_char="1680">into</TOKEN>
<TOKEN end_char="1691" id="token-15-20" morph="none" pos="word" start_char="1685">contact</TOKEN>
<TOKEN end_char="1696" id="token-15-21" morph="none" pos="word" start_char="1693">with</TOKEN>
<TOKEN end_char="1703" id="token-15-22" morph="none" pos="word" start_char="1698">humans</TOKEN>
<TOKEN end_char="1705" id="token-15-23" morph="none" pos="punct" start_char="1704">,"</TOKEN>
<TOKEN end_char="1715" id="token-15-24" morph="none" pos="word" start_char="1707">Robertson</TOKEN>
<TOKEN end_char="1720" id="token-15-25" morph="none" pos="word" start_char="1717">said</TOKEN>
<TOKEN end_char="1723" id="token-15-26" morph="none" pos="word" start_char="1722">of</TOKEN>
<TOKEN end_char="1727" id="token-15-27" morph="none" pos="word" start_char="1725">the</TOKEN>
<TOKEN end_char="1734" id="token-15-28" morph="none" pos="word" start_char="1729">theory</TOKEN>
<TOKEN end_char="1739" id="token-15-29" morph="none" pos="word" start_char="1736">that</TOKEN>
<TOKEN end_char="1749" id="token-15-30" morph="none" pos="word" start_char="1741">pangolins</TOKEN>
<TOKEN end_char="1750" id="token-15-31" morph="none" pos="punct" start_char="1750">,</TOKEN>
<TOKEN end_char="1757" id="token-15-32" morph="none" pos="word" start_char="1752">banned</TOKEN>
<TOKEN end_char="1761" id="token-15-33" morph="none" pos="word" start_char="1759">but</TOKEN>
<TOKEN end_char="1767" id="token-15-34" morph="none" pos="word" start_char="1763">still</TOKEN>
<TOKEN end_char="1774" id="token-15-35" morph="none" pos="word" start_char="1769">traded</TOKEN>
<TOKEN end_char="1784" id="token-15-36" morph="none" pos="word" start_char="1776">illegally</TOKEN>
<TOKEN end_char="1787" id="token-15-37" morph="none" pos="word" start_char="1786">in</TOKEN>
<TOKEN end_char="1792" id="token-15-38" morph="none" pos="word" start_char="1789">huge</TOKEN>
<TOKEN end_char="1800" id="token-15-39" morph="none" pos="word" start_char="1794">numbers</TOKEN>
<TOKEN end_char="1803" id="token-15-40" morph="none" pos="word" start_char="1802">in</TOKEN>
<TOKEN end_char="1809" id="token-15-41" morph="none" pos="word" start_char="1805">China</TOKEN>
<TOKEN end_char="1810" id="token-15-42" morph="none" pos="punct" start_char="1810">,</TOKEN>
<TOKEN end_char="1815" id="token-15-43" morph="none" pos="word" start_char="1812">were</TOKEN>
<TOKEN end_char="1818" id="token-15-44" morph="none" pos="word" start_char="1817">an</TOKEN>
<TOKEN end_char="1831" id="token-15-45" morph="none" pos="word" start_char="1820">intermediate</TOKEN>
<TOKEN end_char="1836" id="token-15-46" morph="none" pos="word" start_char="1833">host</TOKEN>
<TOKEN end_char="1840" id="token-15-47" morph="none" pos="word" start_char="1838">for</TOKEN>
<TOKEN end_char="1849" id="token-15-48" morph="none" pos="unknown" start_char="1842">COVID-19</TOKEN>
<TOKEN end_char="1850" id="token-15-49" morph="none" pos="punct" start_char="1850">.</TOKEN>
</SEG>
<SEG end_char="2099" id="segment-16" start_char="1853">
<ORIGINAL_TEXT>Since these reports began appearing, Chinese state media have published a number of stories and comments claiming that while the pandemic originated in Wuhan, it didn't necessarily make its first jump to humans there, and could have been imported.</ORIGINAL_TEXT>
<TOKEN end_char="1857" id="token-16-0" morph="none" pos="word" start_char="1853">Since</TOKEN>
<TOKEN end_char="1863" id="token-16-1" morph="none" pos="word" start_char="1859">these</TOKEN>
<TOKEN end_char="1871" id="token-16-2" morph="none" pos="word" start_char="1865">reports</TOKEN>
<TOKEN end_char="1877" id="token-16-3" morph="none" pos="word" start_char="1873">began</TOKEN>
<TOKEN end_char="1887" id="token-16-4" morph="none" pos="word" start_char="1879">appearing</TOKEN>
<TOKEN end_char="1888" id="token-16-5" morph="none" pos="punct" start_char="1888">,</TOKEN>
<TOKEN end_char="1896" id="token-16-6" morph="none" pos="word" start_char="1890">Chinese</TOKEN>
<TOKEN end_char="1902" id="token-16-7" morph="none" pos="word" start_char="1898">state</TOKEN>
<TOKEN end_char="1908" id="token-16-8" morph="none" pos="word" start_char="1904">media</TOKEN>
<TOKEN end_char="1913" id="token-16-9" morph="none" pos="word" start_char="1910">have</TOKEN>
<TOKEN end_char="1923" id="token-16-10" morph="none" pos="word" start_char="1915">published</TOKEN>
<TOKEN end_char="1925" id="token-16-11" morph="none" pos="word" start_char="1925">a</TOKEN>
<TOKEN end_char="1932" id="token-16-12" morph="none" pos="word" start_char="1927">number</TOKEN>
<TOKEN end_char="1935" id="token-16-13" morph="none" pos="word" start_char="1934">of</TOKEN>
<TOKEN end_char="1943" id="token-16-14" morph="none" pos="word" start_char="1937">stories</TOKEN>
<TOKEN end_char="1947" id="token-16-15" morph="none" pos="word" start_char="1945">and</TOKEN>
<TOKEN end_char="1956" id="token-16-16" morph="none" pos="word" start_char="1949">comments</TOKEN>
<TOKEN end_char="1965" id="token-16-17" morph="none" pos="word" start_char="1958">claiming</TOKEN>
<TOKEN end_char="1970" id="token-16-18" morph="none" pos="word" start_char="1967">that</TOKEN>
<TOKEN end_char="1976" id="token-16-19" morph="none" pos="word" start_char="1972">while</TOKEN>
<TOKEN end_char="1980" id="token-16-20" morph="none" pos="word" start_char="1978">the</TOKEN>
<TOKEN end_char="1989" id="token-16-21" morph="none" pos="word" start_char="1982">pandemic</TOKEN>
<TOKEN end_char="2000" id="token-16-22" morph="none" pos="word" start_char="1991">originated</TOKEN>
<TOKEN end_char="2003" id="token-16-23" morph="none" pos="word" start_char="2002">in</TOKEN>
<TOKEN end_char="2009" id="token-16-24" morph="none" pos="word" start_char="2005">Wuhan</TOKEN>
<TOKEN end_char="2010" id="token-16-25" morph="none" pos="punct" start_char="2010">,</TOKEN>
<TOKEN end_char="2013" id="token-16-26" morph="none" pos="word" start_char="2012">it</TOKEN>
<TOKEN end_char="2020" id="token-16-27" morph="none" pos="word" start_char="2015">didn't</TOKEN>
<TOKEN end_char="2032" id="token-16-28" morph="none" pos="word" start_char="2022">necessarily</TOKEN>
<TOKEN end_char="2037" id="token-16-29" morph="none" pos="word" start_char="2034">make</TOKEN>
<TOKEN end_char="2041" id="token-16-30" morph="none" pos="word" start_char="2039">its</TOKEN>
<TOKEN end_char="2047" id="token-16-31" morph="none" pos="word" start_char="2043">first</TOKEN>
<TOKEN end_char="2052" id="token-16-32" morph="none" pos="word" start_char="2049">jump</TOKEN>
<TOKEN end_char="2055" id="token-16-33" morph="none" pos="word" start_char="2054">to</TOKEN>
<TOKEN end_char="2062" id="token-16-34" morph="none" pos="word" start_char="2057">humans</TOKEN>
<TOKEN end_char="2068" id="token-16-35" morph="none" pos="word" start_char="2064">there</TOKEN>
<TOKEN end_char="2069" id="token-16-36" morph="none" pos="punct" start_char="2069">,</TOKEN>
<TOKEN end_char="2073" id="token-16-37" morph="none" pos="word" start_char="2071">and</TOKEN>
<TOKEN end_char="2079" id="token-16-38" morph="none" pos="word" start_char="2075">could</TOKEN>
<TOKEN end_char="2084" id="token-16-39" morph="none" pos="word" start_char="2081">have</TOKEN>
<TOKEN end_char="2089" id="token-16-40" morph="none" pos="word" start_char="2086">been</TOKEN>
<TOKEN end_char="2098" id="token-16-41" morph="none" pos="word" start_char="2091">imported</TOKEN>
<TOKEN end_char="2099" id="token-16-42" morph="none" pos="punct" start_char="2099">.</TOKEN>
</SEG>
<SEG end_char="2289" id="segment-17" start_char="2102">
<ORIGINAL_TEXT>Apolone said the detection of SARS-CoV-2 antibodies in Italian study participants who may have been infected as early as September 2020 didn't necessarily support Beijing's claim, however.</ORIGINAL_TEXT>
<TOKEN end_char="2108" id="token-17-0" morph="none" pos="word" start_char="2102">Apolone</TOKEN>
<TOKEN end_char="2113" id="token-17-1" morph="none" pos="word" start_char="2110">said</TOKEN>
<TOKEN end_char="2117" id="token-17-2" morph="none" pos="word" start_char="2115">the</TOKEN>
<TOKEN end_char="2127" id="token-17-3" morph="none" pos="word" start_char="2119">detection</TOKEN>
<TOKEN end_char="2130" id="token-17-4" morph="none" pos="word" start_char="2129">of</TOKEN>
<TOKEN end_char="2141" id="token-17-5" morph="none" pos="unknown" start_char="2132">SARS-CoV-2</TOKEN>
<TOKEN end_char="2152" id="token-17-6" morph="none" pos="word" start_char="2143">antibodies</TOKEN>
<TOKEN end_char="2155" id="token-17-7" morph="none" pos="word" start_char="2154">in</TOKEN>
<TOKEN end_char="2163" id="token-17-8" morph="none" pos="word" start_char="2157">Italian</TOKEN>
<TOKEN end_char="2169" id="token-17-9" morph="none" pos="word" start_char="2165">study</TOKEN>
<TOKEN end_char="2182" id="token-17-10" morph="none" pos="word" start_char="2171">participants</TOKEN>
<TOKEN end_char="2186" id="token-17-11" morph="none" pos="word" start_char="2184">who</TOKEN>
<TOKEN end_char="2190" id="token-17-12" morph="none" pos="word" start_char="2188">may</TOKEN>
<TOKEN end_char="2195" id="token-17-13" morph="none" pos="word" start_char="2192">have</TOKEN>
<TOKEN end_char="2200" id="token-17-14" morph="none" pos="word" start_char="2197">been</TOKEN>
<TOKEN end_char="2209" id="token-17-15" morph="none" pos="word" start_char="2202">infected</TOKEN>
<TOKEN end_char="2212" id="token-17-16" morph="none" pos="word" start_char="2211">as</TOKEN>
<TOKEN end_char="2218" id="token-17-17" morph="none" pos="word" start_char="2214">early</TOKEN>
<TOKEN end_char="2221" id="token-17-18" morph="none" pos="word" start_char="2220">as</TOKEN>
<TOKEN end_char="2231" id="token-17-19" morph="none" pos="word" start_char="2223">September</TOKEN>
<TOKEN end_char="2236" id="token-17-20" morph="none" pos="word" start_char="2233">2020</TOKEN>
<TOKEN end_char="2243" id="token-17-21" morph="none" pos="word" start_char="2238">didn't</TOKEN>
<TOKEN end_char="2255" id="token-17-22" morph="none" pos="word" start_char="2245">necessarily</TOKEN>
<TOKEN end_char="2263" id="token-17-23" morph="none" pos="word" start_char="2257">support</TOKEN>
<TOKEN end_char="2273" id="token-17-24" morph="none" pos="word" start_char="2265">Beijing's</TOKEN>
<TOKEN end_char="2279" id="token-17-25" morph="none" pos="word" start_char="2275">claim</TOKEN>
<TOKEN end_char="2280" id="token-17-26" morph="none" pos="punct" start_char="2280">,</TOKEN>
<TOKEN end_char="2288" id="token-17-27" morph="none" pos="word" start_char="2282">however</TOKEN>
<TOKEN end_char="2289" id="token-17-28" morph="none" pos="punct" start_char="2289">.</TOKEN>
</SEG>
<SEG end_char="2424" id="segment-18" start_char="2292">
<ORIGINAL_TEXT>"All the findings so far suggest that the origin of the virus and of the pandemic is from Wuhan," he said in comments emailed to RFA.</ORIGINAL_TEXT>
<TOKEN end_char="2292" id="token-18-0" morph="none" pos="punct" start_char="2292">"</TOKEN>
<TOKEN end_char="2295" id="token-18-1" morph="none" pos="word" start_char="2293">All</TOKEN>
<TOKEN end_char="2299" id="token-18-2" morph="none" pos="word" start_char="2297">the</TOKEN>
<TOKEN end_char="2308" id="token-18-3" morph="none" pos="word" start_char="2301">findings</TOKEN>
<TOKEN end_char="2311" id="token-18-4" morph="none" pos="word" start_char="2310">so</TOKEN>
<TOKEN end_char="2315" id="token-18-5" morph="none" pos="word" start_char="2313">far</TOKEN>
<TOKEN end_char="2323" id="token-18-6" morph="none" pos="word" start_char="2317">suggest</TOKEN>
<TOKEN end_char="2328" id="token-18-7" morph="none" pos="word" start_char="2325">that</TOKEN>
<TOKEN end_char="2332" id="token-18-8" morph="none" pos="word" start_char="2330">the</TOKEN>
<TOKEN end_char="2339" id="token-18-9" morph="none" pos="word" start_char="2334">origin</TOKEN>
<TOKEN end_char="2342" id="token-18-10" morph="none" pos="word" start_char="2341">of</TOKEN>
<TOKEN end_char="2346" id="token-18-11" morph="none" pos="word" start_char="2344">the</TOKEN>
<TOKEN end_char="2352" id="token-18-12" morph="none" pos="word" start_char="2348">virus</TOKEN>
<TOKEN end_char="2356" id="token-18-13" morph="none" pos="word" start_char="2354">and</TOKEN>
<TOKEN end_char="2359" id="token-18-14" morph="none" pos="word" start_char="2358">of</TOKEN>
<TOKEN end_char="2363" id="token-18-15" morph="none" pos="word" start_char="2361">the</TOKEN>
<TOKEN end_char="2372" id="token-18-16" morph="none" pos="word" start_char="2365">pandemic</TOKEN>
<TOKEN end_char="2375" id="token-18-17" morph="none" pos="word" start_char="2374">is</TOKEN>
<TOKEN end_char="2380" id="token-18-18" morph="none" pos="word" start_char="2377">from</TOKEN>
<TOKEN end_char="2386" id="token-18-19" morph="none" pos="word" start_char="2382">Wuhan</TOKEN>
<TOKEN end_char="2388" id="token-18-20" morph="none" pos="punct" start_char="2387">,"</TOKEN>
<TOKEN end_char="2391" id="token-18-21" morph="none" pos="word" start_char="2390">he</TOKEN>
<TOKEN end_char="2396" id="token-18-22" morph="none" pos="word" start_char="2393">said</TOKEN>
<TOKEN end_char="2399" id="token-18-23" morph="none" pos="word" start_char="2398">in</TOKEN>
<TOKEN end_char="2408" id="token-18-24" morph="none" pos="word" start_char="2401">comments</TOKEN>
<TOKEN end_char="2416" id="token-18-25" morph="none" pos="word" start_char="2410">emailed</TOKEN>
<TOKEN end_char="2419" id="token-18-26" morph="none" pos="word" start_char="2418">to</TOKEN>
<TOKEN end_char="2423" id="token-18-27" morph="none" pos="word" start_char="2421">RFA</TOKEN>
<TOKEN end_char="2424" id="token-18-28" morph="none" pos="punct" start_char="2424">.</TOKEN>
</SEG>
<SEG end_char="2687" id="segment-19" start_char="2427">
<ORIGINAL_TEXT>He said other recent papers, including one based on data from the U.S. Centers for Disease Control and Prevention (CDC), had shown that the virus was "present and circulating" in France and the United States before the first official Chinese case was confirmed.</ORIGINAL_TEXT>
<TOKEN end_char="2428" id="token-19-0" morph="none" pos="word" start_char="2427">He</TOKEN>
<TOKEN end_char="2433" id="token-19-1" morph="none" pos="word" start_char="2430">said</TOKEN>
<TOKEN end_char="2439" id="token-19-2" morph="none" pos="word" start_char="2435">other</TOKEN>
<TOKEN end_char="2446" id="token-19-3" morph="none" pos="word" start_char="2441">recent</TOKEN>
<TOKEN end_char="2453" id="token-19-4" morph="none" pos="word" start_char="2448">papers</TOKEN>
<TOKEN end_char="2454" id="token-19-5" morph="none" pos="punct" start_char="2454">,</TOKEN>
<TOKEN end_char="2464" id="token-19-6" morph="none" pos="word" start_char="2456">including</TOKEN>
<TOKEN end_char="2468" id="token-19-7" morph="none" pos="word" start_char="2466">one</TOKEN>
<TOKEN end_char="2474" id="token-19-8" morph="none" pos="word" start_char="2470">based</TOKEN>
<TOKEN end_char="2477" id="token-19-9" morph="none" pos="word" start_char="2476">on</TOKEN>
<TOKEN end_char="2482" id="token-19-10" morph="none" pos="word" start_char="2479">data</TOKEN>
<TOKEN end_char="2487" id="token-19-11" morph="none" pos="word" start_char="2484">from</TOKEN>
<TOKEN end_char="2491" id="token-19-12" morph="none" pos="word" start_char="2489">the</TOKEN>
<TOKEN end_char="2495" id="token-19-13" morph="none" pos="unknown" start_char="2493">U.S</TOKEN>
<TOKEN end_char="2496" id="token-19-14" morph="none" pos="punct" start_char="2496">.</TOKEN>
<TOKEN end_char="2504" id="token-19-15" morph="none" pos="word" start_char="2498">Centers</TOKEN>
<TOKEN end_char="2508" id="token-19-16" morph="none" pos="word" start_char="2506">for</TOKEN>
<TOKEN end_char="2516" id="token-19-17" morph="none" pos="word" start_char="2510">Disease</TOKEN>
<TOKEN end_char="2524" id="token-19-18" morph="none" pos="word" start_char="2518">Control</TOKEN>
<TOKEN end_char="2528" id="token-19-19" morph="none" pos="word" start_char="2526">and</TOKEN>
<TOKEN end_char="2539" id="token-19-20" morph="none" pos="word" start_char="2530">Prevention</TOKEN>
<TOKEN end_char="2541" id="token-19-21" morph="none" pos="punct" start_char="2541">(</TOKEN>
<TOKEN end_char="2544" id="token-19-22" morph="none" pos="word" start_char="2542">CDC</TOKEN>
<TOKEN end_char="2546" id="token-19-23" morph="none" pos="punct" start_char="2545">),</TOKEN>
<TOKEN end_char="2550" id="token-19-24" morph="none" pos="word" start_char="2548">had</TOKEN>
<TOKEN end_char="2556" id="token-19-25" morph="none" pos="word" start_char="2552">shown</TOKEN>
<TOKEN end_char="2561" id="token-19-26" morph="none" pos="word" start_char="2558">that</TOKEN>
<TOKEN end_char="2565" id="token-19-27" morph="none" pos="word" start_char="2563">the</TOKEN>
<TOKEN end_char="2571" id="token-19-28" morph="none" pos="word" start_char="2567">virus</TOKEN>
<TOKEN end_char="2575" id="token-19-29" morph="none" pos="word" start_char="2573">was</TOKEN>
<TOKEN end_char="2577" id="token-19-30" morph="none" pos="punct" start_char="2577">"</TOKEN>
<TOKEN end_char="2584" id="token-19-31" morph="none" pos="word" start_char="2578">present</TOKEN>
<TOKEN end_char="2588" id="token-19-32" morph="none" pos="word" start_char="2586">and</TOKEN>
<TOKEN end_char="2600" id="token-19-33" morph="none" pos="word" start_char="2590">circulating</TOKEN>
<TOKEN end_char="2601" id="token-19-34" morph="none" pos="punct" start_char="2601">"</TOKEN>
<TOKEN end_char="2604" id="token-19-35" morph="none" pos="word" start_char="2603">in</TOKEN>
<TOKEN end_char="2611" id="token-19-36" morph="none" pos="word" start_char="2606">France</TOKEN>
<TOKEN end_char="2615" id="token-19-37" morph="none" pos="word" start_char="2613">and</TOKEN>
<TOKEN end_char="2619" id="token-19-38" morph="none" pos="word" start_char="2617">the</TOKEN>
<TOKEN end_char="2626" id="token-19-39" morph="none" pos="word" start_char="2621">United</TOKEN>
<TOKEN end_char="2633" id="token-19-40" morph="none" pos="word" start_char="2628">States</TOKEN>
<TOKEN end_char="2640" id="token-19-41" morph="none" pos="word" start_char="2635">before</TOKEN>
<TOKEN end_char="2644" id="token-19-42" morph="none" pos="word" start_char="2642">the</TOKEN>
<TOKEN end_char="2650" id="token-19-43" morph="none" pos="word" start_char="2646">first</TOKEN>
<TOKEN end_char="2659" id="token-19-44" morph="none" pos="word" start_char="2652">official</TOKEN>
<TOKEN end_char="2667" id="token-19-45" morph="none" pos="word" start_char="2661">Chinese</TOKEN>
<TOKEN end_char="2672" id="token-19-46" morph="none" pos="word" start_char="2669">case</TOKEN>
<TOKEN end_char="2676" id="token-19-47" morph="none" pos="word" start_char="2674">was</TOKEN>
<TOKEN end_char="2686" id="token-19-48" morph="none" pos="word" start_char="2678">confirmed</TOKEN>
<TOKEN end_char="2687" id="token-19-49" morph="none" pos="punct" start_char="2687">.</TOKEN>
</SEG>
<SEG end_char="2987" id="segment-20" start_char="2690">
<ORIGINAL_TEXT>"The fact that the virus was present and circulating in Italy, France, and USA ...much before the official communication by China ... is likely due to two factors: the lack of identification of the problem during the first phase and/or a delay of communication for political reasons," Apolone said.</ORIGINAL_TEXT>
<TOKEN end_char="2690" id="token-20-0" morph="none" pos="punct" start_char="2690">"</TOKEN>
<TOKEN end_char="2693" id="token-20-1" morph="none" pos="word" start_char="2691">The</TOKEN>
<TOKEN end_char="2698" id="token-20-2" morph="none" pos="word" start_char="2695">fact</TOKEN>
<TOKEN end_char="2703" id="token-20-3" morph="none" pos="word" start_char="2700">that</TOKEN>
<TOKEN end_char="2707" id="token-20-4" morph="none" pos="word" start_char="2705">the</TOKEN>
<TOKEN end_char="2713" id="token-20-5" morph="none" pos="word" start_char="2709">virus</TOKEN>
<TOKEN end_char="2717" id="token-20-6" morph="none" pos="word" start_char="2715">was</TOKEN>
<TOKEN end_char="2725" id="token-20-7" morph="none" pos="word" start_char="2719">present</TOKEN>
<TOKEN end_char="2729" id="token-20-8" morph="none" pos="word" start_char="2727">and</TOKEN>
<TOKEN end_char="2741" id="token-20-9" morph="none" pos="word" start_char="2731">circulating</TOKEN>
<TOKEN end_char="2744" id="token-20-10" morph="none" pos="word" start_char="2743">in</TOKEN>
<TOKEN end_char="2750" id="token-20-11" morph="none" pos="word" start_char="2746">Italy</TOKEN>
<TOKEN end_char="2751" id="token-20-12" morph="none" pos="punct" start_char="2751">,</TOKEN>
<TOKEN end_char="2758" id="token-20-13" morph="none" pos="word" start_char="2753">France</TOKEN>
<TOKEN end_char="2759" id="token-20-14" morph="none" pos="punct" start_char="2759">,</TOKEN>
<TOKEN end_char="2763" id="token-20-15" morph="none" pos="word" start_char="2761">and</TOKEN>
<TOKEN end_char="2767" id="token-20-16" morph="none" pos="word" start_char="2765">USA</TOKEN>
<TOKEN end_char="2771" id="token-20-17" morph="none" pos="punct" start_char="2769">...</TOKEN>
<TOKEN end_char="2775" id="token-20-18" morph="none" pos="word" start_char="2772">much</TOKEN>
<TOKEN end_char="2782" id="token-20-19" morph="none" pos="word" start_char="2777">before</TOKEN>
<TOKEN end_char="2786" id="token-20-20" morph="none" pos="word" start_char="2784">the</TOKEN>
<TOKEN end_char="2795" id="token-20-21" morph="none" pos="word" start_char="2788">official</TOKEN>
<TOKEN end_char="2809" id="token-20-22" morph="none" pos="word" start_char="2797">communication</TOKEN>
<TOKEN end_char="2812" id="token-20-23" morph="none" pos="word" start_char="2811">by</TOKEN>
<TOKEN end_char="2818" id="token-20-24" morph="none" pos="word" start_char="2814">China</TOKEN>
<TOKEN end_char="2822" id="token-20-25" morph="none" pos="punct" start_char="2820">...</TOKEN>
<TOKEN end_char="2825" id="token-20-26" morph="none" pos="word" start_char="2824">is</TOKEN>
<TOKEN end_char="2832" id="token-20-27" morph="none" pos="word" start_char="2827">likely</TOKEN>
<TOKEN end_char="2836" id="token-20-28" morph="none" pos="word" start_char="2834">due</TOKEN>
<TOKEN end_char="2839" id="token-20-29" morph="none" pos="word" start_char="2838">to</TOKEN>
<TOKEN end_char="2843" id="token-20-30" morph="none" pos="word" start_char="2841">two</TOKEN>
<TOKEN end_char="2851" id="token-20-31" morph="none" pos="word" start_char="2845">factors</TOKEN>
<TOKEN end_char="2852" id="token-20-32" morph="none" pos="punct" start_char="2852">:</TOKEN>
<TOKEN end_char="2856" id="token-20-33" morph="none" pos="word" start_char="2854">the</TOKEN>
<TOKEN end_char="2861" id="token-20-34" morph="none" pos="word" start_char="2858">lack</TOKEN>
<TOKEN end_char="2864" id="token-20-35" morph="none" pos="word" start_char="2863">of</TOKEN>
<TOKEN end_char="2879" id="token-20-36" morph="none" pos="word" start_char="2866">identification</TOKEN>
<TOKEN end_char="2882" id="token-20-37" morph="none" pos="word" start_char="2881">of</TOKEN>
<TOKEN end_char="2886" id="token-20-38" morph="none" pos="word" start_char="2884">the</TOKEN>
<TOKEN end_char="2894" id="token-20-39" morph="none" pos="word" start_char="2888">problem</TOKEN>
<TOKEN end_char="2901" id="token-20-40" morph="none" pos="word" start_char="2896">during</TOKEN>
<TOKEN end_char="2905" id="token-20-41" morph="none" pos="word" start_char="2903">the</TOKEN>
<TOKEN end_char="2911" id="token-20-42" morph="none" pos="word" start_char="2907">first</TOKEN>
<TOKEN end_char="2917" id="token-20-43" morph="none" pos="word" start_char="2913">phase</TOKEN>
<TOKEN end_char="2924" id="token-20-44" morph="none" pos="unknown" start_char="2919">and/or</TOKEN>
<TOKEN end_char="2926" id="token-20-45" morph="none" pos="word" start_char="2926">a</TOKEN>
<TOKEN end_char="2932" id="token-20-46" morph="none" pos="word" start_char="2928">delay</TOKEN>
<TOKEN end_char="2935" id="token-20-47" morph="none" pos="word" start_char="2934">of</TOKEN>
<TOKEN end_char="2949" id="token-20-48" morph="none" pos="word" start_char="2937">communication</TOKEN>
<TOKEN end_char="2953" id="token-20-49" morph="none" pos="word" start_char="2951">for</TOKEN>
<TOKEN end_char="2963" id="token-20-50" morph="none" pos="word" start_char="2955">political</TOKEN>
<TOKEN end_char="2971" id="token-20-51" morph="none" pos="word" start_char="2965">reasons</TOKEN>
<TOKEN end_char="2973" id="token-20-52" morph="none" pos="punct" start_char="2972">,"</TOKEN>
<TOKEN end_char="2981" id="token-20-53" morph="none" pos="word" start_char="2975">Apolone</TOKEN>
<TOKEN end_char="2986" id="token-20-54" morph="none" pos="word" start_char="2983">said</TOKEN>
<TOKEN end_char="2987" id="token-20-55" morph="none" pos="punct" start_char="2987">.</TOKEN>
</SEG>
<SEG end_char="3113" id="segment-21" start_char="2990">
<ORIGINAL_TEXT>The Italian study, the CDC study, and a French study based on the retesting of old pneumonia samples were all used in a Dec.</ORIGINAL_TEXT>
<TOKEN end_char="2992" id="token-21-0" morph="none" pos="word" start_char="2990">The</TOKEN>
<TOKEN end_char="3000" id="token-21-1" morph="none" pos="word" start_char="2994">Italian</TOKEN>
<TOKEN end_char="3006" id="token-21-2" morph="none" pos="word" start_char="3002">study</TOKEN>
<TOKEN end_char="3007" id="token-21-3" morph="none" pos="punct" start_char="3007">,</TOKEN>
<TOKEN end_char="3011" id="token-21-4" morph="none" pos="word" start_char="3009">the</TOKEN>
<TOKEN end_char="3015" id="token-21-5" morph="none" pos="word" start_char="3013">CDC</TOKEN>
<TOKEN end_char="3021" id="token-21-6" morph="none" pos="word" start_char="3017">study</TOKEN>
<TOKEN end_char="3022" id="token-21-7" morph="none" pos="punct" start_char="3022">,</TOKEN>
<TOKEN end_char="3026" id="token-21-8" morph="none" pos="word" start_char="3024">and</TOKEN>
<TOKEN end_char="3028" id="token-21-9" morph="none" pos="word" start_char="3028">a</TOKEN>
<TOKEN end_char="3035" id="token-21-10" morph="none" pos="word" start_char="3030">French</TOKEN>
<TOKEN end_char="3041" id="token-21-11" morph="none" pos="word" start_char="3037">study</TOKEN>
<TOKEN end_char="3047" id="token-21-12" morph="none" pos="word" start_char="3043">based</TOKEN>
<TOKEN end_char="3050" id="token-21-13" morph="none" pos="word" start_char="3049">on</TOKEN>
<TOKEN end_char="3054" id="token-21-14" morph="none" pos="word" start_char="3052">the</TOKEN>
<TOKEN end_char="3064" id="token-21-15" morph="none" pos="word" start_char="3056">retesting</TOKEN>
<TOKEN end_char="3067" id="token-21-16" morph="none" pos="word" start_char="3066">of</TOKEN>
<TOKEN end_char="3071" id="token-21-17" morph="none" pos="word" start_char="3069">old</TOKEN>
<TOKEN end_char="3081" id="token-21-18" morph="none" pos="word" start_char="3073">pneumonia</TOKEN>
<TOKEN end_char="3089" id="token-21-19" morph="none" pos="word" start_char="3083">samples</TOKEN>
<TOKEN end_char="3094" id="token-21-20" morph="none" pos="word" start_char="3091">were</TOKEN>
<TOKEN end_char="3098" id="token-21-21" morph="none" pos="word" start_char="3096">all</TOKEN>
<TOKEN end_char="3103" id="token-21-22" morph="none" pos="word" start_char="3100">used</TOKEN>
<TOKEN end_char="3106" id="token-21-23" morph="none" pos="word" start_char="3105">in</TOKEN>
<TOKEN end_char="3108" id="token-21-24" morph="none" pos="word" start_char="3108">a</TOKEN>
<TOKEN end_char="3112" id="token-21-25" morph="none" pos="word" start_char="3110">Dec</TOKEN>
<TOKEN end_char="3113" id="token-21-26" morph="none" pos="punct" start_char="3113">.</TOKEN>
</SEG>
<SEG end_char="3234" id="segment-22" start_char="3115">
<ORIGINAL_TEXT>2 Facebook video published by China's state news agency Xinhua to support claims of a non-Chinese origin for SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN end_char="3115" id="token-22-0" morph="none" pos="word" start_char="3115">2</TOKEN>
<TOKEN end_char="3124" id="token-22-1" morph="none" pos="word" start_char="3117">Facebook</TOKEN>
<TOKEN end_char="3130" id="token-22-2" morph="none" pos="word" start_char="3126">video</TOKEN>
<TOKEN end_char="3140" id="token-22-3" morph="none" pos="word" start_char="3132">published</TOKEN>
<TOKEN end_char="3143" id="token-22-4" morph="none" pos="word" start_char="3142">by</TOKEN>
<TOKEN end_char="3151" id="token-22-5" morph="none" pos="word" start_char="3145">China's</TOKEN>
<TOKEN end_char="3157" id="token-22-6" morph="none" pos="word" start_char="3153">state</TOKEN>
<TOKEN end_char="3162" id="token-22-7" morph="none" pos="word" start_char="3159">news</TOKEN>
<TOKEN end_char="3169" id="token-22-8" morph="none" pos="word" start_char="3164">agency</TOKEN>
<TOKEN end_char="3176" id="token-22-9" morph="none" pos="word" start_char="3171">Xinhua</TOKEN>
<TOKEN end_char="3179" id="token-22-10" morph="none" pos="word" start_char="3178">to</TOKEN>
<TOKEN end_char="3187" id="token-22-11" morph="none" pos="word" start_char="3181">support</TOKEN>
<TOKEN end_char="3194" id="token-22-12" morph="none" pos="word" start_char="3189">claims</TOKEN>
<TOKEN end_char="3197" id="token-22-13" morph="none" pos="word" start_char="3196">of</TOKEN>
<TOKEN end_char="3199" id="token-22-14" morph="none" pos="word" start_char="3199">a</TOKEN>
<TOKEN end_char="3211" id="token-22-15" morph="none" pos="unknown" start_char="3201">non-Chinese</TOKEN>
<TOKEN end_char="3218" id="token-22-16" morph="none" pos="word" start_char="3213">origin</TOKEN>
<TOKEN end_char="3222" id="token-22-17" morph="none" pos="word" start_char="3220">for</TOKEN>
<TOKEN end_char="3233" id="token-22-18" morph="none" pos="unknown" start_char="3224">SARS-CoV-2</TOKEN>
<TOKEN end_char="3234" id="token-22-19" morph="none" pos="punct" start_char="3234">.</TOKEN>
</SEG>
<SEG end_char="3263" id="segment-23" start_char="3237">
<ORIGINAL_TEXT>Interview censored by China</ORIGINAL_TEXT>
<TOKEN end_char="3245" id="token-23-0" morph="none" pos="word" start_char="3237">Interview</TOKEN>
<TOKEN end_char="3254" id="token-23-1" morph="none" pos="word" start_char="3247">censored</TOKEN>
<TOKEN end_char="3257" id="token-23-2" morph="none" pos="word" start_char="3256">by</TOKEN>
<TOKEN end_char="3263" id="token-23-3" morph="none" pos="word" start_char="3259">China</TOKEN>
</SEG>
<SEG end_char="3372" id="segment-24" start_char="3266">
<ORIGINAL_TEXT>Apolone said he had been interviewed by Chinese state media, but his comments hadn't been reported in full.</ORIGINAL_TEXT>
<TOKEN end_char="3272" id="token-24-0" morph="none" pos="word" start_char="3266">Apolone</TOKEN>
<TOKEN end_char="3277" id="token-24-1" morph="none" pos="word" start_char="3274">said</TOKEN>
<TOKEN end_char="3280" id="token-24-2" morph="none" pos="word" start_char="3279">he</TOKEN>
<TOKEN end_char="3284" id="token-24-3" morph="none" pos="word" start_char="3282">had</TOKEN>
<TOKEN end_char="3289" id="token-24-4" morph="none" pos="word" start_char="3286">been</TOKEN>
<TOKEN end_char="3301" id="token-24-5" morph="none" pos="word" start_char="3291">interviewed</TOKEN>
<TOKEN end_char="3304" id="token-24-6" morph="none" pos="word" start_char="3303">by</TOKEN>
<TOKEN end_char="3312" id="token-24-7" morph="none" pos="word" start_char="3306">Chinese</TOKEN>
<TOKEN end_char="3318" id="token-24-8" morph="none" pos="word" start_char="3314">state</TOKEN>
<TOKEN end_char="3324" id="token-24-9" morph="none" pos="word" start_char="3320">media</TOKEN>
<TOKEN end_char="3325" id="token-24-10" morph="none" pos="punct" start_char="3325">,</TOKEN>
<TOKEN end_char="3329" id="token-24-11" morph="none" pos="word" start_char="3327">but</TOKEN>
<TOKEN end_char="3333" id="token-24-12" morph="none" pos="word" start_char="3331">his</TOKEN>
<TOKEN end_char="3342" id="token-24-13" morph="none" pos="word" start_char="3335">comments</TOKEN>
<TOKEN end_char="3349" id="token-24-14" morph="none" pos="word" start_char="3344">hadn't</TOKEN>
<TOKEN end_char="3354" id="token-24-15" morph="none" pos="word" start_char="3351">been</TOKEN>
<TOKEN end_char="3363" id="token-24-16" morph="none" pos="word" start_char="3356">reported</TOKEN>
<TOKEN end_char="3366" id="token-24-17" morph="none" pos="word" start_char="3365">in</TOKEN>
<TOKEN end_char="3371" id="token-24-18" morph="none" pos="word" start_char="3368">full</TOKEN>
<TOKEN end_char="3372" id="token-24-19" morph="none" pos="punct" start_char="3372">.</TOKEN>
</SEG>
<SEG end_char="3504" id="segment-25" start_char="3375">
<ORIGINAL_TEXT>"I was interviewed by some Chinese media but my response about the origin of the virus was censored," he said in his email to RFA.</ORIGINAL_TEXT>
<TOKEN end_char="3375" id="token-25-0" morph="none" pos="punct" start_char="3375">"</TOKEN>
<TOKEN end_char="3376" id="token-25-1" morph="none" pos="word" start_char="3376">I</TOKEN>
<TOKEN end_char="3380" id="token-25-2" morph="none" pos="word" start_char="3378">was</TOKEN>
<TOKEN end_char="3392" id="token-25-3" morph="none" pos="word" start_char="3382">interviewed</TOKEN>
<TOKEN end_char="3395" id="token-25-4" morph="none" pos="word" start_char="3394">by</TOKEN>
<TOKEN end_char="3400" id="token-25-5" morph="none" pos="word" start_char="3397">some</TOKEN>
<TOKEN end_char="3408" id="token-25-6" morph="none" pos="word" start_char="3402">Chinese</TOKEN>
<TOKEN end_char="3414" id="token-25-7" morph="none" pos="word" start_char="3410">media</TOKEN>
<TOKEN end_char="3418" id="token-25-8" morph="none" pos="word" start_char="3416">but</TOKEN>
<TOKEN end_char="3421" id="token-25-9" morph="none" pos="word" start_char="3420">my</TOKEN>
<TOKEN end_char="3430" id="token-25-10" morph="none" pos="word" start_char="3423">response</TOKEN>
<TOKEN end_char="3436" id="token-25-11" morph="none" pos="word" start_char="3432">about</TOKEN>
<TOKEN end_char="3440" id="token-25-12" morph="none" pos="word" start_char="3438">the</TOKEN>
<TOKEN end_char="3447" id="token-25-13" morph="none" pos="word" start_char="3442">origin</TOKEN>
<TOKEN end_char="3450" id="token-25-14" morph="none" pos="word" start_char="3449">of</TOKEN>
<TOKEN end_char="3454" id="token-25-15" morph="none" pos="word" start_char="3452">the</TOKEN>
<TOKEN end_char="3460" id="token-25-16" morph="none" pos="word" start_char="3456">virus</TOKEN>
<TOKEN end_char="3464" id="token-25-17" morph="none" pos="word" start_char="3462">was</TOKEN>
<TOKEN end_char="3473" id="token-25-18" morph="none" pos="word" start_char="3466">censored</TOKEN>
<TOKEN end_char="3475" id="token-25-19" morph="none" pos="punct" start_char="3474">,"</TOKEN>
<TOKEN end_char="3478" id="token-25-20" morph="none" pos="word" start_char="3477">he</TOKEN>
<TOKEN end_char="3483" id="token-25-21" morph="none" pos="word" start_char="3480">said</TOKEN>
<TOKEN end_char="3486" id="token-25-22" morph="none" pos="word" start_char="3485">in</TOKEN>
<TOKEN end_char="3490" id="token-25-23" morph="none" pos="word" start_char="3488">his</TOKEN>
<TOKEN end_char="3496" id="token-25-24" morph="none" pos="word" start_char="3492">email</TOKEN>
<TOKEN end_char="3499" id="token-25-25" morph="none" pos="word" start_char="3498">to</TOKEN>
<TOKEN end_char="3503" id="token-25-26" morph="none" pos="word" start_char="3501">RFA</TOKEN>
<TOKEN end_char="3504" id="token-25-27" morph="none" pos="punct" start_char="3504">.</TOKEN>
</SEG>
<SEG end_char="3659" id="segment-26" start_char="3507">
<ORIGINAL_TEXT>He said the researchers plan to follow up on the study with more in-depth interviews of participants, and that current theories have yet to be confirmed.</ORIGINAL_TEXT>
<TOKEN end_char="3508" id="token-26-0" morph="none" pos="word" start_char="3507">He</TOKEN>
<TOKEN end_char="3513" id="token-26-1" morph="none" pos="word" start_char="3510">said</TOKEN>
<TOKEN end_char="3517" id="token-26-2" morph="none" pos="word" start_char="3515">the</TOKEN>
<TOKEN end_char="3529" id="token-26-3" morph="none" pos="word" start_char="3519">researchers</TOKEN>
<TOKEN end_char="3534" id="token-26-4" morph="none" pos="word" start_char="3531">plan</TOKEN>
<TOKEN end_char="3537" id="token-26-5" morph="none" pos="word" start_char="3536">to</TOKEN>
<TOKEN end_char="3544" id="token-26-6" morph="none" pos="word" start_char="3539">follow</TOKEN>
<TOKEN end_char="3547" id="token-26-7" morph="none" pos="word" start_char="3546">up</TOKEN>
<TOKEN end_char="3550" id="token-26-8" morph="none" pos="word" start_char="3549">on</TOKEN>
<TOKEN end_char="3554" id="token-26-9" morph="none" pos="word" start_char="3552">the</TOKEN>
<TOKEN end_char="3560" id="token-26-10" morph="none" pos="word" start_char="3556">study</TOKEN>
<TOKEN end_char="3565" id="token-26-11" morph="none" pos="word" start_char="3562">with</TOKEN>
<TOKEN end_char="3570" id="token-26-12" morph="none" pos="word" start_char="3567">more</TOKEN>
<TOKEN end_char="3579" id="token-26-13" morph="none" pos="unknown" start_char="3572">in-depth</TOKEN>
<TOKEN end_char="3590" id="token-26-14" morph="none" pos="word" start_char="3581">interviews</TOKEN>
<TOKEN end_char="3593" id="token-26-15" morph="none" pos="word" start_char="3592">of</TOKEN>
<TOKEN end_char="3606" id="token-26-16" morph="none" pos="word" start_char="3595">participants</TOKEN>
<TOKEN end_char="3607" id="token-26-17" morph="none" pos="punct" start_char="3607">,</TOKEN>
<TOKEN end_char="3611" id="token-26-18" morph="none" pos="word" start_char="3609">and</TOKEN>
<TOKEN end_char="3616" id="token-26-19" morph="none" pos="word" start_char="3613">that</TOKEN>
<TOKEN end_char="3624" id="token-26-20" morph="none" pos="word" start_char="3618">current</TOKEN>
<TOKEN end_char="3633" id="token-26-21" morph="none" pos="word" start_char="3626">theories</TOKEN>
<TOKEN end_char="3638" id="token-26-22" morph="none" pos="word" start_char="3635">have</TOKEN>
<TOKEN end_char="3642" id="token-26-23" morph="none" pos="word" start_char="3640">yet</TOKEN>
<TOKEN end_char="3645" id="token-26-24" morph="none" pos="word" start_char="3644">to</TOKEN>
<TOKEN end_char="3648" id="token-26-25" morph="none" pos="word" start_char="3647">be</TOKEN>
<TOKEN end_char="3658" id="token-26-26" morph="none" pos="word" start_char="3650">confirmed</TOKEN>
<TOKEN end_char="3659" id="token-26-27" morph="none" pos="punct" start_char="3659">.</TOKEN>
</SEG>
<SEG end_char="3791" id="segment-27" start_char="3662">
<ORIGINAL_TEXT>"We are conducting additional analysis to understand better the signal we found in the context of our wider study," Apolone wrote.</ORIGINAL_TEXT>
<TOKEN end_char="3662" id="token-27-0" morph="none" pos="punct" start_char="3662">"</TOKEN>
<TOKEN end_char="3664" id="token-27-1" morph="none" pos="word" start_char="3663">We</TOKEN>
<TOKEN end_char="3668" id="token-27-2" morph="none" pos="word" start_char="3666">are</TOKEN>
<TOKEN end_char="3679" id="token-27-3" morph="none" pos="word" start_char="3670">conducting</TOKEN>
<TOKEN end_char="3690" id="token-27-4" morph="none" pos="word" start_char="3681">additional</TOKEN>
<TOKEN end_char="3699" id="token-27-5" morph="none" pos="word" start_char="3692">analysis</TOKEN>
<TOKEN end_char="3702" id="token-27-6" morph="none" pos="word" start_char="3701">to</TOKEN>
<TOKEN end_char="3713" id="token-27-7" morph="none" pos="word" start_char="3704">understand</TOKEN>
<TOKEN end_char="3720" id="token-27-8" morph="none" pos="word" start_char="3715">better</TOKEN>
<TOKEN end_char="3724" id="token-27-9" morph="none" pos="word" start_char="3722">the</TOKEN>
<TOKEN end_char="3731" id="token-27-10" morph="none" pos="word" start_char="3726">signal</TOKEN>
<TOKEN end_char="3734" id="token-27-11" morph="none" pos="word" start_char="3733">we</TOKEN>
<TOKEN end_char="3740" id="token-27-12" morph="none" pos="word" start_char="3736">found</TOKEN>
<TOKEN end_char="3743" id="token-27-13" morph="none" pos="word" start_char="3742">in</TOKEN>
<TOKEN end_char="3747" id="token-27-14" morph="none" pos="word" start_char="3745">the</TOKEN>
<TOKEN end_char="3755" id="token-27-15" morph="none" pos="word" start_char="3749">context</TOKEN>
<TOKEN end_char="3758" id="token-27-16" morph="none" pos="word" start_char="3757">of</TOKEN>
<TOKEN end_char="3762" id="token-27-17" morph="none" pos="word" start_char="3760">our</TOKEN>
<TOKEN end_char="3768" id="token-27-18" morph="none" pos="word" start_char="3764">wider</TOKEN>
<TOKEN end_char="3774" id="token-27-19" morph="none" pos="word" start_char="3770">study</TOKEN>
<TOKEN end_char="3776" id="token-27-20" morph="none" pos="punct" start_char="3775">,"</TOKEN>
<TOKEN end_char="3784" id="token-27-21" morph="none" pos="word" start_char="3778">Apolone</TOKEN>
<TOKEN end_char="3790" id="token-27-22" morph="none" pos="word" start_char="3786">wrote</TOKEN>
<TOKEN end_char="3791" id="token-27-23" morph="none" pos="punct" start_char="3791">.</TOKEN>
</SEG>
<SEG end_char="3967" id="segment-28" start_char="3794">
<ORIGINAL_TEXT>"I expect that after our publication other teams will report data regarding the origin, timing, and spread of the pandemic virus that will be useful to clarify the argument."</ORIGINAL_TEXT>
<TOKEN end_char="3794" id="token-28-0" morph="none" pos="punct" start_char="3794">"</TOKEN>
<TOKEN end_char="3795" id="token-28-1" morph="none" pos="word" start_char="3795">I</TOKEN>
<TOKEN end_char="3802" id="token-28-2" morph="none" pos="word" start_char="3797">expect</TOKEN>
<TOKEN end_char="3807" id="token-28-3" morph="none" pos="word" start_char="3804">that</TOKEN>
<TOKEN end_char="3813" id="token-28-4" morph="none" pos="word" start_char="3809">after</TOKEN>
<TOKEN end_char="3817" id="token-28-5" morph="none" pos="word" start_char="3815">our</TOKEN>
<TOKEN end_char="3829" id="token-28-6" morph="none" pos="word" start_char="3819">publication</TOKEN>
<TOKEN end_char="3835" id="token-28-7" morph="none" pos="word" start_char="3831">other</TOKEN>
<TOKEN end_char="3841" id="token-28-8" morph="none" pos="word" start_char="3837">teams</TOKEN>
<TOKEN end_char="3846" id="token-28-9" morph="none" pos="word" start_char="3843">will</TOKEN>
<TOKEN end_char="3853" id="token-28-10" morph="none" pos="word" start_char="3848">report</TOKEN>
<TOKEN end_char="3858" id="token-28-11" morph="none" pos="word" start_char="3855">data</TOKEN>
<TOKEN end_char="3868" id="token-28-12" morph="none" pos="word" start_char="3860">regarding</TOKEN>
<TOKEN end_char="3872" id="token-28-13" morph="none" pos="word" start_char="3870">the</TOKEN>
<TOKEN end_char="3879" id="token-28-14" morph="none" pos="word" start_char="3874">origin</TOKEN>
<TOKEN end_char="3880" id="token-28-15" morph="none" pos="punct" start_char="3880">,</TOKEN>
<TOKEN end_char="3887" id="token-28-16" morph="none" pos="word" start_char="3882">timing</TOKEN>
<TOKEN end_char="3888" id="token-28-17" morph="none" pos="punct" start_char="3888">,</TOKEN>
<TOKEN end_char="3892" id="token-28-18" morph="none" pos="word" start_char="3890">and</TOKEN>
<TOKEN end_char="3899" id="token-28-19" morph="none" pos="word" start_char="3894">spread</TOKEN>
<TOKEN end_char="3902" id="token-28-20" morph="none" pos="word" start_char="3901">of</TOKEN>
<TOKEN end_char="3906" id="token-28-21" morph="none" pos="word" start_char="3904">the</TOKEN>
<TOKEN end_char="3915" id="token-28-22" morph="none" pos="word" start_char="3908">pandemic</TOKEN>
<TOKEN end_char="3921" id="token-28-23" morph="none" pos="word" start_char="3917">virus</TOKEN>
<TOKEN end_char="3926" id="token-28-24" morph="none" pos="word" start_char="3923">that</TOKEN>
<TOKEN end_char="3931" id="token-28-25" morph="none" pos="word" start_char="3928">will</TOKEN>
<TOKEN end_char="3934" id="token-28-26" morph="none" pos="word" start_char="3933">be</TOKEN>
<TOKEN end_char="3941" id="token-28-27" morph="none" pos="word" start_char="3936">useful</TOKEN>
<TOKEN end_char="3944" id="token-28-28" morph="none" pos="word" start_char="3943">to</TOKEN>
<TOKEN end_char="3952" id="token-28-29" morph="none" pos="word" start_char="3946">clarify</TOKEN>
<TOKEN end_char="3956" id="token-28-30" morph="none" pos="word" start_char="3954">the</TOKEN>
<TOKEN end_char="3965" id="token-28-31" morph="none" pos="word" start_char="3958">argument</TOKEN>
<TOKEN end_char="3967" id="token-28-32" morph="none" pos="punct" start_char="3966">."</TOKEN>
</SEG>
<SEG end_char="4038" id="segment-29" start_char="3970">
<ORIGINAL_TEXT>While China didn't officially confirm its first COVID case until Dec.</ORIGINAL_TEXT>
<TOKEN end_char="3974" id="token-29-0" morph="none" pos="word" start_char="3970">While</TOKEN>
<TOKEN end_char="3980" id="token-29-1" morph="none" pos="word" start_char="3976">China</TOKEN>
<TOKEN end_char="3987" id="token-29-2" morph="none" pos="word" start_char="3982">didn't</TOKEN>
<TOKEN end_char="3998" id="token-29-3" morph="none" pos="word" start_char="3989">officially</TOKEN>
<TOKEN end_char="4006" id="token-29-4" morph="none" pos="word" start_char="4000">confirm</TOKEN>
<TOKEN end_char="4010" id="token-29-5" morph="none" pos="word" start_char="4008">its</TOKEN>
<TOKEN end_char="4016" id="token-29-6" morph="none" pos="word" start_char="4012">first</TOKEN>
<TOKEN end_char="4022" id="token-29-7" morph="none" pos="word" start_char="4018">COVID</TOKEN>
<TOKEN end_char="4027" id="token-29-8" morph="none" pos="word" start_char="4024">case</TOKEN>
<TOKEN end_char="4033" id="token-29-9" morph="none" pos="word" start_char="4029">until</TOKEN>
<TOKEN end_char="4037" id="token-29-10" morph="none" pos="word" start_char="4035">Dec</TOKEN>
<TOKEN end_char="4038" id="token-29-11" morph="none" pos="punct" start_char="4038">.</TOKEN>
</SEG>
<SEG end_char="4129" id="segment-30" start_char="4040">
<ORIGINAL_TEXT>31, a directive issued by the Wuhan municipal health committee called on hospitals on Dec.</ORIGINAL_TEXT>
<TOKEN end_char="4041" id="token-30-0" morph="none" pos="word" start_char="4040">31</TOKEN>
<TOKEN end_char="4042" id="token-30-1" morph="none" pos="punct" start_char="4042">,</TOKEN>
<TOKEN end_char="4044" id="token-30-2" morph="none" pos="word" start_char="4044">a</TOKEN>
<TOKEN end_char="4054" id="token-30-3" morph="none" pos="word" start_char="4046">directive</TOKEN>
<TOKEN end_char="4061" id="token-30-4" morph="none" pos="word" start_char="4056">issued</TOKEN>
<TOKEN end_char="4064" id="token-30-5" morph="none" pos="word" start_char="4063">by</TOKEN>
<TOKEN end_char="4068" id="token-30-6" morph="none" pos="word" start_char="4066">the</TOKEN>
<TOKEN end_char="4074" id="token-30-7" morph="none" pos="word" start_char="4070">Wuhan</TOKEN>
<TOKEN end_char="4084" id="token-30-8" morph="none" pos="word" start_char="4076">municipal</TOKEN>
<TOKEN end_char="4091" id="token-30-9" morph="none" pos="word" start_char="4086">health</TOKEN>
<TOKEN end_char="4101" id="token-30-10" morph="none" pos="word" start_char="4093">committee</TOKEN>
<TOKEN end_char="4108" id="token-30-11" morph="none" pos="word" start_char="4103">called</TOKEN>
<TOKEN end_char="4111" id="token-30-12" morph="none" pos="word" start_char="4110">on</TOKEN>
<TOKEN end_char="4121" id="token-30-13" morph="none" pos="word" start_char="4113">hospitals</TOKEN>
<TOKEN end_char="4124" id="token-30-14" morph="none" pos="word" start_char="4123">on</TOKEN>
<TOKEN end_char="4128" id="token-30-15" morph="none" pos="word" start_char="4126">Dec</TOKEN>
<TOKEN end_char="4129" id="token-30-16" morph="none" pos="punct" start_char="4129">.</TOKEN>
</SEG>
<SEG end_char="4244" id="segment-31" start_char="4131">
<ORIGINAL_TEXT>30 to follow guidelines when treating cases of "pneumonia of unknown cause that have been appearing" in hospitals.</ORIGINAL_TEXT>
<TOKEN end_char="4132" id="token-31-0" morph="none" pos="word" start_char="4131">30</TOKEN>
<TOKEN end_char="4135" id="token-31-1" morph="none" pos="word" start_char="4134">to</TOKEN>
<TOKEN end_char="4142" id="token-31-2" morph="none" pos="word" start_char="4137">follow</TOKEN>
<TOKEN end_char="4153" id="token-31-3" morph="none" pos="word" start_char="4144">guidelines</TOKEN>
<TOKEN end_char="4158" id="token-31-4" morph="none" pos="word" start_char="4155">when</TOKEN>
<TOKEN end_char="4167" id="token-31-5" morph="none" pos="word" start_char="4160">treating</TOKEN>
<TOKEN end_char="4173" id="token-31-6" morph="none" pos="word" start_char="4169">cases</TOKEN>
<TOKEN end_char="4176" id="token-31-7" morph="none" pos="word" start_char="4175">of</TOKEN>
<TOKEN end_char="4178" id="token-31-8" morph="none" pos="punct" start_char="4178">"</TOKEN>
<TOKEN end_char="4187" id="token-31-9" morph="none" pos="word" start_char="4179">pneumonia</TOKEN>
<TOKEN end_char="4190" id="token-31-10" morph="none" pos="word" start_char="4189">of</TOKEN>
<TOKEN end_char="4198" id="token-31-11" morph="none" pos="word" start_char="4192">unknown</TOKEN>
<TOKEN end_char="4204" id="token-31-12" morph="none" pos="word" start_char="4200">cause</TOKEN>
<TOKEN end_char="4209" id="token-31-13" morph="none" pos="word" start_char="4206">that</TOKEN>
<TOKEN end_char="4214" id="token-31-14" morph="none" pos="word" start_char="4211">have</TOKEN>
<TOKEN end_char="4219" id="token-31-15" morph="none" pos="word" start_char="4216">been</TOKEN>
<TOKEN end_char="4229" id="token-31-16" morph="none" pos="word" start_char="4221">appearing</TOKEN>
<TOKEN end_char="4230" id="token-31-17" morph="none" pos="punct" start_char="4230">"</TOKEN>
<TOKEN end_char="4233" id="token-31-18" morph="none" pos="word" start_char="4232">in</TOKEN>
<TOKEN end_char="4243" id="token-31-19" morph="none" pos="word" start_char="4235">hospitals</TOKEN>
<TOKEN end_char="4244" id="token-31-20" morph="none" pos="punct" start_char="4244">.</TOKEN>
</SEG>
<SEG end_char="4354" id="segment-32" start_char="4247">
<ORIGINAL_TEXT>The directive also banned hospital staff from sharing any information on the disease with the outside world.</ORIGINAL_TEXT>
<TOKEN end_char="4249" id="token-32-0" morph="none" pos="word" start_char="4247">The</TOKEN>
<TOKEN end_char="4259" id="token-32-1" morph="none" pos="word" start_char="4251">directive</TOKEN>
<TOKEN end_char="4264" id="token-32-2" morph="none" pos="word" start_char="4261">also</TOKEN>
<TOKEN end_char="4271" id="token-32-3" morph="none" pos="word" start_char="4266">banned</TOKEN>
<TOKEN end_char="4280" id="token-32-4" morph="none" pos="word" start_char="4273">hospital</TOKEN>
<TOKEN end_char="4286" id="token-32-5" morph="none" pos="word" start_char="4282">staff</TOKEN>
<TOKEN end_char="4291" id="token-32-6" morph="none" pos="word" start_char="4288">from</TOKEN>
<TOKEN end_char="4299" id="token-32-7" morph="none" pos="word" start_char="4293">sharing</TOKEN>
<TOKEN end_char="4303" id="token-32-8" morph="none" pos="word" start_char="4301">any</TOKEN>
<TOKEN end_char="4315" id="token-32-9" morph="none" pos="word" start_char="4305">information</TOKEN>
<TOKEN end_char="4318" id="token-32-10" morph="none" pos="word" start_char="4317">on</TOKEN>
<TOKEN end_char="4322" id="token-32-11" morph="none" pos="word" start_char="4320">the</TOKEN>
<TOKEN end_char="4330" id="token-32-12" morph="none" pos="word" start_char="4324">disease</TOKEN>
<TOKEN end_char="4335" id="token-32-13" morph="none" pos="word" start_char="4332">with</TOKEN>
<TOKEN end_char="4339" id="token-32-14" morph="none" pos="word" start_char="4337">the</TOKEN>
<TOKEN end_char="4347" id="token-32-15" morph="none" pos="word" start_char="4341">outside</TOKEN>
<TOKEN end_char="4353" id="token-32-16" morph="none" pos="word" start_char="4349">world</TOKEN>
<TOKEN end_char="4354" id="token-32-17" morph="none" pos="punct" start_char="4354">.</TOKEN>
</SEG>
<SEG end_char="4487" id="segment-33" start_char="4357">
<ORIGINAL_TEXT>"No organization or individual shall make public any medical information, unless they are authorized to do so," the directive said.</ORIGINAL_TEXT>
<TOKEN end_char="4357" id="token-33-0" morph="none" pos="punct" start_char="4357">"</TOKEN>
<TOKEN end_char="4359" id="token-33-1" morph="none" pos="word" start_char="4358">No</TOKEN>
<TOKEN end_char="4372" id="token-33-2" morph="none" pos="word" start_char="4361">organization</TOKEN>
<TOKEN end_char="4375" id="token-33-3" morph="none" pos="word" start_char="4374">or</TOKEN>
<TOKEN end_char="4386" id="token-33-4" morph="none" pos="word" start_char="4377">individual</TOKEN>
<TOKEN end_char="4392" id="token-33-5" morph="none" pos="word" start_char="4388">shall</TOKEN>
<TOKEN end_char="4397" id="token-33-6" morph="none" pos="word" start_char="4394">make</TOKEN>
<TOKEN end_char="4404" id="token-33-7" morph="none" pos="word" start_char="4399">public</TOKEN>
<TOKEN end_char="4408" id="token-33-8" morph="none" pos="word" start_char="4406">any</TOKEN>
<TOKEN end_char="4416" id="token-33-9" morph="none" pos="word" start_char="4410">medical</TOKEN>
<TOKEN end_char="4428" id="token-33-10" morph="none" pos="word" start_char="4418">information</TOKEN>
<TOKEN end_char="4429" id="token-33-11" morph="none" pos="punct" start_char="4429">,</TOKEN>
<TOKEN end_char="4436" id="token-33-12" morph="none" pos="word" start_char="4431">unless</TOKEN>
<TOKEN end_char="4441" id="token-33-13" morph="none" pos="word" start_char="4438">they</TOKEN>
<TOKEN end_char="4445" id="token-33-14" morph="none" pos="word" start_char="4443">are</TOKEN>
<TOKEN end_char="4456" id="token-33-15" morph="none" pos="word" start_char="4447">authorized</TOKEN>
<TOKEN end_char="4459" id="token-33-16" morph="none" pos="word" start_char="4458">to</TOKEN>
<TOKEN end_char="4462" id="token-33-17" morph="none" pos="word" start_char="4461">do</TOKEN>
<TOKEN end_char="4465" id="token-33-18" morph="none" pos="word" start_char="4464">so</TOKEN>
<TOKEN end_char="4467" id="token-33-19" morph="none" pos="punct" start_char="4466">,"</TOKEN>
<TOKEN end_char="4471" id="token-33-20" morph="none" pos="word" start_char="4469">the</TOKEN>
<TOKEN end_char="4481" id="token-33-21" morph="none" pos="word" start_char="4473">directive</TOKEN>
<TOKEN end_char="4486" id="token-33-22" morph="none" pos="word" start_char="4483">said</TOKEN>
<TOKEN end_char="4487" id="token-33-23" morph="none" pos="punct" start_char="4487">.</TOKEN>
</SEG>
<SEG end_char="4532" id="segment-34" start_char="4490">
<ORIGINAL_TEXT>Hong Kong's Hospital Authority said on Jan.</ORIGINAL_TEXT>
<TOKEN end_char="4493" id="token-34-0" morph="none" pos="word" start_char="4490">Hong</TOKEN>
<TOKEN end_char="4500" id="token-34-1" morph="none" pos="word" start_char="4495">Kong's</TOKEN>
<TOKEN end_char="4509" id="token-34-2" morph="none" pos="word" start_char="4502">Hospital</TOKEN>
<TOKEN end_char="4519" id="token-34-3" morph="none" pos="word" start_char="4511">Authority</TOKEN>
<TOKEN end_char="4524" id="token-34-4" morph="none" pos="word" start_char="4521">said</TOKEN>
<TOKEN end_char="4527" id="token-34-5" morph="none" pos="word" start_char="4526">on</TOKEN>
<TOKEN end_char="4531" id="token-34-6" morph="none" pos="word" start_char="4529">Jan</TOKEN>
<TOKEN end_char="4532" id="token-34-7" morph="none" pos="punct" start_char="4532">.</TOKEN>
</SEG>
<SEG end_char="4689" id="segment-35" start_char="4534">
<ORIGINAL_TEXT>2 it had isolated a pneumonia patient who arrived from the central Chinese city of Wuhan, who had tested negative for SARS and avian and seasonal influenza.</ORIGINAL_TEXT>
<TOKEN end_char="4534" id="token-35-0" morph="none" pos="word" start_char="4534">2</TOKEN>
<TOKEN end_char="4537" id="token-35-1" morph="none" pos="word" start_char="4536">it</TOKEN>
<TOKEN end_char="4541" id="token-35-2" morph="none" pos="word" start_char="4539">had</TOKEN>
<TOKEN end_char="4550" id="token-35-3" morph="none" pos="word" start_char="4543">isolated</TOKEN>
<TOKEN end_char="4552" id="token-35-4" morph="none" pos="word" start_char="4552">a</TOKEN>
<TOKEN end_char="4562" id="token-35-5" morph="none" pos="word" start_char="4554">pneumonia</TOKEN>
<TOKEN end_char="4570" id="token-35-6" morph="none" pos="word" start_char="4564">patient</TOKEN>
<TOKEN end_char="4574" id="token-35-7" morph="none" pos="word" start_char="4572">who</TOKEN>
<TOKEN end_char="4582" id="token-35-8" morph="none" pos="word" start_char="4576">arrived</TOKEN>
<TOKEN end_char="4587" id="token-35-9" morph="none" pos="word" start_char="4584">from</TOKEN>
<TOKEN end_char="4591" id="token-35-10" morph="none" pos="word" start_char="4589">the</TOKEN>
<TOKEN end_char="4599" id="token-35-11" morph="none" pos="word" start_char="4593">central</TOKEN>
<TOKEN end_char="4607" id="token-35-12" morph="none" pos="word" start_char="4601">Chinese</TOKEN>
<TOKEN end_char="4612" id="token-35-13" morph="none" pos="word" start_char="4609">city</TOKEN>
<TOKEN end_char="4615" id="token-35-14" morph="none" pos="word" start_char="4614">of</TOKEN>
<TOKEN end_char="4621" id="token-35-15" morph="none" pos="word" start_char="4617">Wuhan</TOKEN>
<TOKEN end_char="4622" id="token-35-16" morph="none" pos="punct" start_char="4622">,</TOKEN>
<TOKEN end_char="4626" id="token-35-17" morph="none" pos="word" start_char="4624">who</TOKEN>
<TOKEN end_char="4630" id="token-35-18" morph="none" pos="word" start_char="4628">had</TOKEN>
<TOKEN end_char="4637" id="token-35-19" morph="none" pos="word" start_char="4632">tested</TOKEN>
<TOKEN end_char="4646" id="token-35-20" morph="none" pos="word" start_char="4639">negative</TOKEN>
<TOKEN end_char="4650" id="token-35-21" morph="none" pos="word" start_char="4648">for</TOKEN>
<TOKEN end_char="4655" id="token-35-22" morph="none" pos="word" start_char="4652">SARS</TOKEN>
<TOKEN end_char="4659" id="token-35-23" morph="none" pos="word" start_char="4657">and</TOKEN>
<TOKEN end_char="4665" id="token-35-24" morph="none" pos="word" start_char="4661">avian</TOKEN>
<TOKEN end_char="4669" id="token-35-25" morph="none" pos="word" start_char="4667">and</TOKEN>
<TOKEN end_char="4678" id="token-35-26" morph="none" pos="word" start_char="4671">seasonal</TOKEN>
<TOKEN end_char="4688" id="token-35-27" morph="none" pos="word" start_char="4680">influenza</TOKEN>
<TOKEN end_char="4689" id="token-35-28" morph="none" pos="punct" start_char="4689">.</TOKEN>
</SEG>
<SEG end_char="4698" id="segment-36" start_char="4692">
<ORIGINAL_TEXT>By Jan.</ORIGINAL_TEXT>
<TOKEN end_char="4693" id="token-36-0" morph="none" pos="word" start_char="4692">By</TOKEN>
<TOKEN end_char="4697" id="token-36-1" morph="none" pos="word" start_char="4695">Jan</TOKEN>
<TOKEN end_char="4698" id="token-36-2" morph="none" pos="punct" start_char="4698">.</TOKEN>
</SEG>
<SEG end_char="4924" id="segment-37" start_char="4700">
<ORIGINAL_TEXT>6, the number of patients in Hong Kong isolation wards had risen to 21 people, none of whom had visited Wuhan's South China Seafood Market mentioned as a possible link between early cases by mainland Chinese health officials.</ORIGINAL_TEXT>
<TOKEN end_char="4700" id="token-37-0" morph="none" pos="word" start_char="4700">6</TOKEN>
<TOKEN end_char="4701" id="token-37-1" morph="none" pos="punct" start_char="4701">,</TOKEN>
<TOKEN end_char="4705" id="token-37-2" morph="none" pos="word" start_char="4703">the</TOKEN>
<TOKEN end_char="4712" id="token-37-3" morph="none" pos="word" start_char="4707">number</TOKEN>
<TOKEN end_char="4715" id="token-37-4" morph="none" pos="word" start_char="4714">of</TOKEN>
<TOKEN end_char="4724" id="token-37-5" morph="none" pos="word" start_char="4717">patients</TOKEN>
<TOKEN end_char="4727" id="token-37-6" morph="none" pos="word" start_char="4726">in</TOKEN>
<TOKEN end_char="4732" id="token-37-7" morph="none" pos="word" start_char="4729">Hong</TOKEN>
<TOKEN end_char="4737" id="token-37-8" morph="none" pos="word" start_char="4734">Kong</TOKEN>
<TOKEN end_char="4747" id="token-37-9" morph="none" pos="word" start_char="4739">isolation</TOKEN>
<TOKEN end_char="4753" id="token-37-10" morph="none" pos="word" start_char="4749">wards</TOKEN>
<TOKEN end_char="4757" id="token-37-11" morph="none" pos="word" start_char="4755">had</TOKEN>
<TOKEN end_char="4763" id="token-37-12" morph="none" pos="word" start_char="4759">risen</TOKEN>
<TOKEN end_char="4766" id="token-37-13" morph="none" pos="word" start_char="4765">to</TOKEN>
<TOKEN end_char="4769" id="token-37-14" morph="none" pos="word" start_char="4768">21</TOKEN>
<TOKEN end_char="4776" id="token-37-15" morph="none" pos="word" start_char="4771">people</TOKEN>
<TOKEN end_char="4777" id="token-37-16" morph="none" pos="punct" start_char="4777">,</TOKEN>
<TOKEN end_char="4782" id="token-37-17" morph="none" pos="word" start_char="4779">none</TOKEN>
<TOKEN end_char="4785" id="token-37-18" morph="none" pos="word" start_char="4784">of</TOKEN>
<TOKEN end_char="4790" id="token-37-19" morph="none" pos="word" start_char="4787">whom</TOKEN>
<TOKEN end_char="4794" id="token-37-20" morph="none" pos="word" start_char="4792">had</TOKEN>
<TOKEN end_char="4802" id="token-37-21" morph="none" pos="word" start_char="4796">visited</TOKEN>
<TOKEN end_char="4810" id="token-37-22" morph="none" pos="word" start_char="4804">Wuhan's</TOKEN>
<TOKEN end_char="4816" id="token-37-23" morph="none" pos="word" start_char="4812">South</TOKEN>
<TOKEN end_char="4822" id="token-37-24" morph="none" pos="word" start_char="4818">China</TOKEN>
<TOKEN end_char="4830" id="token-37-25" morph="none" pos="word" start_char="4824">Seafood</TOKEN>
<TOKEN end_char="4837" id="token-37-26" morph="none" pos="word" start_char="4832">Market</TOKEN>
<TOKEN end_char="4847" id="token-37-27" morph="none" pos="word" start_char="4839">mentioned</TOKEN>
<TOKEN end_char="4850" id="token-37-28" morph="none" pos="word" start_char="4849">as</TOKEN>
<TOKEN end_char="4852" id="token-37-29" morph="none" pos="word" start_char="4852">a</TOKEN>
<TOKEN end_char="4861" id="token-37-30" morph="none" pos="word" start_char="4854">possible</TOKEN>
<TOKEN end_char="4866" id="token-37-31" morph="none" pos="word" start_char="4863">link</TOKEN>
<TOKEN end_char="4874" id="token-37-32" morph="none" pos="word" start_char="4868">between</TOKEN>
<TOKEN end_char="4880" id="token-37-33" morph="none" pos="word" start_char="4876">early</TOKEN>
<TOKEN end_char="4886" id="token-37-34" morph="none" pos="word" start_char="4882">cases</TOKEN>
<TOKEN end_char="4889" id="token-37-35" morph="none" pos="word" start_char="4888">by</TOKEN>
<TOKEN end_char="4898" id="token-37-36" morph="none" pos="word" start_char="4891">mainland</TOKEN>
<TOKEN end_char="4906" id="token-37-37" morph="none" pos="word" start_char="4900">Chinese</TOKEN>
<TOKEN end_char="4913" id="token-37-38" morph="none" pos="word" start_char="4908">health</TOKEN>
<TOKEN end_char="4923" id="token-37-39" morph="none" pos="word" start_char="4915">officials</TOKEN>
<TOKEN end_char="4924" id="token-37-40" morph="none" pos="punct" start_char="4924">.</TOKEN>
</SEG>
<SEG end_char="5067" id="segment-38" start_char="4927">
<ORIGINAL_TEXT>Hong Kong raised its response level to serious and announced that the then unknown pathogen had "public health significance" on the same day.</ORIGINAL_TEXT>
<TOKEN end_char="4930" id="token-38-0" morph="none" pos="word" start_char="4927">Hong</TOKEN>
<TOKEN end_char="4935" id="token-38-1" morph="none" pos="word" start_char="4932">Kong</TOKEN>
<TOKEN end_char="4942" id="token-38-2" morph="none" pos="word" start_char="4937">raised</TOKEN>
<TOKEN end_char="4946" id="token-38-3" morph="none" pos="word" start_char="4944">its</TOKEN>
<TOKEN end_char="4955" id="token-38-4" morph="none" pos="word" start_char="4948">response</TOKEN>
<TOKEN end_char="4961" id="token-38-5" morph="none" pos="word" start_char="4957">level</TOKEN>
<TOKEN end_char="4964" id="token-38-6" morph="none" pos="word" start_char="4963">to</TOKEN>
<TOKEN end_char="4972" id="token-38-7" morph="none" pos="word" start_char="4966">serious</TOKEN>
<TOKEN end_char="4976" id="token-38-8" morph="none" pos="word" start_char="4974">and</TOKEN>
<TOKEN end_char="4986" id="token-38-9" morph="none" pos="word" start_char="4978">announced</TOKEN>
<TOKEN end_char="4991" id="token-38-10" morph="none" pos="word" start_char="4988">that</TOKEN>
<TOKEN end_char="4995" id="token-38-11" morph="none" pos="word" start_char="4993">the</TOKEN>
<TOKEN end_char="5000" id="token-38-12" morph="none" pos="word" start_char="4997">then</TOKEN>
<TOKEN end_char="5008" id="token-38-13" morph="none" pos="word" start_char="5002">unknown</TOKEN>
<TOKEN end_char="5017" id="token-38-14" morph="none" pos="word" start_char="5010">pathogen</TOKEN>
<TOKEN end_char="5021" id="token-38-15" morph="none" pos="word" start_char="5019">had</TOKEN>
<TOKEN end_char="5023" id="token-38-16" morph="none" pos="punct" start_char="5023">"</TOKEN>
<TOKEN end_char="5029" id="token-38-17" morph="none" pos="word" start_char="5024">public</TOKEN>
<TOKEN end_char="5036" id="token-38-18" morph="none" pos="word" start_char="5031">health</TOKEN>
<TOKEN end_char="5049" id="token-38-19" morph="none" pos="word" start_char="5038">significance</TOKEN>
<TOKEN end_char="5050" id="token-38-20" morph="none" pos="punct" start_char="5050">"</TOKEN>
<TOKEN end_char="5053" id="token-38-21" morph="none" pos="word" start_char="5052">on</TOKEN>
<TOKEN end_char="5057" id="token-38-22" morph="none" pos="word" start_char="5055">the</TOKEN>
<TOKEN end_char="5062" id="token-38-23" morph="none" pos="word" start_char="5059">same</TOKEN>
<TOKEN end_char="5066" id="token-38-24" morph="none" pos="word" start_char="5064">day</TOKEN>
<TOKEN end_char="5067" id="token-38-25" morph="none" pos="punct" start_char="5067">.</TOKEN>
</SEG>
<SEG end_char="5157" id="segment-39" start_char="5070">
<ORIGINAL_TEXT>Ho Pak-leung, head of the University of Hong Kong's Centre for Infection, warned on Jan.</ORIGINAL_TEXT>
<TOKEN end_char="5071" id="token-39-0" morph="none" pos="word" start_char="5070">Ho</TOKEN>
<TOKEN end_char="5081" id="token-39-1" morph="none" pos="unknown" start_char="5073">Pak-leung</TOKEN>
<TOKEN end_char="5082" id="token-39-2" morph="none" pos="punct" start_char="5082">,</TOKEN>
<TOKEN end_char="5087" id="token-39-3" morph="none" pos="word" start_char="5084">head</TOKEN>
<TOKEN end_char="5090" id="token-39-4" morph="none" pos="word" start_char="5089">of</TOKEN>
<TOKEN end_char="5094" id="token-39-5" morph="none" pos="word" start_char="5092">the</TOKEN>
<TOKEN end_char="5105" id="token-39-6" morph="none" pos="word" start_char="5096">University</TOKEN>
<TOKEN end_char="5108" id="token-39-7" morph="none" pos="word" start_char="5107">of</TOKEN>
<TOKEN end_char="5113" id="token-39-8" morph="none" pos="word" start_char="5110">Hong</TOKEN>
<TOKEN end_char="5120" id="token-39-9" morph="none" pos="word" start_char="5115">Kong's</TOKEN>
<TOKEN end_char="5127" id="token-39-10" morph="none" pos="word" start_char="5122">Centre</TOKEN>
<TOKEN end_char="5131" id="token-39-11" morph="none" pos="word" start_char="5129">for</TOKEN>
<TOKEN end_char="5141" id="token-39-12" morph="none" pos="word" start_char="5133">Infection</TOKEN>
<TOKEN end_char="5142" id="token-39-13" morph="none" pos="punct" start_char="5142">,</TOKEN>
<TOKEN end_char="5149" id="token-39-14" morph="none" pos="word" start_char="5144">warned</TOKEN>
<TOKEN end_char="5152" id="token-39-15" morph="none" pos="word" start_char="5151">on</TOKEN>
<TOKEN end_char="5156" id="token-39-16" morph="none" pos="word" start_char="5154">Jan</TOKEN>
<TOKEN end_char="5157" id="token-39-17" morph="none" pos="punct" start_char="5157">.</TOKEN>
</SEG>
<SEG end_char="5318" id="segment-40" start_char="5159">
<ORIGINAL_TEXT>4 that it was highly possible that the illness was spreading from human to human, given the sheer number of cases that had mushroomed in a short period of time.</ORIGINAL_TEXT>
<TOKEN end_char="5159" id="token-40-0" morph="none" pos="word" start_char="5159">4</TOKEN>
<TOKEN end_char="5164" id="token-40-1" morph="none" pos="word" start_char="5161">that</TOKEN>
<TOKEN end_char="5167" id="token-40-2" morph="none" pos="word" start_char="5166">it</TOKEN>
<TOKEN end_char="5171" id="token-40-3" morph="none" pos="word" start_char="5169">was</TOKEN>
<TOKEN end_char="5178" id="token-40-4" morph="none" pos="word" start_char="5173">highly</TOKEN>
<TOKEN end_char="5187" id="token-40-5" morph="none" pos="word" start_char="5180">possible</TOKEN>
<TOKEN end_char="5192" id="token-40-6" morph="none" pos="word" start_char="5189">that</TOKEN>
<TOKEN end_char="5196" id="token-40-7" morph="none" pos="word" start_char="5194">the</TOKEN>
<TOKEN end_char="5204" id="token-40-8" morph="none" pos="word" start_char="5198">illness</TOKEN>
<TOKEN end_char="5208" id="token-40-9" morph="none" pos="word" start_char="5206">was</TOKEN>
<TOKEN end_char="5218" id="token-40-10" morph="none" pos="word" start_char="5210">spreading</TOKEN>
<TOKEN end_char="5223" id="token-40-11" morph="none" pos="word" start_char="5220">from</TOKEN>
<TOKEN end_char="5229" id="token-40-12" morph="none" pos="word" start_char="5225">human</TOKEN>
<TOKEN end_char="5232" id="token-40-13" morph="none" pos="word" start_char="5231">to</TOKEN>
<TOKEN end_char="5238" id="token-40-14" morph="none" pos="word" start_char="5234">human</TOKEN>
<TOKEN end_char="5239" id="token-40-15" morph="none" pos="punct" start_char="5239">,</TOKEN>
<TOKEN end_char="5245" id="token-40-16" morph="none" pos="word" start_char="5241">given</TOKEN>
<TOKEN end_char="5249" id="token-40-17" morph="none" pos="word" start_char="5247">the</TOKEN>
<TOKEN end_char="5255" id="token-40-18" morph="none" pos="word" start_char="5251">sheer</TOKEN>
<TOKEN end_char="5262" id="token-40-19" morph="none" pos="word" start_char="5257">number</TOKEN>
<TOKEN end_char="5265" id="token-40-20" morph="none" pos="word" start_char="5264">of</TOKEN>
<TOKEN end_char="5271" id="token-40-21" morph="none" pos="word" start_char="5267">cases</TOKEN>
<TOKEN end_char="5276" id="token-40-22" morph="none" pos="word" start_char="5273">that</TOKEN>
<TOKEN end_char="5280" id="token-40-23" morph="none" pos="word" start_char="5278">had</TOKEN>
<TOKEN end_char="5291" id="token-40-24" morph="none" pos="word" start_char="5282">mushroomed</TOKEN>
<TOKEN end_char="5294" id="token-40-25" morph="none" pos="word" start_char="5293">in</TOKEN>
<TOKEN end_char="5296" id="token-40-26" morph="none" pos="word" start_char="5296">a</TOKEN>
<TOKEN end_char="5302" id="token-40-27" morph="none" pos="word" start_char="5298">short</TOKEN>
<TOKEN end_char="5309" id="token-40-28" morph="none" pos="word" start_char="5304">period</TOKEN>
<TOKEN end_char="5312" id="token-40-29" morph="none" pos="word" start_char="5311">of</TOKEN>
<TOKEN end_char="5317" id="token-40-30" morph="none" pos="word" start_char="5314">time</TOKEN>
<TOKEN end_char="5318" id="token-40-31" morph="none" pos="punct" start_char="5318">.</TOKEN>
</SEG>
<SEG end_char="5421" id="segment-41" start_char="5321">
<ORIGINAL_TEXT>However, China didn't publicly confirm that the virus was being transmitted between people until Jan.</ORIGINAL_TEXT>
<TOKEN end_char="5327" id="token-41-0" morph="none" pos="word" start_char="5321">However</TOKEN>
<TOKEN end_char="5328" id="token-41-1" morph="none" pos="punct" start_char="5328">,</TOKEN>
<TOKEN end_char="5334" id="token-41-2" morph="none" pos="word" start_char="5330">China</TOKEN>
<TOKEN end_char="5341" id="token-41-3" morph="none" pos="word" start_char="5336">didn't</TOKEN>
<TOKEN end_char="5350" id="token-41-4" morph="none" pos="word" start_char="5343">publicly</TOKEN>
<TOKEN end_char="5358" id="token-41-5" morph="none" pos="word" start_char="5352">confirm</TOKEN>
<TOKEN end_char="5363" id="token-41-6" morph="none" pos="word" start_char="5360">that</TOKEN>
<TOKEN end_char="5367" id="token-41-7" morph="none" pos="word" start_char="5365">the</TOKEN>
<TOKEN end_char="5373" id="token-41-8" morph="none" pos="word" start_char="5369">virus</TOKEN>
<TOKEN end_char="5377" id="token-41-9" morph="none" pos="word" start_char="5375">was</TOKEN>
<TOKEN end_char="5383" id="token-41-10" morph="none" pos="word" start_char="5379">being</TOKEN>
<TOKEN end_char="5395" id="token-41-11" morph="none" pos="word" start_char="5385">transmitted</TOKEN>
<TOKEN end_char="5403" id="token-41-12" morph="none" pos="word" start_char="5397">between</TOKEN>
<TOKEN end_char="5410" id="token-41-13" morph="none" pos="word" start_char="5405">people</TOKEN>
<TOKEN end_char="5416" id="token-41-14" morph="none" pos="word" start_char="5412">until</TOKEN>
<TOKEN end_char="5420" id="token-41-15" morph="none" pos="word" start_char="5418">Jan</TOKEN>
<TOKEN end_char="5421" id="token-41-16" morph="none" pos="punct" start_char="5421">.</TOKEN>
</SEG>
<SEG end_char="5558" id="segment-42" start_char="5423">
<ORIGINAL_TEXT>22, and Wuhan residents told RFA in early January that nobody in the city was wearing masks, nor appeared concerned about the new virus.</ORIGINAL_TEXT>
<TOKEN end_char="5424" id="token-42-0" morph="none" pos="word" start_char="5423">22</TOKEN>
<TOKEN end_char="5425" id="token-42-1" morph="none" pos="punct" start_char="5425">,</TOKEN>
<TOKEN end_char="5429" id="token-42-2" morph="none" pos="word" start_char="5427">and</TOKEN>
<TOKEN end_char="5435" id="token-42-3" morph="none" pos="word" start_char="5431">Wuhan</TOKEN>
<TOKEN end_char="5445" id="token-42-4" morph="none" pos="word" start_char="5437">residents</TOKEN>
<TOKEN end_char="5450" id="token-42-5" morph="none" pos="word" start_char="5447">told</TOKEN>
<TOKEN end_char="5454" id="token-42-6" morph="none" pos="word" start_char="5452">RFA</TOKEN>
<TOKEN end_char="5457" id="token-42-7" morph="none" pos="word" start_char="5456">in</TOKEN>
<TOKEN end_char="5463" id="token-42-8" morph="none" pos="word" start_char="5459">early</TOKEN>
<TOKEN end_char="5471" id="token-42-9" morph="none" pos="word" start_char="5465">January</TOKEN>
<TOKEN end_char="5476" id="token-42-10" morph="none" pos="word" start_char="5473">that</TOKEN>
<TOKEN end_char="5483" id="token-42-11" morph="none" pos="word" start_char="5478">nobody</TOKEN>
<TOKEN end_char="5486" id="token-42-12" morph="none" pos="word" start_char="5485">in</TOKEN>
<TOKEN end_char="5490" id="token-42-13" morph="none" pos="word" start_char="5488">the</TOKEN>
<TOKEN end_char="5495" id="token-42-14" morph="none" pos="word" start_char="5492">city</TOKEN>
<TOKEN end_char="5499" id="token-42-15" morph="none" pos="word" start_char="5497">was</TOKEN>
<TOKEN end_char="5507" id="token-42-16" morph="none" pos="word" start_char="5501">wearing</TOKEN>
<TOKEN end_char="5513" id="token-42-17" morph="none" pos="word" start_char="5509">masks</TOKEN>
<TOKEN end_char="5514" id="token-42-18" morph="none" pos="punct" start_char="5514">,</TOKEN>
<TOKEN end_char="5518" id="token-42-19" morph="none" pos="word" start_char="5516">nor</TOKEN>
<TOKEN end_char="5527" id="token-42-20" morph="none" pos="word" start_char="5520">appeared</TOKEN>
<TOKEN end_char="5537" id="token-42-21" morph="none" pos="word" start_char="5529">concerned</TOKEN>
<TOKEN end_char="5543" id="token-42-22" morph="none" pos="word" start_char="5539">about</TOKEN>
<TOKEN end_char="5547" id="token-42-23" morph="none" pos="word" start_char="5545">the</TOKEN>
<TOKEN end_char="5551" id="token-42-24" morph="none" pos="word" start_char="5549">new</TOKEN>
<TOKEN end_char="5557" id="token-42-25" morph="none" pos="word" start_char="5553">virus</TOKEN>
<TOKEN end_char="5558" id="token-42-26" morph="none" pos="punct" start_char="5558">.</TOKEN>
</SEG>
<SEG end_char="5610" id="segment-43" start_char="5561">
<ORIGINAL_TEXT>Reported by Carmen Wu for RFA's Cantonese Service.</ORIGINAL_TEXT>
<TOKEN end_char="5568" id="token-43-0" morph="none" pos="word" start_char="5561">Reported</TOKEN>
<TOKEN end_char="5571" id="token-43-1" morph="none" pos="word" start_char="5570">by</TOKEN>
<TOKEN end_char="5578" id="token-43-2" morph="none" pos="word" start_char="5573">Carmen</TOKEN>
<TOKEN end_char="5581" id="token-43-3" morph="none" pos="word" start_char="5580">Wu</TOKEN>
<TOKEN end_char="5585" id="token-43-4" morph="none" pos="word" start_char="5583">for</TOKEN>
<TOKEN end_char="5591" id="token-43-5" morph="none" pos="word" start_char="5587">RFA's</TOKEN>
<TOKEN end_char="5601" id="token-43-6" morph="none" pos="word" start_char="5593">Cantonese</TOKEN>
<TOKEN end_char="5609" id="token-43-7" morph="none" pos="word" start_char="5603">Service</TOKEN>
<TOKEN end_char="5610" id="token-43-8" morph="none" pos="punct" start_char="5610">.</TOKEN>
</SEG>
<SEG end_char="5651" id="segment-44" start_char="5612">
<ORIGINAL_TEXT>Translated and edited by Luisetta Mudie.</ORIGINAL_TEXT>
<TOKEN end_char="5621" id="token-44-0" morph="none" pos="word" start_char="5612">Translated</TOKEN>
<TOKEN end_char="5625" id="token-44-1" morph="none" pos="word" start_char="5623">and</TOKEN>
<TOKEN end_char="5632" id="token-44-2" morph="none" pos="word" start_char="5627">edited</TOKEN>
<TOKEN end_char="5635" id="token-44-3" morph="none" pos="word" start_char="5634">by</TOKEN>
<TOKEN end_char="5644" id="token-44-4" morph="none" pos="word" start_char="5637">Luisetta</TOKEN>
<TOKEN end_char="5650" id="token-44-5" morph="none" pos="word" start_char="5646">Mudie</TOKEN>
<TOKEN end_char="5651" id="token-44-6" morph="none" pos="punct" start_char="5651">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>