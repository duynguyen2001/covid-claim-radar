<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CVBL" lang="spa" raw_text_char_length="4383" raw_text_md5="9c27fa05cd2ccf42f9ad0ecbfd206876" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="79" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Nurse who made accusations about horrific NYC COVID patient abuses in hospitals</ORIGINAL_TEXT>
<TOKEN end_char="5" id="token-0-0" morph="none" pos="word" start_char="1">Nurse</TOKEN>
<TOKEN end_char="9" id="token-0-1" morph="none" pos="word" start_char="7">who</TOKEN>
<TOKEN end_char="14" id="token-0-2" morph="none" pos="word" start_char="11">made</TOKEN>
<TOKEN end_char="26" id="token-0-3" morph="none" pos="word" start_char="16">accusations</TOKEN>
<TOKEN end_char="32" id="token-0-4" morph="none" pos="word" start_char="28">about</TOKEN>
<TOKEN end_char="41" id="token-0-5" morph="none" pos="word" start_char="34">horrific</TOKEN>
<TOKEN end_char="45" id="token-0-6" morph="none" pos="word" start_char="43">NYC</TOKEN>
<TOKEN end_char="51" id="token-0-7" morph="none" pos="word" start_char="47">COVID</TOKEN>
<TOKEN end_char="59" id="token-0-8" morph="none" pos="word" start_char="53">patient</TOKEN>
<TOKEN end_char="66" id="token-0-9" morph="none" pos="word" start_char="61">abuses</TOKEN>
<TOKEN end_char="69" id="token-0-10" morph="none" pos="word" start_char="68">in</TOKEN>
<TOKEN end_char="79" id="token-0-11" morph="none" pos="word" start_char="71">hospitals</TOKEN>
</SEG>
<SEG end_char="203" id="segment-1" start_char="83">
<ORIGINAL_TEXT>The nurse who made the above accusations is on the State of Nevada Board of Nursing, Nursing Practice Advisory Committee.</ORIGINAL_TEXT>
<TOKEN end_char="85" id="token-1-0" morph="none" pos="word" start_char="83">The</TOKEN>
<TOKEN end_char="91" id="token-1-1" morph="none" pos="word" start_char="87">nurse</TOKEN>
<TOKEN end_char="95" id="token-1-2" morph="none" pos="word" start_char="93">who</TOKEN>
<TOKEN end_char="100" id="token-1-3" morph="none" pos="word" start_char="97">made</TOKEN>
<TOKEN end_char="104" id="token-1-4" morph="none" pos="word" start_char="102">the</TOKEN>
<TOKEN end_char="110" id="token-1-5" morph="none" pos="word" start_char="106">above</TOKEN>
<TOKEN end_char="122" id="token-1-6" morph="none" pos="word" start_char="112">accusations</TOKEN>
<TOKEN end_char="125" id="token-1-7" morph="none" pos="word" start_char="124">is</TOKEN>
<TOKEN end_char="128" id="token-1-8" morph="none" pos="word" start_char="127">on</TOKEN>
<TOKEN end_char="132" id="token-1-9" morph="none" pos="word" start_char="130">the</TOKEN>
<TOKEN end_char="138" id="token-1-10" morph="none" pos="word" start_char="134">State</TOKEN>
<TOKEN end_char="141" id="token-1-11" morph="none" pos="word" start_char="140">of</TOKEN>
<TOKEN end_char="148" id="token-1-12" morph="none" pos="word" start_char="143">Nevada</TOKEN>
<TOKEN end_char="154" id="token-1-13" morph="none" pos="word" start_char="150">Board</TOKEN>
<TOKEN end_char="157" id="token-1-14" morph="none" pos="word" start_char="156">of</TOKEN>
<TOKEN end_char="165" id="token-1-15" morph="none" pos="word" start_char="159">Nursing</TOKEN>
<TOKEN end_char="166" id="token-1-16" morph="none" pos="punct" start_char="166">,</TOKEN>
<TOKEN end_char="174" id="token-1-17" morph="none" pos="word" start_char="168">Nursing</TOKEN>
<TOKEN end_char="183" id="token-1-18" morph="none" pos="word" start_char="176">Practice</TOKEN>
<TOKEN end_char="192" id="token-1-19" morph="none" pos="word" start_char="185">Advisory</TOKEN>
<TOKEN end_char="202" id="token-1-20" morph="none" pos="word" start_char="194">Committee</TOKEN>
<TOKEN end_char="203" id="token-1-21" morph="none" pos="punct" start_char="203">.</TOKEN>
</SEG>
<SEG end_char="302" id="segment-2" start_char="206">
<ORIGINAL_TEXT>Nicole Sirotek, RN, She made the trip to NYC to help when NYC was begging for more medical staff.</ORIGINAL_TEXT>
<TOKEN end_char="211" id="token-2-0" morph="none" pos="word" start_char="206">Nicole</TOKEN>
<TOKEN end_char="219" id="token-2-1" morph="none" pos="word" start_char="213">Sirotek</TOKEN>
<TOKEN end_char="220" id="token-2-2" morph="none" pos="punct" start_char="220">,</TOKEN>
<TOKEN end_char="223" id="token-2-3" morph="none" pos="word" start_char="222">RN</TOKEN>
<TOKEN end_char="224" id="token-2-4" morph="none" pos="punct" start_char="224">,</TOKEN>
<TOKEN end_char="228" id="token-2-5" morph="none" pos="word" start_char="226">She</TOKEN>
<TOKEN end_char="233" id="token-2-6" morph="none" pos="word" start_char="230">made</TOKEN>
<TOKEN end_char="237" id="token-2-7" morph="none" pos="word" start_char="235">the</TOKEN>
<TOKEN end_char="242" id="token-2-8" morph="none" pos="word" start_char="239">trip</TOKEN>
<TOKEN end_char="245" id="token-2-9" morph="none" pos="word" start_char="244">to</TOKEN>
<TOKEN end_char="249" id="token-2-10" morph="none" pos="word" start_char="247">NYC</TOKEN>
<TOKEN end_char="252" id="token-2-11" morph="none" pos="word" start_char="251">to</TOKEN>
<TOKEN end_char="257" id="token-2-12" morph="none" pos="word" start_char="254">help</TOKEN>
<TOKEN end_char="262" id="token-2-13" morph="none" pos="word" start_char="259">when</TOKEN>
<TOKEN end_char="266" id="token-2-14" morph="none" pos="word" start_char="264">NYC</TOKEN>
<TOKEN end_char="270" id="token-2-15" morph="none" pos="word" start_char="268">was</TOKEN>
<TOKEN end_char="278" id="token-2-16" morph="none" pos="word" start_char="272">begging</TOKEN>
<TOKEN end_char="282" id="token-2-17" morph="none" pos="word" start_char="280">for</TOKEN>
<TOKEN end_char="287" id="token-2-18" morph="none" pos="word" start_char="284">more</TOKEN>
<TOKEN end_char="295" id="token-2-19" morph="none" pos="word" start_char="289">medical</TOKEN>
<TOKEN end_char="301" id="token-2-20" morph="none" pos="word" start_char="297">staff</TOKEN>
<TOKEN end_char="302" id="token-2-21" morph="none" pos="punct" start_char="302">.</TOKEN>
</SEG>
<SEG end_char="360" id="segment-3" start_char="304">
<ORIGINAL_TEXT>She deserves to have her warnings heard and investigated.</ORIGINAL_TEXT>
<TOKEN end_char="306" id="token-3-0" morph="none" pos="word" start_char="304">She</TOKEN>
<TOKEN end_char="315" id="token-3-1" morph="none" pos="word" start_char="308">deserves</TOKEN>
<TOKEN end_char="318" id="token-3-2" morph="none" pos="word" start_char="317">to</TOKEN>
<TOKEN end_char="323" id="token-3-3" morph="none" pos="word" start_char="320">have</TOKEN>
<TOKEN end_char="327" id="token-3-4" morph="none" pos="word" start_char="325">her</TOKEN>
<TOKEN end_char="336" id="token-3-5" morph="none" pos="word" start_char="329">warnings</TOKEN>
<TOKEN end_char="342" id="token-3-6" morph="none" pos="word" start_char="338">heard</TOKEN>
<TOKEN end_char="346" id="token-3-7" morph="none" pos="word" start_char="344">and</TOKEN>
<TOKEN end_char="359" id="token-3-8" morph="none" pos="word" start_char="348">investigated</TOKEN>
<TOKEN end_char="360" id="token-3-9" morph="none" pos="punct" start_char="360">.</TOKEN>
</SEG>
<SEG end_char="399" id="segment-4" start_char="362">
<ORIGINAL_TEXT>Not rejected with knee-jerk reactions.</ORIGINAL_TEXT>
<TOKEN end_char="364" id="token-4-0" morph="none" pos="word" start_char="362">Not</TOKEN>
<TOKEN end_char="373" id="token-4-1" morph="none" pos="word" start_char="366">rejected</TOKEN>
<TOKEN end_char="378" id="token-4-2" morph="none" pos="word" start_char="375">with</TOKEN>
<TOKEN end_char="388" id="token-4-3" morph="none" pos="unknown" start_char="380">knee-jerk</TOKEN>
<TOKEN end_char="398" id="token-4-4" morph="none" pos="word" start_char="390">reactions</TOKEN>
<TOKEN end_char="399" id="token-4-5" morph="none" pos="punct" start_char="399">.</TOKEN>
</SEG>
<SEG end_char="422" id="segment-5" start_char="402">
<ORIGINAL_TEXT>"Nobody is listening.</ORIGINAL_TEXT>
<TOKEN end_char="402" id="token-5-0" morph="none" pos="punct" start_char="402">"</TOKEN>
<TOKEN end_char="408" id="token-5-1" morph="none" pos="word" start_char="403">Nobody</TOKEN>
<TOKEN end_char="411" id="token-5-2" morph="none" pos="word" start_char="410">is</TOKEN>
<TOKEN end_char="421" id="token-5-3" morph="none" pos="word" start_char="413">listening</TOKEN>
<TOKEN end_char="422" id="token-5-4" morph="none" pos="punct" start_char="422">.</TOKEN>
</SEG>
<SEG end_char="473" id="segment-6" start_char="424">
<ORIGINAL_TEXT>They don't care what is happening to these people.</ORIGINAL_TEXT>
<TOKEN end_char="427" id="token-6-0" morph="none" pos="word" start_char="424">They</TOKEN>
<TOKEN end_char="433" id="token-6-1" morph="none" pos="word" start_char="429">don't</TOKEN>
<TOKEN end_char="438" id="token-6-2" morph="none" pos="word" start_char="435">care</TOKEN>
<TOKEN end_char="443" id="token-6-3" morph="none" pos="word" start_char="440">what</TOKEN>
<TOKEN end_char="446" id="token-6-4" morph="none" pos="word" start_char="445">is</TOKEN>
<TOKEN end_char="456" id="token-6-5" morph="none" pos="word" start_char="448">happening</TOKEN>
<TOKEN end_char="459" id="token-6-6" morph="none" pos="word" start_char="458">to</TOKEN>
<TOKEN end_char="465" id="token-6-7" morph="none" pos="word" start_char="461">these</TOKEN>
<TOKEN end_char="472" id="token-6-8" morph="none" pos="word" start_char="467">people</TOKEN>
<TOKEN end_char="473" id="token-6-9" morph="none" pos="punct" start_char="473">.</TOKEN>
</SEG>
<SEG end_char="507" id="segment-7" start_char="475">
<ORIGINAL_TEXT>They don't." - Nicole Sirotek, RN</ORIGINAL_TEXT>
<TOKEN end_char="478" id="token-7-0" morph="none" pos="word" start_char="475">They</TOKEN>
<TOKEN end_char="484" id="token-7-1" morph="none" pos="word" start_char="480">don't</TOKEN>
<TOKEN end_char="486" id="token-7-2" morph="none" pos="punct" start_char="485">."</TOKEN>
<TOKEN end_char="488" id="token-7-3" morph="none" pos="punct" start_char="488">-</TOKEN>
<TOKEN end_char="495" id="token-7-4" morph="none" pos="word" start_char="490">Nicole</TOKEN>
<TOKEN end_char="503" id="token-7-5" morph="none" pos="word" start_char="497">Sirotek</TOKEN>
<TOKEN end_char="504" id="token-7-6" morph="none" pos="punct" start_char="504">,</TOKEN>
<TOKEN end_char="507" id="token-7-7" morph="none" pos="word" start_char="506">RN</TOKEN>
</SEG>
<SEG end_char="545" id="segment-8" start_char="510">
<ORIGINAL_TEXT>Link to Nevada Board of Nursing LINK</ORIGINAL_TEXT>
<TOKEN end_char="513" id="token-8-0" morph="none" pos="word" start_char="510">Link</TOKEN>
<TOKEN end_char="516" id="token-8-1" morph="none" pos="word" start_char="515">to</TOKEN>
<TOKEN end_char="523" id="token-8-2" morph="none" pos="word" start_char="518">Nevada</TOKEN>
<TOKEN end_char="529" id="token-8-3" morph="none" pos="word" start_char="525">Board</TOKEN>
<TOKEN end_char="532" id="token-8-4" morph="none" pos="word" start_char="531">of</TOKEN>
<TOKEN end_char="540" id="token-8-5" morph="none" pos="word" start_char="534">Nursing</TOKEN>
<TOKEN end_char="545" id="token-8-6" morph="none" pos="word" start_char="542">LINK</TOKEN>
</SEG>
<SEG end_char="600" id="segment-9" start_char="548">
<ORIGINAL_TEXT>Link to her video describing NYC hospital abses: LINK</ORIGINAL_TEXT>
<TOKEN end_char="551" id="token-9-0" morph="none" pos="word" start_char="548">Link</TOKEN>
<TOKEN end_char="554" id="token-9-1" morph="none" pos="word" start_char="553">to</TOKEN>
<TOKEN end_char="558" id="token-9-2" morph="none" pos="word" start_char="556">her</TOKEN>
<TOKEN end_char="564" id="token-9-3" morph="none" pos="word" start_char="560">video</TOKEN>
<TOKEN end_char="575" id="token-9-4" morph="none" pos="word" start_char="566">describing</TOKEN>
<TOKEN end_char="579" id="token-9-5" morph="none" pos="word" start_char="577">NYC</TOKEN>
<TOKEN end_char="588" id="token-9-6" morph="none" pos="word" start_char="581">hospital</TOKEN>
<TOKEN end_char="594" id="token-9-7" morph="none" pos="word" start_char="590">abses</TOKEN>
<TOKEN end_char="595" id="token-9-8" morph="none" pos="punct" start_char="595">:</TOKEN>
<TOKEN end_char="600" id="token-9-9" morph="none" pos="word" start_char="597">LINK</TOKEN>
</SEG>
<SEG end_char="641" id="segment-10" start_char="603">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:35 pm</ORIGINAL_TEXT>
<TOKEN end_char="606" id="token-10-0" morph="none" pos="word" start_char="603">This</TOKEN>
<TOKEN end_char="611" id="token-10-1" morph="none" pos="word" start_char="608">post</TOKEN>
<TOKEN end_char="615" id="token-10-2" morph="none" pos="word" start_char="613">was</TOKEN>
<TOKEN end_char="622" id="token-10-3" morph="none" pos="word" start_char="617">edited</TOKEN>
<TOKEN end_char="625" id="token-10-4" morph="none" pos="word" start_char="624">on</TOKEN>
<TOKEN end_char="629" id="token-10-5" morph="none" pos="unknown" start_char="627">5/5</TOKEN>
<TOKEN end_char="632" id="token-10-6" morph="none" pos="word" start_char="631">at</TOKEN>
<TOKEN end_char="638" id="token-10-7" morph="none" pos="unknown" start_char="634">12:35</TOKEN>
<TOKEN end_char="641" id="token-10-8" morph="none" pos="word" start_char="640">pm</TOKEN>
</SEG>
<SEG end_char="677" id="segment-11" start_char="645">
<ORIGINAL_TEXT>That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN end_char="648" id="token-11-0" morph="none" pos="word" start_char="645">That</TOKEN>
<TOKEN end_char="654" id="token-11-1" morph="none" pos="word" start_char="650">still</TOKEN>
<TOKEN end_char="659" id="token-11-2" morph="none" pos="word" start_char="656">does</TOKEN>
<TOKEN end_char="663" id="token-11-3" morph="none" pos="word" start_char="661">not</TOKEN>
<TOKEN end_char="669" id="token-11-4" morph="none" pos="word" start_char="665">equal</TOKEN>
<TOKEN end_char="676" id="token-11-5" morph="none" pos="word" start_char="671">doctor</TOKEN>
<TOKEN end_char="677" id="token-11-6" morph="none" pos="punct" start_char="677">.</TOKEN>
<TRANSLATED_TEXT>Dit is nog niet gelijk aan de arts.</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="707" id="segment-12" start_char="679">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN end_char="682" id="token-12-0" morph="none" pos="word" start_char="679">Does</TOKEN>
<TOKEN end_char="686" id="token-12-1" morph="none" pos="word" start_char="684">she</TOKEN>
<TOKEN end_char="694" id="token-12-2" morph="none" pos="word" start_char="688">deserve</TOKEN>
<TOKEN end_char="697" id="token-12-3" morph="none" pos="word" start_char="696">to</TOKEN>
<TOKEN end_char="700" id="token-12-4" morph="none" pos="word" start_char="699">be</TOKEN>
<TOKEN end_char="706" id="token-12-5" morph="none" pos="word" start_char="702">heard</TOKEN>
<TOKEN end_char="707" id="token-12-6" morph="none" pos="punct" start_char="707">?</TOKEN>
</SEG>
<SEG end_char="717" id="segment-13" start_char="709">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN end_char="717" id="token-13-0" morph="none" pos="word" start_char="709">Certainly</TOKEN>
<TRANSLATED_TEXT>Assuramente</TRANSLATED_TEXT><DETECTED_LANGUAGE>cy</DETECTED_LANGUAGE></SEG>
<SEG end_char="816" id="segment-14" start_char="721">
<ORIGINAL_TEXT>It puts her a cut above the Nurse Jackie Tik Tok crew who are the face of nursing at the moment.</ORIGINAL_TEXT>
<TOKEN end_char="722" id="token-14-0" morph="none" pos="word" start_char="721">It</TOKEN>
<TOKEN end_char="727" id="token-14-1" morph="none" pos="word" start_char="724">puts</TOKEN>
<TOKEN end_char="731" id="token-14-2" morph="none" pos="word" start_char="729">her</TOKEN>
<TOKEN end_char="733" id="token-14-3" morph="none" pos="word" start_char="733">a</TOKEN>
<TOKEN end_char="737" id="token-14-4" morph="none" pos="word" start_char="735">cut</TOKEN>
<TOKEN end_char="743" id="token-14-5" morph="none" pos="word" start_char="739">above</TOKEN>
<TOKEN end_char="747" id="token-14-6" morph="none" pos="word" start_char="745">the</TOKEN>
<TOKEN end_char="753" id="token-14-7" morph="none" pos="word" start_char="749">Nurse</TOKEN>
<TOKEN end_char="760" id="token-14-8" morph="none" pos="word" start_char="755">Jackie</TOKEN>
<TOKEN end_char="764" id="token-14-9" morph="none" pos="word" start_char="762">Tik</TOKEN>
<TOKEN end_char="768" id="token-14-10" morph="none" pos="word" start_char="766">Tok</TOKEN>
<TOKEN end_char="773" id="token-14-11" morph="none" pos="word" start_char="770">crew</TOKEN>
<TOKEN end_char="777" id="token-14-12" morph="none" pos="word" start_char="775">who</TOKEN>
<TOKEN end_char="781" id="token-14-13" morph="none" pos="word" start_char="779">are</TOKEN>
<TOKEN end_char="785" id="token-14-14" morph="none" pos="word" start_char="783">the</TOKEN>
<TOKEN end_char="790" id="token-14-15" morph="none" pos="word" start_char="787">face</TOKEN>
<TOKEN end_char="793" id="token-14-16" morph="none" pos="word" start_char="792">of</TOKEN>
<TOKEN end_char="801" id="token-14-17" morph="none" pos="word" start_char="795">nursing</TOKEN>
<TOKEN end_char="804" id="token-14-18" morph="none" pos="word" start_char="803">at</TOKEN>
<TOKEN end_char="808" id="token-14-19" morph="none" pos="word" start_char="806">the</TOKEN>
<TOKEN end_char="815" id="token-14-20" morph="none" pos="word" start_char="810">moment</TOKEN>
<TOKEN end_char="816" id="token-14-21" morph="none" pos="punct" start_char="816">.</TOKEN>
</SEG>
<SEG end_char="858" id="segment-15" start_char="821">
<ORIGINAL_TEXT>quote:That still does not equal doctor</ORIGINAL_TEXT>
<TOKEN end_char="830" id="token-15-0" morph="none" pos="unknown" start_char="821">quote:That</TOKEN>
<TOKEN end_char="836" id="token-15-1" morph="none" pos="word" start_char="832">still</TOKEN>
<TOKEN end_char="841" id="token-15-2" morph="none" pos="word" start_char="838">does</TOKEN>
<TOKEN end_char="845" id="token-15-3" morph="none" pos="word" start_char="843">not</TOKEN>
<TOKEN end_char="851" id="token-15-4" morph="none" pos="word" start_char="847">equal</TOKEN>
<TOKEN end_char="858" id="token-15-5" morph="none" pos="word" start_char="853">doctor</TOKEN>
<TRANSLATED_TEXT>Quote: Dat blijft niet gelijk aan arts</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="932" id="segment-16" start_char="861">
<ORIGINAL_TEXT>You don't have to be a doctor to recognize a death camp mentality, Slick</ORIGINAL_TEXT>
<TOKEN end_char="863" id="token-16-0" morph="none" pos="word" start_char="861">You</TOKEN>
<TOKEN end_char="869" id="token-16-1" morph="none" pos="word" start_char="865">don't</TOKEN>
<TOKEN end_char="874" id="token-16-2" morph="none" pos="word" start_char="871">have</TOKEN>
<TOKEN end_char="877" id="token-16-3" morph="none" pos="word" start_char="876">to</TOKEN>
<TOKEN end_char="880" id="token-16-4" morph="none" pos="word" start_char="879">be</TOKEN>
<TOKEN end_char="882" id="token-16-5" morph="none" pos="word" start_char="882">a</TOKEN>
<TOKEN end_char="889" id="token-16-6" morph="none" pos="word" start_char="884">doctor</TOKEN>
<TOKEN end_char="892" id="token-16-7" morph="none" pos="word" start_char="891">to</TOKEN>
<TOKEN end_char="902" id="token-16-8" morph="none" pos="word" start_char="894">recognize</TOKEN>
<TOKEN end_char="904" id="token-16-9" morph="none" pos="word" start_char="904">a</TOKEN>
<TOKEN end_char="910" id="token-16-10" morph="none" pos="word" start_char="906">death</TOKEN>
<TOKEN end_char="915" id="token-16-11" morph="none" pos="word" start_char="912">camp</TOKEN>
<TOKEN end_char="925" id="token-16-12" morph="none" pos="word" start_char="917">mentality</TOKEN>
<TOKEN end_char="926" id="token-16-13" morph="none" pos="punct" start_char="926">,</TOKEN>
<TOKEN end_char="932" id="token-16-14" morph="none" pos="word" start_char="928">Slick</TOKEN>
</SEG>
<SEG end_char="979" id="segment-17" start_char="937">
<ORIGINAL_TEXT>Then she needs to clean up her nasty mouth.</ORIGINAL_TEXT>
<TOKEN end_char="940" id="token-17-0" morph="none" pos="word" start_char="937">Then</TOKEN>
<TOKEN end_char="944" id="token-17-1" morph="none" pos="word" start_char="942">she</TOKEN>
<TOKEN end_char="950" id="token-17-2" morph="none" pos="word" start_char="946">needs</TOKEN>
<TOKEN end_char="953" id="token-17-3" morph="none" pos="word" start_char="952">to</TOKEN>
<TOKEN end_char="959" id="token-17-4" morph="none" pos="word" start_char="955">clean</TOKEN>
<TOKEN end_char="962" id="token-17-5" morph="none" pos="word" start_char="961">up</TOKEN>
<TOKEN end_char="966" id="token-17-6" morph="none" pos="word" start_char="964">her</TOKEN>
<TOKEN end_char="972" id="token-17-7" morph="none" pos="word" start_char="968">nasty</TOKEN>
<TOKEN end_char="978" id="token-17-8" morph="none" pos="word" start_char="974">mouth</TOKEN>
<TOKEN end_char="979" id="token-17-9" morph="none" pos="punct" start_char="979">.</TOKEN>
</SEG>
<SEG end_char="1069" id="segment-18" start_char="981">
<ORIGINAL_TEXT>IF she is on a nursing board then she should know the proper channels to make complaints.</ORIGINAL_TEXT>
<TOKEN end_char="982" id="token-18-0" morph="none" pos="word" start_char="981">IF</TOKEN>
<TOKEN end_char="986" id="token-18-1" morph="none" pos="word" start_char="984">she</TOKEN>
<TOKEN end_char="989" id="token-18-2" morph="none" pos="word" start_char="988">is</TOKEN>
<TOKEN end_char="992" id="token-18-3" morph="none" pos="word" start_char="991">on</TOKEN>
<TOKEN end_char="994" id="token-18-4" morph="none" pos="word" start_char="994">a</TOKEN>
<TOKEN end_char="1002" id="token-18-5" morph="none" pos="word" start_char="996">nursing</TOKEN>
<TOKEN end_char="1008" id="token-18-6" morph="none" pos="word" start_char="1004">board</TOKEN>
<TOKEN end_char="1013" id="token-18-7" morph="none" pos="word" start_char="1010">then</TOKEN>
<TOKEN end_char="1017" id="token-18-8" morph="none" pos="word" start_char="1015">she</TOKEN>
<TOKEN end_char="1024" id="token-18-9" morph="none" pos="word" start_char="1019">should</TOKEN>
<TOKEN end_char="1029" id="token-18-10" morph="none" pos="word" start_char="1026">know</TOKEN>
<TOKEN end_char="1033" id="token-18-11" morph="none" pos="word" start_char="1031">the</TOKEN>
<TOKEN end_char="1040" id="token-18-12" morph="none" pos="word" start_char="1035">proper</TOKEN>
<TOKEN end_char="1049" id="token-18-13" morph="none" pos="word" start_char="1042">channels</TOKEN>
<TOKEN end_char="1052" id="token-18-14" morph="none" pos="word" start_char="1051">to</TOKEN>
<TOKEN end_char="1057" id="token-18-15" morph="none" pos="word" start_char="1054">make</TOKEN>
<TOKEN end_char="1068" id="token-18-16" morph="none" pos="word" start_char="1059">complaints</TOKEN>
<TOKEN end_char="1069" id="token-18-17" morph="none" pos="punct" start_char="1069">.</TOKEN>
</SEG>
<SEG end_char="1135" id="segment-19" start_char="1071">
<ORIGINAL_TEXT>Certainly an uncensored you tube video is not the proper channel.</ORIGINAL_TEXT>
<TOKEN end_char="1079" id="token-19-0" morph="none" pos="word" start_char="1071">Certainly</TOKEN>
<TOKEN end_char="1082" id="token-19-1" morph="none" pos="word" start_char="1081">an</TOKEN>
<TOKEN end_char="1093" id="token-19-2" morph="none" pos="word" start_char="1084">uncensored</TOKEN>
<TOKEN end_char="1097" id="token-19-3" morph="none" pos="word" start_char="1095">you</TOKEN>
<TOKEN end_char="1102" id="token-19-4" morph="none" pos="word" start_char="1099">tube</TOKEN>
<TOKEN end_char="1108" id="token-19-5" morph="none" pos="word" start_char="1104">video</TOKEN>
<TOKEN end_char="1111" id="token-19-6" morph="none" pos="word" start_char="1110">is</TOKEN>
<TOKEN end_char="1115" id="token-19-7" morph="none" pos="word" start_char="1113">not</TOKEN>
<TOKEN end_char="1119" id="token-19-8" morph="none" pos="word" start_char="1117">the</TOKEN>
<TOKEN end_char="1126" id="token-19-9" morph="none" pos="word" start_char="1121">proper</TOKEN>
<TOKEN end_char="1134" id="token-19-10" morph="none" pos="word" start_char="1128">channel</TOKEN>
<TOKEN end_char="1135" id="token-19-11" morph="none" pos="punct" start_char="1135">.</TOKEN>
</SEG>
<SEG end_char="1178" id="segment-20" start_char="1140">
<ORIGINAL_TEXT>quote:That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN end_char="1149" id="token-20-0" morph="none" pos="unknown" start_char="1140">quote:That</TOKEN>
<TOKEN end_char="1155" id="token-20-1" morph="none" pos="word" start_char="1151">still</TOKEN>
<TOKEN end_char="1160" id="token-20-2" morph="none" pos="word" start_char="1157">does</TOKEN>
<TOKEN end_char="1164" id="token-20-3" morph="none" pos="word" start_char="1162">not</TOKEN>
<TOKEN end_char="1170" id="token-20-4" morph="none" pos="word" start_char="1166">equal</TOKEN>
<TOKEN end_char="1177" id="token-20-5" morph="none" pos="word" start_char="1172">doctor</TOKEN>
<TOKEN end_char="1178" id="token-20-6" morph="none" pos="punct" start_char="1178">.</TOKEN>
<TRANSLATED_TEXT>Quote: Dat is nog niet gelijk aan de arts.</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="1208" id="segment-21" start_char="1180">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN end_char="1183" id="token-21-0" morph="none" pos="word" start_char="1180">Does</TOKEN>
<TOKEN end_char="1187" id="token-21-1" morph="none" pos="word" start_char="1185">she</TOKEN>
<TOKEN end_char="1195" id="token-21-2" morph="none" pos="word" start_char="1189">deserve</TOKEN>
<TOKEN end_char="1198" id="token-21-3" morph="none" pos="word" start_char="1197">to</TOKEN>
<TOKEN end_char="1201" id="token-21-4" morph="none" pos="word" start_char="1200">be</TOKEN>
<TOKEN end_char="1207" id="token-21-5" morph="none" pos="word" start_char="1203">heard</TOKEN>
<TOKEN end_char="1208" id="token-21-6" morph="none" pos="punct" start_char="1208">?</TOKEN>
</SEG>
<SEG end_char="1218" id="segment-22" start_char="1210">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN end_char="1218" id="token-22-0" morph="none" pos="word" start_char="1210">Certainly</TOKEN>
<TRANSLATED_TEXT>Assuramente</TRANSLATED_TEXT><DETECTED_LANGUAGE>cy</DETECTED_LANGUAGE></SEG>
<SEG end_char="1253" id="segment-23" start_char="1221">
<ORIGINAL_TEXT>Help me understand your position.</ORIGINAL_TEXT>
<TOKEN end_char="1224" id="token-23-0" morph="none" pos="word" start_char="1221">Help</TOKEN>
<TOKEN end_char="1227" id="token-23-1" morph="none" pos="word" start_char="1226">me</TOKEN>
<TOKEN end_char="1238" id="token-23-2" morph="none" pos="word" start_char="1229">understand</TOKEN>
<TOKEN end_char="1243" id="token-23-3" morph="none" pos="word" start_char="1240">your</TOKEN>
<TOKEN end_char="1252" id="token-23-4" morph="none" pos="word" start_char="1245">position</TOKEN>
<TOKEN end_char="1253" id="token-23-5" morph="none" pos="punct" start_char="1253">.</TOKEN>
</SEG>
<SEG end_char="1332" id="segment-24" start_char="1255">
<ORIGINAL_TEXT>Why does one need to be a doctor to witness abuses and be able to report them?</ORIGINAL_TEXT>
<TOKEN end_char="1257" id="token-24-0" morph="none" pos="word" start_char="1255">Why</TOKEN>
<TOKEN end_char="1262" id="token-24-1" morph="none" pos="word" start_char="1259">does</TOKEN>
<TOKEN end_char="1266" id="token-24-2" morph="none" pos="word" start_char="1264">one</TOKEN>
<TOKEN end_char="1271" id="token-24-3" morph="none" pos="word" start_char="1268">need</TOKEN>
<TOKEN end_char="1274" id="token-24-4" morph="none" pos="word" start_char="1273">to</TOKEN>
<TOKEN end_char="1277" id="token-24-5" morph="none" pos="word" start_char="1276">be</TOKEN>
<TOKEN end_char="1279" id="token-24-6" morph="none" pos="word" start_char="1279">a</TOKEN>
<TOKEN end_char="1286" id="token-24-7" morph="none" pos="word" start_char="1281">doctor</TOKEN>
<TOKEN end_char="1289" id="token-24-8" morph="none" pos="word" start_char="1288">to</TOKEN>
<TOKEN end_char="1297" id="token-24-9" morph="none" pos="word" start_char="1291">witness</TOKEN>
<TOKEN end_char="1304" id="token-24-10" morph="none" pos="word" start_char="1299">abuses</TOKEN>
<TOKEN end_char="1308" id="token-24-11" morph="none" pos="word" start_char="1306">and</TOKEN>
<TOKEN end_char="1311" id="token-24-12" morph="none" pos="word" start_char="1310">be</TOKEN>
<TOKEN end_char="1316" id="token-24-13" morph="none" pos="word" start_char="1313">able</TOKEN>
<TOKEN end_char="1319" id="token-24-14" morph="none" pos="word" start_char="1318">to</TOKEN>
<TOKEN end_char="1326" id="token-24-15" morph="none" pos="word" start_char="1321">report</TOKEN>
<TOKEN end_char="1331" id="token-24-16" morph="none" pos="word" start_char="1328">them</TOKEN>
<TOKEN end_char="1332" id="token-24-17" morph="none" pos="punct" start_char="1332">?</TOKEN>
</SEG>
<SEG end_char="1399" id="segment-25" start_char="1334">
<ORIGINAL_TEXT>Nurses are supposed to be familiar with procedures and treatments.</ORIGINAL_TEXT>
<TOKEN end_char="1339" id="token-25-0" morph="none" pos="word" start_char="1334">Nurses</TOKEN>
<TOKEN end_char="1343" id="token-25-1" morph="none" pos="word" start_char="1341">are</TOKEN>
<TOKEN end_char="1352" id="token-25-2" morph="none" pos="word" start_char="1345">supposed</TOKEN>
<TOKEN end_char="1355" id="token-25-3" morph="none" pos="word" start_char="1354">to</TOKEN>
<TOKEN end_char="1358" id="token-25-4" morph="none" pos="word" start_char="1357">be</TOKEN>
<TOKEN end_char="1367" id="token-25-5" morph="none" pos="word" start_char="1360">familiar</TOKEN>
<TOKEN end_char="1372" id="token-25-6" morph="none" pos="word" start_char="1369">with</TOKEN>
<TOKEN end_char="1383" id="token-25-7" morph="none" pos="word" start_char="1374">procedures</TOKEN>
<TOKEN end_char="1387" id="token-25-8" morph="none" pos="word" start_char="1385">and</TOKEN>
<TOKEN end_char="1398" id="token-25-9" morph="none" pos="word" start_char="1389">treatments</TOKEN>
<TOKEN end_char="1399" id="token-25-10" morph="none" pos="punct" start_char="1399">.</TOKEN>
</SEG>
<SEG end_char="1519" id="segment-26" start_char="1401">
<ORIGINAL_TEXT>So if something stood out as an obvious infraction to her, I don't know why we need to qualify that she isn't a doctor.</ORIGINAL_TEXT>
<TOKEN end_char="1402" id="token-26-0" morph="none" pos="word" start_char="1401">So</TOKEN>
<TOKEN end_char="1405" id="token-26-1" morph="none" pos="word" start_char="1404">if</TOKEN>
<TOKEN end_char="1415" id="token-26-2" morph="none" pos="word" start_char="1407">something</TOKEN>
<TOKEN end_char="1421" id="token-26-3" morph="none" pos="word" start_char="1417">stood</TOKEN>
<TOKEN end_char="1425" id="token-26-4" morph="none" pos="word" start_char="1423">out</TOKEN>
<TOKEN end_char="1428" id="token-26-5" morph="none" pos="word" start_char="1427">as</TOKEN>
<TOKEN end_char="1431" id="token-26-6" morph="none" pos="word" start_char="1430">an</TOKEN>
<TOKEN end_char="1439" id="token-26-7" morph="none" pos="word" start_char="1433">obvious</TOKEN>
<TOKEN end_char="1450" id="token-26-8" morph="none" pos="word" start_char="1441">infraction</TOKEN>
<TOKEN end_char="1453" id="token-26-9" morph="none" pos="word" start_char="1452">to</TOKEN>
<TOKEN end_char="1457" id="token-26-10" morph="none" pos="word" start_char="1455">her</TOKEN>
<TOKEN end_char="1458" id="token-26-11" morph="none" pos="punct" start_char="1458">,</TOKEN>
<TOKEN end_char="1460" id="token-26-12" morph="none" pos="word" start_char="1460">I</TOKEN>
<TOKEN end_char="1466" id="token-26-13" morph="none" pos="word" start_char="1462">don't</TOKEN>
<TOKEN end_char="1471" id="token-26-14" morph="none" pos="word" start_char="1468">know</TOKEN>
<TOKEN end_char="1475" id="token-26-15" morph="none" pos="word" start_char="1473">why</TOKEN>
<TOKEN end_char="1478" id="token-26-16" morph="none" pos="word" start_char="1477">we</TOKEN>
<TOKEN end_char="1483" id="token-26-17" morph="none" pos="word" start_char="1480">need</TOKEN>
<TOKEN end_char="1486" id="token-26-18" morph="none" pos="word" start_char="1485">to</TOKEN>
<TOKEN end_char="1494" id="token-26-19" morph="none" pos="word" start_char="1488">qualify</TOKEN>
<TOKEN end_char="1499" id="token-26-20" morph="none" pos="word" start_char="1496">that</TOKEN>
<TOKEN end_char="1503" id="token-26-21" morph="none" pos="word" start_char="1501">she</TOKEN>
<TOKEN end_char="1509" id="token-26-22" morph="none" pos="word" start_char="1505">isn't</TOKEN>
<TOKEN end_char="1511" id="token-26-23" morph="none" pos="word" start_char="1511">a</TOKEN>
<TOKEN end_char="1518" id="token-26-24" morph="none" pos="word" start_char="1513">doctor</TOKEN>
<TOKEN end_char="1519" id="token-26-25" morph="none" pos="punct" start_char="1519">.</TOKEN>
</SEG>
<SEG end_char="1626" id="segment-27" start_char="1525">
<ORIGINAL_TEXT>quote:It puts her a cut above the Nurse Jackie Tik Tok crew who are the face of nursing at the moment.</ORIGINAL_TEXT>
<TOKEN end_char="1532" id="token-27-0" morph="none" pos="unknown" start_char="1525">quote:It</TOKEN>
<TOKEN end_char="1537" id="token-27-1" morph="none" pos="word" start_char="1534">puts</TOKEN>
<TOKEN end_char="1541" id="token-27-2" morph="none" pos="word" start_char="1539">her</TOKEN>
<TOKEN end_char="1543" id="token-27-3" morph="none" pos="word" start_char="1543">a</TOKEN>
<TOKEN end_char="1547" id="token-27-4" morph="none" pos="word" start_char="1545">cut</TOKEN>
<TOKEN end_char="1553" id="token-27-5" morph="none" pos="word" start_char="1549">above</TOKEN>
<TOKEN end_char="1557" id="token-27-6" morph="none" pos="word" start_char="1555">the</TOKEN>
<TOKEN end_char="1563" id="token-27-7" morph="none" pos="word" start_char="1559">Nurse</TOKEN>
<TOKEN end_char="1570" id="token-27-8" morph="none" pos="word" start_char="1565">Jackie</TOKEN>
<TOKEN end_char="1574" id="token-27-9" morph="none" pos="word" start_char="1572">Tik</TOKEN>
<TOKEN end_char="1578" id="token-27-10" morph="none" pos="word" start_char="1576">Tok</TOKEN>
<TOKEN end_char="1583" id="token-27-11" morph="none" pos="word" start_char="1580">crew</TOKEN>
<TOKEN end_char="1587" id="token-27-12" morph="none" pos="word" start_char="1585">who</TOKEN>
<TOKEN end_char="1591" id="token-27-13" morph="none" pos="word" start_char="1589">are</TOKEN>
<TOKEN end_char="1595" id="token-27-14" morph="none" pos="word" start_char="1593">the</TOKEN>
<TOKEN end_char="1600" id="token-27-15" morph="none" pos="word" start_char="1597">face</TOKEN>
<TOKEN end_char="1603" id="token-27-16" morph="none" pos="word" start_char="1602">of</TOKEN>
<TOKEN end_char="1611" id="token-27-17" morph="none" pos="word" start_char="1605">nursing</TOKEN>
<TOKEN end_char="1614" id="token-27-18" morph="none" pos="word" start_char="1613">at</TOKEN>
<TOKEN end_char="1618" id="token-27-19" morph="none" pos="word" start_char="1616">the</TOKEN>
<TOKEN end_char="1625" id="token-27-20" morph="none" pos="word" start_char="1620">moment</TOKEN>
<TOKEN end_char="1626" id="token-27-21" morph="none" pos="punct" start_char="1626">.</TOKEN>
</SEG>
<SEG end_char="1632" id="segment-28" start_char="1629">
<ORIGINAL_TEXT>Why?</ORIGINAL_TEXT>
<TOKEN end_char="1631" id="token-28-0" morph="none" pos="word" start_char="1629">Why</TOKEN>
<TOKEN end_char="1632" id="token-28-1" morph="none" pos="punct" start_char="1632">?</TOKEN>
<TRANSLATED_TEXT>Waarom?</TRANSLATED_TEXT><DETECTED_LANGUAGE>cy</DETECTED_LANGUAGE></SEG>
<SEG end_char="1713" id="segment-29" start_char="1634">
<ORIGINAL_TEXT>Are doctors doing similar things to patients in hospitals you are familiar with?</ORIGINAL_TEXT>
<TOKEN end_char="1636" id="token-29-0" morph="none" pos="word" start_char="1634">Are</TOKEN>
<TOKEN end_char="1644" id="token-29-1" morph="none" pos="word" start_char="1638">doctors</TOKEN>
<TOKEN end_char="1650" id="token-29-2" morph="none" pos="word" start_char="1646">doing</TOKEN>
<TOKEN end_char="1658" id="token-29-3" morph="none" pos="word" start_char="1652">similar</TOKEN>
<TOKEN end_char="1665" id="token-29-4" morph="none" pos="word" start_char="1660">things</TOKEN>
<TOKEN end_char="1668" id="token-29-5" morph="none" pos="word" start_char="1667">to</TOKEN>
<TOKEN end_char="1677" id="token-29-6" morph="none" pos="word" start_char="1670">patients</TOKEN>
<TOKEN end_char="1680" id="token-29-7" morph="none" pos="word" start_char="1679">in</TOKEN>
<TOKEN end_char="1690" id="token-29-8" morph="none" pos="word" start_char="1682">hospitals</TOKEN>
<TOKEN end_char="1694" id="token-29-9" morph="none" pos="word" start_char="1692">you</TOKEN>
<TOKEN end_char="1698" id="token-29-10" morph="none" pos="word" start_char="1696">are</TOKEN>
<TOKEN end_char="1707" id="token-29-11" morph="none" pos="word" start_char="1700">familiar</TOKEN>
<TOKEN end_char="1712" id="token-29-12" morph="none" pos="word" start_char="1709">with</TOKEN>
<TOKEN end_char="1713" id="token-29-13" morph="none" pos="punct" start_char="1713">?</TOKEN>
</SEG>
<SEG end_char="1758" id="segment-30" start_char="1715">
<ORIGINAL_TEXT>Standard procedure in 2020 medical practice?</ORIGINAL_TEXT>
<TOKEN end_char="1722" id="token-30-0" morph="none" pos="word" start_char="1715">Standard</TOKEN>
<TOKEN end_char="1732" id="token-30-1" morph="none" pos="word" start_char="1724">procedure</TOKEN>
<TOKEN end_char="1735" id="token-30-2" morph="none" pos="word" start_char="1734">in</TOKEN>
<TOKEN end_char="1740" id="token-30-3" morph="none" pos="word" start_char="1737">2020</TOKEN>
<TOKEN end_char="1748" id="token-30-4" morph="none" pos="word" start_char="1742">medical</TOKEN>
<TOKEN end_char="1757" id="token-30-5" morph="none" pos="word" start_char="1750">practice</TOKEN>
<TOKEN end_char="1758" id="token-30-6" morph="none" pos="punct" start_char="1758">?</TOKEN>
</SEG>
<SEG end_char="1809" id="segment-31" start_char="1761">
<ORIGINAL_TEXT>At least she has the raw courage to warn a nation</ORIGINAL_TEXT>
<TOKEN end_char="1762" id="token-31-0" morph="none" pos="word" start_char="1761">At</TOKEN>
<TOKEN end_char="1768" id="token-31-1" morph="none" pos="word" start_char="1764">least</TOKEN>
<TOKEN end_char="1772" id="token-31-2" morph="none" pos="word" start_char="1770">she</TOKEN>
<TOKEN end_char="1776" id="token-31-3" morph="none" pos="word" start_char="1774">has</TOKEN>
<TOKEN end_char="1780" id="token-31-4" morph="none" pos="word" start_char="1778">the</TOKEN>
<TOKEN end_char="1784" id="token-31-5" morph="none" pos="word" start_char="1782">raw</TOKEN>
<TOKEN end_char="1792" id="token-31-6" morph="none" pos="word" start_char="1786">courage</TOKEN>
<TOKEN end_char="1795" id="token-31-7" morph="none" pos="word" start_char="1794">to</TOKEN>
<TOKEN end_char="1800" id="token-31-8" morph="none" pos="word" start_char="1797">warn</TOKEN>
<TOKEN end_char="1802" id="token-31-9" morph="none" pos="word" start_char="1802">a</TOKEN>
<TOKEN end_char="1809" id="token-31-10" morph="none" pos="word" start_char="1804">nation</TOKEN>
</SEG>
<SEG end_char="1850" id="segment-32" start_char="1812">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:10 pm</ORIGINAL_TEXT>
<TOKEN end_char="1815" id="token-32-0" morph="none" pos="word" start_char="1812">This</TOKEN>
<TOKEN end_char="1820" id="token-32-1" morph="none" pos="word" start_char="1817">post</TOKEN>
<TOKEN end_char="1824" id="token-32-2" morph="none" pos="word" start_char="1822">was</TOKEN>
<TOKEN end_char="1831" id="token-32-3" morph="none" pos="word" start_char="1826">edited</TOKEN>
<TOKEN end_char="1834" id="token-32-4" morph="none" pos="word" start_char="1833">on</TOKEN>
<TOKEN end_char="1838" id="token-32-5" morph="none" pos="unknown" start_char="1836">5/5</TOKEN>
<TOKEN end_char="1841" id="token-32-6" morph="none" pos="word" start_char="1840">at</TOKEN>
<TOKEN end_char="1847" id="token-32-7" morph="none" pos="unknown" start_char="1843">12:10</TOKEN>
<TOKEN end_char="1850" id="token-32-8" morph="none" pos="word" start_char="1849">pm</TOKEN>
</SEG>
<SEG end_char="1910" id="segment-33" start_char="1855">
<ORIGINAL_TEXT>Is this the woman accusing NY hospitals of being racist?</ORIGINAL_TEXT>
<TOKEN end_char="1856" id="token-33-0" morph="none" pos="word" start_char="1855">Is</TOKEN>
<TOKEN end_char="1861" id="token-33-1" morph="none" pos="word" start_char="1858">this</TOKEN>
<TOKEN end_char="1865" id="token-33-2" morph="none" pos="word" start_char="1863">the</TOKEN>
<TOKEN end_char="1871" id="token-33-3" morph="none" pos="word" start_char="1867">woman</TOKEN>
<TOKEN end_char="1880" id="token-33-4" morph="none" pos="word" start_char="1873">accusing</TOKEN>
<TOKEN end_char="1883" id="token-33-5" morph="none" pos="word" start_char="1882">NY</TOKEN>
<TOKEN end_char="1893" id="token-33-6" morph="none" pos="word" start_char="1885">hospitals</TOKEN>
<TOKEN end_char="1896" id="token-33-7" morph="none" pos="word" start_char="1895">of</TOKEN>
<TOKEN end_char="1902" id="token-33-8" morph="none" pos="word" start_char="1898">being</TOKEN>
<TOKEN end_char="1909" id="token-33-9" morph="none" pos="word" start_char="1904">racist</TOKEN>
<TOKEN end_char="1910" id="token-33-10" morph="none" pos="punct" start_char="1910">?</TOKEN>
</SEG>
<SEG end_char="1953" id="segment-34" start_char="1915">
<ORIGINAL_TEXT>quote:That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN end_char="1924" id="token-34-0" morph="none" pos="unknown" start_char="1915">quote:That</TOKEN>
<TOKEN end_char="1930" id="token-34-1" morph="none" pos="word" start_char="1926">still</TOKEN>
<TOKEN end_char="1935" id="token-34-2" morph="none" pos="word" start_char="1932">does</TOKEN>
<TOKEN end_char="1939" id="token-34-3" morph="none" pos="word" start_char="1937">not</TOKEN>
<TOKEN end_char="1945" id="token-34-4" morph="none" pos="word" start_char="1941">equal</TOKEN>
<TOKEN end_char="1952" id="token-34-5" morph="none" pos="word" start_char="1947">doctor</TOKEN>
<TOKEN end_char="1953" id="token-34-6" morph="none" pos="punct" start_char="1953">.</TOKEN>
<TRANSLATED_TEXT>Quote: Dat is nog niet gelijk aan de arts.</TRANSLATED_TEXT><DETECTED_LANGUAGE>ca</DETECTED_LANGUAGE></SEG>
<SEG end_char="1983" id="segment-35" start_char="1955">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN end_char="1958" id="token-35-0" morph="none" pos="word" start_char="1955">Does</TOKEN>
<TOKEN end_char="1962" id="token-35-1" morph="none" pos="word" start_char="1960">she</TOKEN>
<TOKEN end_char="1970" id="token-35-2" morph="none" pos="word" start_char="1964">deserve</TOKEN>
<TOKEN end_char="1973" id="token-35-3" morph="none" pos="word" start_char="1972">to</TOKEN>
<TOKEN end_char="1976" id="token-35-4" morph="none" pos="word" start_char="1975">be</TOKEN>
<TOKEN end_char="1982" id="token-35-5" morph="none" pos="word" start_char="1978">heard</TOKEN>
<TOKEN end_char="1983" id="token-35-6" morph="none" pos="punct" start_char="1983">?</TOKEN>
</SEG>
<SEG end_char="1993" id="segment-36" start_char="1985">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN end_char="1993" id="token-36-0" morph="none" pos="word" start_char="1985">Certainly</TOKEN>
<TRANSLATED_TEXT>Assuramente</TRANSLATED_TEXT><DETECTED_LANGUAGE>cy</DETECTED_LANGUAGE></SEG>
<SEG end_char="2089" id="segment-37" start_char="1996">
<ORIGINAL_TEXT>The opinion of a experienced RN on the treatment of patients is probably more valid than a MD.</ORIGINAL_TEXT>
<TOKEN end_char="1998" id="token-37-0" morph="none" pos="word" start_char="1996">The</TOKEN>
<TOKEN end_char="2006" id="token-37-1" morph="none" pos="word" start_char="2000">opinion</TOKEN>
<TOKEN end_char="2009" id="token-37-2" morph="none" pos="word" start_char="2008">of</TOKEN>
<TOKEN end_char="2011" id="token-37-3" morph="none" pos="word" start_char="2011">a</TOKEN>
<TOKEN end_char="2023" id="token-37-4" morph="none" pos="word" start_char="2013">experienced</TOKEN>
<TOKEN end_char="2026" id="token-37-5" morph="none" pos="word" start_char="2025">RN</TOKEN>
<TOKEN end_char="2029" id="token-37-6" morph="none" pos="word" start_char="2028">on</TOKEN>
<TOKEN end_char="2033" id="token-37-7" morph="none" pos="word" start_char="2031">the</TOKEN>
<TOKEN end_char="2043" id="token-37-8" morph="none" pos="word" start_char="2035">treatment</TOKEN>
<TOKEN end_char="2046" id="token-37-9" morph="none" pos="word" start_char="2045">of</TOKEN>
<TOKEN end_char="2055" id="token-37-10" morph="none" pos="word" start_char="2048">patients</TOKEN>
<TOKEN end_char="2058" id="token-37-11" morph="none" pos="word" start_char="2057">is</TOKEN>
<TOKEN end_char="2067" id="token-37-12" morph="none" pos="word" start_char="2060">probably</TOKEN>
<TOKEN end_char="2072" id="token-37-13" morph="none" pos="word" start_char="2069">more</TOKEN>
<TOKEN end_char="2078" id="token-37-14" morph="none" pos="word" start_char="2074">valid</TOKEN>
<TOKEN end_char="2083" id="token-37-15" morph="none" pos="word" start_char="2080">than</TOKEN>
<TOKEN end_char="2085" id="token-37-16" morph="none" pos="word" start_char="2085">a</TOKEN>
<TOKEN end_char="2088" id="token-37-17" morph="none" pos="word" start_char="2087">MD</TOKEN>
<TOKEN end_char="2089" id="token-37-18" morph="none" pos="punct" start_char="2089">.</TOKEN>
</SEG>
<SEG end_char="2142" id="segment-38" start_char="2094">
<ORIGINAL_TEXT>I would expect a higher level of professionalism.</ORIGINAL_TEXT>
<TOKEN end_char="2094" id="token-38-0" morph="none" pos="word" start_char="2094">I</TOKEN>
<TOKEN end_char="2100" id="token-38-1" morph="none" pos="word" start_char="2096">would</TOKEN>
<TOKEN end_char="2107" id="token-38-2" morph="none" pos="word" start_char="2102">expect</TOKEN>
<TOKEN end_char="2109" id="token-38-3" morph="none" pos="word" start_char="2109">a</TOKEN>
<TOKEN end_char="2116" id="token-38-4" morph="none" pos="word" start_char="2111">higher</TOKEN>
<TOKEN end_char="2122" id="token-38-5" morph="none" pos="word" start_char="2118">level</TOKEN>
<TOKEN end_char="2125" id="token-38-6" morph="none" pos="word" start_char="2124">of</TOKEN>
<TOKEN end_char="2141" id="token-38-7" morph="none" pos="word" start_char="2127">professionalism</TOKEN>
<TOKEN end_char="2142" id="token-38-8" morph="none" pos="punct" start_char="2142">.</TOKEN>
</SEG>
<SEG end_char="2218" id="segment-39" start_char="2144">
<ORIGINAL_TEXT>In her video she immediately stated that the oversight agencies don’t care.</ORIGINAL_TEXT>
<TOKEN end_char="2145" id="token-39-0" morph="none" pos="word" start_char="2144">In</TOKEN>
<TOKEN end_char="2149" id="token-39-1" morph="none" pos="word" start_char="2147">her</TOKEN>
<TOKEN end_char="2155" id="token-39-2" morph="none" pos="word" start_char="2151">video</TOKEN>
<TOKEN end_char="2159" id="token-39-3" morph="none" pos="word" start_char="2157">she</TOKEN>
<TOKEN end_char="2171" id="token-39-4" morph="none" pos="word" start_char="2161">immediately</TOKEN>
<TOKEN end_char="2178" id="token-39-5" morph="none" pos="word" start_char="2173">stated</TOKEN>
<TOKEN end_char="2183" id="token-39-6" morph="none" pos="word" start_char="2180">that</TOKEN>
<TOKEN end_char="2187" id="token-39-7" morph="none" pos="word" start_char="2185">the</TOKEN>
<TOKEN end_char="2197" id="token-39-8" morph="none" pos="word" start_char="2189">oversight</TOKEN>
<TOKEN end_char="2206" id="token-39-9" morph="none" pos="word" start_char="2199">agencies</TOKEN>
<TOKEN end_char="2212" id="token-39-10" morph="none" pos="word" start_char="2208">don’t</TOKEN>
<TOKEN end_char="2217" id="token-39-11" morph="none" pos="word" start_char="2214">care</TOKEN>
<TOKEN end_char="2218" id="token-39-12" morph="none" pos="punct" start_char="2218">.</TOKEN>
</SEG>
<SEG end_char="2253" id="segment-40" start_char="2220">
<ORIGINAL_TEXT>Has she approached those agencies?</ORIGINAL_TEXT>
<TOKEN end_char="2222" id="token-40-0" morph="none" pos="word" start_char="2220">Has</TOKEN>
<TOKEN end_char="2226" id="token-40-1" morph="none" pos="word" start_char="2224">she</TOKEN>
<TOKEN end_char="2237" id="token-40-2" morph="none" pos="word" start_char="2228">approached</TOKEN>
<TOKEN end_char="2243" id="token-40-3" morph="none" pos="word" start_char="2239">those</TOKEN>
<TOKEN end_char="2252" id="token-40-4" morph="none" pos="word" start_char="2245">agencies</TOKEN>
<TOKEN end_char="2253" id="token-40-5" morph="none" pos="punct" start_char="2253">?</TOKEN>
</SEG>
<SEG end_char="2297" id="segment-41" start_char="2255">
<ORIGINAL_TEXT>Does she have documented complaints logged?</ORIGINAL_TEXT>
<TOKEN end_char="2258" id="token-41-0" morph="none" pos="word" start_char="2255">Does</TOKEN>
<TOKEN end_char="2262" id="token-41-1" morph="none" pos="word" start_char="2260">she</TOKEN>
<TOKEN end_char="2267" id="token-41-2" morph="none" pos="word" start_char="2264">have</TOKEN>
<TOKEN end_char="2278" id="token-41-3" morph="none" pos="word" start_char="2269">documented</TOKEN>
<TOKEN end_char="2289" id="token-41-4" morph="none" pos="word" start_char="2280">complaints</TOKEN>
<TOKEN end_char="2296" id="token-41-5" morph="none" pos="word" start_char="2291">logged</TOKEN>
<TOKEN end_char="2297" id="token-41-6" morph="none" pos="punct" start_char="2297">?</TOKEN>
</SEG>
<SEG end_char="2362" id="segment-42" start_char="2300">
<ORIGINAL_TEXT>If not did she go strait to posting videos for internet points?</ORIGINAL_TEXT>
<TOKEN end_char="2301" id="token-42-0" morph="none" pos="word" start_char="2300">If</TOKEN>
<TOKEN end_char="2305" id="token-42-1" morph="none" pos="word" start_char="2303">not</TOKEN>
<TOKEN end_char="2309" id="token-42-2" morph="none" pos="word" start_char="2307">did</TOKEN>
<TOKEN end_char="2313" id="token-42-3" morph="none" pos="word" start_char="2311">she</TOKEN>
<TOKEN end_char="2316" id="token-42-4" morph="none" pos="word" start_char="2315">go</TOKEN>
<TOKEN end_char="2323" id="token-42-5" morph="none" pos="word" start_char="2318">strait</TOKEN>
<TOKEN end_char="2326" id="token-42-6" morph="none" pos="word" start_char="2325">to</TOKEN>
<TOKEN end_char="2334" id="token-42-7" morph="none" pos="word" start_char="2328">posting</TOKEN>
<TOKEN end_char="2341" id="token-42-8" morph="none" pos="word" start_char="2336">videos</TOKEN>
<TOKEN end_char="2345" id="token-42-9" morph="none" pos="word" start_char="2343">for</TOKEN>
<TOKEN end_char="2354" id="token-42-10" morph="none" pos="word" start_char="2347">internet</TOKEN>
<TOKEN end_char="2361" id="token-42-11" morph="none" pos="word" start_char="2356">points</TOKEN>
<TOKEN end_char="2362" id="token-42-12" morph="none" pos="punct" start_char="2362">?</TOKEN>
</SEG>
<SEG end_char="2397" id="segment-43" start_char="2366">
<ORIGINAL_TEXT>She is on a State Nursing board.</ORIGINAL_TEXT>
<TOKEN end_char="2368" id="token-43-0" morph="none" pos="word" start_char="2366">She</TOKEN>
<TOKEN end_char="2371" id="token-43-1" morph="none" pos="word" start_char="2370">is</TOKEN>
<TOKEN end_char="2374" id="token-43-2" morph="none" pos="word" start_char="2373">on</TOKEN>
<TOKEN end_char="2376" id="token-43-3" morph="none" pos="word" start_char="2376">a</TOKEN>
<TOKEN end_char="2382" id="token-43-4" morph="none" pos="word" start_char="2378">State</TOKEN>
<TOKEN end_char="2390" id="token-43-5" morph="none" pos="word" start_char="2384">Nursing</TOKEN>
<TOKEN end_char="2396" id="token-43-6" morph="none" pos="word" start_char="2392">board</TOKEN>
<TOKEN end_char="2397" id="token-43-7" morph="none" pos="punct" start_char="2397">.</TOKEN>
</SEG>
<SEG end_char="2473" id="segment-44" start_char="2399">
<ORIGINAL_TEXT>I would guess you have never been through a Joint Commission or CMS survey.</ORIGINAL_TEXT>
<TOKEN end_char="2399" id="token-44-0" morph="none" pos="word" start_char="2399">I</TOKEN>
<TOKEN end_char="2405" id="token-44-1" morph="none" pos="word" start_char="2401">would</TOKEN>
<TOKEN end_char="2411" id="token-44-2" morph="none" pos="word" start_char="2407">guess</TOKEN>
<TOKEN end_char="2415" id="token-44-3" morph="none" pos="word" start_char="2413">you</TOKEN>
<TOKEN end_char="2420" id="token-44-4" morph="none" pos="word" start_char="2417">have</TOKEN>
<TOKEN end_char="2426" id="token-44-5" morph="none" pos="word" start_char="2422">never</TOKEN>
<TOKEN end_char="2431" id="token-44-6" morph="none" pos="word" start_char="2428">been</TOKEN>
<TOKEN end_char="2439" id="token-44-7" morph="none" pos="word" start_char="2433">through</TOKEN>
<TOKEN end_char="2441" id="token-44-8" morph="none" pos="word" start_char="2441">a</TOKEN>
<TOKEN end_char="2447" id="token-44-9" morph="none" pos="word" start_char="2443">Joint</TOKEN>
<TOKEN end_char="2458" id="token-44-10" morph="none" pos="word" start_char="2449">Commission</TOKEN>
<TOKEN end_char="2461" id="token-44-11" morph="none" pos="word" start_char="2460">or</TOKEN>
<TOKEN end_char="2465" id="token-44-12" morph="none" pos="word" start_char="2463">CMS</TOKEN>
<TOKEN end_char="2472" id="token-44-13" morph="none" pos="word" start_char="2467">survey</TOKEN>
<TOKEN end_char="2473" id="token-44-14" morph="none" pos="punct" start_char="2473">.</TOKEN>
</SEG>
<SEG end_char="2525" id="segment-45" start_char="2475">
<ORIGINAL_TEXT>As a Director of Pharmacy I have been through many.</ORIGINAL_TEXT>
<TOKEN end_char="2476" id="token-45-0" morph="none" pos="word" start_char="2475">As</TOKEN>
<TOKEN end_char="2478" id="token-45-1" morph="none" pos="word" start_char="2478">a</TOKEN>
<TOKEN end_char="2487" id="token-45-2" morph="none" pos="word" start_char="2480">Director</TOKEN>
<TOKEN end_char="2490" id="token-45-3" morph="none" pos="word" start_char="2489">of</TOKEN>
<TOKEN end_char="2499" id="token-45-4" morph="none" pos="word" start_char="2492">Pharmacy</TOKEN>
<TOKEN end_char="2501" id="token-45-5" morph="none" pos="word" start_char="2501">I</TOKEN>
<TOKEN end_char="2506" id="token-45-6" morph="none" pos="word" start_char="2503">have</TOKEN>
<TOKEN end_char="2511" id="token-45-7" morph="none" pos="word" start_char="2508">been</TOKEN>
<TOKEN end_char="2519" id="token-45-8" morph="none" pos="word" start_char="2513">through</TOKEN>
<TOKEN end_char="2524" id="token-45-9" morph="none" pos="word" start_char="2521">many</TOKEN>
<TOKEN end_char="2525" id="token-45-10" morph="none" pos="punct" start_char="2525">.</TOKEN>
</SEG>
<SEG end_char="2565" id="segment-46" start_char="2527">
<ORIGINAL_TEXT>Nursing Leadership is heavily involved.</ORIGINAL_TEXT>
<TOKEN end_char="2533" id="token-46-0" morph="none" pos="word" start_char="2527">Nursing</TOKEN>
<TOKEN end_char="2544" id="token-46-1" morph="none" pos="word" start_char="2535">Leadership</TOKEN>
<TOKEN end_char="2547" id="token-46-2" morph="none" pos="word" start_char="2546">is</TOKEN>
<TOKEN end_char="2555" id="token-46-3" morph="none" pos="word" start_char="2549">heavily</TOKEN>
<TOKEN end_char="2564" id="token-46-4" morph="none" pos="word" start_char="2557">involved</TOKEN>
<TOKEN end_char="2565" id="token-46-5" morph="none" pos="punct" start_char="2565">.</TOKEN>
</SEG>
<SEG end_char="2607" id="segment-47" start_char="2567">
<ORIGINAL_TEXT>They know plenty about Standards of Care.</ORIGINAL_TEXT>
<TOKEN end_char="2570" id="token-47-0" morph="none" pos="word" start_char="2567">They</TOKEN>
<TOKEN end_char="2575" id="token-47-1" morph="none" pos="word" start_char="2572">know</TOKEN>
<TOKEN end_char="2582" id="token-47-2" morph="none" pos="word" start_char="2577">plenty</TOKEN>
<TOKEN end_char="2588" id="token-47-3" morph="none" pos="word" start_char="2584">about</TOKEN>
<TOKEN end_char="2598" id="token-47-4" morph="none" pos="word" start_char="2590">Standards</TOKEN>
<TOKEN end_char="2601" id="token-47-5" morph="none" pos="word" start_char="2600">of</TOKEN>
<TOKEN end_char="2606" id="token-47-6" morph="none" pos="word" start_char="2603">Care</TOKEN>
<TOKEN end_char="2607" id="token-47-7" morph="none" pos="punct" start_char="2607">.</TOKEN>
</SEG>
<SEG end_char="2673" id="segment-48" start_char="2612">
<ORIGINAL_TEXT>quote:Is this the woman accusing NY hospitals of being racist?</ORIGINAL_TEXT>
<TOKEN end_char="2619" id="token-48-0" morph="none" pos="unknown" start_char="2612">quote:Is</TOKEN>
<TOKEN end_char="2624" id="token-48-1" morph="none" pos="word" start_char="2621">this</TOKEN>
<TOKEN end_char="2628" id="token-48-2" morph="none" pos="word" start_char="2626">the</TOKEN>
<TOKEN end_char="2634" id="token-48-3" morph="none" pos="word" start_char="2630">woman</TOKEN>
<TOKEN end_char="2643" id="token-48-4" morph="none" pos="word" start_char="2636">accusing</TOKEN>
<TOKEN end_char="2646" id="token-48-5" morph="none" pos="word" start_char="2645">NY</TOKEN>
<TOKEN end_char="2656" id="token-48-6" morph="none" pos="word" start_char="2648">hospitals</TOKEN>
<TOKEN end_char="2659" id="token-48-7" morph="none" pos="word" start_char="2658">of</TOKEN>
<TOKEN end_char="2665" id="token-48-8" morph="none" pos="word" start_char="2661">being</TOKEN>
<TOKEN end_char="2672" id="token-48-9" morph="none" pos="word" start_char="2667">racist</TOKEN>
<TOKEN end_char="2673" id="token-48-10" morph="none" pos="punct" start_char="2673">?</TOKEN>
</SEG>
<SEG end_char="2678" id="segment-49" start_char="2676">
<ORIGINAL_TEXT>NO.</ORIGINAL_TEXT>
<TOKEN end_char="2677" id="token-49-0" morph="none" pos="word" start_char="2676">NO</TOKEN>
<TOKEN end_char="2678" id="token-49-1" morph="none" pos="punct" start_char="2678">.</TOKEN>
</SEG>
<SEG end_char="2748" id="segment-50" start_char="2680">
<ORIGINAL_TEXT>She is the nurse who is accusing NYC hospitals of Murdering patients.</ORIGINAL_TEXT>
<TOKEN end_char="2682" id="token-50-0" morph="none" pos="word" start_char="2680">She</TOKEN>
<TOKEN end_char="2685" id="token-50-1" morph="none" pos="word" start_char="2684">is</TOKEN>
<TOKEN end_char="2689" id="token-50-2" morph="none" pos="word" start_char="2687">the</TOKEN>
<TOKEN end_char="2695" id="token-50-3" morph="none" pos="word" start_char="2691">nurse</TOKEN>
<TOKEN end_char="2699" id="token-50-4" morph="none" pos="word" start_char="2697">who</TOKEN>
<TOKEN end_char="2702" id="token-50-5" morph="none" pos="word" start_char="2701">is</TOKEN>
<TOKEN end_char="2711" id="token-50-6" morph="none" pos="word" start_char="2704">accusing</TOKEN>
<TOKEN end_char="2715" id="token-50-7" morph="none" pos="word" start_char="2713">NYC</TOKEN>
<TOKEN end_char="2725" id="token-50-8" morph="none" pos="word" start_char="2717">hospitals</TOKEN>
<TOKEN end_char="2728" id="token-50-9" morph="none" pos="word" start_char="2727">of</TOKEN>
<TOKEN end_char="2738" id="token-50-10" morph="none" pos="word" start_char="2730">Murdering</TOKEN>
<TOKEN end_char="2747" id="token-50-11" morph="none" pos="word" start_char="2740">patients</TOKEN>
<TOKEN end_char="2748" id="token-50-12" morph="none" pos="punct" start_char="2748">.</TOKEN>
</SEG>
<SEG end_char="2853" id="segment-51" start_char="2754">
<ORIGINAL_TEXT>quote:The opinion of a experienced RN on the treatment of patients is probably more valid than a MD.</ORIGINAL_TEXT>
<TOKEN end_char="2762" id="token-51-0" morph="none" pos="unknown" start_char="2754">quote:The</TOKEN>
<TOKEN end_char="2770" id="token-51-1" morph="none" pos="word" start_char="2764">opinion</TOKEN>
<TOKEN end_char="2773" id="token-51-2" morph="none" pos="word" start_char="2772">of</TOKEN>
<TOKEN end_char="2775" id="token-51-3" morph="none" pos="word" start_char="2775">a</TOKEN>
<TOKEN end_char="2787" id="token-51-4" morph="none" pos="word" start_char="2777">experienced</TOKEN>
<TOKEN end_char="2790" id="token-51-5" morph="none" pos="word" start_char="2789">RN</TOKEN>
<TOKEN end_char="2793" id="token-51-6" morph="none" pos="word" start_char="2792">on</TOKEN>
<TOKEN end_char="2797" id="token-51-7" morph="none" pos="word" start_char="2795">the</TOKEN>
<TOKEN end_char="2807" id="token-51-8" morph="none" pos="word" start_char="2799">treatment</TOKEN>
<TOKEN end_char="2810" id="token-51-9" morph="none" pos="word" start_char="2809">of</TOKEN>
<TOKEN end_char="2819" id="token-51-10" morph="none" pos="word" start_char="2812">patients</TOKEN>
<TOKEN end_char="2822" id="token-51-11" morph="none" pos="word" start_char="2821">is</TOKEN>
<TOKEN end_char="2831" id="token-51-12" morph="none" pos="word" start_char="2824">probably</TOKEN>
<TOKEN end_char="2836" id="token-51-13" morph="none" pos="word" start_char="2833">more</TOKEN>
<TOKEN end_char="2842" id="token-51-14" morph="none" pos="word" start_char="2838">valid</TOKEN>
<TOKEN end_char="2847" id="token-51-15" morph="none" pos="word" start_char="2844">than</TOKEN>
<TOKEN end_char="2849" id="token-51-16" morph="none" pos="word" start_char="2849">a</TOKEN>
<TOKEN end_char="2852" id="token-51-17" morph="none" pos="word" start_char="2851">MD</TOKEN>
<TOKEN end_char="2853" id="token-51-18" morph="none" pos="punct" start_char="2853">.</TOKEN>
</SEG>
<SEG end_char="3132" id="segment-52" start_char="2856">
<ORIGINAL_TEXT>Most folks who apply Appeals to Authority logical fallacies seem to think that someone with letters behind their name are better at reading and critical thinking than other people who don't have letters behind their name ... but usually nothing could be further from the truth.</ORIGINAL_TEXT>
<TOKEN end_char="2859" id="token-52-0" morph="none" pos="word" start_char="2856">Most</TOKEN>
<TOKEN end_char="2865" id="token-52-1" morph="none" pos="word" start_char="2861">folks</TOKEN>
<TOKEN end_char="2869" id="token-52-2" morph="none" pos="word" start_char="2867">who</TOKEN>
<TOKEN end_char="2875" id="token-52-3" morph="none" pos="word" start_char="2871">apply</TOKEN>
<TOKEN end_char="2883" id="token-52-4" morph="none" pos="word" start_char="2877">Appeals</TOKEN>
<TOKEN end_char="2886" id="token-52-5" morph="none" pos="word" start_char="2885">to</TOKEN>
<TOKEN end_char="2896" id="token-52-6" morph="none" pos="word" start_char="2888">Authority</TOKEN>
<TOKEN end_char="2904" id="token-52-7" morph="none" pos="word" start_char="2898">logical</TOKEN>
<TOKEN end_char="2914" id="token-52-8" morph="none" pos="word" start_char="2906">fallacies</TOKEN>
<TOKEN end_char="2919" id="token-52-9" morph="none" pos="word" start_char="2916">seem</TOKEN>
<TOKEN end_char="2922" id="token-52-10" morph="none" pos="word" start_char="2921">to</TOKEN>
<TOKEN end_char="2928" id="token-52-11" morph="none" pos="word" start_char="2924">think</TOKEN>
<TOKEN end_char="2933" id="token-52-12" morph="none" pos="word" start_char="2930">that</TOKEN>
<TOKEN end_char="2941" id="token-52-13" morph="none" pos="word" start_char="2935">someone</TOKEN>
<TOKEN end_char="2946" id="token-52-14" morph="none" pos="word" start_char="2943">with</TOKEN>
<TOKEN end_char="2954" id="token-52-15" morph="none" pos="word" start_char="2948">letters</TOKEN>
<TOKEN end_char="2961" id="token-52-16" morph="none" pos="word" start_char="2956">behind</TOKEN>
<TOKEN end_char="2967" id="token-52-17" morph="none" pos="word" start_char="2963">their</TOKEN>
<TOKEN end_char="2972" id="token-52-18" morph="none" pos="word" start_char="2969">name</TOKEN>
<TOKEN end_char="2976" id="token-52-19" morph="none" pos="word" start_char="2974">are</TOKEN>
<TOKEN end_char="2983" id="token-52-20" morph="none" pos="word" start_char="2978">better</TOKEN>
<TOKEN end_char="2986" id="token-52-21" morph="none" pos="word" start_char="2985">at</TOKEN>
<TOKEN end_char="2994" id="token-52-22" morph="none" pos="word" start_char="2988">reading</TOKEN>
<TOKEN end_char="2998" id="token-52-23" morph="none" pos="word" start_char="2996">and</TOKEN>
<TOKEN end_char="3007" id="token-52-24" morph="none" pos="word" start_char="3000">critical</TOKEN>
<TOKEN end_char="3016" id="token-52-25" morph="none" pos="word" start_char="3009">thinking</TOKEN>
<TOKEN end_char="3021" id="token-52-26" morph="none" pos="word" start_char="3018">than</TOKEN>
<TOKEN end_char="3027" id="token-52-27" morph="none" pos="word" start_char="3023">other</TOKEN>
<TOKEN end_char="3034" id="token-52-28" morph="none" pos="word" start_char="3029">people</TOKEN>
<TOKEN end_char="3038" id="token-52-29" morph="none" pos="word" start_char="3036">who</TOKEN>
<TOKEN end_char="3044" id="token-52-30" morph="none" pos="word" start_char="3040">don't</TOKEN>
<TOKEN end_char="3049" id="token-52-31" morph="none" pos="word" start_char="3046">have</TOKEN>
<TOKEN end_char="3057" id="token-52-32" morph="none" pos="word" start_char="3051">letters</TOKEN>
<TOKEN end_char="3064" id="token-52-33" morph="none" pos="word" start_char="3059">behind</TOKEN>
<TOKEN end_char="3070" id="token-52-34" morph="none" pos="word" start_char="3066">their</TOKEN>
<TOKEN end_char="3075" id="token-52-35" morph="none" pos="word" start_char="3072">name</TOKEN>
<TOKEN end_char="3079" id="token-52-36" morph="none" pos="punct" start_char="3077">...</TOKEN>
<TOKEN end_char="3083" id="token-52-37" morph="none" pos="word" start_char="3081">but</TOKEN>
<TOKEN end_char="3091" id="token-52-38" morph="none" pos="word" start_char="3085">usually</TOKEN>
<TOKEN end_char="3099" id="token-52-39" morph="none" pos="word" start_char="3093">nothing</TOKEN>
<TOKEN end_char="3105" id="token-52-40" morph="none" pos="word" start_char="3101">could</TOKEN>
<TOKEN end_char="3108" id="token-52-41" morph="none" pos="word" start_char="3107">be</TOKEN>
<TOKEN end_char="3116" id="token-52-42" morph="none" pos="word" start_char="3110">further</TOKEN>
<TOKEN end_char="3121" id="token-52-43" morph="none" pos="word" start_char="3118">from</TOKEN>
<TOKEN end_char="3125" id="token-52-44" morph="none" pos="word" start_char="3123">the</TOKEN>
<TOKEN end_char="3131" id="token-52-45" morph="none" pos="word" start_char="3127">truth</TOKEN>
<TOKEN end_char="3132" id="token-52-46" morph="none" pos="punct" start_char="3132">.</TOKEN>
</SEG>
<SEG end_char="3237" id="segment-53" start_char="3135">
<ORIGINAL_TEXT>Piled High and Deep ... inability to critically think but usually well read and relies on consensus vs.</ORIGINAL_TEXT>
<TOKEN end_char="3139" id="token-53-0" morph="none" pos="word" start_char="3135">Piled</TOKEN>
<TOKEN end_char="3144" id="token-53-1" morph="none" pos="word" start_char="3141">High</TOKEN>
<TOKEN end_char="3148" id="token-53-2" morph="none" pos="word" start_char="3146">and</TOKEN>
<TOKEN end_char="3153" id="token-53-3" morph="none" pos="word" start_char="3150">Deep</TOKEN>
<TOKEN end_char="3157" id="token-53-4" morph="none" pos="punct" start_char="3155">...</TOKEN>
<TOKEN end_char="3167" id="token-53-5" morph="none" pos="word" start_char="3159">inability</TOKEN>
<TOKEN end_char="3170" id="token-53-6" morph="none" pos="word" start_char="3169">to</TOKEN>
<TOKEN end_char="3181" id="token-53-7" morph="none" pos="word" start_char="3172">critically</TOKEN>
<TOKEN end_char="3187" id="token-53-8" morph="none" pos="word" start_char="3183">think</TOKEN>
<TOKEN end_char="3191" id="token-53-9" morph="none" pos="word" start_char="3189">but</TOKEN>
<TOKEN end_char="3199" id="token-53-10" morph="none" pos="word" start_char="3193">usually</TOKEN>
<TOKEN end_char="3204" id="token-53-11" morph="none" pos="word" start_char="3201">well</TOKEN>
<TOKEN end_char="3209" id="token-53-12" morph="none" pos="word" start_char="3206">read</TOKEN>
<TOKEN end_char="3213" id="token-53-13" morph="none" pos="word" start_char="3211">and</TOKEN>
<TOKEN end_char="3220" id="token-53-14" morph="none" pos="word" start_char="3215">relies</TOKEN>
<TOKEN end_char="3223" id="token-53-15" morph="none" pos="word" start_char="3222">on</TOKEN>
<TOKEN end_char="3233" id="token-53-16" morph="none" pos="word" start_char="3225">consensus</TOKEN>
<TOKEN end_char="3236" id="token-53-17" morph="none" pos="word" start_char="3235">vs</TOKEN>
<TOKEN end_char="3237" id="token-53-18" morph="none" pos="punct" start_char="3237">.</TOKEN>
</SEG>
<SEG end_char="3251" id="segment-54" start_char="3239">
<ORIGINAL_TEXT>intuition ...</ORIGINAL_TEXT>
<TOKEN end_char="3247" id="token-54-0" morph="none" pos="word" start_char="3239">intuition</TOKEN>
<TOKEN end_char="3251" id="token-54-1" morph="none" pos="punct" start_char="3249">...</TOKEN>
</SEG>
<SEG end_char="3312" id="segment-55" start_char="3256">
<ORIGINAL_TEXT>Why did she say black lives don’t matter to these people?</ORIGINAL_TEXT>
<TOKEN end_char="3258" id="token-55-0" morph="none" pos="word" start_char="3256">Why</TOKEN>
<TOKEN end_char="3262" id="token-55-1" morph="none" pos="word" start_char="3260">did</TOKEN>
<TOKEN end_char="3266" id="token-55-2" morph="none" pos="word" start_char="3264">she</TOKEN>
<TOKEN end_char="3270" id="token-55-3" morph="none" pos="word" start_char="3268">say</TOKEN>
<TOKEN end_char="3276" id="token-55-4" morph="none" pos="word" start_char="3272">black</TOKEN>
<TOKEN end_char="3282" id="token-55-5" morph="none" pos="word" start_char="3278">lives</TOKEN>
<TOKEN end_char="3288" id="token-55-6" morph="none" pos="word" start_char="3284">don’t</TOKEN>
<TOKEN end_char="3295" id="token-55-7" morph="none" pos="word" start_char="3290">matter</TOKEN>
<TOKEN end_char="3298" id="token-55-8" morph="none" pos="word" start_char="3297">to</TOKEN>
<TOKEN end_char="3304" id="token-55-9" morph="none" pos="word" start_char="3300">these</TOKEN>
<TOKEN end_char="3311" id="token-55-10" morph="none" pos="word" start_char="3306">people</TOKEN>
<TOKEN end_char="3312" id="token-55-11" morph="none" pos="punct" start_char="3312">?</TOKEN>
</SEG>
<SEG end_char="3436" id="segment-56" start_char="3316">
<ORIGINAL_TEXT>Then why not go through the channels of reporting the complaints to the state health Department, Joint Commission or CMS?</ORIGINAL_TEXT>
<TOKEN end_char="3319" id="token-56-0" morph="none" pos="word" start_char="3316">Then</TOKEN>
<TOKEN end_char="3323" id="token-56-1" morph="none" pos="word" start_char="3321">why</TOKEN>
<TOKEN end_char="3327" id="token-56-2" morph="none" pos="word" start_char="3325">not</TOKEN>
<TOKEN end_char="3330" id="token-56-3" morph="none" pos="word" start_char="3329">go</TOKEN>
<TOKEN end_char="3338" id="token-56-4" morph="none" pos="word" start_char="3332">through</TOKEN>
<TOKEN end_char="3342" id="token-56-5" morph="none" pos="word" start_char="3340">the</TOKEN>
<TOKEN end_char="3351" id="token-56-6" morph="none" pos="word" start_char="3344">channels</TOKEN>
<TOKEN end_char="3354" id="token-56-7" morph="none" pos="word" start_char="3353">of</TOKEN>
<TOKEN end_char="3364" id="token-56-8" morph="none" pos="word" start_char="3356">reporting</TOKEN>
<TOKEN end_char="3368" id="token-56-9" morph="none" pos="word" start_char="3366">the</TOKEN>
<TOKEN end_char="3379" id="token-56-10" morph="none" pos="word" start_char="3370">complaints</TOKEN>
<TOKEN end_char="3382" id="token-56-11" morph="none" pos="word" start_char="3381">to</TOKEN>
<TOKEN end_char="3386" id="token-56-12" morph="none" pos="word" start_char="3384">the</TOKEN>
<TOKEN end_char="3392" id="token-56-13" morph="none" pos="word" start_char="3388">state</TOKEN>
<TOKEN end_char="3399" id="token-56-14" morph="none" pos="word" start_char="3394">health</TOKEN>
<TOKEN end_char="3410" id="token-56-15" morph="none" pos="word" start_char="3401">Department</TOKEN>
<TOKEN end_char="3411" id="token-56-16" morph="none" pos="punct" start_char="3411">,</TOKEN>
<TOKEN end_char="3417" id="token-56-17" morph="none" pos="word" start_char="3413">Joint</TOKEN>
<TOKEN end_char="3428" id="token-56-18" morph="none" pos="word" start_char="3419">Commission</TOKEN>
<TOKEN end_char="3431" id="token-56-19" morph="none" pos="word" start_char="3430">or</TOKEN>
<TOKEN end_char="3435" id="token-56-20" morph="none" pos="word" start_char="3433">CMS</TOKEN>
<TOKEN end_char="3436" id="token-56-21" morph="none" pos="punct" start_char="3436">?</TOKEN>
</SEG>
<SEG end_char="3546" id="segment-57" start_char="3439">
<ORIGINAL_TEXT>I find it extremely difficult to believe she reported potential wrongful deaths and JCO said nah we’re good.</ORIGINAL_TEXT>
<TOKEN end_char="3439" id="token-57-0" morph="none" pos="word" start_char="3439">I</TOKEN>
<TOKEN end_char="3444" id="token-57-1" morph="none" pos="word" start_char="3441">find</TOKEN>
<TOKEN end_char="3447" id="token-57-2" morph="none" pos="word" start_char="3446">it</TOKEN>
<TOKEN end_char="3457" id="token-57-3" morph="none" pos="word" start_char="3449">extremely</TOKEN>
<TOKEN end_char="3467" id="token-57-4" morph="none" pos="word" start_char="3459">difficult</TOKEN>
<TOKEN end_char="3470" id="token-57-5" morph="none" pos="word" start_char="3469">to</TOKEN>
<TOKEN end_char="3478" id="token-57-6" morph="none" pos="word" start_char="3472">believe</TOKEN>
<TOKEN end_char="3482" id="token-57-7" morph="none" pos="word" start_char="3480">she</TOKEN>
<TOKEN end_char="3491" id="token-57-8" morph="none" pos="word" start_char="3484">reported</TOKEN>
<TOKEN end_char="3501" id="token-57-9" morph="none" pos="word" start_char="3493">potential</TOKEN>
<TOKEN end_char="3510" id="token-57-10" morph="none" pos="word" start_char="3503">wrongful</TOKEN>
<TOKEN end_char="3517" id="token-57-11" morph="none" pos="word" start_char="3512">deaths</TOKEN>
<TOKEN end_char="3521" id="token-57-12" morph="none" pos="word" start_char="3519">and</TOKEN>
<TOKEN end_char="3525" id="token-57-13" morph="none" pos="word" start_char="3523">JCO</TOKEN>
<TOKEN end_char="3530" id="token-57-14" morph="none" pos="word" start_char="3527">said</TOKEN>
<TOKEN end_char="3534" id="token-57-15" morph="none" pos="word" start_char="3532">nah</TOKEN>
<TOKEN end_char="3540" id="token-57-16" morph="none" pos="word" start_char="3536">we’re</TOKEN>
<TOKEN end_char="3545" id="token-57-17" morph="none" pos="word" start_char="3542">good</TOKEN>
<TOKEN end_char="3546" id="token-57-18" morph="none" pos="punct" start_char="3546">.</TOKEN>
</SEG>
<SEG end_char="3620" id="segment-58" start_char="3551">
<ORIGINAL_TEXT>quote: If not did she go strait to posting videos for internet points?</ORIGINAL_TEXT>
<TOKEN end_char="3555" id="token-58-0" morph="none" pos="word" start_char="3551">quote</TOKEN>
<TOKEN end_char="3556" id="token-58-1" morph="none" pos="punct" start_char="3556">:</TOKEN>
<TOKEN end_char="3559" id="token-58-2" morph="none" pos="word" start_char="3558">If</TOKEN>
<TOKEN end_char="3563" id="token-58-3" morph="none" pos="word" start_char="3561">not</TOKEN>
<TOKEN end_char="3567" id="token-58-4" morph="none" pos="word" start_char="3565">did</TOKEN>
<TOKEN end_char="3571" id="token-58-5" morph="none" pos="word" start_char="3569">she</TOKEN>
<TOKEN end_char="3574" id="token-58-6" morph="none" pos="word" start_char="3573">go</TOKEN>
<TOKEN end_char="3581" id="token-58-7" morph="none" pos="word" start_char="3576">strait</TOKEN>
<TOKEN end_char="3584" id="token-58-8" morph="none" pos="word" start_char="3583">to</TOKEN>
<TOKEN end_char="3592" id="token-58-9" morph="none" pos="word" start_char="3586">posting</TOKEN>
<TOKEN end_char="3599" id="token-58-10" morph="none" pos="word" start_char="3594">videos</TOKEN>
<TOKEN end_char="3603" id="token-58-11" morph="none" pos="word" start_char="3601">for</TOKEN>
<TOKEN end_char="3612" id="token-58-12" morph="none" pos="word" start_char="3605">internet</TOKEN>
<TOKEN end_char="3619" id="token-58-13" morph="none" pos="word" start_char="3614">points</TOKEN>
<TOKEN end_char="3620" id="token-58-14" morph="none" pos="punct" start_char="3620">?</TOKEN>
</SEG>
<SEG end_char="3644" id="segment-59" start_char="3623">
<ORIGINAL_TEXT>This is 2020 not 1955.</ORIGINAL_TEXT>
<TOKEN end_char="3626" id="token-59-0" morph="none" pos="word" start_char="3623">This</TOKEN>
<TOKEN end_char="3629" id="token-59-1" morph="none" pos="word" start_char="3628">is</TOKEN>
<TOKEN end_char="3634" id="token-59-2" morph="none" pos="word" start_char="3631">2020</TOKEN>
<TOKEN end_char="3638" id="token-59-3" morph="none" pos="word" start_char="3636">not</TOKEN>
<TOKEN end_char="3643" id="token-59-4" morph="none" pos="word" start_char="3640">1955</TOKEN>
<TOKEN end_char="3644" id="token-59-5" morph="none" pos="punct" start_char="3644">.</TOKEN>
</SEG>
<SEG end_char="3694" id="segment-60" start_char="3646">
<ORIGINAL_TEXT>An effective YouTube video could save lives, NOW.</ORIGINAL_TEXT>
<TOKEN end_char="3647" id="token-60-0" morph="none" pos="word" start_char="3646">An</TOKEN>
<TOKEN end_char="3657" id="token-60-1" morph="none" pos="word" start_char="3649">effective</TOKEN>
<TOKEN end_char="3665" id="token-60-2" morph="none" pos="word" start_char="3659">YouTube</TOKEN>
<TOKEN end_char="3671" id="token-60-3" morph="none" pos="word" start_char="3667">video</TOKEN>
<TOKEN end_char="3677" id="token-60-4" morph="none" pos="word" start_char="3673">could</TOKEN>
<TOKEN end_char="3682" id="token-60-5" morph="none" pos="word" start_char="3679">save</TOKEN>
<TOKEN end_char="3688" id="token-60-6" morph="none" pos="word" start_char="3684">lives</TOKEN>
<TOKEN end_char="3689" id="token-60-7" morph="none" pos="punct" start_char="3689">,</TOKEN>
<TOKEN end_char="3693" id="token-60-8" morph="none" pos="word" start_char="3691">NOW</TOKEN>
<TOKEN end_char="3694" id="token-60-9" morph="none" pos="punct" start_char="3694">.</TOKEN>
</SEG>
<SEG end_char="3768" id="segment-61" start_char="3696">
<ORIGINAL_TEXT>Formal complaints may or may not be reviewed within the next year or two.</ORIGINAL_TEXT>
<TOKEN end_char="3701" id="token-61-0" morph="none" pos="word" start_char="3696">Formal</TOKEN>
<TOKEN end_char="3712" id="token-61-1" morph="none" pos="word" start_char="3703">complaints</TOKEN>
<TOKEN end_char="3716" id="token-61-2" morph="none" pos="word" start_char="3714">may</TOKEN>
<TOKEN end_char="3719" id="token-61-3" morph="none" pos="word" start_char="3718">or</TOKEN>
<TOKEN end_char="3723" id="token-61-4" morph="none" pos="word" start_char="3721">may</TOKEN>
<TOKEN end_char="3727" id="token-61-5" morph="none" pos="word" start_char="3725">not</TOKEN>
<TOKEN end_char="3730" id="token-61-6" morph="none" pos="word" start_char="3729">be</TOKEN>
<TOKEN end_char="3739" id="token-61-7" morph="none" pos="word" start_char="3732">reviewed</TOKEN>
<TOKEN end_char="3746" id="token-61-8" morph="none" pos="word" start_char="3741">within</TOKEN>
<TOKEN end_char="3750" id="token-61-9" morph="none" pos="word" start_char="3748">the</TOKEN>
<TOKEN end_char="3755" id="token-61-10" morph="none" pos="word" start_char="3752">next</TOKEN>
<TOKEN end_char="3760" id="token-61-11" morph="none" pos="word" start_char="3757">year</TOKEN>
<TOKEN end_char="3763" id="token-61-12" morph="none" pos="word" start_char="3762">or</TOKEN>
<TOKEN end_char="3767" id="token-61-13" morph="none" pos="word" start_char="3765">two</TOKEN>
<TOKEN end_char="3768" id="token-61-14" morph="none" pos="punct" start_char="3768">.</TOKEN>
</SEG>
<SEG end_char="3871" id="segment-62" start_char="3771">
<ORIGINAL_TEXT>Judging by the support that is mounting for her in Nevada, there will be complaints filed... aplenty.</ORIGINAL_TEXT>
<TOKEN end_char="3777" id="token-62-0" morph="none" pos="word" start_char="3771">Judging</TOKEN>
<TOKEN end_char="3780" id="token-62-1" morph="none" pos="word" start_char="3779">by</TOKEN>
<TOKEN end_char="3784" id="token-62-2" morph="none" pos="word" start_char="3782">the</TOKEN>
<TOKEN end_char="3792" id="token-62-3" morph="none" pos="word" start_char="3786">support</TOKEN>
<TOKEN end_char="3797" id="token-62-4" morph="none" pos="word" start_char="3794">that</TOKEN>
<TOKEN end_char="3800" id="token-62-5" morph="none" pos="word" start_char="3799">is</TOKEN>
<TOKEN end_char="3809" id="token-62-6" morph="none" pos="word" start_char="3802">mounting</TOKEN>
<TOKEN end_char="3813" id="token-62-7" morph="none" pos="word" start_char="3811">for</TOKEN>
<TOKEN end_char="3817" id="token-62-8" morph="none" pos="word" start_char="3815">her</TOKEN>
<TOKEN end_char="3820" id="token-62-9" morph="none" pos="word" start_char="3819">in</TOKEN>
<TOKEN end_char="3827" id="token-62-10" morph="none" pos="word" start_char="3822">Nevada</TOKEN>
<TOKEN end_char="3828" id="token-62-11" morph="none" pos="punct" start_char="3828">,</TOKEN>
<TOKEN end_char="3834" id="token-62-12" morph="none" pos="word" start_char="3830">there</TOKEN>
<TOKEN end_char="3839" id="token-62-13" morph="none" pos="word" start_char="3836">will</TOKEN>
<TOKEN end_char="3842" id="token-62-14" morph="none" pos="word" start_char="3841">be</TOKEN>
<TOKEN end_char="3853" id="token-62-15" morph="none" pos="word" start_char="3844">complaints</TOKEN>
<TOKEN end_char="3859" id="token-62-16" morph="none" pos="word" start_char="3855">filed</TOKEN>
<TOKEN end_char="3862" id="token-62-17" morph="none" pos="punct" start_char="3860">...</TOKEN>
<TOKEN end_char="3870" id="token-62-18" morph="none" pos="word" start_char="3864">aplenty</TOKEN>
<TOKEN end_char="3871" id="token-62-19" morph="none" pos="punct" start_char="3871">.</TOKEN>
</SEG>
<SEG end_char="3939" id="segment-63" start_char="3877">
<ORIGINAL_TEXT>quote:Why did she say black lives don’t matter to these people?</ORIGINAL_TEXT>
<TOKEN end_char="3885" id="token-63-0" morph="none" pos="unknown" start_char="3877">quote:Why</TOKEN>
<TOKEN end_char="3889" id="token-63-1" morph="none" pos="word" start_char="3887">did</TOKEN>
<TOKEN end_char="3893" id="token-63-2" morph="none" pos="word" start_char="3891">she</TOKEN>
<TOKEN end_char="3897" id="token-63-3" morph="none" pos="word" start_char="3895">say</TOKEN>
<TOKEN end_char="3903" id="token-63-4" morph="none" pos="word" start_char="3899">black</TOKEN>
<TOKEN end_char="3909" id="token-63-5" morph="none" pos="word" start_char="3905">lives</TOKEN>
<TOKEN end_char="3915" id="token-63-6" morph="none" pos="word" start_char="3911">don’t</TOKEN>
<TOKEN end_char="3922" id="token-63-7" morph="none" pos="word" start_char="3917">matter</TOKEN>
<TOKEN end_char="3925" id="token-63-8" morph="none" pos="word" start_char="3924">to</TOKEN>
<TOKEN end_char="3931" id="token-63-9" morph="none" pos="word" start_char="3927">these</TOKEN>
<TOKEN end_char="3938" id="token-63-10" morph="none" pos="word" start_char="3933">people</TOKEN>
<TOKEN end_char="3939" id="token-63-11" morph="none" pos="punct" start_char="3939">?</TOKEN>
</SEG>
<SEG end_char="3986" id="segment-64" start_char="3942">
<ORIGINAL_TEXT>Was that one of the many things she reported?</ORIGINAL_TEXT>
<TOKEN end_char="3944" id="token-64-0" morph="none" pos="word" start_char="3942">Was</TOKEN>
<TOKEN end_char="3949" id="token-64-1" morph="none" pos="word" start_char="3946">that</TOKEN>
<TOKEN end_char="3953" id="token-64-2" morph="none" pos="word" start_char="3951">one</TOKEN>
<TOKEN end_char="3956" id="token-64-3" morph="none" pos="word" start_char="3955">of</TOKEN>
<TOKEN end_char="3960" id="token-64-4" morph="none" pos="word" start_char="3958">the</TOKEN>
<TOKEN end_char="3965" id="token-64-5" morph="none" pos="word" start_char="3962">many</TOKEN>
<TOKEN end_char="3972" id="token-64-6" morph="none" pos="word" start_char="3967">things</TOKEN>
<TOKEN end_char="3976" id="token-64-7" morph="none" pos="word" start_char="3974">she</TOKEN>
<TOKEN end_char="3985" id="token-64-8" morph="none" pos="word" start_char="3978">reported</TOKEN>
<TOKEN end_char="3986" id="token-64-9" morph="none" pos="punct" start_char="3986">?</TOKEN>
</SEG>
<SEG end_char="4017" id="segment-65" start_char="3988">
<ORIGINAL_TEXT>Probably true, if she said it.</ORIGINAL_TEXT>
<TOKEN end_char="3995" id="token-65-0" morph="none" pos="word" start_char="3988">Probably</TOKEN>
<TOKEN end_char="4000" id="token-65-1" morph="none" pos="word" start_char="3997">true</TOKEN>
<TOKEN end_char="4001" id="token-65-2" morph="none" pos="punct" start_char="4001">,</TOKEN>
<TOKEN end_char="4004" id="token-65-3" morph="none" pos="word" start_char="4003">if</TOKEN>
<TOKEN end_char="4008" id="token-65-4" morph="none" pos="word" start_char="4006">she</TOKEN>
<TOKEN end_char="4013" id="token-65-5" morph="none" pos="word" start_char="4010">said</TOKEN>
<TOKEN end_char="4016" id="token-65-6" morph="none" pos="word" start_char="4015">it</TOKEN>
<TOKEN end_char="4017" id="token-65-7" morph="none" pos="punct" start_char="4017">.</TOKEN>
</SEG>
<SEG end_char="4084" id="segment-66" start_char="4022">
<ORIGINAL_TEXT>She is some bastion of truth or a prog looking for hero status?</ORIGINAL_TEXT>
<TOKEN end_char="4024" id="token-66-0" morph="none" pos="word" start_char="4022">She</TOKEN>
<TOKEN end_char="4027" id="token-66-1" morph="none" pos="word" start_char="4026">is</TOKEN>
<TOKEN end_char="4032" id="token-66-2" morph="none" pos="word" start_char="4029">some</TOKEN>
<TOKEN end_char="4040" id="token-66-3" morph="none" pos="word" start_char="4034">bastion</TOKEN>
<TOKEN end_char="4043" id="token-66-4" morph="none" pos="word" start_char="4042">of</TOKEN>
<TOKEN end_char="4049" id="token-66-5" morph="none" pos="word" start_char="4045">truth</TOKEN>
<TOKEN end_char="4052" id="token-66-6" morph="none" pos="word" start_char="4051">or</TOKEN>
<TOKEN end_char="4054" id="token-66-7" morph="none" pos="word" start_char="4054">a</TOKEN>
<TOKEN end_char="4059" id="token-66-8" morph="none" pos="word" start_char="4056">prog</TOKEN>
<TOKEN end_char="4067" id="token-66-9" morph="none" pos="word" start_char="4061">looking</TOKEN>
<TOKEN end_char="4071" id="token-66-10" morph="none" pos="word" start_char="4069">for</TOKEN>
<TOKEN end_char="4076" id="token-66-11" morph="none" pos="word" start_char="4073">hero</TOKEN>
<TOKEN end_char="4083" id="token-66-12" morph="none" pos="word" start_char="4078">status</TOKEN>
<TOKEN end_char="4084" id="token-66-13" morph="none" pos="punct" start_char="4084">?</TOKEN>
</SEG>
<SEG end_char="4131" id="segment-67" start_char="4087">
<ORIGINAL_TEXT>You think NYC, prog heaven, is a racist city?</ORIGINAL_TEXT>
<TOKEN end_char="4089" id="token-67-0" morph="none" pos="word" start_char="4087">You</TOKEN>
<TOKEN end_char="4095" id="token-67-1" morph="none" pos="word" start_char="4091">think</TOKEN>
<TOKEN end_char="4099" id="token-67-2" morph="none" pos="word" start_char="4097">NYC</TOKEN>
<TOKEN end_char="4100" id="token-67-3" morph="none" pos="punct" start_char="4100">,</TOKEN>
<TOKEN end_char="4105" id="token-67-4" morph="none" pos="word" start_char="4102">prog</TOKEN>
<TOKEN end_char="4112" id="token-67-5" morph="none" pos="word" start_char="4107">heaven</TOKEN>
<TOKEN end_char="4113" id="token-67-6" morph="none" pos="punct" start_char="4113">,</TOKEN>
<TOKEN end_char="4116" id="token-67-7" morph="none" pos="word" start_char="4115">is</TOKEN>
<TOKEN end_char="4118" id="token-67-8" morph="none" pos="word" start_char="4118">a</TOKEN>
<TOKEN end_char="4125" id="token-67-9" morph="none" pos="word" start_char="4120">racist</TOKEN>
<TOKEN end_char="4130" id="token-67-10" morph="none" pos="word" start_char="4127">city</TOKEN>
<TOKEN end_char="4131" id="token-67-11" morph="none" pos="punct" start_char="4131">?</TOKEN>
</SEG>
<SEG end_char="4172" id="segment-68" start_char="4134">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:21 pm</ORIGINAL_TEXT>
<TOKEN end_char="4137" id="token-68-0" morph="none" pos="word" start_char="4134">This</TOKEN>
<TOKEN end_char="4142" id="token-68-1" morph="none" pos="word" start_char="4139">post</TOKEN>
<TOKEN end_char="4146" id="token-68-2" morph="none" pos="word" start_char="4144">was</TOKEN>
<TOKEN end_char="4153" id="token-68-3" morph="none" pos="word" start_char="4148">edited</TOKEN>
<TOKEN end_char="4156" id="token-68-4" morph="none" pos="word" start_char="4155">on</TOKEN>
<TOKEN end_char="4160" id="token-68-5" morph="none" pos="unknown" start_char="4158">5/5</TOKEN>
<TOKEN end_char="4163" id="token-68-6" morph="none" pos="word" start_char="4162">at</TOKEN>
<TOKEN end_char="4169" id="token-68-7" morph="none" pos="unknown" start_char="4165">12:21</TOKEN>
<TOKEN end_char="4172" id="token-68-8" morph="none" pos="word" start_char="4171">pm</TOKEN>
</SEG>
<SEG end_char="4227" id="segment-69" start_char="4177">
<ORIGINAL_TEXT>quote:You think NYC, prog heaven, is a racist city?</ORIGINAL_TEXT>
<TOKEN end_char="4185" id="token-69-0" morph="none" pos="unknown" start_char="4177">quote:You</TOKEN>
<TOKEN end_char="4191" id="token-69-1" morph="none" pos="word" start_char="4187">think</TOKEN>
<TOKEN end_char="4195" id="token-69-2" morph="none" pos="word" start_char="4193">NYC</TOKEN>
<TOKEN end_char="4196" id="token-69-3" morph="none" pos="punct" start_char="4196">,</TOKEN>
<TOKEN end_char="4201" id="token-69-4" morph="none" pos="word" start_char="4198">prog</TOKEN>
<TOKEN end_char="4208" id="token-69-5" morph="none" pos="word" start_char="4203">heaven</TOKEN>
<TOKEN end_char="4209" id="token-69-6" morph="none" pos="punct" start_char="4209">,</TOKEN>
<TOKEN end_char="4212" id="token-69-7" morph="none" pos="word" start_char="4211">is</TOKEN>
<TOKEN end_char="4214" id="token-69-8" morph="none" pos="word" start_char="4214">a</TOKEN>
<TOKEN end_char="4221" id="token-69-9" morph="none" pos="word" start_char="4216">racist</TOKEN>
<TOKEN end_char="4226" id="token-69-10" morph="none" pos="word" start_char="4223">city</TOKEN>
<TOKEN end_char="4227" id="token-69-11" morph="none" pos="punct" start_char="4227">?</TOKEN>
</SEG>
<SEG end_char="4257" id="segment-70" start_char="4230">
<ORIGINAL_TEXT>You have never lived in NYC.</ORIGINAL_TEXT>
<TOKEN end_char="4232" id="token-70-0" morph="none" pos="word" start_char="4230">You</TOKEN>
<TOKEN end_char="4237" id="token-70-1" morph="none" pos="word" start_char="4234">have</TOKEN>
<TOKEN end_char="4243" id="token-70-2" morph="none" pos="word" start_char="4239">never</TOKEN>
<TOKEN end_char="4249" id="token-70-3" morph="none" pos="word" start_char="4245">lived</TOKEN>
<TOKEN end_char="4252" id="token-70-4" morph="none" pos="word" start_char="4251">in</TOKEN>
<TOKEN end_char="4256" id="token-70-5" morph="none" pos="word" start_char="4254">NYC</TOKEN>
<TOKEN end_char="4257" id="token-70-6" morph="none" pos="punct" start_char="4257">.</TOKEN>
</SEG>
<SEG end_char="4275" id="segment-71" start_char="4259">
<ORIGINAL_TEXT>Bless your heart.</ORIGINAL_TEXT>
<TOKEN end_char="4263" id="token-71-0" morph="none" pos="word" start_char="4259">Bless</TOKEN>
<TOKEN end_char="4268" id="token-71-1" morph="none" pos="word" start_char="4265">your</TOKEN>
<TOKEN end_char="4274" id="token-71-2" morph="none" pos="word" start_char="4270">heart</TOKEN>
<TOKEN end_char="4275" id="token-71-3" morph="none" pos="punct" start_char="4275">.</TOKEN>
</SEG>
<SEG end_char="4331" id="segment-72" start_char="4281">
<ORIGINAL_TEXT>quote:Was that one of the many things she reported?</ORIGINAL_TEXT>
<TOKEN end_char="4289" id="token-72-0" morph="none" pos="unknown" start_char="4281">quote:Was</TOKEN>
<TOKEN end_char="4294" id="token-72-1" morph="none" pos="word" start_char="4291">that</TOKEN>
<TOKEN end_char="4298" id="token-72-2" morph="none" pos="word" start_char="4296">one</TOKEN>
<TOKEN end_char="4301" id="token-72-3" morph="none" pos="word" start_char="4300">of</TOKEN>
<TOKEN end_char="4305" id="token-72-4" morph="none" pos="word" start_char="4303">the</TOKEN>
<TOKEN end_char="4310" id="token-72-5" morph="none" pos="word" start_char="4307">many</TOKEN>
<TOKEN end_char="4317" id="token-72-6" morph="none" pos="word" start_char="4312">things</TOKEN>
<TOKEN end_char="4321" id="token-72-7" morph="none" pos="word" start_char="4319">she</TOKEN>
<TOKEN end_char="4330" id="token-72-8" morph="none" pos="word" start_char="4323">reported</TOKEN>
<TOKEN end_char="4331" id="token-72-9" morph="none" pos="punct" start_char="4331">?</TOKEN>
</SEG>
<SEG end_char="4378" id="segment-73" start_char="4334">
<ORIGINAL_TEXT>Wait, you didn’t watch her now deleted video?</ORIGINAL_TEXT>
<TOKEN end_char="4337" id="token-73-0" morph="none" pos="word" start_char="4334">Wait</TOKEN>
<TOKEN end_char="4338" id="token-73-1" morph="none" pos="punct" start_char="4338">,</TOKEN>
<TOKEN end_char="4342" id="token-73-2" morph="none" pos="word" start_char="4340">you</TOKEN>
<TOKEN end_char="4349" id="token-73-3" morph="none" pos="word" start_char="4344">didn’t</TOKEN>
<TOKEN end_char="4355" id="token-73-4" morph="none" pos="word" start_char="4351">watch</TOKEN>
<TOKEN end_char="4359" id="token-73-5" morph="none" pos="word" start_char="4357">her</TOKEN>
<TOKEN end_char="4363" id="token-73-6" morph="none" pos="word" start_char="4361">now</TOKEN>
<TOKEN end_char="4371" id="token-73-7" morph="none" pos="word" start_char="4365">deleted</TOKEN>
<TOKEN end_char="4377" id="token-73-8" morph="none" pos="word" start_char="4373">video</TOKEN>
<TOKEN end_char="4378" id="token-73-9" morph="none" pos="punct" start_char="4378">?</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>