<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C049DQS" lang="eng" raw_text_char_length="4752" raw_text_md5="bd227d4cd679345e81a47eaeb9f34308" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="60" id="segment-0" start_char="1">
<ORIGINAL_TEXT>A Nobel Laureate Said the New Coronavirus Was Made in a Lab.</ORIGINAL_TEXT>
<TOKEN end_char="1" id="token-0-0" morph="none" pos="word" start_char="1">A</TOKEN>
<TOKEN end_char="7" id="token-0-1" morph="none" pos="word" start_char="3">Nobel</TOKEN>
<TOKEN end_char="16" id="token-0-2" morph="none" pos="word" start_char="9">Laureate</TOKEN>
<TOKEN end_char="21" id="token-0-3" morph="none" pos="word" start_char="18">Said</TOKEN>
<TOKEN end_char="25" id="token-0-4" morph="none" pos="word" start_char="23">the</TOKEN>
<TOKEN end_char="29" id="token-0-5" morph="none" pos="word" start_char="27">New</TOKEN>
<TOKEN end_char="41" id="token-0-6" morph="none" pos="word" start_char="31">Coronavirus</TOKEN>
<TOKEN end_char="45" id="token-0-7" morph="none" pos="word" start_char="43">Was</TOKEN>
<TOKEN end_char="50" id="token-0-8" morph="none" pos="word" start_char="47">Made</TOKEN>
<TOKEN end_char="53" id="token-0-9" morph="none" pos="word" start_char="52">in</TOKEN>
<TOKEN end_char="55" id="token-0-10" morph="none" pos="word" start_char="55">a</TOKEN>
<TOKEN end_char="59" id="token-0-11" morph="none" pos="word" start_char="57">Lab</TOKEN>
<TOKEN end_char="60" id="token-0-12" morph="none" pos="punct" start_char="60">.</TOKEN>
</SEG>
<SEG end_char="72" id="segment-1" start_char="62">
<ORIGINAL_TEXT>He’s Wrong.</ORIGINAL_TEXT>
<TOKEN end_char="65" id="token-1-0" morph="none" pos="word" start_char="62">He’s</TOKEN>
<TOKEN end_char="71" id="token-1-1" morph="none" pos="word" start_char="67">Wrong</TOKEN>
<TOKEN end_char="72" id="token-1-2" morph="none" pos="punct" start_char="72">.</TOKEN>
</SEG>
<SEG end_char="114" id="segment-2" start_char="76">
<ORIGINAL_TEXT>Luc Montagnier during the TV interview.</ORIGINAL_TEXT>
<TOKEN end_char="78" id="token-2-0" morph="none" pos="word" start_char="76">Luc</TOKEN>
<TOKEN end_char="89" id="token-2-1" morph="none" pos="word" start_char="80">Montagnier</TOKEN>
<TOKEN end_char="96" id="token-2-2" morph="none" pos="word" start_char="91">during</TOKEN>
<TOKEN end_char="100" id="token-2-3" morph="none" pos="word" start_char="98">the</TOKEN>
<TOKEN end_char="103" id="token-2-4" morph="none" pos="word" start_char="102">TV</TOKEN>
<TOKEN end_char="113" id="token-2-5" morph="none" pos="word" start_char="105">interview</TOKEN>
<TOKEN end_char="114" id="token-2-6" morph="none" pos="punct" start_char="114">.</TOKEN>
</SEG>
<SEG end_char="130" id="segment-3" start_char="116">
<ORIGINAL_TEXT>Photo: YouTube.</ORIGINAL_TEXT>
<TOKEN end_char="120" id="token-3-0" morph="none" pos="word" start_char="116">Photo</TOKEN>
<TOKEN end_char="121" id="token-3-1" morph="none" pos="punct" start_char="121">:</TOKEN>
<TOKEN end_char="129" id="token-3-2" morph="none" pos="word" start_char="123">YouTube</TOKEN>
<TOKEN end_char="130" id="token-3-3" morph="none" pos="punct" start_char="130">.</TOKEN>
</SEG>
<SEG end_char="336" id="segment-4" start_char="134">
<ORIGINAL_TEXT>The 2008 Nobel Laureate for physiology or medicine from France, Luc Antoine Montagnier, caught the media’s attention when he recently endorsed a COVID-19 conspiracy theory – that the virus is human-made.</ORIGINAL_TEXT>
<TOKEN end_char="136" id="token-4-0" morph="none" pos="word" start_char="134">The</TOKEN>
<TOKEN end_char="141" id="token-4-1" morph="none" pos="word" start_char="138">2008</TOKEN>
<TOKEN end_char="147" id="token-4-2" morph="none" pos="word" start_char="143">Nobel</TOKEN>
<TOKEN end_char="156" id="token-4-3" morph="none" pos="word" start_char="149">Laureate</TOKEN>
<TOKEN end_char="160" id="token-4-4" morph="none" pos="word" start_char="158">for</TOKEN>
<TOKEN end_char="171" id="token-4-5" morph="none" pos="word" start_char="162">physiology</TOKEN>
<TOKEN end_char="174" id="token-4-6" morph="none" pos="word" start_char="173">or</TOKEN>
<TOKEN end_char="183" id="token-4-7" morph="none" pos="word" start_char="176">medicine</TOKEN>
<TOKEN end_char="188" id="token-4-8" morph="none" pos="word" start_char="185">from</TOKEN>
<TOKEN end_char="195" id="token-4-9" morph="none" pos="word" start_char="190">France</TOKEN>
<TOKEN end_char="196" id="token-4-10" morph="none" pos="punct" start_char="196">,</TOKEN>
<TOKEN end_char="200" id="token-4-11" morph="none" pos="word" start_char="198">Luc</TOKEN>
<TOKEN end_char="208" id="token-4-12" morph="none" pos="word" start_char="202">Antoine</TOKEN>
<TOKEN end_char="219" id="token-4-13" morph="none" pos="word" start_char="210">Montagnier</TOKEN>
<TOKEN end_char="220" id="token-4-14" morph="none" pos="punct" start_char="220">,</TOKEN>
<TOKEN end_char="227" id="token-4-15" morph="none" pos="word" start_char="222">caught</TOKEN>
<TOKEN end_char="231" id="token-4-16" morph="none" pos="word" start_char="229">the</TOKEN>
<TOKEN end_char="239" id="token-4-17" morph="none" pos="word" start_char="233">media’s</TOKEN>
<TOKEN end_char="249" id="token-4-18" morph="none" pos="word" start_char="241">attention</TOKEN>
<TOKEN end_char="254" id="token-4-19" morph="none" pos="word" start_char="251">when</TOKEN>
<TOKEN end_char="257" id="token-4-20" morph="none" pos="word" start_char="256">he</TOKEN>
<TOKEN end_char="266" id="token-4-21" morph="none" pos="word" start_char="259">recently</TOKEN>
<TOKEN end_char="275" id="token-4-22" morph="none" pos="word" start_char="268">endorsed</TOKEN>
<TOKEN end_char="277" id="token-4-23" morph="none" pos="word" start_char="277">a</TOKEN>
<TOKEN end_char="286" id="token-4-24" morph="none" pos="unknown" start_char="279">COVID-19</TOKEN>
<TOKEN end_char="297" id="token-4-25" morph="none" pos="word" start_char="288">conspiracy</TOKEN>
<TOKEN end_char="304" id="token-4-26" morph="none" pos="word" start_char="299">theory</TOKEN>
<TOKEN end_char="306" id="token-4-27" morph="none" pos="punct" start_char="306">–</TOKEN>
<TOKEN end_char="311" id="token-4-28" morph="none" pos="word" start_char="308">that</TOKEN>
<TOKEN end_char="315" id="token-4-29" morph="none" pos="word" start_char="313">the</TOKEN>
<TOKEN end_char="321" id="token-4-30" morph="none" pos="word" start_char="317">virus</TOKEN>
<TOKEN end_char="324" id="token-4-31" morph="none" pos="word" start_char="323">is</TOKEN>
<TOKEN end_char="335" id="token-4-32" morph="none" pos="unknown" start_char="326">human-made</TOKEN>
<TOKEN end_char="336" id="token-4-33" morph="none" pos="punct" start_char="336">.</TOKEN>
</SEG>
<SEG end_char="436" id="segment-5" start_char="338">
<ORIGINAL_TEXT>His proclamation was subsequently magnified by various news outlets, including many in India (e.g.,</ORIGINAL_TEXT>
<TOKEN end_char="340" id="token-5-0" morph="none" pos="word" start_char="338">His</TOKEN>
<TOKEN end_char="353" id="token-5-1" morph="none" pos="word" start_char="342">proclamation</TOKEN>
<TOKEN end_char="357" id="token-5-2" morph="none" pos="word" start_char="355">was</TOKEN>
<TOKEN end_char="370" id="token-5-3" morph="none" pos="word" start_char="359">subsequently</TOKEN>
<TOKEN end_char="380" id="token-5-4" morph="none" pos="word" start_char="372">magnified</TOKEN>
<TOKEN end_char="383" id="token-5-5" morph="none" pos="word" start_char="382">by</TOKEN>
<TOKEN end_char="391" id="token-5-6" morph="none" pos="word" start_char="385">various</TOKEN>
<TOKEN end_char="396" id="token-5-7" morph="none" pos="word" start_char="393">news</TOKEN>
<TOKEN end_char="404" id="token-5-8" morph="none" pos="word" start_char="398">outlets</TOKEN>
<TOKEN end_char="405" id="token-5-9" morph="none" pos="punct" start_char="405">,</TOKEN>
<TOKEN end_char="415" id="token-5-10" morph="none" pos="word" start_char="407">including</TOKEN>
<TOKEN end_char="420" id="token-5-11" morph="none" pos="word" start_char="417">many</TOKEN>
<TOKEN end_char="423" id="token-5-12" morph="none" pos="word" start_char="422">in</TOKEN>
<TOKEN end_char="429" id="token-5-13" morph="none" pos="word" start_char="425">India</TOKEN>
<TOKEN end_char="431" id="token-5-14" morph="none" pos="punct" start_char="431">(</TOKEN>
<TOKEN end_char="434" id="token-5-15" morph="none" pos="unknown" start_char="432">e.g</TOKEN>
<TOKEN end_char="436" id="token-5-16" morph="none" pos="punct" start_char="435">.,</TOKEN>
</SEG>
<SEG end_char="446" id="segment-6" start_char="439">
<ORIGINAL_TEXT>The Week</ORIGINAL_TEXT>
<TOKEN end_char="441" id="token-6-0" morph="none" pos="word" start_char="439">The</TOKEN>
<TOKEN end_char="446" id="token-6-1" morph="none" pos="word" start_char="443">Week</TOKEN>
</SEG>
<SEG end_char="449" id="segment-7" start_char="449">
<ORIGINAL_TEXT>,</ORIGINAL_TEXT>
<TOKEN end_char="449" id="token-7-0" morph="none" pos="punct" start_char="449">,</TOKEN>
</SEG>
<SEG end_char="473" id="segment-8" start_char="452">
<ORIGINAL_TEXT>The Hindu Businessline</ORIGINAL_TEXT>
<TOKEN end_char="454" id="token-8-0" morph="none" pos="word" start_char="452">The</TOKEN>
<TOKEN end_char="460" id="token-8-1" morph="none" pos="word" start_char="456">Hindu</TOKEN>
<TOKEN end_char="473" id="token-8-2" morph="none" pos="word" start_char="462">Businessline</TOKEN>
<TRANSLATED_TEXT>The Hindu Business Line</TRANSLATED_TEXT><DETECTED_LANGUAGE>de</DETECTED_LANGUAGE></SEG>
<SEG end_char="480" id="segment-9" start_char="476">
<ORIGINAL_TEXT>, and</ORIGINAL_TEXT>
<TOKEN end_char="476" id="token-9-0" morph="none" pos="punct" start_char="476">,</TOKEN>
<TOKEN end_char="480" id="token-9-1" morph="none" pos="word" start_char="478">and</TOKEN>
</SEG>
<SEG end_char="496" id="segment-10" start_char="483">
<ORIGINAL_TEXT>Times of India</ORIGINAL_TEXT>
<TOKEN end_char="487" id="token-10-0" morph="none" pos="word" start_char="483">Times</TOKEN>
<TOKEN end_char="490" id="token-10-1" morph="none" pos="word" start_char="489">of</TOKEN>
<TOKEN end_char="496" id="token-10-2" morph="none" pos="word" start_char="492">India</TOKEN>
</SEG>
<SEG end_char="500" id="segment-11" start_char="499">
<ORIGINAL_TEXT>).</ORIGINAL_TEXT>
<TOKEN end_char="500" id="token-11-0" morph="none" pos="punct" start_char="499">).</TOKEN>
</SEG>
<SEG end_char="688" id="segment-12" start_char="503">
<ORIGINAL_TEXT>Montagnier argued during a TV interview with a French TV channel that elements of the HIV-1 retrovirus, which he co-discovered in 1983, can be found in the genome of the new coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="512" id="token-12-0" morph="none" pos="word" start_char="503">Montagnier</TOKEN>
<TOKEN end_char="519" id="token-12-1" morph="none" pos="word" start_char="514">argued</TOKEN>
<TOKEN end_char="526" id="token-12-2" morph="none" pos="word" start_char="521">during</TOKEN>
<TOKEN end_char="528" id="token-12-3" morph="none" pos="word" start_char="528">a</TOKEN>
<TOKEN end_char="531" id="token-12-4" morph="none" pos="word" start_char="530">TV</TOKEN>
<TOKEN end_char="541" id="token-12-5" morph="none" pos="word" start_char="533">interview</TOKEN>
<TOKEN end_char="546" id="token-12-6" morph="none" pos="word" start_char="543">with</TOKEN>
<TOKEN end_char="548" id="token-12-7" morph="none" pos="word" start_char="548">a</TOKEN>
<TOKEN end_char="555" id="token-12-8" morph="none" pos="word" start_char="550">French</TOKEN>
<TOKEN end_char="558" id="token-12-9" morph="none" pos="word" start_char="557">TV</TOKEN>
<TOKEN end_char="566" id="token-12-10" morph="none" pos="word" start_char="560">channel</TOKEN>
<TOKEN end_char="571" id="token-12-11" morph="none" pos="word" start_char="568">that</TOKEN>
<TOKEN end_char="580" id="token-12-12" morph="none" pos="word" start_char="573">elements</TOKEN>
<TOKEN end_char="583" id="token-12-13" morph="none" pos="word" start_char="582">of</TOKEN>
<TOKEN end_char="587" id="token-12-14" morph="none" pos="word" start_char="585">the</TOKEN>
<TOKEN end_char="593" id="token-12-15" morph="none" pos="unknown" start_char="589">HIV-1</TOKEN>
<TOKEN end_char="604" id="token-12-16" morph="none" pos="word" start_char="595">retrovirus</TOKEN>
<TOKEN end_char="605" id="token-12-17" morph="none" pos="punct" start_char="605">,</TOKEN>
<TOKEN end_char="611" id="token-12-18" morph="none" pos="word" start_char="607">which</TOKEN>
<TOKEN end_char="614" id="token-12-19" morph="none" pos="word" start_char="613">he</TOKEN>
<TOKEN end_char="628" id="token-12-20" morph="none" pos="unknown" start_char="616">co-discovered</TOKEN>
<TOKEN end_char="631" id="token-12-21" morph="none" pos="word" start_char="630">in</TOKEN>
<TOKEN end_char="636" id="token-12-22" morph="none" pos="word" start_char="633">1983</TOKEN>
<TOKEN end_char="637" id="token-12-23" morph="none" pos="punct" start_char="637">,</TOKEN>
<TOKEN end_char="641" id="token-12-24" morph="none" pos="word" start_char="639">can</TOKEN>
<TOKEN end_char="644" id="token-12-25" morph="none" pos="word" start_char="643">be</TOKEN>
<TOKEN end_char="650" id="token-12-26" morph="none" pos="word" start_char="646">found</TOKEN>
<TOKEN end_char="653" id="token-12-27" morph="none" pos="word" start_char="652">in</TOKEN>
<TOKEN end_char="657" id="token-12-28" morph="none" pos="word" start_char="655">the</TOKEN>
<TOKEN end_char="664" id="token-12-29" morph="none" pos="word" start_char="659">genome</TOKEN>
<TOKEN end_char="667" id="token-12-30" morph="none" pos="word" start_char="666">of</TOKEN>
<TOKEN end_char="671" id="token-12-31" morph="none" pos="word" start_char="669">the</TOKEN>
<TOKEN end_char="675" id="token-12-32" morph="none" pos="word" start_char="673">new</TOKEN>
<TOKEN end_char="687" id="token-12-33" morph="none" pos="word" start_char="677">coronavirus</TOKEN>
<TOKEN end_char="688" id="token-12-34" morph="none" pos="punct" start_char="688">.</TOKEN>
</SEG>
<SEG end_char="747" id="segment-13" start_char="690">
<ORIGINAL_TEXT>He also said elements of the "malaria germ" – the parasite</ORIGINAL_TEXT>
<TOKEN end_char="691" id="token-13-0" morph="none" pos="word" start_char="690">He</TOKEN>
<TOKEN end_char="696" id="token-13-1" morph="none" pos="word" start_char="693">also</TOKEN>
<TOKEN end_char="701" id="token-13-2" morph="none" pos="word" start_char="698">said</TOKEN>
<TOKEN end_char="710" id="token-13-3" morph="none" pos="word" start_char="703">elements</TOKEN>
<TOKEN end_char="713" id="token-13-4" morph="none" pos="word" start_char="712">of</TOKEN>
<TOKEN end_char="717" id="token-13-5" morph="none" pos="word" start_char="715">the</TOKEN>
<TOKEN end_char="719" id="token-13-6" morph="none" pos="punct" start_char="719">"</TOKEN>
<TOKEN end_char="726" id="token-13-7" morph="none" pos="word" start_char="720">malaria</TOKEN>
<TOKEN end_char="731" id="token-13-8" morph="none" pos="word" start_char="728">germ</TOKEN>
<TOKEN end_char="732" id="token-13-9" morph="none" pos="punct" start_char="732">"</TOKEN>
<TOKEN end_char="734" id="token-13-10" morph="none" pos="punct" start_char="734">–</TOKEN>
<TOKEN end_char="738" id="token-13-11" morph="none" pos="word" start_char="736">the</TOKEN>
<TOKEN end_char="747" id="token-13-12" morph="none" pos="word" start_char="740">parasite</TOKEN>
</SEG>
<SEG end_char="770" id="segment-14" start_char="750">
<ORIGINAL_TEXT>Plasmodium falciparum</ORIGINAL_TEXT>
<TOKEN end_char="759" id="token-14-0" morph="none" pos="word" start_char="750">Plasmodium</TOKEN>
<TOKEN end_char="770" id="token-14-1" morph="none" pos="word" start_char="761">falciparum</TOKEN>
</SEG>
<SEG end_char="813" id="segment-15" start_char="773">
<ORIGINAL_TEXT>– can also be seen in the virus’s genome.</ORIGINAL_TEXT>
<TOKEN end_char="773" id="token-15-0" morph="none" pos="punct" start_char="773">–</TOKEN>
<TOKEN end_char="777" id="token-15-1" morph="none" pos="word" start_char="775">can</TOKEN>
<TOKEN end_char="782" id="token-15-2" morph="none" pos="word" start_char="779">also</TOKEN>
<TOKEN end_char="785" id="token-15-3" morph="none" pos="word" start_char="784">be</TOKEN>
<TOKEN end_char="790" id="token-15-4" morph="none" pos="word" start_char="787">seen</TOKEN>
<TOKEN end_char="793" id="token-15-5" morph="none" pos="word" start_char="792">in</TOKEN>
<TOKEN end_char="797" id="token-15-6" morph="none" pos="word" start_char="795">the</TOKEN>
<TOKEN end_char="805" id="token-15-7" morph="none" pos="word" start_char="799">virus’s</TOKEN>
<TOKEN end_char="812" id="token-15-8" morph="none" pos="word" start_char="807">genome</TOKEN>
<TOKEN end_char="813" id="token-15-9" morph="none" pos="punct" start_char="813">.</TOKEN>
</SEG>
<SEG end_char="1021" id="segment-16" start_char="816">
<ORIGINAL_TEXT>His full quote: "We were not the first since a group of Indian researchers tried to publish a study which showed that the complete genome of this coronavirus [has] sequences of another virus, which is HIV."</ORIGINAL_TEXT>
<TOKEN end_char="818" id="token-16-0" morph="none" pos="word" start_char="816">His</TOKEN>
<TOKEN end_char="823" id="token-16-1" morph="none" pos="word" start_char="820">full</TOKEN>
<TOKEN end_char="829" id="token-16-2" morph="none" pos="word" start_char="825">quote</TOKEN>
<TOKEN end_char="830" id="token-16-3" morph="none" pos="punct" start_char="830">:</TOKEN>
<TOKEN end_char="832" id="token-16-4" morph="none" pos="punct" start_char="832">"</TOKEN>
<TOKEN end_char="834" id="token-16-5" morph="none" pos="word" start_char="833">We</TOKEN>
<TOKEN end_char="839" id="token-16-6" morph="none" pos="word" start_char="836">were</TOKEN>
<TOKEN end_char="843" id="token-16-7" morph="none" pos="word" start_char="841">not</TOKEN>
<TOKEN end_char="847" id="token-16-8" morph="none" pos="word" start_char="845">the</TOKEN>
<TOKEN end_char="853" id="token-16-9" morph="none" pos="word" start_char="849">first</TOKEN>
<TOKEN end_char="859" id="token-16-10" morph="none" pos="word" start_char="855">since</TOKEN>
<TOKEN end_char="861" id="token-16-11" morph="none" pos="word" start_char="861">a</TOKEN>
<TOKEN end_char="867" id="token-16-12" morph="none" pos="word" start_char="863">group</TOKEN>
<TOKEN end_char="870" id="token-16-13" morph="none" pos="word" start_char="869">of</TOKEN>
<TOKEN end_char="877" id="token-16-14" morph="none" pos="word" start_char="872">Indian</TOKEN>
<TOKEN end_char="889" id="token-16-15" morph="none" pos="word" start_char="879">researchers</TOKEN>
<TOKEN end_char="895" id="token-16-16" morph="none" pos="word" start_char="891">tried</TOKEN>
<TOKEN end_char="898" id="token-16-17" morph="none" pos="word" start_char="897">to</TOKEN>
<TOKEN end_char="906" id="token-16-18" morph="none" pos="word" start_char="900">publish</TOKEN>
<TOKEN end_char="908" id="token-16-19" morph="none" pos="word" start_char="908">a</TOKEN>
<TOKEN end_char="914" id="token-16-20" morph="none" pos="word" start_char="910">study</TOKEN>
<TOKEN end_char="920" id="token-16-21" morph="none" pos="word" start_char="916">which</TOKEN>
<TOKEN end_char="927" id="token-16-22" morph="none" pos="word" start_char="922">showed</TOKEN>
<TOKEN end_char="932" id="token-16-23" morph="none" pos="word" start_char="929">that</TOKEN>
<TOKEN end_char="936" id="token-16-24" morph="none" pos="word" start_char="934">the</TOKEN>
<TOKEN end_char="945" id="token-16-25" morph="none" pos="word" start_char="938">complete</TOKEN>
<TOKEN end_char="952" id="token-16-26" morph="none" pos="word" start_char="947">genome</TOKEN>
<TOKEN end_char="955" id="token-16-27" morph="none" pos="word" start_char="954">of</TOKEN>
<TOKEN end_char="960" id="token-16-28" morph="none" pos="word" start_char="957">this</TOKEN>
<TOKEN end_char="972" id="token-16-29" morph="none" pos="word" start_char="962">coronavirus</TOKEN>
<TOKEN end_char="974" id="token-16-30" morph="none" pos="punct" start_char="974">[</TOKEN>
<TOKEN end_char="977" id="token-16-31" morph="none" pos="word" start_char="975">has</TOKEN>
<TOKEN end_char="978" id="token-16-32" morph="none" pos="punct" start_char="978">]</TOKEN>
<TOKEN end_char="988" id="token-16-33" morph="none" pos="word" start_char="980">sequences</TOKEN>
<TOKEN end_char="991" id="token-16-34" morph="none" pos="word" start_char="990">of</TOKEN>
<TOKEN end_char="999" id="token-16-35" morph="none" pos="word" start_char="993">another</TOKEN>
<TOKEN end_char="1005" id="token-16-36" morph="none" pos="word" start_char="1001">virus</TOKEN>
<TOKEN end_char="1006" id="token-16-37" morph="none" pos="punct" start_char="1006">,</TOKEN>
<TOKEN end_char="1012" id="token-16-38" morph="none" pos="word" start_char="1008">which</TOKEN>
<TOKEN end_char="1015" id="token-16-39" morph="none" pos="word" start_char="1014">is</TOKEN>
<TOKEN end_char="1019" id="token-16-40" morph="none" pos="word" start_char="1017">HIV</TOKEN>
<TOKEN end_char="1021" id="token-16-41" morph="none" pos="punct" start_char="1020">."</TOKEN>
</SEG>
<SEG end_char="1256" id="segment-17" start_char="1024">
<ORIGINAL_TEXT>In a separate podcast episode with a different outlet, Montagnier further said the virus had escaped in an "industrial accident" from the Wuhan city laboratory when Chinese scientists were attempting to develop a vaccine against HIV.</ORIGINAL_TEXT>
<TOKEN end_char="1025" id="token-17-0" morph="none" pos="word" start_char="1024">In</TOKEN>
<TOKEN end_char="1027" id="token-17-1" morph="none" pos="word" start_char="1027">a</TOKEN>
<TOKEN end_char="1036" id="token-17-2" morph="none" pos="word" start_char="1029">separate</TOKEN>
<TOKEN end_char="1044" id="token-17-3" morph="none" pos="word" start_char="1038">podcast</TOKEN>
<TOKEN end_char="1052" id="token-17-4" morph="none" pos="word" start_char="1046">episode</TOKEN>
<TOKEN end_char="1057" id="token-17-5" morph="none" pos="word" start_char="1054">with</TOKEN>
<TOKEN end_char="1059" id="token-17-6" morph="none" pos="word" start_char="1059">a</TOKEN>
<TOKEN end_char="1069" id="token-17-7" morph="none" pos="word" start_char="1061">different</TOKEN>
<TOKEN end_char="1076" id="token-17-8" morph="none" pos="word" start_char="1071">outlet</TOKEN>
<TOKEN end_char="1077" id="token-17-9" morph="none" pos="punct" start_char="1077">,</TOKEN>
<TOKEN end_char="1088" id="token-17-10" morph="none" pos="word" start_char="1079">Montagnier</TOKEN>
<TOKEN end_char="1096" id="token-17-11" morph="none" pos="word" start_char="1090">further</TOKEN>
<TOKEN end_char="1101" id="token-17-12" morph="none" pos="word" start_char="1098">said</TOKEN>
<TOKEN end_char="1105" id="token-17-13" morph="none" pos="word" start_char="1103">the</TOKEN>
<TOKEN end_char="1111" id="token-17-14" morph="none" pos="word" start_char="1107">virus</TOKEN>
<TOKEN end_char="1115" id="token-17-15" morph="none" pos="word" start_char="1113">had</TOKEN>
<TOKEN end_char="1123" id="token-17-16" morph="none" pos="word" start_char="1117">escaped</TOKEN>
<TOKEN end_char="1126" id="token-17-17" morph="none" pos="word" start_char="1125">in</TOKEN>
<TOKEN end_char="1129" id="token-17-18" morph="none" pos="word" start_char="1128">an</TOKEN>
<TOKEN end_char="1131" id="token-17-19" morph="none" pos="punct" start_char="1131">"</TOKEN>
<TOKEN end_char="1141" id="token-17-20" morph="none" pos="word" start_char="1132">industrial</TOKEN>
<TOKEN end_char="1150" id="token-17-21" morph="none" pos="word" start_char="1143">accident</TOKEN>
<TOKEN end_char="1151" id="token-17-22" morph="none" pos="punct" start_char="1151">"</TOKEN>
<TOKEN end_char="1156" id="token-17-23" morph="none" pos="word" start_char="1153">from</TOKEN>
<TOKEN end_char="1160" id="token-17-24" morph="none" pos="word" start_char="1158">the</TOKEN>
<TOKEN end_char="1166" id="token-17-25" morph="none" pos="word" start_char="1162">Wuhan</TOKEN>
<TOKEN end_char="1171" id="token-17-26" morph="none" pos="word" start_char="1168">city</TOKEN>
<TOKEN end_char="1182" id="token-17-27" morph="none" pos="word" start_char="1173">laboratory</TOKEN>
<TOKEN end_char="1187" id="token-17-28" morph="none" pos="word" start_char="1184">when</TOKEN>
<TOKEN end_char="1195" id="token-17-29" morph="none" pos="word" start_char="1189">Chinese</TOKEN>
<TOKEN end_char="1206" id="token-17-30" morph="none" pos="word" start_char="1197">scientists</TOKEN>
<TOKEN end_char="1211" id="token-17-31" morph="none" pos="word" start_char="1208">were</TOKEN>
<TOKEN end_char="1222" id="token-17-32" morph="none" pos="word" start_char="1213">attempting</TOKEN>
<TOKEN end_char="1225" id="token-17-33" morph="none" pos="word" start_char="1224">to</TOKEN>
<TOKEN end_char="1233" id="token-17-34" morph="none" pos="word" start_char="1227">develop</TOKEN>
<TOKEN end_char="1235" id="token-17-35" morph="none" pos="word" start_char="1235">a</TOKEN>
<TOKEN end_char="1243" id="token-17-36" morph="none" pos="word" start_char="1237">vaccine</TOKEN>
<TOKEN end_char="1251" id="token-17-37" morph="none" pos="word" start_char="1245">against</TOKEN>
<TOKEN end_char="1255" id="token-17-38" morph="none" pos="word" start_char="1253">HIV</TOKEN>
<TOKEN end_char="1256" id="token-17-39" morph="none" pos="punct" start_char="1256">.</TOKEN>
</SEG>
<SEG end_char="1304" id="segment-18" start_char="1259">
<ORIGINAL_TEXT>The new coronavirus is an RNA virus, like HIV.</ORIGINAL_TEXT>
<TOKEN end_char="1261" id="token-18-0" morph="none" pos="word" start_char="1259">The</TOKEN>
<TOKEN end_char="1265" id="token-18-1" morph="none" pos="word" start_char="1263">new</TOKEN>
<TOKEN end_char="1277" id="token-18-2" morph="none" pos="word" start_char="1267">coronavirus</TOKEN>
<TOKEN end_char="1280" id="token-18-3" morph="none" pos="word" start_char="1279">is</TOKEN>
<TOKEN end_char="1283" id="token-18-4" morph="none" pos="word" start_char="1282">an</TOKEN>
<TOKEN end_char="1287" id="token-18-5" morph="none" pos="word" start_char="1285">RNA</TOKEN>
<TOKEN end_char="1293" id="token-18-6" morph="none" pos="word" start_char="1289">virus</TOKEN>
<TOKEN end_char="1294" id="token-18-7" morph="none" pos="punct" start_char="1294">,</TOKEN>
<TOKEN end_char="1299" id="token-18-8" morph="none" pos="word" start_char="1296">like</TOKEN>
<TOKEN end_char="1303" id="token-18-9" morph="none" pos="word" start_char="1301">HIV</TOKEN>
<TOKEN end_char="1304" id="token-18-10" morph="none" pos="punct" start_char="1304">.</TOKEN>
</SEG>
<SEG end_char="1461" id="segment-19" start_char="1306">
<ORIGINAL_TEXT>Scientists already know that many viruses incorporate pieces of other genomes into their own in the natural course of evolution, both of plants and animals.</ORIGINAL_TEXT>
<TOKEN end_char="1315" id="token-19-0" morph="none" pos="word" start_char="1306">Scientists</TOKEN>
<TOKEN end_char="1323" id="token-19-1" morph="none" pos="word" start_char="1317">already</TOKEN>
<TOKEN end_char="1328" id="token-19-2" morph="none" pos="word" start_char="1325">know</TOKEN>
<TOKEN end_char="1333" id="token-19-3" morph="none" pos="word" start_char="1330">that</TOKEN>
<TOKEN end_char="1338" id="token-19-4" morph="none" pos="word" start_char="1335">many</TOKEN>
<TOKEN end_char="1346" id="token-19-5" morph="none" pos="word" start_char="1340">viruses</TOKEN>
<TOKEN end_char="1358" id="token-19-6" morph="none" pos="word" start_char="1348">incorporate</TOKEN>
<TOKEN end_char="1365" id="token-19-7" morph="none" pos="word" start_char="1360">pieces</TOKEN>
<TOKEN end_char="1368" id="token-19-8" morph="none" pos="word" start_char="1367">of</TOKEN>
<TOKEN end_char="1374" id="token-19-9" morph="none" pos="word" start_char="1370">other</TOKEN>
<TOKEN end_char="1382" id="token-19-10" morph="none" pos="word" start_char="1376">genomes</TOKEN>
<TOKEN end_char="1387" id="token-19-11" morph="none" pos="word" start_char="1384">into</TOKEN>
<TOKEN end_char="1393" id="token-19-12" morph="none" pos="word" start_char="1389">their</TOKEN>
<TOKEN end_char="1397" id="token-19-13" morph="none" pos="word" start_char="1395">own</TOKEN>
<TOKEN end_char="1400" id="token-19-14" morph="none" pos="word" start_char="1399">in</TOKEN>
<TOKEN end_char="1404" id="token-19-15" morph="none" pos="word" start_char="1402">the</TOKEN>
<TOKEN end_char="1412" id="token-19-16" morph="none" pos="word" start_char="1406">natural</TOKEN>
<TOKEN end_char="1419" id="token-19-17" morph="none" pos="word" start_char="1414">course</TOKEN>
<TOKEN end_char="1422" id="token-19-18" morph="none" pos="word" start_char="1421">of</TOKEN>
<TOKEN end_char="1432" id="token-19-19" morph="none" pos="word" start_char="1424">evolution</TOKEN>
<TOKEN end_char="1433" id="token-19-20" morph="none" pos="punct" start_char="1433">,</TOKEN>
<TOKEN end_char="1438" id="token-19-21" morph="none" pos="word" start_char="1435">both</TOKEN>
<TOKEN end_char="1441" id="token-19-22" morph="none" pos="word" start_char="1440">of</TOKEN>
<TOKEN end_char="1448" id="token-19-23" morph="none" pos="word" start_char="1443">plants</TOKEN>
<TOKEN end_char="1452" id="token-19-24" morph="none" pos="word" start_char="1450">and</TOKEN>
<TOKEN end_char="1460" id="token-19-25" morph="none" pos="word" start_char="1454">animals</TOKEN>
<TOKEN end_char="1461" id="token-19-26" morph="none" pos="punct" start_char="1461">.</TOKEN>
</SEG>
<SEG end_char="1652" id="segment-20" start_char="1463">
<ORIGINAL_TEXT>Indeed, fully 43% of the human genome is composed of mobile genetic element sequences, which are the leftovers of viral infections that our ancestors experienced over the last 300,000 years.</ORIGINAL_TEXT>
<TOKEN end_char="1468" id="token-20-0" morph="none" pos="word" start_char="1463">Indeed</TOKEN>
<TOKEN end_char="1469" id="token-20-1" morph="none" pos="punct" start_char="1469">,</TOKEN>
<TOKEN end_char="1475" id="token-20-2" morph="none" pos="word" start_char="1471">fully</TOKEN>
<TOKEN end_char="1478" id="token-20-3" morph="none" pos="word" start_char="1477">43</TOKEN>
<TOKEN end_char="1479" id="token-20-4" morph="none" pos="punct" start_char="1479">%</TOKEN>
<TOKEN end_char="1482" id="token-20-5" morph="none" pos="word" start_char="1481">of</TOKEN>
<TOKEN end_char="1486" id="token-20-6" morph="none" pos="word" start_char="1484">the</TOKEN>
<TOKEN end_char="1492" id="token-20-7" morph="none" pos="word" start_char="1488">human</TOKEN>
<TOKEN end_char="1499" id="token-20-8" morph="none" pos="word" start_char="1494">genome</TOKEN>
<TOKEN end_char="1502" id="token-20-9" morph="none" pos="word" start_char="1501">is</TOKEN>
<TOKEN end_char="1511" id="token-20-10" morph="none" pos="word" start_char="1504">composed</TOKEN>
<TOKEN end_char="1514" id="token-20-11" morph="none" pos="word" start_char="1513">of</TOKEN>
<TOKEN end_char="1521" id="token-20-12" morph="none" pos="word" start_char="1516">mobile</TOKEN>
<TOKEN end_char="1529" id="token-20-13" morph="none" pos="word" start_char="1523">genetic</TOKEN>
<TOKEN end_char="1537" id="token-20-14" morph="none" pos="word" start_char="1531">element</TOKEN>
<TOKEN end_char="1547" id="token-20-15" morph="none" pos="word" start_char="1539">sequences</TOKEN>
<TOKEN end_char="1548" id="token-20-16" morph="none" pos="punct" start_char="1548">,</TOKEN>
<TOKEN end_char="1554" id="token-20-17" morph="none" pos="word" start_char="1550">which</TOKEN>
<TOKEN end_char="1558" id="token-20-18" morph="none" pos="word" start_char="1556">are</TOKEN>
<TOKEN end_char="1562" id="token-20-19" morph="none" pos="word" start_char="1560">the</TOKEN>
<TOKEN end_char="1572" id="token-20-20" morph="none" pos="word" start_char="1564">leftovers</TOKEN>
<TOKEN end_char="1575" id="token-20-21" morph="none" pos="word" start_char="1574">of</TOKEN>
<TOKEN end_char="1581" id="token-20-22" morph="none" pos="word" start_char="1577">viral</TOKEN>
<TOKEN end_char="1592" id="token-20-23" morph="none" pos="word" start_char="1583">infections</TOKEN>
<TOKEN end_char="1597" id="token-20-24" morph="none" pos="word" start_char="1594">that</TOKEN>
<TOKEN end_char="1601" id="token-20-25" morph="none" pos="word" start_char="1599">our</TOKEN>
<TOKEN end_char="1611" id="token-20-26" morph="none" pos="word" start_char="1603">ancestors</TOKEN>
<TOKEN end_char="1623" id="token-20-27" morph="none" pos="word" start_char="1613">experienced</TOKEN>
<TOKEN end_char="1628" id="token-20-28" morph="none" pos="word" start_char="1625">over</TOKEN>
<TOKEN end_char="1632" id="token-20-29" morph="none" pos="word" start_char="1630">the</TOKEN>
<TOKEN end_char="1637" id="token-20-30" morph="none" pos="word" start_char="1634">last</TOKEN>
<TOKEN end_char="1645" id="token-20-31" morph="none" pos="unknown" start_char="1639">300,000</TOKEN>
<TOKEN end_char="1651" id="token-20-32" morph="none" pos="word" start_char="1647">years</TOKEN>
<TOKEN end_char="1652" id="token-20-33" morph="none" pos="punct" start_char="1652">.</TOKEN>
</SEG>
<SEG end_char="1736" id="segment-21" start_char="1655">
<ORIGINAL_TEXT>The new virus also has an exceptionally large genome, of about 30,000 nucleobases.</ORIGINAL_TEXT>
<TOKEN end_char="1657" id="token-21-0" morph="none" pos="word" start_char="1655">The</TOKEN>
<TOKEN end_char="1661" id="token-21-1" morph="none" pos="word" start_char="1659">new</TOKEN>
<TOKEN end_char="1667" id="token-21-2" morph="none" pos="word" start_char="1663">virus</TOKEN>
<TOKEN end_char="1672" id="token-21-3" morph="none" pos="word" start_char="1669">also</TOKEN>
<TOKEN end_char="1676" id="token-21-4" morph="none" pos="word" start_char="1674">has</TOKEN>
<TOKEN end_char="1679" id="token-21-5" morph="none" pos="word" start_char="1678">an</TOKEN>
<TOKEN end_char="1693" id="token-21-6" morph="none" pos="word" start_char="1681">exceptionally</TOKEN>
<TOKEN end_char="1699" id="token-21-7" morph="none" pos="word" start_char="1695">large</TOKEN>
<TOKEN end_char="1706" id="token-21-8" morph="none" pos="word" start_char="1701">genome</TOKEN>
<TOKEN end_char="1707" id="token-21-9" morph="none" pos="punct" start_char="1707">,</TOKEN>
<TOKEN end_char="1710" id="token-21-10" morph="none" pos="word" start_char="1709">of</TOKEN>
<TOKEN end_char="1716" id="token-21-11" morph="none" pos="word" start_char="1712">about</TOKEN>
<TOKEN end_char="1723" id="token-21-12" morph="none" pos="unknown" start_char="1718">30,000</TOKEN>
<TOKEN end_char="1735" id="token-21-13" morph="none" pos="word" start_char="1725">nucleobases</TOKEN>
<TOKEN end_char="1736" id="token-21-14" morph="none" pos="punct" start_char="1736">.</TOKEN>
</SEG>
<SEG end_char="1842" id="segment-22" start_char="1738">
<ORIGINAL_TEXT>Mobile genetic elements have been discovered in many viruses with large genomes, including coronaviruses.</ORIGINAL_TEXT>
<TOKEN end_char="1743" id="token-22-0" morph="none" pos="word" start_char="1738">Mobile</TOKEN>
<TOKEN end_char="1751" id="token-22-1" morph="none" pos="word" start_char="1745">genetic</TOKEN>
<TOKEN end_char="1760" id="token-22-2" morph="none" pos="word" start_char="1753">elements</TOKEN>
<TOKEN end_char="1765" id="token-22-3" morph="none" pos="word" start_char="1762">have</TOKEN>
<TOKEN end_char="1770" id="token-22-4" morph="none" pos="word" start_char="1767">been</TOKEN>
<TOKEN end_char="1781" id="token-22-5" morph="none" pos="word" start_char="1772">discovered</TOKEN>
<TOKEN end_char="1784" id="token-22-6" morph="none" pos="word" start_char="1783">in</TOKEN>
<TOKEN end_char="1789" id="token-22-7" morph="none" pos="word" start_char="1786">many</TOKEN>
<TOKEN end_char="1797" id="token-22-8" morph="none" pos="word" start_char="1791">viruses</TOKEN>
<TOKEN end_char="1802" id="token-22-9" morph="none" pos="word" start_char="1799">with</TOKEN>
<TOKEN end_char="1808" id="token-22-10" morph="none" pos="word" start_char="1804">large</TOKEN>
<TOKEN end_char="1816" id="token-22-11" morph="none" pos="word" start_char="1810">genomes</TOKEN>
<TOKEN end_char="1817" id="token-22-12" morph="none" pos="punct" start_char="1817">,</TOKEN>
<TOKEN end_char="1827" id="token-22-13" morph="none" pos="word" start_char="1819">including</TOKEN>
<TOKEN end_char="1841" id="token-22-14" morph="none" pos="word" start_char="1829">coronaviruses</TOKEN>
<TOKEN end_char="1842" id="token-22-15" morph="none" pos="punct" start_char="1842">.</TOKEN>
</SEG>
<SEG end_char="1940" id="segment-23" start_char="1844">
<ORIGINAL_TEXT>The Indian study Montagnier referred to had been authored by a team from IIT Delhi, among others.</ORIGINAL_TEXT>
<TOKEN end_char="1846" id="token-23-0" morph="none" pos="word" start_char="1844">The</TOKEN>
<TOKEN end_char="1853" id="token-23-1" morph="none" pos="word" start_char="1848">Indian</TOKEN>
<TOKEN end_char="1859" id="token-23-2" morph="none" pos="word" start_char="1855">study</TOKEN>
<TOKEN end_char="1870" id="token-23-3" morph="none" pos="word" start_char="1861">Montagnier</TOKEN>
<TOKEN end_char="1879" id="token-23-4" morph="none" pos="word" start_char="1872">referred</TOKEN>
<TOKEN end_char="1882" id="token-23-5" morph="none" pos="word" start_char="1881">to</TOKEN>
<TOKEN end_char="1886" id="token-23-6" morph="none" pos="word" start_char="1884">had</TOKEN>
<TOKEN end_char="1891" id="token-23-7" morph="none" pos="word" start_char="1888">been</TOKEN>
<TOKEN end_char="1900" id="token-23-8" morph="none" pos="word" start_char="1893">authored</TOKEN>
<TOKEN end_char="1903" id="token-23-9" morph="none" pos="word" start_char="1902">by</TOKEN>
<TOKEN end_char="1905" id="token-23-10" morph="none" pos="word" start_char="1905">a</TOKEN>
<TOKEN end_char="1910" id="token-23-11" morph="none" pos="word" start_char="1907">team</TOKEN>
<TOKEN end_char="1915" id="token-23-12" morph="none" pos="word" start_char="1912">from</TOKEN>
<TOKEN end_char="1919" id="token-23-13" morph="none" pos="word" start_char="1917">IIT</TOKEN>
<TOKEN end_char="1925" id="token-23-14" morph="none" pos="word" start_char="1921">Delhi</TOKEN>
<TOKEN end_char="1926" id="token-23-15" morph="none" pos="punct" start_char="1926">,</TOKEN>
<TOKEN end_char="1932" id="token-23-16" morph="none" pos="word" start_char="1928">among</TOKEN>
<TOKEN end_char="1939" id="token-23-17" morph="none" pos="word" start_char="1934">others</TOKEN>
<TOKEN end_char="1940" id="token-23-18" morph="none" pos="punct" start_char="1940">.</TOKEN>
</SEG>
<SEG end_char="2105" id="segment-24" start_char="1942">
<ORIGINAL_TEXT>They had uploaded their manuscript to the bioRxiv preprint repository only to quickly take it down after commentators pointed out numerous errors in their analysis.</ORIGINAL_TEXT>
<TOKEN end_char="1945" id="token-24-0" morph="none" pos="word" start_char="1942">They</TOKEN>
<TOKEN end_char="1949" id="token-24-1" morph="none" pos="word" start_char="1947">had</TOKEN>
<TOKEN end_char="1958" id="token-24-2" morph="none" pos="word" start_char="1951">uploaded</TOKEN>
<TOKEN end_char="1964" id="token-24-3" morph="none" pos="word" start_char="1960">their</TOKEN>
<TOKEN end_char="1975" id="token-24-4" morph="none" pos="word" start_char="1966">manuscript</TOKEN>
<TOKEN end_char="1978" id="token-24-5" morph="none" pos="word" start_char="1977">to</TOKEN>
<TOKEN end_char="1982" id="token-24-6" morph="none" pos="word" start_char="1980">the</TOKEN>
<TOKEN end_char="1990" id="token-24-7" morph="none" pos="word" start_char="1984">bioRxiv</TOKEN>
<TOKEN end_char="1999" id="token-24-8" morph="none" pos="word" start_char="1992">preprint</TOKEN>
<TOKEN end_char="2010" id="token-24-9" morph="none" pos="word" start_char="2001">repository</TOKEN>
<TOKEN end_char="2015" id="token-24-10" morph="none" pos="word" start_char="2012">only</TOKEN>
<TOKEN end_char="2018" id="token-24-11" morph="none" pos="word" start_char="2017">to</TOKEN>
<TOKEN end_char="2026" id="token-24-12" morph="none" pos="word" start_char="2020">quickly</TOKEN>
<TOKEN end_char="2031" id="token-24-13" morph="none" pos="word" start_char="2028">take</TOKEN>
<TOKEN end_char="2034" id="token-24-14" morph="none" pos="word" start_char="2033">it</TOKEN>
<TOKEN end_char="2039" id="token-24-15" morph="none" pos="word" start_char="2036">down</TOKEN>
<TOKEN end_char="2045" id="token-24-16" morph="none" pos="word" start_char="2041">after</TOKEN>
<TOKEN end_char="2058" id="token-24-17" morph="none" pos="word" start_char="2047">commentators</TOKEN>
<TOKEN end_char="2066" id="token-24-18" morph="none" pos="word" start_char="2060">pointed</TOKEN>
<TOKEN end_char="2070" id="token-24-19" morph="none" pos="word" start_char="2068">out</TOKEN>
<TOKEN end_char="2079" id="token-24-20" morph="none" pos="word" start_char="2072">numerous</TOKEN>
<TOKEN end_char="2086" id="token-24-21" morph="none" pos="word" start_char="2081">errors</TOKEN>
<TOKEN end_char="2089" id="token-24-22" morph="none" pos="word" start_char="2088">in</TOKEN>
<TOKEN end_char="2095" id="token-24-23" morph="none" pos="word" start_char="2091">their</TOKEN>
<TOKEN end_char="2104" id="token-24-24" morph="none" pos="word" start_char="2097">analysis</TOKEN>
<TOKEN end_char="2105" id="token-24-25" morph="none" pos="punct" start_char="2105">.</TOKEN>
</SEG>
<SEG end_char="2156" id="segment-25" start_char="2108">
<ORIGINAL_TEXT>An article published more recently in the journal</ORIGINAL_TEXT>
<TOKEN end_char="2109" id="token-25-0" morph="none" pos="word" start_char="2108">An</TOKEN>
<TOKEN end_char="2117" id="token-25-1" morph="none" pos="word" start_char="2111">article</TOKEN>
<TOKEN end_char="2127" id="token-25-2" morph="none" pos="word" start_char="2119">published</TOKEN>
<TOKEN end_char="2132" id="token-25-3" morph="none" pos="word" start_char="2129">more</TOKEN>
<TOKEN end_char="2141" id="token-25-4" morph="none" pos="word" start_char="2134">recently</TOKEN>
<TOKEN end_char="2144" id="token-25-5" morph="none" pos="word" start_char="2143">in</TOKEN>
<TOKEN end_char="2148" id="token-25-6" morph="none" pos="word" start_char="2146">the</TOKEN>
<TOKEN end_char="2156" id="token-25-7" morph="none" pos="word" start_char="2150">journal</TOKEN>
</SEG>
<SEG end_char="2173" id="segment-26" start_char="2159">
<ORIGINAL_TEXT>Nature Medicine</ORIGINAL_TEXT>
<TOKEN end_char="2164" id="token-26-0" morph="none" pos="word" start_char="2159">Nature</TOKEN>
<TOKEN end_char="2173" id="token-26-1" morph="none" pos="word" start_char="2166">Medicine</TOKEN>
<TRANSLATED_TEXT>Naturmedicin</TRANSLATED_TEXT><DETECTED_LANGUAGE>ro</DETECTED_LANGUAGE></SEG>
<SEG end_char="2334" id="segment-27" start_char="2176">
<ORIGINAL_TEXT>analysing the new virus’s genome concluded thus: "Our analyses clearly show that SARS-CoV-2 is not a laboratory construct or a purposefully manipulated virus."</ORIGINAL_TEXT>
<TOKEN end_char="2184" id="token-27-0" morph="none" pos="word" start_char="2176">analysing</TOKEN>
<TOKEN end_char="2188" id="token-27-1" morph="none" pos="word" start_char="2186">the</TOKEN>
<TOKEN end_char="2192" id="token-27-2" morph="none" pos="word" start_char="2190">new</TOKEN>
<TOKEN end_char="2200" id="token-27-3" morph="none" pos="word" start_char="2194">virus’s</TOKEN>
<TOKEN end_char="2207" id="token-27-4" morph="none" pos="word" start_char="2202">genome</TOKEN>
<TOKEN end_char="2217" id="token-27-5" morph="none" pos="word" start_char="2209">concluded</TOKEN>
<TOKEN end_char="2222" id="token-27-6" morph="none" pos="word" start_char="2219">thus</TOKEN>
<TOKEN end_char="2223" id="token-27-7" morph="none" pos="punct" start_char="2223">:</TOKEN>
<TOKEN end_char="2225" id="token-27-8" morph="none" pos="punct" start_char="2225">"</TOKEN>
<TOKEN end_char="2228" id="token-27-9" morph="none" pos="word" start_char="2226">Our</TOKEN>
<TOKEN end_char="2237" id="token-27-10" morph="none" pos="word" start_char="2230">analyses</TOKEN>
<TOKEN end_char="2245" id="token-27-11" morph="none" pos="word" start_char="2239">clearly</TOKEN>
<TOKEN end_char="2250" id="token-27-12" morph="none" pos="word" start_char="2247">show</TOKEN>
<TOKEN end_char="2255" id="token-27-13" morph="none" pos="word" start_char="2252">that</TOKEN>
<TOKEN end_char="2266" id="token-27-14" morph="none" pos="unknown" start_char="2257">SARS-CoV-2</TOKEN>
<TOKEN end_char="2269" id="token-27-15" morph="none" pos="word" start_char="2268">is</TOKEN>
<TOKEN end_char="2273" id="token-27-16" morph="none" pos="word" start_char="2271">not</TOKEN>
<TOKEN end_char="2275" id="token-27-17" morph="none" pos="word" start_char="2275">a</TOKEN>
<TOKEN end_char="2286" id="token-27-18" morph="none" pos="word" start_char="2277">laboratory</TOKEN>
<TOKEN end_char="2296" id="token-27-19" morph="none" pos="word" start_char="2288">construct</TOKEN>
<TOKEN end_char="2299" id="token-27-20" morph="none" pos="word" start_char="2298">or</TOKEN>
<TOKEN end_char="2301" id="token-27-21" morph="none" pos="word" start_char="2301">a</TOKEN>
<TOKEN end_char="2314" id="token-27-22" morph="none" pos="word" start_char="2303">purposefully</TOKEN>
<TOKEN end_char="2326" id="token-27-23" morph="none" pos="word" start_char="2316">manipulated</TOKEN>
<TOKEN end_char="2332" id="token-27-24" morph="none" pos="word" start_char="2328">virus</TOKEN>
<TOKEN end_char="2334" id="token-27-25" morph="none" pos="punct" start_char="2333">."</TOKEN>
</SEG>
<SEG end_char="2411" id="segment-28" start_char="2337">
<ORIGINAL_TEXT>What Montagnier called the "elements" of HIV were short cis-acting elements</ORIGINAL_TEXT>
<TOKEN end_char="2340" id="token-28-0" morph="none" pos="word" start_char="2337">What</TOKEN>
<TOKEN end_char="2351" id="token-28-1" morph="none" pos="word" start_char="2342">Montagnier</TOKEN>
<TOKEN end_char="2358" id="token-28-2" morph="none" pos="word" start_char="2353">called</TOKEN>
<TOKEN end_char="2362" id="token-28-3" morph="none" pos="word" start_char="2360">the</TOKEN>
<TOKEN end_char="2364" id="token-28-4" morph="none" pos="punct" start_char="2364">"</TOKEN>
<TOKEN end_char="2372" id="token-28-5" morph="none" pos="word" start_char="2365">elements</TOKEN>
<TOKEN end_char="2373" id="token-28-6" morph="none" pos="punct" start_char="2373">"</TOKEN>
<TOKEN end_char="2376" id="token-28-7" morph="none" pos="word" start_char="2375">of</TOKEN>
<TOKEN end_char="2380" id="token-28-8" morph="none" pos="word" start_char="2378">HIV</TOKEN>
<TOKEN end_char="2385" id="token-28-9" morph="none" pos="word" start_char="2382">were</TOKEN>
<TOKEN end_char="2391" id="token-28-10" morph="none" pos="word" start_char="2387">short</TOKEN>
<TOKEN end_char="2402" id="token-28-11" morph="none" pos="unknown" start_char="2393">cis-acting</TOKEN>
<TOKEN end_char="2411" id="token-28-12" morph="none" pos="word" start_char="2404">elements</TOKEN>
</SEG>
<SEG end_char="2483" id="segment-29" start_char="2414">
<ORIGINAL_TEXT>that scientists had discovered in the genome of coronaviruses in 2005.</ORIGINAL_TEXT>
<TOKEN end_char="2417" id="token-29-0" morph="none" pos="word" start_char="2414">that</TOKEN>
<TOKEN end_char="2428" id="token-29-1" morph="none" pos="word" start_char="2419">scientists</TOKEN>
<TOKEN end_char="2432" id="token-29-2" morph="none" pos="word" start_char="2430">had</TOKEN>
<TOKEN end_char="2443" id="token-29-3" morph="none" pos="word" start_char="2434">discovered</TOKEN>
<TOKEN end_char="2446" id="token-29-4" morph="none" pos="word" start_char="2445">in</TOKEN>
<TOKEN end_char="2450" id="token-29-5" morph="none" pos="word" start_char="2448">the</TOKEN>
<TOKEN end_char="2457" id="token-29-6" morph="none" pos="word" start_char="2452">genome</TOKEN>
<TOKEN end_char="2460" id="token-29-7" morph="none" pos="word" start_char="2459">of</TOKEN>
<TOKEN end_char="2474" id="token-29-8" morph="none" pos="word" start_char="2462">coronaviruses</TOKEN>
<TOKEN end_char="2477" id="token-29-9" morph="none" pos="word" start_char="2476">in</TOKEN>
<TOKEN end_char="2482" id="token-29-10" morph="none" pos="word" start_char="2479">2005</TOKEN>
<TOKEN end_char="2483" id="token-29-11" morph="none" pos="punct" start_char="2483">.</TOKEN>
</SEG>
<SEG end_char="2562" id="segment-30" start_char="2485">
<ORIGINAL_TEXT>They are required for genome replication and are shared by many coronaviruses.</ORIGINAL_TEXT>
<TOKEN end_char="2488" id="token-30-0" morph="none" pos="word" start_char="2485">They</TOKEN>
<TOKEN end_char="2492" id="token-30-1" morph="none" pos="word" start_char="2490">are</TOKEN>
<TOKEN end_char="2501" id="token-30-2" morph="none" pos="word" start_char="2494">required</TOKEN>
<TOKEN end_char="2505" id="token-30-3" morph="none" pos="word" start_char="2503">for</TOKEN>
<TOKEN end_char="2512" id="token-30-4" morph="none" pos="word" start_char="2507">genome</TOKEN>
<TOKEN end_char="2524" id="token-30-5" morph="none" pos="word" start_char="2514">replication</TOKEN>
<TOKEN end_char="2528" id="token-30-6" morph="none" pos="word" start_char="2526">and</TOKEN>
<TOKEN end_char="2532" id="token-30-7" morph="none" pos="word" start_char="2530">are</TOKEN>
<TOKEN end_char="2539" id="token-30-8" morph="none" pos="word" start_char="2534">shared</TOKEN>
<TOKEN end_char="2542" id="token-30-9" morph="none" pos="word" start_char="2541">by</TOKEN>
<TOKEN end_char="2547" id="token-30-10" morph="none" pos="word" start_char="2544">many</TOKEN>
<TOKEN end_char="2561" id="token-30-11" morph="none" pos="word" start_char="2549">coronaviruses</TOKEN>
<TOKEN end_char="2562" id="token-30-12" morph="none" pos="punct" start_char="2562">.</TOKEN>
</SEG>
<SEG end_char="2737" id="segment-31" start_char="2564">
<ORIGINAL_TEXT>So if what Montagnier said is true, the whole family of coronaviruses – which originated over 10,000 years ago – would have to be lab-made, and this is obviously nonsensical.</ORIGINAL_TEXT>
<TOKEN end_char="2565" id="token-31-0" morph="none" pos="word" start_char="2564">So</TOKEN>
<TOKEN end_char="2568" id="token-31-1" morph="none" pos="word" start_char="2567">if</TOKEN>
<TOKEN end_char="2573" id="token-31-2" morph="none" pos="word" start_char="2570">what</TOKEN>
<TOKEN end_char="2584" id="token-31-3" morph="none" pos="word" start_char="2575">Montagnier</TOKEN>
<TOKEN end_char="2589" id="token-31-4" morph="none" pos="word" start_char="2586">said</TOKEN>
<TOKEN end_char="2592" id="token-31-5" morph="none" pos="word" start_char="2591">is</TOKEN>
<TOKEN end_char="2597" id="token-31-6" morph="none" pos="word" start_char="2594">true</TOKEN>
<TOKEN end_char="2598" id="token-31-7" morph="none" pos="punct" start_char="2598">,</TOKEN>
<TOKEN end_char="2602" id="token-31-8" morph="none" pos="word" start_char="2600">the</TOKEN>
<TOKEN end_char="2608" id="token-31-9" morph="none" pos="word" start_char="2604">whole</TOKEN>
<TOKEN end_char="2615" id="token-31-10" morph="none" pos="word" start_char="2610">family</TOKEN>
<TOKEN end_char="2618" id="token-31-11" morph="none" pos="word" start_char="2617">of</TOKEN>
<TOKEN end_char="2632" id="token-31-12" morph="none" pos="word" start_char="2620">coronaviruses</TOKEN>
<TOKEN end_char="2634" id="token-31-13" morph="none" pos="punct" start_char="2634">–</TOKEN>
<TOKEN end_char="2640" id="token-31-14" morph="none" pos="word" start_char="2636">which</TOKEN>
<TOKEN end_char="2651" id="token-31-15" morph="none" pos="word" start_char="2642">originated</TOKEN>
<TOKEN end_char="2656" id="token-31-16" morph="none" pos="word" start_char="2653">over</TOKEN>
<TOKEN end_char="2663" id="token-31-17" morph="none" pos="unknown" start_char="2658">10,000</TOKEN>
<TOKEN end_char="2669" id="token-31-18" morph="none" pos="word" start_char="2665">years</TOKEN>
<TOKEN end_char="2673" id="token-31-19" morph="none" pos="word" start_char="2671">ago</TOKEN>
<TOKEN end_char="2675" id="token-31-20" morph="none" pos="punct" start_char="2675">–</TOKEN>
<TOKEN end_char="2681" id="token-31-21" morph="none" pos="word" start_char="2677">would</TOKEN>
<TOKEN end_char="2686" id="token-31-22" morph="none" pos="word" start_char="2683">have</TOKEN>
<TOKEN end_char="2689" id="token-31-23" morph="none" pos="word" start_char="2688">to</TOKEN>
<TOKEN end_char="2692" id="token-31-24" morph="none" pos="word" start_char="2691">be</TOKEN>
<TOKEN end_char="2701" id="token-31-25" morph="none" pos="unknown" start_char="2694">lab-made</TOKEN>
<TOKEN end_char="2702" id="token-31-26" morph="none" pos="punct" start_char="2702">,</TOKEN>
<TOKEN end_char="2706" id="token-31-27" morph="none" pos="word" start_char="2704">and</TOKEN>
<TOKEN end_char="2711" id="token-31-28" morph="none" pos="word" start_char="2708">this</TOKEN>
<TOKEN end_char="2714" id="token-31-29" morph="none" pos="word" start_char="2713">is</TOKEN>
<TOKEN end_char="2724" id="token-31-30" morph="none" pos="word" start_char="2716">obviously</TOKEN>
<TOKEN end_char="2736" id="token-31-31" morph="none" pos="word" start_char="2726">nonsensical</TOKEN>
<TOKEN end_char="2737" id="token-31-32" morph="none" pos="punct" start_char="2737">.</TOKEN>
</SEG>
<SEG end_char="2820" id="segment-32" start_char="2740">
<ORIGINAL_TEXT>Many experts have already pointed out this obvious flaw in Montagnier’s argument.</ORIGINAL_TEXT>
<TOKEN end_char="2743" id="token-32-0" morph="none" pos="word" start_char="2740">Many</TOKEN>
<TOKEN end_char="2751" id="token-32-1" morph="none" pos="word" start_char="2745">experts</TOKEN>
<TOKEN end_char="2756" id="token-32-2" morph="none" pos="word" start_char="2753">have</TOKEN>
<TOKEN end_char="2764" id="token-32-3" morph="none" pos="word" start_char="2758">already</TOKEN>
<TOKEN end_char="2772" id="token-32-4" morph="none" pos="word" start_char="2766">pointed</TOKEN>
<TOKEN end_char="2776" id="token-32-5" morph="none" pos="word" start_char="2774">out</TOKEN>
<TOKEN end_char="2781" id="token-32-6" morph="none" pos="word" start_char="2778">this</TOKEN>
<TOKEN end_char="2789" id="token-32-7" morph="none" pos="word" start_char="2783">obvious</TOKEN>
<TOKEN end_char="2794" id="token-32-8" morph="none" pos="word" start_char="2791">flaw</TOKEN>
<TOKEN end_char="2797" id="token-32-9" morph="none" pos="word" start_char="2796">in</TOKEN>
<TOKEN end_char="2810" id="token-32-10" morph="none" pos="word" start_char="2799">Montagnier’s</TOKEN>
<TOKEN end_char="2819" id="token-32-11" morph="none" pos="word" start_char="2812">argument</TOKEN>
<TOKEN end_char="2820" id="token-32-12" morph="none" pos="punct" start_char="2820">.</TOKEN>
</SEG>
<SEG end_char="3008" id="segment-33" start_char="2822">
<ORIGINAL_TEXT>As Étienne Simon-Lorière, a professor at the Institut Pasteur in Paris, said, "If we take a word from a book and it looks like another word, can we say that one has copied from the other?</ORIGINAL_TEXT>
<TOKEN end_char="2823" id="token-33-0" morph="none" pos="word" start_char="2822">As</TOKEN>
<TOKEN end_char="2831" id="token-33-1" morph="none" pos="word" start_char="2825">Étienne</TOKEN>
<TOKEN end_char="2845" id="token-33-2" morph="none" pos="unknown" start_char="2833">Simon-Lorière</TOKEN>
<TOKEN end_char="2846" id="token-33-3" morph="none" pos="punct" start_char="2846">,</TOKEN>
<TOKEN end_char="2848" id="token-33-4" morph="none" pos="word" start_char="2848">a</TOKEN>
<TOKEN end_char="2858" id="token-33-5" morph="none" pos="word" start_char="2850">professor</TOKEN>
<TOKEN end_char="2861" id="token-33-6" morph="none" pos="word" start_char="2860">at</TOKEN>
<TOKEN end_char="2865" id="token-33-7" morph="none" pos="word" start_char="2863">the</TOKEN>
<TOKEN end_char="2874" id="token-33-8" morph="none" pos="word" start_char="2867">Institut</TOKEN>
<TOKEN end_char="2882" id="token-33-9" morph="none" pos="word" start_char="2876">Pasteur</TOKEN>
<TOKEN end_char="2885" id="token-33-10" morph="none" pos="word" start_char="2884">in</TOKEN>
<TOKEN end_char="2891" id="token-33-11" morph="none" pos="word" start_char="2887">Paris</TOKEN>
<TOKEN end_char="2892" id="token-33-12" morph="none" pos="punct" start_char="2892">,</TOKEN>
<TOKEN end_char="2897" id="token-33-13" morph="none" pos="word" start_char="2894">said</TOKEN>
<TOKEN end_char="2898" id="token-33-14" morph="none" pos="punct" start_char="2898">,</TOKEN>
<TOKEN end_char="2900" id="token-33-15" morph="none" pos="punct" start_char="2900">"</TOKEN>
<TOKEN end_char="2902" id="token-33-16" morph="none" pos="word" start_char="2901">If</TOKEN>
<TOKEN end_char="2905" id="token-33-17" morph="none" pos="word" start_char="2904">we</TOKEN>
<TOKEN end_char="2910" id="token-33-18" morph="none" pos="word" start_char="2907">take</TOKEN>
<TOKEN end_char="2912" id="token-33-19" morph="none" pos="word" start_char="2912">a</TOKEN>
<TOKEN end_char="2917" id="token-33-20" morph="none" pos="word" start_char="2914">word</TOKEN>
<TOKEN end_char="2922" id="token-33-21" morph="none" pos="word" start_char="2919">from</TOKEN>
<TOKEN end_char="2924" id="token-33-22" morph="none" pos="word" start_char="2924">a</TOKEN>
<TOKEN end_char="2929" id="token-33-23" morph="none" pos="word" start_char="2926">book</TOKEN>
<TOKEN end_char="2933" id="token-33-24" morph="none" pos="word" start_char="2931">and</TOKEN>
<TOKEN end_char="2936" id="token-33-25" morph="none" pos="word" start_char="2935">it</TOKEN>
<TOKEN end_char="2942" id="token-33-26" morph="none" pos="word" start_char="2938">looks</TOKEN>
<TOKEN end_char="2947" id="token-33-27" morph="none" pos="word" start_char="2944">like</TOKEN>
<TOKEN end_char="2955" id="token-33-28" morph="none" pos="word" start_char="2949">another</TOKEN>
<TOKEN end_char="2960" id="token-33-29" morph="none" pos="word" start_char="2957">word</TOKEN>
<TOKEN end_char="2961" id="token-33-30" morph="none" pos="punct" start_char="2961">,</TOKEN>
<TOKEN end_char="2965" id="token-33-31" morph="none" pos="word" start_char="2963">can</TOKEN>
<TOKEN end_char="2968" id="token-33-32" morph="none" pos="word" start_char="2967">we</TOKEN>
<TOKEN end_char="2972" id="token-33-33" morph="none" pos="word" start_char="2970">say</TOKEN>
<TOKEN end_char="2977" id="token-33-34" morph="none" pos="word" start_char="2974">that</TOKEN>
<TOKEN end_char="2981" id="token-33-35" morph="none" pos="word" start_char="2979">one</TOKEN>
<TOKEN end_char="2985" id="token-33-36" morph="none" pos="word" start_char="2983">has</TOKEN>
<TOKEN end_char="2992" id="token-33-37" morph="none" pos="word" start_char="2987">copied</TOKEN>
<TOKEN end_char="2997" id="token-33-38" morph="none" pos="word" start_char="2994">from</TOKEN>
<TOKEN end_char="3001" id="token-33-39" morph="none" pos="word" start_char="2999">the</TOKEN>
<TOKEN end_char="3007" id="token-33-40" morph="none" pos="word" start_char="3003">other</TOKEN>
<TOKEN end_char="3008" id="token-33-41" morph="none" pos="punct" start_char="3008">?</TOKEN>
</SEG>
<SEG end_char="3025" id="segment-34" start_char="3010">
<ORIGINAL_TEXT>This is absurd!"</ORIGINAL_TEXT>
<TOKEN end_char="3013" id="token-34-0" morph="none" pos="word" start_char="3010">This</TOKEN>
<TOKEN end_char="3016" id="token-34-1" morph="none" pos="word" start_char="3015">is</TOKEN>
<TOKEN end_char="3023" id="token-34-2" morph="none" pos="word" start_char="3018">absurd</TOKEN>
<TOKEN end_char="3025" id="token-34-3" morph="none" pos="punct" start_char="3024">!"</TOKEN>
</SEG>
<SEG end_char="3179" id="segment-35" start_char="3028">
<ORIGINAL_TEXT>It is surprising to have a scientist of Montagnier’s stature utter such questionable statements – although Montagnier himself is a controversial figure.</ORIGINAL_TEXT>
<TOKEN end_char="3029" id="token-35-0" morph="none" pos="word" start_char="3028">It</TOKEN>
<TOKEN end_char="3032" id="token-35-1" morph="none" pos="word" start_char="3031">is</TOKEN>
<TOKEN end_char="3043" id="token-35-2" morph="none" pos="word" start_char="3034">surprising</TOKEN>
<TOKEN end_char="3046" id="token-35-3" morph="none" pos="word" start_char="3045">to</TOKEN>
<TOKEN end_char="3051" id="token-35-4" morph="none" pos="word" start_char="3048">have</TOKEN>
<TOKEN end_char="3053" id="token-35-5" morph="none" pos="word" start_char="3053">a</TOKEN>
<TOKEN end_char="3063" id="token-35-6" morph="none" pos="word" start_char="3055">scientist</TOKEN>
<TOKEN end_char="3066" id="token-35-7" morph="none" pos="word" start_char="3065">of</TOKEN>
<TOKEN end_char="3079" id="token-35-8" morph="none" pos="word" start_char="3068">Montagnier’s</TOKEN>
<TOKEN end_char="3087" id="token-35-9" morph="none" pos="word" start_char="3081">stature</TOKEN>
<TOKEN end_char="3093" id="token-35-10" morph="none" pos="word" start_char="3089">utter</TOKEN>
<TOKEN end_char="3098" id="token-35-11" morph="none" pos="word" start_char="3095">such</TOKEN>
<TOKEN end_char="3111" id="token-35-12" morph="none" pos="word" start_char="3100">questionable</TOKEN>
<TOKEN end_char="3122" id="token-35-13" morph="none" pos="word" start_char="3113">statements</TOKEN>
<TOKEN end_char="3124" id="token-35-14" morph="none" pos="punct" start_char="3124">–</TOKEN>
<TOKEN end_char="3133" id="token-35-15" morph="none" pos="word" start_char="3126">although</TOKEN>
<TOKEN end_char="3144" id="token-35-16" morph="none" pos="word" start_char="3135">Montagnier</TOKEN>
<TOKEN end_char="3152" id="token-35-17" morph="none" pos="word" start_char="3146">himself</TOKEN>
<TOKEN end_char="3155" id="token-35-18" morph="none" pos="word" start_char="3154">is</TOKEN>
<TOKEN end_char="3157" id="token-35-19" morph="none" pos="word" start_char="3157">a</TOKEN>
<TOKEN end_char="3171" id="token-35-20" morph="none" pos="word" start_char="3159">controversial</TOKEN>
<TOKEN end_char="3178" id="token-35-21" morph="none" pos="word" start_char="3173">figure</TOKEN>
<TOKEN end_char="3179" id="token-35-22" morph="none" pos="punct" start_char="3179">.</TOKEN>
</SEG>
<SEG end_char="3299" id="segment-36" start_char="3181">
<ORIGINAL_TEXT>Among other causes, he has supported anti-vaxxers, homeopathy and a silly claim that DNA emits "electromagnetic waves".</ORIGINAL_TEXT>
<TOKEN end_char="3185" id="token-36-0" morph="none" pos="word" start_char="3181">Among</TOKEN>
<TOKEN end_char="3191" id="token-36-1" morph="none" pos="word" start_char="3187">other</TOKEN>
<TOKEN end_char="3198" id="token-36-2" morph="none" pos="word" start_char="3193">causes</TOKEN>
<TOKEN end_char="3199" id="token-36-3" morph="none" pos="punct" start_char="3199">,</TOKEN>
<TOKEN end_char="3202" id="token-36-4" morph="none" pos="word" start_char="3201">he</TOKEN>
<TOKEN end_char="3206" id="token-36-5" morph="none" pos="word" start_char="3204">has</TOKEN>
<TOKEN end_char="3216" id="token-36-6" morph="none" pos="word" start_char="3208">supported</TOKEN>
<TOKEN end_char="3229" id="token-36-7" morph="none" pos="unknown" start_char="3218">anti-vaxxers</TOKEN>
<TOKEN end_char="3230" id="token-36-8" morph="none" pos="punct" start_char="3230">,</TOKEN>
<TOKEN end_char="3241" id="token-36-9" morph="none" pos="word" start_char="3232">homeopathy</TOKEN>
<TOKEN end_char="3245" id="token-36-10" morph="none" pos="word" start_char="3243">and</TOKEN>
<TOKEN end_char="3247" id="token-36-11" morph="none" pos="word" start_char="3247">a</TOKEN>
<TOKEN end_char="3253" id="token-36-12" morph="none" pos="word" start_char="3249">silly</TOKEN>
<TOKEN end_char="3259" id="token-36-13" morph="none" pos="word" start_char="3255">claim</TOKEN>
<TOKEN end_char="3264" id="token-36-14" morph="none" pos="word" start_char="3261">that</TOKEN>
<TOKEN end_char="3268" id="token-36-15" morph="none" pos="word" start_char="3266">DNA</TOKEN>
<TOKEN end_char="3274" id="token-36-16" morph="none" pos="word" start_char="3270">emits</TOKEN>
<TOKEN end_char="3276" id="token-36-17" morph="none" pos="punct" start_char="3276">"</TOKEN>
<TOKEN end_char="3291" id="token-36-18" morph="none" pos="word" start_char="3277">electromagnetic</TOKEN>
<TOKEN end_char="3297" id="token-36-19" morph="none" pos="word" start_char="3293">waves</TOKEN>
<TOKEN end_char="3299" id="token-36-20" morph="none" pos="punct" start_char="3298">".</TOKEN>
</SEG>
<SEG end_char="3476" id="segment-37" start_char="3302">
<ORIGINAL_TEXT>As he lost credibility among his peers, scientific agencies around Europe began to reject his grant applications, and eventually he was left with no money to pursue his ideas.</ORIGINAL_TEXT>
<TOKEN end_char="3303" id="token-37-0" morph="none" pos="word" start_char="3302">As</TOKEN>
<TOKEN end_char="3306" id="token-37-1" morph="none" pos="word" start_char="3305">he</TOKEN>
<TOKEN end_char="3311" id="token-37-2" morph="none" pos="word" start_char="3308">lost</TOKEN>
<TOKEN end_char="3323" id="token-37-3" morph="none" pos="word" start_char="3313">credibility</TOKEN>
<TOKEN end_char="3329" id="token-37-4" morph="none" pos="word" start_char="3325">among</TOKEN>
<TOKEN end_char="3333" id="token-37-5" morph="none" pos="word" start_char="3331">his</TOKEN>
<TOKEN end_char="3339" id="token-37-6" morph="none" pos="word" start_char="3335">peers</TOKEN>
<TOKEN end_char="3340" id="token-37-7" morph="none" pos="punct" start_char="3340">,</TOKEN>
<TOKEN end_char="3351" id="token-37-8" morph="none" pos="word" start_char="3342">scientific</TOKEN>
<TOKEN end_char="3360" id="token-37-9" morph="none" pos="word" start_char="3353">agencies</TOKEN>
<TOKEN end_char="3367" id="token-37-10" morph="none" pos="word" start_char="3362">around</TOKEN>
<TOKEN end_char="3374" id="token-37-11" morph="none" pos="word" start_char="3369">Europe</TOKEN>
<TOKEN end_char="3380" id="token-37-12" morph="none" pos="word" start_char="3376">began</TOKEN>
<TOKEN end_char="3383" id="token-37-13" morph="none" pos="word" start_char="3382">to</TOKEN>
<TOKEN end_char="3390" id="token-37-14" morph="none" pos="word" start_char="3385">reject</TOKEN>
<TOKEN end_char="3394" id="token-37-15" morph="none" pos="word" start_char="3392">his</TOKEN>
<TOKEN end_char="3400" id="token-37-16" morph="none" pos="word" start_char="3396">grant</TOKEN>
<TOKEN end_char="3413" id="token-37-17" morph="none" pos="word" start_char="3402">applications</TOKEN>
<TOKEN end_char="3414" id="token-37-18" morph="none" pos="punct" start_char="3414">,</TOKEN>
<TOKEN end_char="3418" id="token-37-19" morph="none" pos="word" start_char="3416">and</TOKEN>
<TOKEN end_char="3429" id="token-37-20" morph="none" pos="word" start_char="3420">eventually</TOKEN>
<TOKEN end_char="3432" id="token-37-21" morph="none" pos="word" start_char="3431">he</TOKEN>
<TOKEN end_char="3436" id="token-37-22" morph="none" pos="word" start_char="3434">was</TOKEN>
<TOKEN end_char="3441" id="token-37-23" morph="none" pos="word" start_char="3438">left</TOKEN>
<TOKEN end_char="3446" id="token-37-24" morph="none" pos="word" start_char="3443">with</TOKEN>
<TOKEN end_char="3449" id="token-37-25" morph="none" pos="word" start_char="3448">no</TOKEN>
<TOKEN end_char="3455" id="token-37-26" morph="none" pos="word" start_char="3451">money</TOKEN>
<TOKEN end_char="3458" id="token-37-27" morph="none" pos="word" start_char="3457">to</TOKEN>
<TOKEN end_char="3465" id="token-37-28" morph="none" pos="word" start_char="3460">pursue</TOKEN>
<TOKEN end_char="3469" id="token-37-29" morph="none" pos="word" start_char="3467">his</TOKEN>
<TOKEN end_char="3475" id="token-37-30" morph="none" pos="word" start_char="3471">ideas</TOKEN>
<TOKEN end_char="3476" id="token-37-31" morph="none" pos="punct" start_char="3476">.</TOKEN>
</SEG>
<SEG end_char="3572" id="segment-38" start_char="3478">
<ORIGINAL_TEXT>In a 2010 interview, Montagnier said he was leaving Europe to "escape the intellectual terror."</ORIGINAL_TEXT>
<TOKEN end_char="3479" id="token-38-0" morph="none" pos="word" start_char="3478">In</TOKEN>
<TOKEN end_char="3481" id="token-38-1" morph="none" pos="word" start_char="3481">a</TOKEN>
<TOKEN end_char="3486" id="token-38-2" morph="none" pos="word" start_char="3483">2010</TOKEN>
<TOKEN end_char="3496" id="token-38-3" morph="none" pos="word" start_char="3488">interview</TOKEN>
<TOKEN end_char="3497" id="token-38-4" morph="none" pos="punct" start_char="3497">,</TOKEN>
<TOKEN end_char="3508" id="token-38-5" morph="none" pos="word" start_char="3499">Montagnier</TOKEN>
<TOKEN end_char="3513" id="token-38-6" morph="none" pos="word" start_char="3510">said</TOKEN>
<TOKEN end_char="3516" id="token-38-7" morph="none" pos="word" start_char="3515">he</TOKEN>
<TOKEN end_char="3520" id="token-38-8" morph="none" pos="word" start_char="3518">was</TOKEN>
<TOKEN end_char="3528" id="token-38-9" morph="none" pos="word" start_char="3522">leaving</TOKEN>
<TOKEN end_char="3535" id="token-38-10" morph="none" pos="word" start_char="3530">Europe</TOKEN>
<TOKEN end_char="3538" id="token-38-11" morph="none" pos="word" start_char="3537">to</TOKEN>
<TOKEN end_char="3540" id="token-38-12" morph="none" pos="punct" start_char="3540">"</TOKEN>
<TOKEN end_char="3546" id="token-38-13" morph="none" pos="word" start_char="3541">escape</TOKEN>
<TOKEN end_char="3550" id="token-38-14" morph="none" pos="word" start_char="3548">the</TOKEN>
<TOKEN end_char="3563" id="token-38-15" morph="none" pos="word" start_char="3552">intellectual</TOKEN>
<TOKEN end_char="3570" id="token-38-16" morph="none" pos="word" start_char="3565">terror</TOKEN>
<TOKEN end_char="3572" id="token-38-17" morph="none" pos="punct" start_char="3571">."</TOKEN>
</SEG>
<SEG end_char="3648" id="segment-39" start_char="3574">
<ORIGINAL_TEXT>He added, "I’m no longer allowed to work at a public institute (in France).</ORIGINAL_TEXT>
<TOKEN end_char="3575" id="token-39-0" morph="none" pos="word" start_char="3574">He</TOKEN>
<TOKEN end_char="3581" id="token-39-1" morph="none" pos="word" start_char="3577">added</TOKEN>
<TOKEN end_char="3582" id="token-39-2" morph="none" pos="punct" start_char="3582">,</TOKEN>
<TOKEN end_char="3584" id="token-39-3" morph="none" pos="punct" start_char="3584">"</TOKEN>
<TOKEN end_char="3587" id="token-39-4" morph="none" pos="word" start_char="3585">I’m</TOKEN>
<TOKEN end_char="3590" id="token-39-5" morph="none" pos="word" start_char="3589">no</TOKEN>
<TOKEN end_char="3597" id="token-39-6" morph="none" pos="word" start_char="3592">longer</TOKEN>
<TOKEN end_char="3605" id="token-39-7" morph="none" pos="word" start_char="3599">allowed</TOKEN>
<TOKEN end_char="3608" id="token-39-8" morph="none" pos="word" start_char="3607">to</TOKEN>
<TOKEN end_char="3613" id="token-39-9" morph="none" pos="word" start_char="3610">work</TOKEN>
<TOKEN end_char="3616" id="token-39-10" morph="none" pos="word" start_char="3615">at</TOKEN>
<TOKEN end_char="3618" id="token-39-11" morph="none" pos="word" start_char="3618">a</TOKEN>
<TOKEN end_char="3625" id="token-39-12" morph="none" pos="word" start_char="3620">public</TOKEN>
<TOKEN end_char="3635" id="token-39-13" morph="none" pos="word" start_char="3627">institute</TOKEN>
<TOKEN end_char="3637" id="token-39-14" morph="none" pos="punct" start_char="3637">(</TOKEN>
<TOKEN end_char="3639" id="token-39-15" morph="none" pos="word" start_char="3638">in</TOKEN>
<TOKEN end_char="3646" id="token-39-16" morph="none" pos="word" start_char="3641">France</TOKEN>
<TOKEN end_char="3648" id="token-39-17" morph="none" pos="punct" start_char="3647">).</TOKEN>
</SEG>
<SEG end_char="3725" id="segment-40" start_char="3650">
<ORIGINAL_TEXT>I have applied for funding from other sources, but I have been turned down."</ORIGINAL_TEXT>
<TOKEN end_char="3650" id="token-40-0" morph="none" pos="word" start_char="3650">I</TOKEN>
<TOKEN end_char="3655" id="token-40-1" morph="none" pos="word" start_char="3652">have</TOKEN>
<TOKEN end_char="3663" id="token-40-2" morph="none" pos="word" start_char="3657">applied</TOKEN>
<TOKEN end_char="3667" id="token-40-3" morph="none" pos="word" start_char="3665">for</TOKEN>
<TOKEN end_char="3675" id="token-40-4" morph="none" pos="word" start_char="3669">funding</TOKEN>
<TOKEN end_char="3680" id="token-40-5" morph="none" pos="word" start_char="3677">from</TOKEN>
<TOKEN end_char="3686" id="token-40-6" morph="none" pos="word" start_char="3682">other</TOKEN>
<TOKEN end_char="3694" id="token-40-7" morph="none" pos="word" start_char="3688">sources</TOKEN>
<TOKEN end_char="3695" id="token-40-8" morph="none" pos="punct" start_char="3695">,</TOKEN>
<TOKEN end_char="3699" id="token-40-9" morph="none" pos="word" start_char="3697">but</TOKEN>
<TOKEN end_char="3701" id="token-40-10" morph="none" pos="word" start_char="3701">I</TOKEN>
<TOKEN end_char="3706" id="token-40-11" morph="none" pos="word" start_char="3703">have</TOKEN>
<TOKEN end_char="3711" id="token-40-12" morph="none" pos="word" start_char="3708">been</TOKEN>
<TOKEN end_char="3718" id="token-40-13" morph="none" pos="word" start_char="3713">turned</TOKEN>
<TOKEN end_char="3723" id="token-40-14" morph="none" pos="word" start_char="3720">down</TOKEN>
<TOKEN end_char="3725" id="token-40-15" morph="none" pos="punct" start_char="3724">."</TOKEN>
</SEG>
<SEG end_char="3815" id="segment-41" start_char="3728">
<ORIGINAL_TEXT>Pandemics have historically been breeding grounds for fake news and conspiracy theories.</ORIGINAL_TEXT>
<TOKEN end_char="3736" id="token-41-0" morph="none" pos="word" start_char="3728">Pandemics</TOKEN>
<TOKEN end_char="3741" id="token-41-1" morph="none" pos="word" start_char="3738">have</TOKEN>
<TOKEN end_char="3754" id="token-41-2" morph="none" pos="word" start_char="3743">historically</TOKEN>
<TOKEN end_char="3759" id="token-41-3" morph="none" pos="word" start_char="3756">been</TOKEN>
<TOKEN end_char="3768" id="token-41-4" morph="none" pos="word" start_char="3761">breeding</TOKEN>
<TOKEN end_char="3776" id="token-41-5" morph="none" pos="word" start_char="3770">grounds</TOKEN>
<TOKEN end_char="3780" id="token-41-6" morph="none" pos="word" start_char="3778">for</TOKEN>
<TOKEN end_char="3785" id="token-41-7" morph="none" pos="word" start_char="3782">fake</TOKEN>
<TOKEN end_char="3790" id="token-41-8" morph="none" pos="word" start_char="3787">news</TOKEN>
<TOKEN end_char="3794" id="token-41-9" morph="none" pos="word" start_char="3792">and</TOKEN>
<TOKEN end_char="3805" id="token-41-10" morph="none" pos="word" start_char="3796">conspiracy</TOKEN>
<TOKEN end_char="3814" id="token-41-11" morph="none" pos="word" start_char="3807">theories</TOKEN>
<TOKEN end_char="3815" id="token-41-12" morph="none" pos="punct" start_char="3815">.</TOKEN>
</SEG>
<SEG end_char="4050" id="segment-42" start_char="3817">
<ORIGINAL_TEXT>For example, in the 14th century, the bubonic plague epidemic in Europe fuelled a misbelief among Christians that the Jews were deliberately poisoning wells and rivers with infectious "miasma", leading to the mass persecution of Jews.</ORIGINAL_TEXT>
<TOKEN end_char="3819" id="token-42-0" morph="none" pos="word" start_char="3817">For</TOKEN>
<TOKEN end_char="3827" id="token-42-1" morph="none" pos="word" start_char="3821">example</TOKEN>
<TOKEN end_char="3828" id="token-42-2" morph="none" pos="punct" start_char="3828">,</TOKEN>
<TOKEN end_char="3831" id="token-42-3" morph="none" pos="word" start_char="3830">in</TOKEN>
<TOKEN end_char="3835" id="token-42-4" morph="none" pos="word" start_char="3833">the</TOKEN>
<TOKEN end_char="3840" id="token-42-5" morph="none" pos="word" start_char="3837">14th</TOKEN>
<TOKEN end_char="3848" id="token-42-6" morph="none" pos="word" start_char="3842">century</TOKEN>
<TOKEN end_char="3849" id="token-42-7" morph="none" pos="punct" start_char="3849">,</TOKEN>
<TOKEN end_char="3853" id="token-42-8" morph="none" pos="word" start_char="3851">the</TOKEN>
<TOKEN end_char="3861" id="token-42-9" morph="none" pos="word" start_char="3855">bubonic</TOKEN>
<TOKEN end_char="3868" id="token-42-10" morph="none" pos="word" start_char="3863">plague</TOKEN>
<TOKEN end_char="3877" id="token-42-11" morph="none" pos="word" start_char="3870">epidemic</TOKEN>
<TOKEN end_char="3880" id="token-42-12" morph="none" pos="word" start_char="3879">in</TOKEN>
<TOKEN end_char="3887" id="token-42-13" morph="none" pos="word" start_char="3882">Europe</TOKEN>
<TOKEN end_char="3895" id="token-42-14" morph="none" pos="word" start_char="3889">fuelled</TOKEN>
<TOKEN end_char="3897" id="token-42-15" morph="none" pos="word" start_char="3897">a</TOKEN>
<TOKEN end_char="3907" id="token-42-16" morph="none" pos="word" start_char="3899">misbelief</TOKEN>
<TOKEN end_char="3913" id="token-42-17" morph="none" pos="word" start_char="3909">among</TOKEN>
<TOKEN end_char="3924" id="token-42-18" morph="none" pos="word" start_char="3915">Christians</TOKEN>
<TOKEN end_char="3929" id="token-42-19" morph="none" pos="word" start_char="3926">that</TOKEN>
<TOKEN end_char="3933" id="token-42-20" morph="none" pos="word" start_char="3931">the</TOKEN>
<TOKEN end_char="3938" id="token-42-21" morph="none" pos="word" start_char="3935">Jews</TOKEN>
<TOKEN end_char="3943" id="token-42-22" morph="none" pos="word" start_char="3940">were</TOKEN>
<TOKEN end_char="3956" id="token-42-23" morph="none" pos="word" start_char="3945">deliberately</TOKEN>
<TOKEN end_char="3966" id="token-42-24" morph="none" pos="word" start_char="3958">poisoning</TOKEN>
<TOKEN end_char="3972" id="token-42-25" morph="none" pos="word" start_char="3968">wells</TOKEN>
<TOKEN end_char="3976" id="token-42-26" morph="none" pos="word" start_char="3974">and</TOKEN>
<TOKEN end_char="3983" id="token-42-27" morph="none" pos="word" start_char="3978">rivers</TOKEN>
<TOKEN end_char="3988" id="token-42-28" morph="none" pos="word" start_char="3985">with</TOKEN>
<TOKEN end_char="3999" id="token-42-29" morph="none" pos="word" start_char="3990">infectious</TOKEN>
<TOKEN end_char="4001" id="token-42-30" morph="none" pos="punct" start_char="4001">"</TOKEN>
<TOKEN end_char="4007" id="token-42-31" morph="none" pos="word" start_char="4002">miasma</TOKEN>
<TOKEN end_char="4009" id="token-42-32" morph="none" pos="punct" start_char="4008">",</TOKEN>
<TOKEN end_char="4017" id="token-42-33" morph="none" pos="word" start_char="4011">leading</TOKEN>
<TOKEN end_char="4020" id="token-42-34" morph="none" pos="word" start_char="4019">to</TOKEN>
<TOKEN end_char="4024" id="token-42-35" morph="none" pos="word" start_char="4022">the</TOKEN>
<TOKEN end_char="4029" id="token-42-36" morph="none" pos="word" start_char="4026">mass</TOKEN>
<TOKEN end_char="4041" id="token-42-37" morph="none" pos="word" start_char="4031">persecution</TOKEN>
<TOKEN end_char="4044" id="token-42-38" morph="none" pos="word" start_char="4043">of</TOKEN>
<TOKEN end_char="4049" id="token-42-39" morph="none" pos="word" start_char="4046">Jews</TOKEN>
<TOKEN end_char="4050" id="token-42-40" morph="none" pos="punct" start_char="4050">.</TOKEN>
</SEG>
<SEG end_char="4291" id="segment-43" start_char="4052">
<ORIGINAL_TEXT>Even when Montagnier helped discover the HIV virus (alongside Françoise Barré-Sinoussi) in the early 1980s, a prominent conspiracy theory in the US was that HIV is a human-made virus that the government had created to wipe out black people.</ORIGINAL_TEXT>
<TOKEN end_char="4055" id="token-43-0" morph="none" pos="word" start_char="4052">Even</TOKEN>
<TOKEN end_char="4060" id="token-43-1" morph="none" pos="word" start_char="4057">when</TOKEN>
<TOKEN end_char="4071" id="token-43-2" morph="none" pos="word" start_char="4062">Montagnier</TOKEN>
<TOKEN end_char="4078" id="token-43-3" morph="none" pos="word" start_char="4073">helped</TOKEN>
<TOKEN end_char="4087" id="token-43-4" morph="none" pos="word" start_char="4080">discover</TOKEN>
<TOKEN end_char="4091" id="token-43-5" morph="none" pos="word" start_char="4089">the</TOKEN>
<TOKEN end_char="4095" id="token-43-6" morph="none" pos="word" start_char="4093">HIV</TOKEN>
<TOKEN end_char="4101" id="token-43-7" morph="none" pos="word" start_char="4097">virus</TOKEN>
<TOKEN end_char="4103" id="token-43-8" morph="none" pos="punct" start_char="4103">(</TOKEN>
<TOKEN end_char="4112" id="token-43-9" morph="none" pos="word" start_char="4104">alongside</TOKEN>
<TOKEN end_char="4122" id="token-43-10" morph="none" pos="word" start_char="4114">Françoise</TOKEN>
<TOKEN end_char="4137" id="token-43-11" morph="none" pos="unknown" start_char="4124">Barré-Sinoussi</TOKEN>
<TOKEN end_char="4138" id="token-43-12" morph="none" pos="punct" start_char="4138">)</TOKEN>
<TOKEN end_char="4141" id="token-43-13" morph="none" pos="word" start_char="4140">in</TOKEN>
<TOKEN end_char="4145" id="token-43-14" morph="none" pos="word" start_char="4143">the</TOKEN>
<TOKEN end_char="4151" id="token-43-15" morph="none" pos="word" start_char="4147">early</TOKEN>
<TOKEN end_char="4157" id="token-43-16" morph="none" pos="word" start_char="4153">1980s</TOKEN>
<TOKEN end_char="4158" id="token-43-17" morph="none" pos="punct" start_char="4158">,</TOKEN>
<TOKEN end_char="4160" id="token-43-18" morph="none" pos="word" start_char="4160">a</TOKEN>
<TOKEN end_char="4170" id="token-43-19" morph="none" pos="word" start_char="4162">prominent</TOKEN>
<TOKEN end_char="4181" id="token-43-20" morph="none" pos="word" start_char="4172">conspiracy</TOKEN>
<TOKEN end_char="4188" id="token-43-21" morph="none" pos="word" start_char="4183">theory</TOKEN>
<TOKEN end_char="4191" id="token-43-22" morph="none" pos="word" start_char="4190">in</TOKEN>
<TOKEN end_char="4195" id="token-43-23" morph="none" pos="word" start_char="4193">the</TOKEN>
<TOKEN end_char="4198" id="token-43-24" morph="none" pos="word" start_char="4197">US</TOKEN>
<TOKEN end_char="4202" id="token-43-25" morph="none" pos="word" start_char="4200">was</TOKEN>
<TOKEN end_char="4207" id="token-43-26" morph="none" pos="word" start_char="4204">that</TOKEN>
<TOKEN end_char="4211" id="token-43-27" morph="none" pos="word" start_char="4209">HIV</TOKEN>
<TOKEN end_char="4214" id="token-43-28" morph="none" pos="word" start_char="4213">is</TOKEN>
<TOKEN end_char="4216" id="token-43-29" morph="none" pos="word" start_char="4216">a</TOKEN>
<TOKEN end_char="4227" id="token-43-30" morph="none" pos="unknown" start_char="4218">human-made</TOKEN>
<TOKEN end_char="4233" id="token-43-31" morph="none" pos="word" start_char="4229">virus</TOKEN>
<TOKEN end_char="4238" id="token-43-32" morph="none" pos="word" start_char="4235">that</TOKEN>
<TOKEN end_char="4242" id="token-43-33" morph="none" pos="word" start_char="4240">the</TOKEN>
<TOKEN end_char="4253" id="token-43-34" morph="none" pos="word" start_char="4244">government</TOKEN>
<TOKEN end_char="4257" id="token-43-35" morph="none" pos="word" start_char="4255">had</TOKEN>
<TOKEN end_char="4265" id="token-43-36" morph="none" pos="word" start_char="4259">created</TOKEN>
<TOKEN end_char="4268" id="token-43-37" morph="none" pos="word" start_char="4267">to</TOKEN>
<TOKEN end_char="4273" id="token-43-38" morph="none" pos="word" start_char="4270">wipe</TOKEN>
<TOKEN end_char="4277" id="token-43-39" morph="none" pos="word" start_char="4275">out</TOKEN>
<TOKEN end_char="4283" id="token-43-40" morph="none" pos="word" start_char="4279">black</TOKEN>
<TOKEN end_char="4290" id="token-43-41" morph="none" pos="word" start_char="4285">people</TOKEN>
<TOKEN end_char="4291" id="token-43-42" morph="none" pos="punct" start_char="4291">.</TOKEN>
</SEG>
<SEG end_char="4557" id="segment-44" start_char="4294">
<ORIGINAL_TEXT>And because pandemics are so fraught with misinformation, they also make for an important time to communicate good science, double-check suspicious comments, refuse to accept claims without good reason, and not amplify pseudoscience without suitable qualification.</ORIGINAL_TEXT>
<TOKEN end_char="4296" id="token-44-0" morph="none" pos="word" start_char="4294">And</TOKEN>
<TOKEN end_char="4304" id="token-44-1" morph="none" pos="word" start_char="4298">because</TOKEN>
<TOKEN end_char="4314" id="token-44-2" morph="none" pos="word" start_char="4306">pandemics</TOKEN>
<TOKEN end_char="4318" id="token-44-3" morph="none" pos="word" start_char="4316">are</TOKEN>
<TOKEN end_char="4321" id="token-44-4" morph="none" pos="word" start_char="4320">so</TOKEN>
<TOKEN end_char="4329" id="token-44-5" morph="none" pos="word" start_char="4323">fraught</TOKEN>
<TOKEN end_char="4334" id="token-44-6" morph="none" pos="word" start_char="4331">with</TOKEN>
<TOKEN end_char="4349" id="token-44-7" morph="none" pos="word" start_char="4336">misinformation</TOKEN>
<TOKEN end_char="4350" id="token-44-8" morph="none" pos="punct" start_char="4350">,</TOKEN>
<TOKEN end_char="4355" id="token-44-9" morph="none" pos="word" start_char="4352">they</TOKEN>
<TOKEN end_char="4360" id="token-44-10" morph="none" pos="word" start_char="4357">also</TOKEN>
<TOKEN end_char="4365" id="token-44-11" morph="none" pos="word" start_char="4362">make</TOKEN>
<TOKEN end_char="4369" id="token-44-12" morph="none" pos="word" start_char="4367">for</TOKEN>
<TOKEN end_char="4372" id="token-44-13" morph="none" pos="word" start_char="4371">an</TOKEN>
<TOKEN end_char="4382" id="token-44-14" morph="none" pos="word" start_char="4374">important</TOKEN>
<TOKEN end_char="4387" id="token-44-15" morph="none" pos="word" start_char="4384">time</TOKEN>
<TOKEN end_char="4390" id="token-44-16" morph="none" pos="word" start_char="4389">to</TOKEN>
<TOKEN end_char="4402" id="token-44-17" morph="none" pos="word" start_char="4392">communicate</TOKEN>
<TOKEN end_char="4407" id="token-44-18" morph="none" pos="word" start_char="4404">good</TOKEN>
<TOKEN end_char="4415" id="token-44-19" morph="none" pos="word" start_char="4409">science</TOKEN>
<TOKEN end_char="4416" id="token-44-20" morph="none" pos="punct" start_char="4416">,</TOKEN>
<TOKEN end_char="4429" id="token-44-21" morph="none" pos="unknown" start_char="4418">double-check</TOKEN>
<TOKEN end_char="4440" id="token-44-22" morph="none" pos="word" start_char="4431">suspicious</TOKEN>
<TOKEN end_char="4449" id="token-44-23" morph="none" pos="word" start_char="4442">comments</TOKEN>
<TOKEN end_char="4450" id="token-44-24" morph="none" pos="punct" start_char="4450">,</TOKEN>
<TOKEN end_char="4457" id="token-44-25" morph="none" pos="word" start_char="4452">refuse</TOKEN>
<TOKEN end_char="4460" id="token-44-26" morph="none" pos="word" start_char="4459">to</TOKEN>
<TOKEN end_char="4467" id="token-44-27" morph="none" pos="word" start_char="4462">accept</TOKEN>
<TOKEN end_char="4474" id="token-44-28" morph="none" pos="word" start_char="4469">claims</TOKEN>
<TOKEN end_char="4482" id="token-44-29" morph="none" pos="word" start_char="4476">without</TOKEN>
<TOKEN end_char="4487" id="token-44-30" morph="none" pos="word" start_char="4484">good</TOKEN>
<TOKEN end_char="4494" id="token-44-31" morph="none" pos="word" start_char="4489">reason</TOKEN>
<TOKEN end_char="4495" id="token-44-32" morph="none" pos="punct" start_char="4495">,</TOKEN>
<TOKEN end_char="4499" id="token-44-33" morph="none" pos="word" start_char="4497">and</TOKEN>
<TOKEN end_char="4503" id="token-44-34" morph="none" pos="word" start_char="4501">not</TOKEN>
<TOKEN end_char="4511" id="token-44-35" morph="none" pos="word" start_char="4505">amplify</TOKEN>
<TOKEN end_char="4525" id="token-44-36" morph="none" pos="word" start_char="4513">pseudoscience</TOKEN>
<TOKEN end_char="4533" id="token-44-37" morph="none" pos="word" start_char="4527">without</TOKEN>
<TOKEN end_char="4542" id="token-44-38" morph="none" pos="word" start_char="4535">suitable</TOKEN>
<TOKEN end_char="4556" id="token-44-39" morph="none" pos="word" start_char="4544">qualification</TOKEN>
<TOKEN end_char="4557" id="token-44-40" morph="none" pos="punct" start_char="4557">.</TOKEN>
</SEG>
<SEG end_char="4653" id="segment-45" start_char="4560">
<ORIGINAL_TEXT>Felix Bast is a science writer and an associate professor at the Central University of Punjab.</ORIGINAL_TEXT>
<TOKEN end_char="4564" id="token-45-0" morph="none" pos="word" start_char="4560">Felix</TOKEN>
<TOKEN end_char="4569" id="token-45-1" morph="none" pos="word" start_char="4566">Bast</TOKEN>
<TOKEN end_char="4572" id="token-45-2" morph="none" pos="word" start_char="4571">is</TOKEN>
<TOKEN end_char="4574" id="token-45-3" morph="none" pos="word" start_char="4574">a</TOKEN>
<TOKEN end_char="4582" id="token-45-4" morph="none" pos="word" start_char="4576">science</TOKEN>
<TOKEN end_char="4589" id="token-45-5" morph="none" pos="word" start_char="4584">writer</TOKEN>
<TOKEN end_char="4593" id="token-45-6" morph="none" pos="word" start_char="4591">and</TOKEN>
<TOKEN end_char="4596" id="token-45-7" morph="none" pos="word" start_char="4595">an</TOKEN>
<TOKEN end_char="4606" id="token-45-8" morph="none" pos="word" start_char="4598">associate</TOKEN>
<TOKEN end_char="4616" id="token-45-9" morph="none" pos="word" start_char="4608">professor</TOKEN>
<TOKEN end_char="4619" id="token-45-10" morph="none" pos="word" start_char="4618">at</TOKEN>
<TOKEN end_char="4623" id="token-45-11" morph="none" pos="word" start_char="4621">the</TOKEN>
<TOKEN end_char="4631" id="token-45-12" morph="none" pos="word" start_char="4625">Central</TOKEN>
<TOKEN end_char="4642" id="token-45-13" morph="none" pos="word" start_char="4633">University</TOKEN>
<TOKEN end_char="4645" id="token-45-14" morph="none" pos="word" start_char="4644">of</TOKEN>
<TOKEN end_char="4652" id="token-45-15" morph="none" pos="word" start_char="4647">Punjab</TOKEN>
<TOKEN end_char="4653" id="token-45-16" morph="none" pos="punct" start_char="4653">.</TOKEN>
</SEG>
<SEG end_char="4748" id="segment-46" start_char="4655">
<ORIGINAL_TEXT>This article was originally published on Medium and has been republished here with permission.</ORIGINAL_TEXT>
<TOKEN end_char="4658" id="token-46-0" morph="none" pos="word" start_char="4655">This</TOKEN>
<TOKEN end_char="4666" id="token-46-1" morph="none" pos="word" start_char="4660">article</TOKEN>
<TOKEN end_char="4670" id="token-46-2" morph="none" pos="word" start_char="4668">was</TOKEN>
<TOKEN end_char="4681" id="token-46-3" morph="none" pos="word" start_char="4672">originally</TOKEN>
<TOKEN end_char="4691" id="token-46-4" morph="none" pos="word" start_char="4683">published</TOKEN>
<TOKEN end_char="4694" id="token-46-5" morph="none" pos="word" start_char="4693">on</TOKEN>
<TOKEN end_char="4701" id="token-46-6" morph="none" pos="word" start_char="4696">Medium</TOKEN>
<TOKEN end_char="4705" id="token-46-7" morph="none" pos="word" start_char="4703">and</TOKEN>
<TOKEN end_char="4709" id="token-46-8" morph="none" pos="word" start_char="4707">has</TOKEN>
<TOKEN end_char="4714" id="token-46-9" morph="none" pos="word" start_char="4711">been</TOKEN>
<TOKEN end_char="4726" id="token-46-10" morph="none" pos="word" start_char="4716">republished</TOKEN>
<TOKEN end_char="4731" id="token-46-11" morph="none" pos="word" start_char="4728">here</TOKEN>
<TOKEN end_char="4736" id="token-46-12" morph="none" pos="word" start_char="4733">with</TOKEN>
<TOKEN end_char="4747" id="token-46-13" morph="none" pos="word" start_char="4738">permission</TOKEN>
<TOKEN end_char="4748" id="token-46-14" morph="none" pos="punct" start_char="4748">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>