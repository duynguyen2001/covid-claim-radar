<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAAC" lang="spa" raw_text_char_length="4949" raw_text_md5="ee22824752432723d1860a8735f2c98e" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="47" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Was coronavirus really in Europe in March 2019?</ORIGINAL_TEXT>
<TOKEN end_char="3" id="token-0-0" morph="none" pos="word" start_char="1">Was</TOKEN>
<TOKEN end_char="15" id="token-0-1" morph="none" pos="word" start_char="5">coronavirus</TOKEN>
<TOKEN end_char="22" id="token-0-2" morph="none" pos="word" start_char="17">really</TOKEN>
<TOKEN end_char="25" id="token-0-3" morph="none" pos="word" start_char="24">in</TOKEN>
<TOKEN end_char="32" id="token-0-4" morph="none" pos="word" start_char="27">Europe</TOKEN>
<TOKEN end_char="35" id="token-0-5" morph="none" pos="word" start_char="34">in</TOKEN>
<TOKEN end_char="41" id="token-0-6" morph="none" pos="word" start_char="37">March</TOKEN>
<TOKEN end_char="46" id="token-0-7" morph="none" pos="word" start_char="43">2019</TOKEN>
<TOKEN end_char="47" id="token-0-8" morph="none" pos="punct" start_char="47">?</TOKEN>
</SEG>
<SEG end_char="98" id="segment-1" start_char="51">
<ORIGINAL_TEXT>Did coronavirus arrive in Spain over a year ago?</ORIGINAL_TEXT>
<TOKEN end_char="53" id="token-1-0" morph="none" pos="word" start_char="51">Did</TOKEN>
<TOKEN end_char="65" id="token-1-1" morph="none" pos="word" start_char="55">coronavirus</TOKEN>
<TOKEN end_char="72" id="token-1-2" morph="none" pos="word" start_char="67">arrive</TOKEN>
<TOKEN end_char="75" id="token-1-3" morph="none" pos="word" start_char="74">in</TOKEN>
<TOKEN end_char="81" id="token-1-4" morph="none" pos="word" start_char="77">Spain</TOKEN>
<TOKEN end_char="86" id="token-1-5" morph="none" pos="word" start_char="83">over</TOKEN>
<TOKEN end_char="88" id="token-1-6" morph="none" pos="word" start_char="88">a</TOKEN>
<TOKEN end_char="93" id="token-1-7" morph="none" pos="word" start_char="90">year</TOKEN>
<TOKEN end_char="97" id="token-1-8" morph="none" pos="word" start_char="95">ago</TOKEN>
<TOKEN end_char="98" id="token-1-9" morph="none" pos="punct" start_char="98">?</TOKEN>
</SEG>
<SEG end_char="197" id="segment-2" start_char="102">
<ORIGINAL_TEXT>The novel coronavirus – SARS-CoV-2 – may have been in Europe for longer than previously thought.</ORIGINAL_TEXT>
<TOKEN end_char="104" id="token-2-0" morph="none" pos="word" start_char="102">The</TOKEN>
<TOKEN end_char="110" id="token-2-1" morph="none" pos="word" start_char="106">novel</TOKEN>
<TOKEN end_char="122" id="token-2-2" morph="none" pos="word" start_char="112">coronavirus</TOKEN>
<TOKEN end_char="124" id="token-2-3" morph="none" pos="punct" start_char="124">–</TOKEN>
<TOKEN end_char="135" id="token-2-4" morph="none" pos="unknown" start_char="126">SARS-CoV-2</TOKEN>
<TOKEN end_char="137" id="token-2-5" morph="none" pos="punct" start_char="137">–</TOKEN>
<TOKEN end_char="141" id="token-2-6" morph="none" pos="word" start_char="139">may</TOKEN>
<TOKEN end_char="146" id="token-2-7" morph="none" pos="word" start_char="143">have</TOKEN>
<TOKEN end_char="151" id="token-2-8" morph="none" pos="word" start_char="148">been</TOKEN>
<TOKEN end_char="154" id="token-2-9" morph="none" pos="word" start_char="153">in</TOKEN>
<TOKEN end_char="161" id="token-2-10" morph="none" pos="word" start_char="156">Europe</TOKEN>
<TOKEN end_char="165" id="token-2-11" morph="none" pos="word" start_char="163">for</TOKEN>
<TOKEN end_char="172" id="token-2-12" morph="none" pos="word" start_char="167">longer</TOKEN>
<TOKEN end_char="177" id="token-2-13" morph="none" pos="word" start_char="174">than</TOKEN>
<TOKEN end_char="188" id="token-2-14" morph="none" pos="word" start_char="179">previously</TOKEN>
<TOKEN end_char="196" id="token-2-15" morph="none" pos="word" start_char="190">thought</TOKEN>
<TOKEN end_char="197" id="token-2-16" morph="none" pos="punct" start_char="197">.</TOKEN>
</SEG>
<SEG end_char="287" id="segment-3" start_char="199">
<ORIGINAL_TEXT>Recent studies have suggested that it was circulating in Italy as early as December 2019.</ORIGINAL_TEXT>
<TOKEN end_char="204" id="token-3-0" morph="none" pos="word" start_char="199">Recent</TOKEN>
<TOKEN end_char="212" id="token-3-1" morph="none" pos="word" start_char="206">studies</TOKEN>
<TOKEN end_char="217" id="token-3-2" morph="none" pos="word" start_char="214">have</TOKEN>
<TOKEN end_char="227" id="token-3-3" morph="none" pos="word" start_char="219">suggested</TOKEN>
<TOKEN end_char="232" id="token-3-4" morph="none" pos="word" start_char="229">that</TOKEN>
<TOKEN end_char="235" id="token-3-5" morph="none" pos="word" start_char="234">it</TOKEN>
<TOKEN end_char="239" id="token-3-6" morph="none" pos="word" start_char="237">was</TOKEN>
<TOKEN end_char="251" id="token-3-7" morph="none" pos="word" start_char="241">circulating</TOKEN>
<TOKEN end_char="254" id="token-3-8" morph="none" pos="word" start_char="253">in</TOKEN>
<TOKEN end_char="260" id="token-3-9" morph="none" pos="word" start_char="256">Italy</TOKEN>
<TOKEN end_char="263" id="token-3-10" morph="none" pos="word" start_char="262">as</TOKEN>
<TOKEN end_char="269" id="token-3-11" morph="none" pos="word" start_char="265">early</TOKEN>
<TOKEN end_char="272" id="token-3-12" morph="none" pos="word" start_char="271">as</TOKEN>
<TOKEN end_char="281" id="token-3-13" morph="none" pos="word" start_char="274">December</TOKEN>
<TOKEN end_char="286" id="token-3-14" morph="none" pos="word" start_char="283">2019</TOKEN>
<TOKEN end_char="287" id="token-3-15" morph="none" pos="punct" start_char="287">.</TOKEN>
</SEG>
<SEG end_char="439" id="segment-4" start_char="289">
<ORIGINAL_TEXT>More surprisingly, researchers at the University of Barcelona found traces of the virus when testing untreated wastewater samples dated March 12, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="292" id="token-4-0" morph="none" pos="word" start_char="289">More</TOKEN>
<TOKEN end_char="305" id="token-4-1" morph="none" pos="word" start_char="294">surprisingly</TOKEN>
<TOKEN end_char="306" id="token-4-2" morph="none" pos="punct" start_char="306">,</TOKEN>
<TOKEN end_char="318" id="token-4-3" morph="none" pos="word" start_char="308">researchers</TOKEN>
<TOKEN end_char="321" id="token-4-4" morph="none" pos="word" start_char="320">at</TOKEN>
<TOKEN end_char="325" id="token-4-5" morph="none" pos="word" start_char="323">the</TOKEN>
<TOKEN end_char="336" id="token-4-6" morph="none" pos="word" start_char="327">University</TOKEN>
<TOKEN end_char="339" id="token-4-7" morph="none" pos="word" start_char="338">of</TOKEN>
<TOKEN end_char="349" id="token-4-8" morph="none" pos="word" start_char="341">Barcelona</TOKEN>
<TOKEN end_char="355" id="token-4-9" morph="none" pos="word" start_char="351">found</TOKEN>
<TOKEN end_char="362" id="token-4-10" morph="none" pos="word" start_char="357">traces</TOKEN>
<TOKEN end_char="365" id="token-4-11" morph="none" pos="word" start_char="364">of</TOKEN>
<TOKEN end_char="369" id="token-4-12" morph="none" pos="word" start_char="367">the</TOKEN>
<TOKEN end_char="375" id="token-4-13" morph="none" pos="word" start_char="371">virus</TOKEN>
<TOKEN end_char="380" id="token-4-14" morph="none" pos="word" start_char="377">when</TOKEN>
<TOKEN end_char="388" id="token-4-15" morph="none" pos="word" start_char="382">testing</TOKEN>
<TOKEN end_char="398" id="token-4-16" morph="none" pos="word" start_char="390">untreated</TOKEN>
<TOKEN end_char="409" id="token-4-17" morph="none" pos="word" start_char="400">wastewater</TOKEN>
<TOKEN end_char="417" id="token-4-18" morph="none" pos="word" start_char="411">samples</TOKEN>
<TOKEN end_char="423" id="token-4-19" morph="none" pos="word" start_char="419">dated</TOKEN>
<TOKEN end_char="429" id="token-4-20" morph="none" pos="word" start_char="425">March</TOKEN>
<TOKEN end_char="432" id="token-4-21" morph="none" pos="word" start_char="431">12</TOKEN>
<TOKEN end_char="433" id="token-4-22" morph="none" pos="punct" start_char="433">,</TOKEN>
<TOKEN end_char="438" id="token-4-23" morph="none" pos="word" start_char="435">2019</TOKEN>
<TOKEN end_char="439" id="token-4-24" morph="none" pos="punct" start_char="439">.</TOKEN>
</SEG>
<SEG end_char="504" id="segment-5" start_char="442">
<ORIGINAL_TEXT>The study was recently published on a preprint server, medRxiv.</ORIGINAL_TEXT>
<TOKEN end_char="444" id="token-5-0" morph="none" pos="word" start_char="442">The</TOKEN>
<TOKEN end_char="450" id="token-5-1" morph="none" pos="word" start_char="446">study</TOKEN>
<TOKEN end_char="454" id="token-5-2" morph="none" pos="word" start_char="452">was</TOKEN>
<TOKEN end_char="463" id="token-5-3" morph="none" pos="word" start_char="456">recently</TOKEN>
<TOKEN end_char="473" id="token-5-4" morph="none" pos="word" start_char="465">published</TOKEN>
<TOKEN end_char="476" id="token-5-5" morph="none" pos="word" start_char="475">on</TOKEN>
<TOKEN end_char="478" id="token-5-6" morph="none" pos="word" start_char="478">a</TOKEN>
<TOKEN end_char="487" id="token-5-7" morph="none" pos="word" start_char="480">preprint</TOKEN>
<TOKEN end_char="494" id="token-5-8" morph="none" pos="word" start_char="489">server</TOKEN>
<TOKEN end_char="495" id="token-5-9" morph="none" pos="punct" start_char="495">,</TOKEN>
<TOKEN end_char="503" id="token-5-10" morph="none" pos="word" start_char="497">medRxiv</TOKEN>
<TOKEN end_char="504" id="token-5-11" morph="none" pos="punct" start_char="504">.</TOKEN>
</SEG>
<SEG end_char="635" id="segment-6" start_char="506">
<ORIGINAL_TEXT>The paper is currently being subject to critical review by outside experts in preparation for publication in a scientific journal.</ORIGINAL_TEXT>
<TOKEN end_char="508" id="token-6-0" morph="none" pos="word" start_char="506">The</TOKEN>
<TOKEN end_char="514" id="token-6-1" morph="none" pos="word" start_char="510">paper</TOKEN>
<TOKEN end_char="517" id="token-6-2" morph="none" pos="word" start_char="516">is</TOKEN>
<TOKEN end_char="527" id="token-6-3" morph="none" pos="word" start_char="519">currently</TOKEN>
<TOKEN end_char="533" id="token-6-4" morph="none" pos="word" start_char="529">being</TOKEN>
<TOKEN end_char="541" id="token-6-5" morph="none" pos="word" start_char="535">subject</TOKEN>
<TOKEN end_char="544" id="token-6-6" morph="none" pos="word" start_char="543">to</TOKEN>
<TOKEN end_char="553" id="token-6-7" morph="none" pos="word" start_char="546">critical</TOKEN>
<TOKEN end_char="560" id="token-6-8" morph="none" pos="word" start_char="555">review</TOKEN>
<TOKEN end_char="563" id="token-6-9" morph="none" pos="word" start_char="562">by</TOKEN>
<TOKEN end_char="571" id="token-6-10" morph="none" pos="word" start_char="565">outside</TOKEN>
<TOKEN end_char="579" id="token-6-11" morph="none" pos="word" start_char="573">experts</TOKEN>
<TOKEN end_char="582" id="token-6-12" morph="none" pos="word" start_char="581">in</TOKEN>
<TOKEN end_char="594" id="token-6-13" morph="none" pos="word" start_char="584">preparation</TOKEN>
<TOKEN end_char="598" id="token-6-14" morph="none" pos="word" start_char="596">for</TOKEN>
<TOKEN end_char="610" id="token-6-15" morph="none" pos="word" start_char="600">publication</TOKEN>
<TOKEN end_char="613" id="token-6-16" morph="none" pos="word" start_char="612">in</TOKEN>
<TOKEN end_char="615" id="token-6-17" morph="none" pos="word" start_char="615">a</TOKEN>
<TOKEN end_char="626" id="token-6-18" morph="none" pos="word" start_char="617">scientific</TOKEN>
<TOKEN end_char="634" id="token-6-19" morph="none" pos="word" start_char="628">journal</TOKEN>
<TOKEN end_char="635" id="token-6-20" morph="none" pos="punct" start_char="635">.</TOKEN>
</SEG>
<SEG end_char="744" id="segment-7" start_char="637">
<ORIGINAL_TEXT>Until this process of peer review has been completed, though, the evidence needs to be treated with caution.</ORIGINAL_TEXT>
<TOKEN end_char="641" id="token-7-0" morph="none" pos="word" start_char="637">Until</TOKEN>
<TOKEN end_char="646" id="token-7-1" morph="none" pos="word" start_char="643">this</TOKEN>
<TOKEN end_char="654" id="token-7-2" morph="none" pos="word" start_char="648">process</TOKEN>
<TOKEN end_char="657" id="token-7-3" morph="none" pos="word" start_char="656">of</TOKEN>
<TOKEN end_char="662" id="token-7-4" morph="none" pos="word" start_char="659">peer</TOKEN>
<TOKEN end_char="669" id="token-7-5" morph="none" pos="word" start_char="664">review</TOKEN>
<TOKEN end_char="673" id="token-7-6" morph="none" pos="word" start_char="671">has</TOKEN>
<TOKEN end_char="678" id="token-7-7" morph="none" pos="word" start_char="675">been</TOKEN>
<TOKEN end_char="688" id="token-7-8" morph="none" pos="word" start_char="680">completed</TOKEN>
<TOKEN end_char="689" id="token-7-9" morph="none" pos="punct" start_char="689">,</TOKEN>
<TOKEN end_char="696" id="token-7-10" morph="none" pos="word" start_char="691">though</TOKEN>
<TOKEN end_char="697" id="token-7-11" morph="none" pos="punct" start_char="697">,</TOKEN>
<TOKEN end_char="701" id="token-7-12" morph="none" pos="word" start_char="699">the</TOKEN>
<TOKEN end_char="710" id="token-7-13" morph="none" pos="word" start_char="703">evidence</TOKEN>
<TOKEN end_char="716" id="token-7-14" morph="none" pos="word" start_char="712">needs</TOKEN>
<TOKEN end_char="719" id="token-7-15" morph="none" pos="word" start_char="718">to</TOKEN>
<TOKEN end_char="722" id="token-7-16" morph="none" pos="word" start_char="721">be</TOKEN>
<TOKEN end_char="730" id="token-7-17" morph="none" pos="word" start_char="724">treated</TOKEN>
<TOKEN end_char="735" id="token-7-18" morph="none" pos="word" start_char="732">with</TOKEN>
<TOKEN end_char="743" id="token-7-19" morph="none" pos="word" start_char="737">caution</TOKEN>
<TOKEN end_char="744" id="token-7-20" morph="none" pos="punct" start_char="744">.</TOKEN>
</SEG>
<SEG end_char="824" id="segment-8" start_char="747">
<ORIGINAL_TEXT>So, how was the experiment conducted and what exactly did the scientists find?</ORIGINAL_TEXT>
<TOKEN end_char="748" id="token-8-0" morph="none" pos="word" start_char="747">So</TOKEN>
<TOKEN end_char="749" id="token-8-1" morph="none" pos="punct" start_char="749">,</TOKEN>
<TOKEN end_char="753" id="token-8-2" morph="none" pos="word" start_char="751">how</TOKEN>
<TOKEN end_char="757" id="token-8-3" morph="none" pos="word" start_char="755">was</TOKEN>
<TOKEN end_char="761" id="token-8-4" morph="none" pos="word" start_char="759">the</TOKEN>
<TOKEN end_char="772" id="token-8-5" morph="none" pos="word" start_char="763">experiment</TOKEN>
<TOKEN end_char="782" id="token-8-6" morph="none" pos="word" start_char="774">conducted</TOKEN>
<TOKEN end_char="786" id="token-8-7" morph="none" pos="word" start_char="784">and</TOKEN>
<TOKEN end_char="791" id="token-8-8" morph="none" pos="word" start_char="788">what</TOKEN>
<TOKEN end_char="799" id="token-8-9" morph="none" pos="word" start_char="793">exactly</TOKEN>
<TOKEN end_char="803" id="token-8-10" morph="none" pos="word" start_char="801">did</TOKEN>
<TOKEN end_char="807" id="token-8-11" morph="none" pos="word" start_char="805">the</TOKEN>
<TOKEN end_char="818" id="token-8-12" morph="none" pos="word" start_char="809">scientists</TOKEN>
<TOKEN end_char="823" id="token-8-13" morph="none" pos="word" start_char="820">find</TOKEN>
<TOKEN end_char="824" id="token-8-14" morph="none" pos="punct" start_char="824">?</TOKEN>
</SEG>
<SEG end_char="922" id="segment-9" start_char="827">
<ORIGINAL_TEXT>One of the early findings about SARS-CoV-2 is that it is found in the faeces of infected people.</ORIGINAL_TEXT>
<TOKEN end_char="829" id="token-9-0" morph="none" pos="word" start_char="827">One</TOKEN>
<TOKEN end_char="832" id="token-9-1" morph="none" pos="word" start_char="831">of</TOKEN>
<TOKEN end_char="836" id="token-9-2" morph="none" pos="word" start_char="834">the</TOKEN>
<TOKEN end_char="842" id="token-9-3" morph="none" pos="word" start_char="838">early</TOKEN>
<TOKEN end_char="851" id="token-9-4" morph="none" pos="word" start_char="844">findings</TOKEN>
<TOKEN end_char="857" id="token-9-5" morph="none" pos="word" start_char="853">about</TOKEN>
<TOKEN end_char="868" id="token-9-6" morph="none" pos="unknown" start_char="859">SARS-CoV-2</TOKEN>
<TOKEN end_char="871" id="token-9-7" morph="none" pos="word" start_char="870">is</TOKEN>
<TOKEN end_char="876" id="token-9-8" morph="none" pos="word" start_char="873">that</TOKEN>
<TOKEN end_char="879" id="token-9-9" morph="none" pos="word" start_char="878">it</TOKEN>
<TOKEN end_char="882" id="token-9-10" morph="none" pos="word" start_char="881">is</TOKEN>
<TOKEN end_char="888" id="token-9-11" morph="none" pos="word" start_char="884">found</TOKEN>
<TOKEN end_char="891" id="token-9-12" morph="none" pos="word" start_char="890">in</TOKEN>
<TOKEN end_char="895" id="token-9-13" morph="none" pos="word" start_char="893">the</TOKEN>
<TOKEN end_char="902" id="token-9-14" morph="none" pos="word" start_char="897">faeces</TOKEN>
<TOKEN end_char="905" id="token-9-15" morph="none" pos="word" start_char="904">of</TOKEN>
<TOKEN end_char="914" id="token-9-16" morph="none" pos="word" start_char="907">infected</TOKEN>
<TOKEN end_char="921" id="token-9-17" morph="none" pos="word" start_char="916">people</TOKEN>
<TOKEN end_char="922" id="token-9-18" morph="none" pos="punct" start_char="922">.</TOKEN>
</SEG>
<SEG end_char="1141" id="segment-10" start_char="924">
<ORIGINAL_TEXT>As the virus makes its way through the gut – where it can cause gastrointestinal symptoms – it loses its outer protein layer, but bits of genetic material called RNA survive the journey intact and are "shed" in faeces.</ORIGINAL_TEXT>
<TOKEN end_char="925" id="token-10-0" morph="none" pos="word" start_char="924">As</TOKEN>
<TOKEN end_char="929" id="token-10-1" morph="none" pos="word" start_char="927">the</TOKEN>
<TOKEN end_char="935" id="token-10-2" morph="none" pos="word" start_char="931">virus</TOKEN>
<TOKEN end_char="941" id="token-10-3" morph="none" pos="word" start_char="937">makes</TOKEN>
<TOKEN end_char="945" id="token-10-4" morph="none" pos="word" start_char="943">its</TOKEN>
<TOKEN end_char="949" id="token-10-5" morph="none" pos="word" start_char="947">way</TOKEN>
<TOKEN end_char="957" id="token-10-6" morph="none" pos="word" start_char="951">through</TOKEN>
<TOKEN end_char="961" id="token-10-7" morph="none" pos="word" start_char="959">the</TOKEN>
<TOKEN end_char="965" id="token-10-8" morph="none" pos="word" start_char="963">gut</TOKEN>
<TOKEN end_char="967" id="token-10-9" morph="none" pos="punct" start_char="967">–</TOKEN>
<TOKEN end_char="973" id="token-10-10" morph="none" pos="word" start_char="969">where</TOKEN>
<TOKEN end_char="976" id="token-10-11" morph="none" pos="word" start_char="975">it</TOKEN>
<TOKEN end_char="980" id="token-10-12" morph="none" pos="word" start_char="978">can</TOKEN>
<TOKEN end_char="986" id="token-10-13" morph="none" pos="word" start_char="982">cause</TOKEN>
<TOKEN end_char="1003" id="token-10-14" morph="none" pos="word" start_char="988">gastrointestinal</TOKEN>
<TOKEN end_char="1012" id="token-10-15" morph="none" pos="word" start_char="1005">symptoms</TOKEN>
<TOKEN end_char="1014" id="token-10-16" morph="none" pos="punct" start_char="1014">–</TOKEN>
<TOKEN end_char="1017" id="token-10-17" morph="none" pos="word" start_char="1016">it</TOKEN>
<TOKEN end_char="1023" id="token-10-18" morph="none" pos="word" start_char="1019">loses</TOKEN>
<TOKEN end_char="1027" id="token-10-19" morph="none" pos="word" start_char="1025">its</TOKEN>
<TOKEN end_char="1033" id="token-10-20" morph="none" pos="word" start_char="1029">outer</TOKEN>
<TOKEN end_char="1041" id="token-10-21" morph="none" pos="word" start_char="1035">protein</TOKEN>
<TOKEN end_char="1047" id="token-10-22" morph="none" pos="word" start_char="1043">layer</TOKEN>
<TOKEN end_char="1048" id="token-10-23" morph="none" pos="punct" start_char="1048">,</TOKEN>
<TOKEN end_char="1052" id="token-10-24" morph="none" pos="word" start_char="1050">but</TOKEN>
<TOKEN end_char="1057" id="token-10-25" morph="none" pos="word" start_char="1054">bits</TOKEN>
<TOKEN end_char="1060" id="token-10-26" morph="none" pos="word" start_char="1059">of</TOKEN>
<TOKEN end_char="1068" id="token-10-27" morph="none" pos="word" start_char="1062">genetic</TOKEN>
<TOKEN end_char="1077" id="token-10-28" morph="none" pos="word" start_char="1070">material</TOKEN>
<TOKEN end_char="1084" id="token-10-29" morph="none" pos="word" start_char="1079">called</TOKEN>
<TOKEN end_char="1088" id="token-10-30" morph="none" pos="word" start_char="1086">RNA</TOKEN>
<TOKEN end_char="1096" id="token-10-31" morph="none" pos="word" start_char="1090">survive</TOKEN>
<TOKEN end_char="1100" id="token-10-32" morph="none" pos="word" start_char="1098">the</TOKEN>
<TOKEN end_char="1108" id="token-10-33" morph="none" pos="word" start_char="1102">journey</TOKEN>
<TOKEN end_char="1115" id="token-10-34" morph="none" pos="word" start_char="1110">intact</TOKEN>
<TOKEN end_char="1119" id="token-10-35" morph="none" pos="word" start_char="1117">and</TOKEN>
<TOKEN end_char="1123" id="token-10-36" morph="none" pos="word" start_char="1121">are</TOKEN>
<TOKEN end_char="1125" id="token-10-37" morph="none" pos="punct" start_char="1125">"</TOKEN>
<TOKEN end_char="1129" id="token-10-38" morph="none" pos="word" start_char="1126">shed</TOKEN>
<TOKEN end_char="1130" id="token-10-39" morph="none" pos="punct" start_char="1130">"</TOKEN>
<TOKEN end_char="1133" id="token-10-40" morph="none" pos="word" start_char="1132">in</TOKEN>
<TOKEN end_char="1140" id="token-10-41" morph="none" pos="word" start_char="1135">faeces</TOKEN>
<TOKEN end_char="1141" id="token-10-42" morph="none" pos="punct" start_char="1141">.</TOKEN>
</SEG>
<SEG end_char="1222" id="segment-11" start_char="1143">
<ORIGINAL_TEXT>At this point, it is no longer infectious – as far as current evidence tells us.</ORIGINAL_TEXT>
<TOKEN end_char="1144" id="token-11-0" morph="none" pos="word" start_char="1143">At</TOKEN>
<TOKEN end_char="1149" id="token-11-1" morph="none" pos="word" start_char="1146">this</TOKEN>
<TOKEN end_char="1155" id="token-11-2" morph="none" pos="word" start_char="1151">point</TOKEN>
<TOKEN end_char="1156" id="token-11-3" morph="none" pos="punct" start_char="1156">,</TOKEN>
<TOKEN end_char="1159" id="token-11-4" morph="none" pos="word" start_char="1158">it</TOKEN>
<TOKEN end_char="1162" id="token-11-5" morph="none" pos="word" start_char="1161">is</TOKEN>
<TOKEN end_char="1165" id="token-11-6" morph="none" pos="word" start_char="1164">no</TOKEN>
<TOKEN end_char="1172" id="token-11-7" morph="none" pos="word" start_char="1167">longer</TOKEN>
<TOKEN end_char="1183" id="token-11-8" morph="none" pos="word" start_char="1174">infectious</TOKEN>
<TOKEN end_char="1185" id="token-11-9" morph="none" pos="punct" start_char="1185">–</TOKEN>
<TOKEN end_char="1188" id="token-11-10" morph="none" pos="word" start_char="1187">as</TOKEN>
<TOKEN end_char="1192" id="token-11-11" morph="none" pos="word" start_char="1190">far</TOKEN>
<TOKEN end_char="1195" id="token-11-12" morph="none" pos="word" start_char="1194">as</TOKEN>
<TOKEN end_char="1203" id="token-11-13" morph="none" pos="word" start_char="1197">current</TOKEN>
<TOKEN end_char="1212" id="token-11-14" morph="none" pos="word" start_char="1205">evidence</TOKEN>
<TOKEN end_char="1218" id="token-11-15" morph="none" pos="word" start_char="1214">tells</TOKEN>
<TOKEN end_char="1221" id="token-11-16" morph="none" pos="word" start_char="1220">us</TOKEN>
<TOKEN end_char="1222" id="token-11-17" morph="none" pos="punct" start_char="1222">.</TOKEN>
</SEG>
<SEG end_char="1364" id="segment-12" start_char="1225">
<ORIGINAL_TEXT>But the fact that these bits of coronavirus RNA can be found in untreated wastewater (known as "influent") is useful for tracking outbreaks.</ORIGINAL_TEXT>
<TOKEN end_char="1227" id="token-12-0" morph="none" pos="word" start_char="1225">But</TOKEN>
<TOKEN end_char="1231" id="token-12-1" morph="none" pos="word" start_char="1229">the</TOKEN>
<TOKEN end_char="1236" id="token-12-2" morph="none" pos="word" start_char="1233">fact</TOKEN>
<TOKEN end_char="1241" id="token-12-3" morph="none" pos="word" start_char="1238">that</TOKEN>
<TOKEN end_char="1247" id="token-12-4" morph="none" pos="word" start_char="1243">these</TOKEN>
<TOKEN end_char="1252" id="token-12-5" morph="none" pos="word" start_char="1249">bits</TOKEN>
<TOKEN end_char="1255" id="token-12-6" morph="none" pos="word" start_char="1254">of</TOKEN>
<TOKEN end_char="1267" id="token-12-7" morph="none" pos="word" start_char="1257">coronavirus</TOKEN>
<TOKEN end_char="1271" id="token-12-8" morph="none" pos="word" start_char="1269">RNA</TOKEN>
<TOKEN end_char="1275" id="token-12-9" morph="none" pos="word" start_char="1273">can</TOKEN>
<TOKEN end_char="1278" id="token-12-10" morph="none" pos="word" start_char="1277">be</TOKEN>
<TOKEN end_char="1284" id="token-12-11" morph="none" pos="word" start_char="1280">found</TOKEN>
<TOKEN end_char="1287" id="token-12-12" morph="none" pos="word" start_char="1286">in</TOKEN>
<TOKEN end_char="1297" id="token-12-13" morph="none" pos="word" start_char="1289">untreated</TOKEN>
<TOKEN end_char="1308" id="token-12-14" morph="none" pos="word" start_char="1299">wastewater</TOKEN>
<TOKEN end_char="1310" id="token-12-15" morph="none" pos="punct" start_char="1310">(</TOKEN>
<TOKEN end_char="1315" id="token-12-16" morph="none" pos="word" start_char="1311">known</TOKEN>
<TOKEN end_char="1318" id="token-12-17" morph="none" pos="word" start_char="1317">as</TOKEN>
<TOKEN end_char="1320" id="token-12-18" morph="none" pos="punct" start_char="1320">"</TOKEN>
<TOKEN end_char="1328" id="token-12-19" morph="none" pos="word" start_char="1321">influent</TOKEN>
<TOKEN end_char="1330" id="token-12-20" morph="none" pos="punct" start_char="1329">")</TOKEN>
<TOKEN end_char="1333" id="token-12-21" morph="none" pos="word" start_char="1332">is</TOKEN>
<TOKEN end_char="1340" id="token-12-22" morph="none" pos="word" start_char="1335">useful</TOKEN>
<TOKEN end_char="1344" id="token-12-23" morph="none" pos="word" start_char="1342">for</TOKEN>
<TOKEN end_char="1353" id="token-12-24" morph="none" pos="word" start_char="1346">tracking</TOKEN>
<TOKEN end_char="1363" id="token-12-25" morph="none" pos="word" start_char="1355">outbreaks</TOKEN>
<TOKEN end_char="1364" id="token-12-26" morph="none" pos="punct" start_char="1364">.</TOKEN>
</SEG>
<SEG end_char="1565" id="segment-13" start_char="1366">
<ORIGINAL_TEXT>Indeed, they can predict where an outbreak is likely to occur a week to ten days before they show up in official figures – the reason being that people shed coronavirus before symptoms become evident.</ORIGINAL_TEXT>
<TOKEN end_char="1371" id="token-13-0" morph="none" pos="word" start_char="1366">Indeed</TOKEN>
<TOKEN end_char="1372" id="token-13-1" morph="none" pos="punct" start_char="1372">,</TOKEN>
<TOKEN end_char="1377" id="token-13-2" morph="none" pos="word" start_char="1374">they</TOKEN>
<TOKEN end_char="1381" id="token-13-3" morph="none" pos="word" start_char="1379">can</TOKEN>
<TOKEN end_char="1389" id="token-13-4" morph="none" pos="word" start_char="1383">predict</TOKEN>
<TOKEN end_char="1395" id="token-13-5" morph="none" pos="word" start_char="1391">where</TOKEN>
<TOKEN end_char="1398" id="token-13-6" morph="none" pos="word" start_char="1397">an</TOKEN>
<TOKEN end_char="1407" id="token-13-7" morph="none" pos="word" start_char="1400">outbreak</TOKEN>
<TOKEN end_char="1410" id="token-13-8" morph="none" pos="word" start_char="1409">is</TOKEN>
<TOKEN end_char="1417" id="token-13-9" morph="none" pos="word" start_char="1412">likely</TOKEN>
<TOKEN end_char="1420" id="token-13-10" morph="none" pos="word" start_char="1419">to</TOKEN>
<TOKEN end_char="1426" id="token-13-11" morph="none" pos="word" start_char="1422">occur</TOKEN>
<TOKEN end_char="1428" id="token-13-12" morph="none" pos="word" start_char="1428">a</TOKEN>
<TOKEN end_char="1433" id="token-13-13" morph="none" pos="word" start_char="1430">week</TOKEN>
<TOKEN end_char="1436" id="token-13-14" morph="none" pos="word" start_char="1435">to</TOKEN>
<TOKEN end_char="1440" id="token-13-15" morph="none" pos="word" start_char="1438">ten</TOKEN>
<TOKEN end_char="1445" id="token-13-16" morph="none" pos="word" start_char="1442">days</TOKEN>
<TOKEN end_char="1452" id="token-13-17" morph="none" pos="word" start_char="1447">before</TOKEN>
<TOKEN end_char="1457" id="token-13-18" morph="none" pos="word" start_char="1454">they</TOKEN>
<TOKEN end_char="1462" id="token-13-19" morph="none" pos="word" start_char="1459">show</TOKEN>
<TOKEN end_char="1465" id="token-13-20" morph="none" pos="word" start_char="1464">up</TOKEN>
<TOKEN end_char="1468" id="token-13-21" morph="none" pos="word" start_char="1467">in</TOKEN>
<TOKEN end_char="1477" id="token-13-22" morph="none" pos="word" start_char="1470">official</TOKEN>
<TOKEN end_char="1485" id="token-13-23" morph="none" pos="word" start_char="1479">figures</TOKEN>
<TOKEN end_char="1487" id="token-13-24" morph="none" pos="punct" start_char="1487">–</TOKEN>
<TOKEN end_char="1491" id="token-13-25" morph="none" pos="word" start_char="1489">the</TOKEN>
<TOKEN end_char="1498" id="token-13-26" morph="none" pos="word" start_char="1493">reason</TOKEN>
<TOKEN end_char="1504" id="token-13-27" morph="none" pos="word" start_char="1500">being</TOKEN>
<TOKEN end_char="1509" id="token-13-28" morph="none" pos="word" start_char="1506">that</TOKEN>
<TOKEN end_char="1516" id="token-13-29" morph="none" pos="word" start_char="1511">people</TOKEN>
<TOKEN end_char="1521" id="token-13-30" morph="none" pos="word" start_char="1518">shed</TOKEN>
<TOKEN end_char="1533" id="token-13-31" morph="none" pos="word" start_char="1523">coronavirus</TOKEN>
<TOKEN end_char="1540" id="token-13-32" morph="none" pos="word" start_char="1535">before</TOKEN>
<TOKEN end_char="1549" id="token-13-33" morph="none" pos="word" start_char="1542">symptoms</TOKEN>
<TOKEN end_char="1556" id="token-13-34" morph="none" pos="word" start_char="1551">become</TOKEN>
<TOKEN end_char="1564" id="token-13-35" morph="none" pos="word" start_char="1558">evident</TOKEN>
<TOKEN end_char="1565" id="token-13-36" morph="none" pos="punct" start_char="1565">.</TOKEN>
</SEG>
<SEG end_char="1735" id="segment-14" start_char="1567">
<ORIGINAL_TEXT>These "pre-symptomatic" people then have to get sick enough to be tested, get the results, and be admitted to a hospital as an official "case", hence the week or so lag.</ORIGINAL_TEXT>
<TOKEN end_char="1571" id="token-14-0" morph="none" pos="word" start_char="1567">These</TOKEN>
<TOKEN end_char="1573" id="token-14-1" morph="none" pos="punct" start_char="1573">"</TOKEN>
<TOKEN end_char="1588" id="token-14-2" morph="none" pos="unknown" start_char="1574">pre-symptomatic</TOKEN>
<TOKEN end_char="1589" id="token-14-3" morph="none" pos="punct" start_char="1589">"</TOKEN>
<TOKEN end_char="1596" id="token-14-4" morph="none" pos="word" start_char="1591">people</TOKEN>
<TOKEN end_char="1601" id="token-14-5" morph="none" pos="word" start_char="1598">then</TOKEN>
<TOKEN end_char="1606" id="token-14-6" morph="none" pos="word" start_char="1603">have</TOKEN>
<TOKEN end_char="1609" id="token-14-7" morph="none" pos="word" start_char="1608">to</TOKEN>
<TOKEN end_char="1613" id="token-14-8" morph="none" pos="word" start_char="1611">get</TOKEN>
<TOKEN end_char="1618" id="token-14-9" morph="none" pos="word" start_char="1615">sick</TOKEN>
<TOKEN end_char="1625" id="token-14-10" morph="none" pos="word" start_char="1620">enough</TOKEN>
<TOKEN end_char="1628" id="token-14-11" morph="none" pos="word" start_char="1627">to</TOKEN>
<TOKEN end_char="1631" id="token-14-12" morph="none" pos="word" start_char="1630">be</TOKEN>
<TOKEN end_char="1638" id="token-14-13" morph="none" pos="word" start_char="1633">tested</TOKEN>
<TOKEN end_char="1639" id="token-14-14" morph="none" pos="punct" start_char="1639">,</TOKEN>
<TOKEN end_char="1643" id="token-14-15" morph="none" pos="word" start_char="1641">get</TOKEN>
<TOKEN end_char="1647" id="token-14-16" morph="none" pos="word" start_char="1645">the</TOKEN>
<TOKEN end_char="1655" id="token-14-17" morph="none" pos="word" start_char="1649">results</TOKEN>
<TOKEN end_char="1656" id="token-14-18" morph="none" pos="punct" start_char="1656">,</TOKEN>
<TOKEN end_char="1660" id="token-14-19" morph="none" pos="word" start_char="1658">and</TOKEN>
<TOKEN end_char="1663" id="token-14-20" morph="none" pos="word" start_char="1662">be</TOKEN>
<TOKEN end_char="1672" id="token-14-21" morph="none" pos="word" start_char="1665">admitted</TOKEN>
<TOKEN end_char="1675" id="token-14-22" morph="none" pos="word" start_char="1674">to</TOKEN>
<TOKEN end_char="1677" id="token-14-23" morph="none" pos="word" start_char="1677">a</TOKEN>
<TOKEN end_char="1686" id="token-14-24" morph="none" pos="word" start_char="1679">hospital</TOKEN>
<TOKEN end_char="1689" id="token-14-25" morph="none" pos="word" start_char="1688">as</TOKEN>
<TOKEN end_char="1692" id="token-14-26" morph="none" pos="word" start_char="1691">an</TOKEN>
<TOKEN end_char="1701" id="token-14-27" morph="none" pos="word" start_char="1694">official</TOKEN>
<TOKEN end_char="1703" id="token-14-28" morph="none" pos="punct" start_char="1703">"</TOKEN>
<TOKEN end_char="1707" id="token-14-29" morph="none" pos="word" start_char="1704">case</TOKEN>
<TOKEN end_char="1709" id="token-14-30" morph="none" pos="punct" start_char="1708">",</TOKEN>
<TOKEN end_char="1715" id="token-14-31" morph="none" pos="word" start_char="1711">hence</TOKEN>
<TOKEN end_char="1719" id="token-14-32" morph="none" pos="word" start_char="1717">the</TOKEN>
<TOKEN end_char="1724" id="token-14-33" morph="none" pos="word" start_char="1721">week</TOKEN>
<TOKEN end_char="1727" id="token-14-34" morph="none" pos="word" start_char="1726">or</TOKEN>
<TOKEN end_char="1730" id="token-14-35" morph="none" pos="word" start_char="1729">so</TOKEN>
<TOKEN end_char="1734" id="token-14-36" morph="none" pos="word" start_char="1732">lag</TOKEN>
<TOKEN end_char="1735" id="token-14-37" morph="none" pos="punct" start_char="1735">.</TOKEN>
</SEG>
<SEG end_char="1839" id="segment-15" start_char="1738">
<ORIGINAL_TEXT>As a result, many countries, including Spain, are now monitoring wastewater for traces of coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="1739" id="token-15-0" morph="none" pos="word" start_char="1738">As</TOKEN>
<TOKEN end_char="1741" id="token-15-1" morph="none" pos="word" start_char="1741">a</TOKEN>
<TOKEN end_char="1748" id="token-15-2" morph="none" pos="word" start_char="1743">result</TOKEN>
<TOKEN end_char="1749" id="token-15-3" morph="none" pos="punct" start_char="1749">,</TOKEN>
<TOKEN end_char="1754" id="token-15-4" morph="none" pos="word" start_char="1751">many</TOKEN>
<TOKEN end_char="1764" id="token-15-5" morph="none" pos="word" start_char="1756">countries</TOKEN>
<TOKEN end_char="1765" id="token-15-6" morph="none" pos="punct" start_char="1765">,</TOKEN>
<TOKEN end_char="1775" id="token-15-7" morph="none" pos="word" start_char="1767">including</TOKEN>
<TOKEN end_char="1781" id="token-15-8" morph="none" pos="word" start_char="1777">Spain</TOKEN>
<TOKEN end_char="1782" id="token-15-9" morph="none" pos="punct" start_char="1782">,</TOKEN>
<TOKEN end_char="1786" id="token-15-10" morph="none" pos="word" start_char="1784">are</TOKEN>
<TOKEN end_char="1790" id="token-15-11" morph="none" pos="word" start_char="1788">now</TOKEN>
<TOKEN end_char="1801" id="token-15-12" morph="none" pos="word" start_char="1792">monitoring</TOKEN>
<TOKEN end_char="1812" id="token-15-13" morph="none" pos="word" start_char="1803">wastewater</TOKEN>
<TOKEN end_char="1816" id="token-15-14" morph="none" pos="word" start_char="1814">for</TOKEN>
<TOKEN end_char="1823" id="token-15-15" morph="none" pos="word" start_char="1818">traces</TOKEN>
<TOKEN end_char="1826" id="token-15-16" morph="none" pos="word" start_char="1825">of</TOKEN>
<TOKEN end_char="1838" id="token-15-17" morph="none" pos="word" start_char="1828">coronavirus</TOKEN>
<TOKEN end_char="1839" id="token-15-18" morph="none" pos="punct" start_char="1839">.</TOKEN>
</SEG>
<SEG end_char="2023" id="segment-16" start_char="1841">
<ORIGINAL_TEXT>In this particular study, wastewater epidemiologists were examining frozen samples of influent between January 2018 and December 2019 to see when the virus made its debut in the city.</ORIGINAL_TEXT>
<TOKEN end_char="1842" id="token-16-0" morph="none" pos="word" start_char="1841">In</TOKEN>
<TOKEN end_char="1847" id="token-16-1" morph="none" pos="word" start_char="1844">this</TOKEN>
<TOKEN end_char="1858" id="token-16-2" morph="none" pos="word" start_char="1849">particular</TOKEN>
<TOKEN end_char="1864" id="token-16-3" morph="none" pos="word" start_char="1860">study</TOKEN>
<TOKEN end_char="1865" id="token-16-4" morph="none" pos="punct" start_char="1865">,</TOKEN>
<TOKEN end_char="1876" id="token-16-5" morph="none" pos="word" start_char="1867">wastewater</TOKEN>
<TOKEN end_char="1892" id="token-16-6" morph="none" pos="word" start_char="1878">epidemiologists</TOKEN>
<TOKEN end_char="1897" id="token-16-7" morph="none" pos="word" start_char="1894">were</TOKEN>
<TOKEN end_char="1907" id="token-16-8" morph="none" pos="word" start_char="1899">examining</TOKEN>
<TOKEN end_char="1914" id="token-16-9" morph="none" pos="word" start_char="1909">frozen</TOKEN>
<TOKEN end_char="1922" id="token-16-10" morph="none" pos="word" start_char="1916">samples</TOKEN>
<TOKEN end_char="1925" id="token-16-11" morph="none" pos="word" start_char="1924">of</TOKEN>
<TOKEN end_char="1934" id="token-16-12" morph="none" pos="word" start_char="1927">influent</TOKEN>
<TOKEN end_char="1942" id="token-16-13" morph="none" pos="word" start_char="1936">between</TOKEN>
<TOKEN end_char="1950" id="token-16-14" morph="none" pos="word" start_char="1944">January</TOKEN>
<TOKEN end_char="1955" id="token-16-15" morph="none" pos="word" start_char="1952">2018</TOKEN>
<TOKEN end_char="1959" id="token-16-16" morph="none" pos="word" start_char="1957">and</TOKEN>
<TOKEN end_char="1968" id="token-16-17" morph="none" pos="word" start_char="1961">December</TOKEN>
<TOKEN end_char="1973" id="token-16-18" morph="none" pos="word" start_char="1970">2019</TOKEN>
<TOKEN end_char="1976" id="token-16-19" morph="none" pos="word" start_char="1975">to</TOKEN>
<TOKEN end_char="1980" id="token-16-20" morph="none" pos="word" start_char="1978">see</TOKEN>
<TOKEN end_char="1985" id="token-16-21" morph="none" pos="word" start_char="1982">when</TOKEN>
<TOKEN end_char="1989" id="token-16-22" morph="none" pos="word" start_char="1987">the</TOKEN>
<TOKEN end_char="1995" id="token-16-23" morph="none" pos="word" start_char="1991">virus</TOKEN>
<TOKEN end_char="2000" id="token-16-24" morph="none" pos="word" start_char="1997">made</TOKEN>
<TOKEN end_char="2004" id="token-16-25" morph="none" pos="word" start_char="2002">its</TOKEN>
<TOKEN end_char="2010" id="token-16-26" morph="none" pos="word" start_char="2006">debut</TOKEN>
<TOKEN end_char="2013" id="token-16-27" morph="none" pos="word" start_char="2012">in</TOKEN>
<TOKEN end_char="2017" id="token-16-28" morph="none" pos="word" start_char="2015">the</TOKEN>
<TOKEN end_char="2022" id="token-16-29" morph="none" pos="word" start_char="2019">city</TOKEN>
<TOKEN end_char="2023" id="token-16-30" morph="none" pos="punct" start_char="2023">.</TOKEN>
</SEG>
<SEG end_char="2101" id="segment-17" start_char="2026">
<ORIGINAL_TEXT>Experts around the world are monitoring wastewater for signs of coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="2032" id="token-17-0" morph="none" pos="word" start_char="2026">Experts</TOKEN>
<TOKEN end_char="2039" id="token-17-1" morph="none" pos="word" start_char="2034">around</TOKEN>
<TOKEN end_char="2043" id="token-17-2" morph="none" pos="word" start_char="2041">the</TOKEN>
<TOKEN end_char="2049" id="token-17-3" morph="none" pos="word" start_char="2045">world</TOKEN>
<TOKEN end_char="2053" id="token-17-4" morph="none" pos="word" start_char="2051">are</TOKEN>
<TOKEN end_char="2064" id="token-17-5" morph="none" pos="word" start_char="2055">monitoring</TOKEN>
<TOKEN end_char="2075" id="token-17-6" morph="none" pos="word" start_char="2066">wastewater</TOKEN>
<TOKEN end_char="2079" id="token-17-7" morph="none" pos="word" start_char="2077">for</TOKEN>
<TOKEN end_char="2085" id="token-17-8" morph="none" pos="word" start_char="2081">signs</TOKEN>
<TOKEN end_char="2088" id="token-17-9" morph="none" pos="word" start_char="2087">of</TOKEN>
<TOKEN end_char="2100" id="token-17-10" morph="none" pos="word" start_char="2090">coronavirus</TOKEN>
<TOKEN end_char="2101" id="token-17-11" morph="none" pos="punct" start_char="2101">.</TOKEN>
</SEG>
<SEG end_char="2231" id="segment-18" start_char="2105">
<ORIGINAL_TEXT>They found evidence of the virus on January 15, 2020, 41 days before the first official case was declared on February 25, 2020.</ORIGINAL_TEXT>
<TOKEN end_char="2108" id="token-18-0" morph="none" pos="word" start_char="2105">They</TOKEN>
<TOKEN end_char="2114" id="token-18-1" morph="none" pos="word" start_char="2110">found</TOKEN>
<TOKEN end_char="2123" id="token-18-2" morph="none" pos="word" start_char="2116">evidence</TOKEN>
<TOKEN end_char="2126" id="token-18-3" morph="none" pos="word" start_char="2125">of</TOKEN>
<TOKEN end_char="2130" id="token-18-4" morph="none" pos="word" start_char="2128">the</TOKEN>
<TOKEN end_char="2136" id="token-18-5" morph="none" pos="word" start_char="2132">virus</TOKEN>
<TOKEN end_char="2139" id="token-18-6" morph="none" pos="word" start_char="2138">on</TOKEN>
<TOKEN end_char="2147" id="token-18-7" morph="none" pos="word" start_char="2141">January</TOKEN>
<TOKEN end_char="2150" id="token-18-8" morph="none" pos="word" start_char="2149">15</TOKEN>
<TOKEN end_char="2151" id="token-18-9" morph="none" pos="punct" start_char="2151">,</TOKEN>
<TOKEN end_char="2156" id="token-18-10" morph="none" pos="word" start_char="2153">2020</TOKEN>
<TOKEN end_char="2157" id="token-18-11" morph="none" pos="punct" start_char="2157">,</TOKEN>
<TOKEN end_char="2160" id="token-18-12" morph="none" pos="word" start_char="2159">41</TOKEN>
<TOKEN end_char="2165" id="token-18-13" morph="none" pos="word" start_char="2162">days</TOKEN>
<TOKEN end_char="2172" id="token-18-14" morph="none" pos="word" start_char="2167">before</TOKEN>
<TOKEN end_char="2176" id="token-18-15" morph="none" pos="word" start_char="2174">the</TOKEN>
<TOKEN end_char="2182" id="token-18-16" morph="none" pos="word" start_char="2178">first</TOKEN>
<TOKEN end_char="2191" id="token-18-17" morph="none" pos="word" start_char="2184">official</TOKEN>
<TOKEN end_char="2196" id="token-18-18" morph="none" pos="word" start_char="2193">case</TOKEN>
<TOKEN end_char="2200" id="token-18-19" morph="none" pos="word" start_char="2198">was</TOKEN>
<TOKEN end_char="2209" id="token-18-20" morph="none" pos="word" start_char="2202">declared</TOKEN>
<TOKEN end_char="2212" id="token-18-21" morph="none" pos="word" start_char="2211">on</TOKEN>
<TOKEN end_char="2221" id="token-18-22" morph="none" pos="word" start_char="2214">February</TOKEN>
<TOKEN end_char="2224" id="token-18-23" morph="none" pos="word" start_char="2223">25</TOKEN>
<TOKEN end_char="2225" id="token-18-24" morph="none" pos="punct" start_char="2225">,</TOKEN>
<TOKEN end_char="2230" id="token-18-25" morph="none" pos="word" start_char="2227">2020</TOKEN>
<TOKEN end_char="2231" id="token-18-26" morph="none" pos="punct" start_char="2231">.</TOKEN>
</SEG>
<SEG end_char="2384" id="segment-19" start_char="2233">
<ORIGINAL_TEXT>All the samples before this date were negative, except for a sample from March 12, 2019, which gave a positive result in their PCR test for coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="2235" id="token-19-0" morph="none" pos="word" start_char="2233">All</TOKEN>
<TOKEN end_char="2239" id="token-19-1" morph="none" pos="word" start_char="2237">the</TOKEN>
<TOKEN end_char="2247" id="token-19-2" morph="none" pos="word" start_char="2241">samples</TOKEN>
<TOKEN end_char="2254" id="token-19-3" morph="none" pos="word" start_char="2249">before</TOKEN>
<TOKEN end_char="2259" id="token-19-4" morph="none" pos="word" start_char="2256">this</TOKEN>
<TOKEN end_char="2264" id="token-19-5" morph="none" pos="word" start_char="2261">date</TOKEN>
<TOKEN end_char="2269" id="token-19-6" morph="none" pos="word" start_char="2266">were</TOKEN>
<TOKEN end_char="2278" id="token-19-7" morph="none" pos="word" start_char="2271">negative</TOKEN>
<TOKEN end_char="2279" id="token-19-8" morph="none" pos="punct" start_char="2279">,</TOKEN>
<TOKEN end_char="2286" id="token-19-9" morph="none" pos="word" start_char="2281">except</TOKEN>
<TOKEN end_char="2290" id="token-19-10" morph="none" pos="word" start_char="2288">for</TOKEN>
<TOKEN end_char="2292" id="token-19-11" morph="none" pos="word" start_char="2292">a</TOKEN>
<TOKEN end_char="2299" id="token-19-12" morph="none" pos="word" start_char="2294">sample</TOKEN>
<TOKEN end_char="2304" id="token-19-13" morph="none" pos="word" start_char="2301">from</TOKEN>
<TOKEN end_char="2310" id="token-19-14" morph="none" pos="word" start_char="2306">March</TOKEN>
<TOKEN end_char="2313" id="token-19-15" morph="none" pos="word" start_char="2312">12</TOKEN>
<TOKEN end_char="2314" id="token-19-16" morph="none" pos="punct" start_char="2314">,</TOKEN>
<TOKEN end_char="2319" id="token-19-17" morph="none" pos="word" start_char="2316">2019</TOKEN>
<TOKEN end_char="2320" id="token-19-18" morph="none" pos="punct" start_char="2320">,</TOKEN>
<TOKEN end_char="2326" id="token-19-19" morph="none" pos="word" start_char="2322">which</TOKEN>
<TOKEN end_char="2331" id="token-19-20" morph="none" pos="word" start_char="2328">gave</TOKEN>
<TOKEN end_char="2333" id="token-19-21" morph="none" pos="word" start_char="2333">a</TOKEN>
<TOKEN end_char="2342" id="token-19-22" morph="none" pos="word" start_char="2335">positive</TOKEN>
<TOKEN end_char="2349" id="token-19-23" morph="none" pos="word" start_char="2344">result</TOKEN>
<TOKEN end_char="2352" id="token-19-24" morph="none" pos="word" start_char="2351">in</TOKEN>
<TOKEN end_char="2358" id="token-19-25" morph="none" pos="word" start_char="2354">their</TOKEN>
<TOKEN end_char="2362" id="token-19-26" morph="none" pos="word" start_char="2360">PCR</TOKEN>
<TOKEN end_char="2367" id="token-19-27" morph="none" pos="word" start_char="2364">test</TOKEN>
<TOKEN end_char="2371" id="token-19-28" morph="none" pos="word" start_char="2369">for</TOKEN>
<TOKEN end_char="2383" id="token-19-29" morph="none" pos="word" start_char="2373">coronavirus</TOKEN>
<TOKEN end_char="2384" id="token-19-30" morph="none" pos="punct" start_char="2384">.</TOKEN>
</SEG>
<SEG end_char="2464" id="segment-20" start_char="2386">
<ORIGINAL_TEXT>PCR is the standard way of testing to see if someone currently has the disease.</ORIGINAL_TEXT>
<TOKEN end_char="2388" id="token-20-0" morph="none" pos="word" start_char="2386">PCR</TOKEN>
<TOKEN end_char="2391" id="token-20-1" morph="none" pos="word" start_char="2390">is</TOKEN>
<TOKEN end_char="2395" id="token-20-2" morph="none" pos="word" start_char="2393">the</TOKEN>
<TOKEN end_char="2404" id="token-20-3" morph="none" pos="word" start_char="2397">standard</TOKEN>
<TOKEN end_char="2408" id="token-20-4" morph="none" pos="word" start_char="2406">way</TOKEN>
<TOKEN end_char="2411" id="token-20-5" morph="none" pos="word" start_char="2410">of</TOKEN>
<TOKEN end_char="2419" id="token-20-6" morph="none" pos="word" start_char="2413">testing</TOKEN>
<TOKEN end_char="2422" id="token-20-7" morph="none" pos="word" start_char="2421">to</TOKEN>
<TOKEN end_char="2426" id="token-20-8" morph="none" pos="word" start_char="2424">see</TOKEN>
<TOKEN end_char="2429" id="token-20-9" morph="none" pos="word" start_char="2428">if</TOKEN>
<TOKEN end_char="2437" id="token-20-10" morph="none" pos="word" start_char="2431">someone</TOKEN>
<TOKEN end_char="2447" id="token-20-11" morph="none" pos="word" start_char="2439">currently</TOKEN>
<TOKEN end_char="2451" id="token-20-12" morph="none" pos="word" start_char="2449">has</TOKEN>
<TOKEN end_char="2455" id="token-20-13" morph="none" pos="word" start_char="2453">the</TOKEN>
<TOKEN end_char="2463" id="token-20-14" morph="none" pos="word" start_char="2457">disease</TOKEN>
<TOKEN end_char="2464" id="token-20-15" morph="none" pos="punct" start_char="2464">.</TOKEN>
</SEG>
<SEG end_char="2758" id="segment-21" start_char="2467">
<ORIGINAL_TEXT>PCR involves getting samples of saliva, mucus, frozen wastewater or whatever else the virus is thought to be lurking in, clearing all the unnecessary stuff out of the sample, then converting the RNA – which is a single strand of genetic material – into DNA (the famous double-stranded helix).</ORIGINAL_TEXT>
<TOKEN end_char="2469" id="token-21-0" morph="none" pos="word" start_char="2467">PCR</TOKEN>
<TOKEN end_char="2478" id="token-21-1" morph="none" pos="word" start_char="2471">involves</TOKEN>
<TOKEN end_char="2486" id="token-21-2" morph="none" pos="word" start_char="2480">getting</TOKEN>
<TOKEN end_char="2494" id="token-21-3" morph="none" pos="word" start_char="2488">samples</TOKEN>
<TOKEN end_char="2497" id="token-21-4" morph="none" pos="word" start_char="2496">of</TOKEN>
<TOKEN end_char="2504" id="token-21-5" morph="none" pos="word" start_char="2499">saliva</TOKEN>
<TOKEN end_char="2505" id="token-21-6" morph="none" pos="punct" start_char="2505">,</TOKEN>
<TOKEN end_char="2511" id="token-21-7" morph="none" pos="word" start_char="2507">mucus</TOKEN>
<TOKEN end_char="2512" id="token-21-8" morph="none" pos="punct" start_char="2512">,</TOKEN>
<TOKEN end_char="2519" id="token-21-9" morph="none" pos="word" start_char="2514">frozen</TOKEN>
<TOKEN end_char="2530" id="token-21-10" morph="none" pos="word" start_char="2521">wastewater</TOKEN>
<TOKEN end_char="2533" id="token-21-11" morph="none" pos="word" start_char="2532">or</TOKEN>
<TOKEN end_char="2542" id="token-21-12" morph="none" pos="word" start_char="2535">whatever</TOKEN>
<TOKEN end_char="2547" id="token-21-13" morph="none" pos="word" start_char="2544">else</TOKEN>
<TOKEN end_char="2551" id="token-21-14" morph="none" pos="word" start_char="2549">the</TOKEN>
<TOKEN end_char="2557" id="token-21-15" morph="none" pos="word" start_char="2553">virus</TOKEN>
<TOKEN end_char="2560" id="token-21-16" morph="none" pos="word" start_char="2559">is</TOKEN>
<TOKEN end_char="2568" id="token-21-17" morph="none" pos="word" start_char="2562">thought</TOKEN>
<TOKEN end_char="2571" id="token-21-18" morph="none" pos="word" start_char="2570">to</TOKEN>
<TOKEN end_char="2574" id="token-21-19" morph="none" pos="word" start_char="2573">be</TOKEN>
<TOKEN end_char="2582" id="token-21-20" morph="none" pos="word" start_char="2576">lurking</TOKEN>
<TOKEN end_char="2585" id="token-21-21" morph="none" pos="word" start_char="2584">in</TOKEN>
<TOKEN end_char="2586" id="token-21-22" morph="none" pos="punct" start_char="2586">,</TOKEN>
<TOKEN end_char="2595" id="token-21-23" morph="none" pos="word" start_char="2588">clearing</TOKEN>
<TOKEN end_char="2599" id="token-21-24" morph="none" pos="word" start_char="2597">all</TOKEN>
<TOKEN end_char="2603" id="token-21-25" morph="none" pos="word" start_char="2601">the</TOKEN>
<TOKEN end_char="2615" id="token-21-26" morph="none" pos="word" start_char="2605">unnecessary</TOKEN>
<TOKEN end_char="2621" id="token-21-27" morph="none" pos="word" start_char="2617">stuff</TOKEN>
<TOKEN end_char="2625" id="token-21-28" morph="none" pos="word" start_char="2623">out</TOKEN>
<TOKEN end_char="2628" id="token-21-29" morph="none" pos="word" start_char="2627">of</TOKEN>
<TOKEN end_char="2632" id="token-21-30" morph="none" pos="word" start_char="2630">the</TOKEN>
<TOKEN end_char="2639" id="token-21-31" morph="none" pos="word" start_char="2634">sample</TOKEN>
<TOKEN end_char="2640" id="token-21-32" morph="none" pos="punct" start_char="2640">,</TOKEN>
<TOKEN end_char="2645" id="token-21-33" morph="none" pos="word" start_char="2642">then</TOKEN>
<TOKEN end_char="2656" id="token-21-34" morph="none" pos="word" start_char="2647">converting</TOKEN>
<TOKEN end_char="2660" id="token-21-35" morph="none" pos="word" start_char="2658">the</TOKEN>
<TOKEN end_char="2664" id="token-21-36" morph="none" pos="word" start_char="2662">RNA</TOKEN>
<TOKEN end_char="2666" id="token-21-37" morph="none" pos="punct" start_char="2666">–</TOKEN>
<TOKEN end_char="2672" id="token-21-38" morph="none" pos="word" start_char="2668">which</TOKEN>
<TOKEN end_char="2675" id="token-21-39" morph="none" pos="word" start_char="2674">is</TOKEN>
<TOKEN end_char="2677" id="token-21-40" morph="none" pos="word" start_char="2677">a</TOKEN>
<TOKEN end_char="2684" id="token-21-41" morph="none" pos="word" start_char="2679">single</TOKEN>
<TOKEN end_char="2691" id="token-21-42" morph="none" pos="word" start_char="2686">strand</TOKEN>
<TOKEN end_char="2694" id="token-21-43" morph="none" pos="word" start_char="2693">of</TOKEN>
<TOKEN end_char="2702" id="token-21-44" morph="none" pos="word" start_char="2696">genetic</TOKEN>
<TOKEN end_char="2711" id="token-21-45" morph="none" pos="word" start_char="2704">material</TOKEN>
<TOKEN end_char="2713" id="token-21-46" morph="none" pos="punct" start_char="2713">–</TOKEN>
<TOKEN end_char="2718" id="token-21-47" morph="none" pos="word" start_char="2715">into</TOKEN>
<TOKEN end_char="2722" id="token-21-48" morph="none" pos="word" start_char="2720">DNA</TOKEN>
<TOKEN end_char="2724" id="token-21-49" morph="none" pos="punct" start_char="2724">(</TOKEN>
<TOKEN end_char="2727" id="token-21-50" morph="none" pos="word" start_char="2725">the</TOKEN>
<TOKEN end_char="2734" id="token-21-51" morph="none" pos="word" start_char="2729">famous</TOKEN>
<TOKEN end_char="2750" id="token-21-52" morph="none" pos="unknown" start_char="2736">double-stranded</TOKEN>
<TOKEN end_char="2756" id="token-21-53" morph="none" pos="word" start_char="2752">helix</TOKEN>
<TOKEN end_char="2758" id="token-21-54" morph="none" pos="punct" start_char="2757">).</TOKEN>
</SEG>
<SEG end_char="2955" id="segment-22" start_char="2760">
<ORIGINAL_TEXT>The DNA is then "amplified" in successive cycles until key bits of genetic material that are known to only exist in a particular virus are plentiful enough to be detected with a fluorescent probe.</ORIGINAL_TEXT>
<TOKEN end_char="2762" id="token-22-0" morph="none" pos="word" start_char="2760">The</TOKEN>
<TOKEN end_char="2766" id="token-22-1" morph="none" pos="word" start_char="2764">DNA</TOKEN>
<TOKEN end_char="2769" id="token-22-2" morph="none" pos="word" start_char="2768">is</TOKEN>
<TOKEN end_char="2774" id="token-22-3" morph="none" pos="word" start_char="2771">then</TOKEN>
<TOKEN end_char="2776" id="token-22-4" morph="none" pos="punct" start_char="2776">"</TOKEN>
<TOKEN end_char="2785" id="token-22-5" morph="none" pos="word" start_char="2777">amplified</TOKEN>
<TOKEN end_char="2786" id="token-22-6" morph="none" pos="punct" start_char="2786">"</TOKEN>
<TOKEN end_char="2789" id="token-22-7" morph="none" pos="word" start_char="2788">in</TOKEN>
<TOKEN end_char="2800" id="token-22-8" morph="none" pos="word" start_char="2791">successive</TOKEN>
<TOKEN end_char="2807" id="token-22-9" morph="none" pos="word" start_char="2802">cycles</TOKEN>
<TOKEN end_char="2813" id="token-22-10" morph="none" pos="word" start_char="2809">until</TOKEN>
<TOKEN end_char="2817" id="token-22-11" morph="none" pos="word" start_char="2815">key</TOKEN>
<TOKEN end_char="2822" id="token-22-12" morph="none" pos="word" start_char="2819">bits</TOKEN>
<TOKEN end_char="2825" id="token-22-13" morph="none" pos="word" start_char="2824">of</TOKEN>
<TOKEN end_char="2833" id="token-22-14" morph="none" pos="word" start_char="2827">genetic</TOKEN>
<TOKEN end_char="2842" id="token-22-15" morph="none" pos="word" start_char="2835">material</TOKEN>
<TOKEN end_char="2847" id="token-22-16" morph="none" pos="word" start_char="2844">that</TOKEN>
<TOKEN end_char="2851" id="token-22-17" morph="none" pos="word" start_char="2849">are</TOKEN>
<TOKEN end_char="2857" id="token-22-18" morph="none" pos="word" start_char="2853">known</TOKEN>
<TOKEN end_char="2860" id="token-22-19" morph="none" pos="word" start_char="2859">to</TOKEN>
<TOKEN end_char="2865" id="token-22-20" morph="none" pos="word" start_char="2862">only</TOKEN>
<TOKEN end_char="2871" id="token-22-21" morph="none" pos="word" start_char="2867">exist</TOKEN>
<TOKEN end_char="2874" id="token-22-22" morph="none" pos="word" start_char="2873">in</TOKEN>
<TOKEN end_char="2876" id="token-22-23" morph="none" pos="word" start_char="2876">a</TOKEN>
<TOKEN end_char="2887" id="token-22-24" morph="none" pos="word" start_char="2878">particular</TOKEN>
<TOKEN end_char="2893" id="token-22-25" morph="none" pos="word" start_char="2889">virus</TOKEN>
<TOKEN end_char="2897" id="token-22-26" morph="none" pos="word" start_char="2895">are</TOKEN>
<TOKEN end_char="2907" id="token-22-27" morph="none" pos="word" start_char="2899">plentiful</TOKEN>
<TOKEN end_char="2914" id="token-22-28" morph="none" pos="word" start_char="2909">enough</TOKEN>
<TOKEN end_char="2917" id="token-22-29" morph="none" pos="word" start_char="2916">to</TOKEN>
<TOKEN end_char="2920" id="token-22-30" morph="none" pos="word" start_char="2919">be</TOKEN>
<TOKEN end_char="2929" id="token-22-31" morph="none" pos="word" start_char="2922">detected</TOKEN>
<TOKEN end_char="2934" id="token-22-32" morph="none" pos="word" start_char="2931">with</TOKEN>
<TOKEN end_char="2936" id="token-22-33" morph="none" pos="word" start_char="2936">a</TOKEN>
<TOKEN end_char="2948" id="token-22-34" morph="none" pos="word" start_char="2938">fluorescent</TOKEN>
<TOKEN end_char="2954" id="token-22-35" morph="none" pos="word" start_char="2950">probe</TOKEN>
<TOKEN end_char="2955" id="token-22-36" morph="none" pos="punct" start_char="2955">.</TOKEN>
</SEG>
<SEG end_char="2976" id="segment-23" start_char="2958">
<ORIGINAL_TEXT>Not highly specific</ORIGINAL_TEXT>
<TOKEN end_char="2960" id="token-23-0" morph="none" pos="word" start_char="2958">Not</TOKEN>
<TOKEN end_char="2967" id="token-23-1" morph="none" pos="word" start_char="2962">highly</TOKEN>
<TOKEN end_char="2976" id="token-23-2" morph="none" pos="word" start_char="2969">specific</TOKEN>
</SEG>
<SEG end_char="3054" id="segment-24" start_char="2980">
<ORIGINAL_TEXT>In coronavirus testing, scientists typically screen for more than one gene.</ORIGINAL_TEXT>
<TOKEN end_char="2981" id="token-24-0" morph="none" pos="word" start_char="2980">In</TOKEN>
<TOKEN end_char="2993" id="token-24-1" morph="none" pos="word" start_char="2983">coronavirus</TOKEN>
<TOKEN end_char="3001" id="token-24-2" morph="none" pos="word" start_char="2995">testing</TOKEN>
<TOKEN end_char="3002" id="token-24-3" morph="none" pos="punct" start_char="3002">,</TOKEN>
<TOKEN end_char="3013" id="token-24-4" morph="none" pos="word" start_char="3004">scientists</TOKEN>
<TOKEN end_char="3023" id="token-24-5" morph="none" pos="word" start_char="3015">typically</TOKEN>
<TOKEN end_char="3030" id="token-24-6" morph="none" pos="word" start_char="3025">screen</TOKEN>
<TOKEN end_char="3034" id="token-24-7" morph="none" pos="word" start_char="3032">for</TOKEN>
<TOKEN end_char="3039" id="token-24-8" morph="none" pos="word" start_char="3036">more</TOKEN>
<TOKEN end_char="3044" id="token-24-9" morph="none" pos="word" start_char="3041">than</TOKEN>
<TOKEN end_char="3048" id="token-24-10" morph="none" pos="word" start_char="3046">one</TOKEN>
<TOKEN end_char="3053" id="token-24-11" morph="none" pos="word" start_char="3050">gene</TOKEN>
<TOKEN end_char="3054" id="token-24-12" morph="none" pos="punct" start_char="3054">.</TOKEN>
</SEG>
<SEG end_char="3102" id="segment-25" start_char="3056">
<ORIGINAL_TEXT>In this case, the researchers tested for three.</ORIGINAL_TEXT>
<TOKEN end_char="3057" id="token-25-0" morph="none" pos="word" start_char="3056">In</TOKEN>
<TOKEN end_char="3062" id="token-25-1" morph="none" pos="word" start_char="3059">this</TOKEN>
<TOKEN end_char="3067" id="token-25-2" morph="none" pos="word" start_char="3064">case</TOKEN>
<TOKEN end_char="3068" id="token-25-3" morph="none" pos="punct" start_char="3068">,</TOKEN>
<TOKEN end_char="3072" id="token-25-4" morph="none" pos="word" start_char="3070">the</TOKEN>
<TOKEN end_char="3084" id="token-25-5" morph="none" pos="word" start_char="3074">researchers</TOKEN>
<TOKEN end_char="3091" id="token-25-6" morph="none" pos="word" start_char="3086">tested</TOKEN>
<TOKEN end_char="3095" id="token-25-7" morph="none" pos="word" start_char="3093">for</TOKEN>
<TOKEN end_char="3101" id="token-25-8" morph="none" pos="word" start_char="3097">three</TOKEN>
<TOKEN end_char="3102" id="token-25-9" morph="none" pos="punct" start_char="3102">.</TOKEN>
</SEG>
<SEG end_char="3205" id="segment-26" start_char="3104">
<ORIGINAL_TEXT>They had a positive result for the March 2019 sample in one of the three genes tested – the RdRp gene.</ORIGINAL_TEXT>
<TOKEN end_char="3107" id="token-26-0" morph="none" pos="word" start_char="3104">They</TOKEN>
<TOKEN end_char="3111" id="token-26-1" morph="none" pos="word" start_char="3109">had</TOKEN>
<TOKEN end_char="3113" id="token-26-2" morph="none" pos="word" start_char="3113">a</TOKEN>
<TOKEN end_char="3122" id="token-26-3" morph="none" pos="word" start_char="3115">positive</TOKEN>
<TOKEN end_char="3129" id="token-26-4" morph="none" pos="word" start_char="3124">result</TOKEN>
<TOKEN end_char="3133" id="token-26-5" morph="none" pos="word" start_char="3131">for</TOKEN>
<TOKEN end_char="3137" id="token-26-6" morph="none" pos="word" start_char="3135">the</TOKEN>
<TOKEN end_char="3143" id="token-26-7" morph="none" pos="word" start_char="3139">March</TOKEN>
<TOKEN end_char="3148" id="token-26-8" morph="none" pos="word" start_char="3145">2019</TOKEN>
<TOKEN end_char="3155" id="token-26-9" morph="none" pos="word" start_char="3150">sample</TOKEN>
<TOKEN end_char="3158" id="token-26-10" morph="none" pos="word" start_char="3157">in</TOKEN>
<TOKEN end_char="3162" id="token-26-11" morph="none" pos="word" start_char="3160">one</TOKEN>
<TOKEN end_char="3165" id="token-26-12" morph="none" pos="word" start_char="3164">of</TOKEN>
<TOKEN end_char="3169" id="token-26-13" morph="none" pos="word" start_char="3167">the</TOKEN>
<TOKEN end_char="3175" id="token-26-14" morph="none" pos="word" start_char="3171">three</TOKEN>
<TOKEN end_char="3181" id="token-26-15" morph="none" pos="word" start_char="3177">genes</TOKEN>
<TOKEN end_char="3188" id="token-26-16" morph="none" pos="word" start_char="3183">tested</TOKEN>
<TOKEN end_char="3190" id="token-26-17" morph="none" pos="punct" start_char="3190">–</TOKEN>
<TOKEN end_char="3194" id="token-26-18" morph="none" pos="word" start_char="3192">the</TOKEN>
<TOKEN end_char="3199" id="token-26-19" morph="none" pos="word" start_char="3196">RdRp</TOKEN>
<TOKEN end_char="3204" id="token-26-20" morph="none" pos="word" start_char="3201">gene</TOKEN>
<TOKEN end_char="3205" id="token-26-21" morph="none" pos="punct" start_char="3205">.</TOKEN>
</SEG>
<SEG end_char="3316" id="segment-27" start_char="3207">
<ORIGINAL_TEXT>They screened for two regions of this gene and both were only detected around the 39th cycle of amplification.</ORIGINAL_TEXT>
<TOKEN end_char="3210" id="token-27-0" morph="none" pos="word" start_char="3207">They</TOKEN>
<TOKEN end_char="3219" id="token-27-1" morph="none" pos="word" start_char="3212">screened</TOKEN>
<TOKEN end_char="3223" id="token-27-2" morph="none" pos="word" start_char="3221">for</TOKEN>
<TOKEN end_char="3227" id="token-27-3" morph="none" pos="word" start_char="3225">two</TOKEN>
<TOKEN end_char="3235" id="token-27-4" morph="none" pos="word" start_char="3229">regions</TOKEN>
<TOKEN end_char="3238" id="token-27-5" morph="none" pos="word" start_char="3237">of</TOKEN>
<TOKEN end_char="3243" id="token-27-6" morph="none" pos="word" start_char="3240">this</TOKEN>
<TOKEN end_char="3248" id="token-27-7" morph="none" pos="word" start_char="3245">gene</TOKEN>
<TOKEN end_char="3252" id="token-27-8" morph="none" pos="word" start_char="3250">and</TOKEN>
<TOKEN end_char="3257" id="token-27-9" morph="none" pos="word" start_char="3254">both</TOKEN>
<TOKEN end_char="3262" id="token-27-10" morph="none" pos="word" start_char="3259">were</TOKEN>
<TOKEN end_char="3267" id="token-27-11" morph="none" pos="word" start_char="3264">only</TOKEN>
<TOKEN end_char="3276" id="token-27-12" morph="none" pos="word" start_char="3269">detected</TOKEN>
<TOKEN end_char="3283" id="token-27-13" morph="none" pos="word" start_char="3278">around</TOKEN>
<TOKEN end_char="3287" id="token-27-14" morph="none" pos="word" start_char="3285">the</TOKEN>
<TOKEN end_char="3292" id="token-27-15" morph="none" pos="word" start_char="3289">39th</TOKEN>
<TOKEN end_char="3298" id="token-27-16" morph="none" pos="word" start_char="3294">cycle</TOKEN>
<TOKEN end_char="3301" id="token-27-17" morph="none" pos="word" start_char="3300">of</TOKEN>
<TOKEN end_char="3315" id="token-27-18" morph="none" pos="word" start_char="3303">amplification</TOKEN>
<TOKEN end_char="3316" id="token-27-19" morph="none" pos="punct" start_char="3316">.</TOKEN>
</SEG>
<SEG end_char="3391" id="segment-28" start_char="3318">
<ORIGINAL_TEXT>(PCR tests become less "specific" with increasing rounds of amplification.</ORIGINAL_TEXT>
<TOKEN end_char="3318" id="token-28-0" morph="none" pos="punct" start_char="3318">(</TOKEN>
<TOKEN end_char="3321" id="token-28-1" morph="none" pos="word" start_char="3319">PCR</TOKEN>
<TOKEN end_char="3327" id="token-28-2" morph="none" pos="word" start_char="3323">tests</TOKEN>
<TOKEN end_char="3334" id="token-28-3" morph="none" pos="word" start_char="3329">become</TOKEN>
<TOKEN end_char="3339" id="token-28-4" morph="none" pos="word" start_char="3336">less</TOKEN>
<TOKEN end_char="3341" id="token-28-5" morph="none" pos="punct" start_char="3341">"</TOKEN>
<TOKEN end_char="3349" id="token-28-6" morph="none" pos="word" start_char="3342">specific</TOKEN>
<TOKEN end_char="3350" id="token-28-7" morph="none" pos="punct" start_char="3350">"</TOKEN>
<TOKEN end_char="3355" id="token-28-8" morph="none" pos="word" start_char="3352">with</TOKEN>
<TOKEN end_char="3366" id="token-28-9" morph="none" pos="word" start_char="3357">increasing</TOKEN>
<TOKEN end_char="3373" id="token-28-10" morph="none" pos="word" start_char="3368">rounds</TOKEN>
<TOKEN end_char="3376" id="token-28-11" morph="none" pos="word" start_char="3375">of</TOKEN>
<TOKEN end_char="3390" id="token-28-12" morph="none" pos="word" start_char="3378">amplification</TOKEN>
<TOKEN end_char="3391" id="token-28-13" morph="none" pos="punct" start_char="3391">.</TOKEN>
</SEG>
<SEG end_char="3451" id="segment-29" start_char="3393">
<ORIGINAL_TEXT>Scientists generally use 40 to 45 rounds of amplification.)</ORIGINAL_TEXT>
<TOKEN end_char="3402" id="token-29-0" morph="none" pos="word" start_char="3393">Scientists</TOKEN>
<TOKEN end_char="3412" id="token-29-1" morph="none" pos="word" start_char="3404">generally</TOKEN>
<TOKEN end_char="3416" id="token-29-2" morph="none" pos="word" start_char="3414">use</TOKEN>
<TOKEN end_char="3419" id="token-29-3" morph="none" pos="word" start_char="3418">40</TOKEN>
<TOKEN end_char="3422" id="token-29-4" morph="none" pos="word" start_char="3421">to</TOKEN>
<TOKEN end_char="3425" id="token-29-5" morph="none" pos="word" start_char="3424">45</TOKEN>
<TOKEN end_char="3432" id="token-29-6" morph="none" pos="word" start_char="3427">rounds</TOKEN>
<TOKEN end_char="3435" id="token-29-7" morph="none" pos="word" start_char="3434">of</TOKEN>
<TOKEN end_char="3449" id="token-29-8" morph="none" pos="word" start_char="3437">amplification</TOKEN>
<TOKEN end_char="3451" id="token-29-9" morph="none" pos="punct" start_char="3450">.)</TOKEN>
</SEG>
<SEG end_char="3509" id="segment-30" start_char="3454">
<ORIGINAL_TEXT>There are several explanations for this positive result.</ORIGINAL_TEXT>
<TOKEN end_char="3458" id="token-30-0" morph="none" pos="word" start_char="3454">There</TOKEN>
<TOKEN end_char="3462" id="token-30-1" morph="none" pos="word" start_char="3460">are</TOKEN>
<TOKEN end_char="3470" id="token-30-2" morph="none" pos="word" start_char="3464">several</TOKEN>
<TOKEN end_char="3483" id="token-30-3" morph="none" pos="word" start_char="3472">explanations</TOKEN>
<TOKEN end_char="3487" id="token-30-4" morph="none" pos="word" start_char="3485">for</TOKEN>
<TOKEN end_char="3492" id="token-30-5" morph="none" pos="word" start_char="3489">this</TOKEN>
<TOKEN end_char="3501" id="token-30-6" morph="none" pos="word" start_char="3494">positive</TOKEN>
<TOKEN end_char="3508" id="token-30-7" morph="none" pos="word" start_char="3503">result</TOKEN>
<TOKEN end_char="3509" id="token-30-8" morph="none" pos="punct" start_char="3509">.</TOKEN>
</SEG>
<SEG end_char="3578" id="segment-31" start_char="3511">
<ORIGINAL_TEXT>One is that SARS-CoV-2 is present in the sewage at a very low level.</ORIGINAL_TEXT>
<TOKEN end_char="3513" id="token-31-0" morph="none" pos="word" start_char="3511">One</TOKEN>
<TOKEN end_char="3516" id="token-31-1" morph="none" pos="word" start_char="3515">is</TOKEN>
<TOKEN end_char="3521" id="token-31-2" morph="none" pos="word" start_char="3518">that</TOKEN>
<TOKEN end_char="3532" id="token-31-3" morph="none" pos="unknown" start_char="3523">SARS-CoV-2</TOKEN>
<TOKEN end_char="3535" id="token-31-4" morph="none" pos="word" start_char="3534">is</TOKEN>
<TOKEN end_char="3543" id="token-31-5" morph="none" pos="word" start_char="3537">present</TOKEN>
<TOKEN end_char="3546" id="token-31-6" morph="none" pos="word" start_char="3545">in</TOKEN>
<TOKEN end_char="3550" id="token-31-7" morph="none" pos="word" start_char="3548">the</TOKEN>
<TOKEN end_char="3557" id="token-31-8" morph="none" pos="word" start_char="3552">sewage</TOKEN>
<TOKEN end_char="3560" id="token-31-9" morph="none" pos="word" start_char="3559">at</TOKEN>
<TOKEN end_char="3562" id="token-31-10" morph="none" pos="word" start_char="3562">a</TOKEN>
<TOKEN end_char="3567" id="token-31-11" morph="none" pos="word" start_char="3564">very</TOKEN>
<TOKEN end_char="3571" id="token-31-12" morph="none" pos="word" start_char="3569">low</TOKEN>
<TOKEN end_char="3577" id="token-31-13" morph="none" pos="word" start_char="3573">level</TOKEN>
<TOKEN end_char="3578" id="token-31-14" morph="none" pos="punct" start_char="3578">.</TOKEN>
</SEG>
<SEG end_char="3677" id="segment-32" start_char="3580">
<ORIGINAL_TEXT>Another is that the test reaction was accidentally contaminated with SARS-CoV-2 in the laboratory.</ORIGINAL_TEXT>
<TOKEN end_char="3586" id="token-32-0" morph="none" pos="word" start_char="3580">Another</TOKEN>
<TOKEN end_char="3589" id="token-32-1" morph="none" pos="word" start_char="3588">is</TOKEN>
<TOKEN end_char="3594" id="token-32-2" morph="none" pos="word" start_char="3591">that</TOKEN>
<TOKEN end_char="3598" id="token-32-3" morph="none" pos="word" start_char="3596">the</TOKEN>
<TOKEN end_char="3603" id="token-32-4" morph="none" pos="word" start_char="3600">test</TOKEN>
<TOKEN end_char="3612" id="token-32-5" morph="none" pos="word" start_char="3605">reaction</TOKEN>
<TOKEN end_char="3616" id="token-32-6" morph="none" pos="word" start_char="3614">was</TOKEN>
<TOKEN end_char="3629" id="token-32-7" morph="none" pos="word" start_char="3618">accidentally</TOKEN>
<TOKEN end_char="3642" id="token-32-8" morph="none" pos="word" start_char="3631">contaminated</TOKEN>
<TOKEN end_char="3647" id="token-32-9" morph="none" pos="word" start_char="3644">with</TOKEN>
<TOKEN end_char="3658" id="token-32-10" morph="none" pos="unknown" start_char="3649">SARS-CoV-2</TOKEN>
<TOKEN end_char="3661" id="token-32-11" morph="none" pos="word" start_char="3660">in</TOKEN>
<TOKEN end_char="3665" id="token-32-12" morph="none" pos="word" start_char="3663">the</TOKEN>
<TOKEN end_char="3676" id="token-32-13" morph="none" pos="word" start_char="3667">laboratory</TOKEN>
<TOKEN end_char="3677" id="token-32-14" morph="none" pos="punct" start_char="3677">.</TOKEN>
</SEG>
<SEG end_char="3851" id="segment-33" start_char="3679">
<ORIGINAL_TEXT>This sometimes happens in labs as positive samples are regularly being handled, and it can be difficult to prevent very small traces of positive sample contaminating others.</ORIGINAL_TEXT>
<TOKEN end_char="3682" id="token-33-0" morph="none" pos="word" start_char="3679">This</TOKEN>
<TOKEN end_char="3692" id="token-33-1" morph="none" pos="word" start_char="3684">sometimes</TOKEN>
<TOKEN end_char="3700" id="token-33-2" morph="none" pos="word" start_char="3694">happens</TOKEN>
<TOKEN end_char="3703" id="token-33-3" morph="none" pos="word" start_char="3702">in</TOKEN>
<TOKEN end_char="3708" id="token-33-4" morph="none" pos="word" start_char="3705">labs</TOKEN>
<TOKEN end_char="3711" id="token-33-5" morph="none" pos="word" start_char="3710">as</TOKEN>
<TOKEN end_char="3720" id="token-33-6" morph="none" pos="word" start_char="3713">positive</TOKEN>
<TOKEN end_char="3728" id="token-33-7" morph="none" pos="word" start_char="3722">samples</TOKEN>
<TOKEN end_char="3732" id="token-33-8" morph="none" pos="word" start_char="3730">are</TOKEN>
<TOKEN end_char="3742" id="token-33-9" morph="none" pos="word" start_char="3734">regularly</TOKEN>
<TOKEN end_char="3748" id="token-33-10" morph="none" pos="word" start_char="3744">being</TOKEN>
<TOKEN end_char="3756" id="token-33-11" morph="none" pos="word" start_char="3750">handled</TOKEN>
<TOKEN end_char="3757" id="token-33-12" morph="none" pos="punct" start_char="3757">,</TOKEN>
<TOKEN end_char="3761" id="token-33-13" morph="none" pos="word" start_char="3759">and</TOKEN>
<TOKEN end_char="3764" id="token-33-14" morph="none" pos="word" start_char="3763">it</TOKEN>
<TOKEN end_char="3768" id="token-33-15" morph="none" pos="word" start_char="3766">can</TOKEN>
<TOKEN end_char="3771" id="token-33-16" morph="none" pos="word" start_char="3770">be</TOKEN>
<TOKEN end_char="3781" id="token-33-17" morph="none" pos="word" start_char="3773">difficult</TOKEN>
<TOKEN end_char="3784" id="token-33-18" morph="none" pos="word" start_char="3783">to</TOKEN>
<TOKEN end_char="3792" id="token-33-19" morph="none" pos="word" start_char="3786">prevent</TOKEN>
<TOKEN end_char="3797" id="token-33-20" morph="none" pos="word" start_char="3794">very</TOKEN>
<TOKEN end_char="3803" id="token-33-21" morph="none" pos="word" start_char="3799">small</TOKEN>
<TOKEN end_char="3810" id="token-33-22" morph="none" pos="word" start_char="3805">traces</TOKEN>
<TOKEN end_char="3813" id="token-33-23" morph="none" pos="word" start_char="3812">of</TOKEN>
<TOKEN end_char="3822" id="token-33-24" morph="none" pos="word" start_char="3815">positive</TOKEN>
<TOKEN end_char="3829" id="token-33-25" morph="none" pos="word" start_char="3824">sample</TOKEN>
<TOKEN end_char="3843" id="token-33-26" morph="none" pos="word" start_char="3831">contaminating</TOKEN>
<TOKEN end_char="3850" id="token-33-27" morph="none" pos="word" start_char="3845">others</TOKEN>
<TOKEN end_char="3851" id="token-33-28" morph="none" pos="punct" start_char="3851">.</TOKEN>
</SEG>
<SEG end_char="4032" id="segment-34" start_char="3854">
<ORIGINAL_TEXT>Another explanation is that there is other RNA or DNA in the sample that resembles the test target site enough for it to give a positive result at the 39th cycle of amplification.</ORIGINAL_TEXT>
<TOKEN end_char="3860" id="token-34-0" morph="none" pos="word" start_char="3854">Another</TOKEN>
<TOKEN end_char="3872" id="token-34-1" morph="none" pos="word" start_char="3862">explanation</TOKEN>
<TOKEN end_char="3875" id="token-34-2" morph="none" pos="word" start_char="3874">is</TOKEN>
<TOKEN end_char="3880" id="token-34-3" morph="none" pos="word" start_char="3877">that</TOKEN>
<TOKEN end_char="3886" id="token-34-4" morph="none" pos="word" start_char="3882">there</TOKEN>
<TOKEN end_char="3889" id="token-34-5" morph="none" pos="word" start_char="3888">is</TOKEN>
<TOKEN end_char="3895" id="token-34-6" morph="none" pos="word" start_char="3891">other</TOKEN>
<TOKEN end_char="3899" id="token-34-7" morph="none" pos="word" start_char="3897">RNA</TOKEN>
<TOKEN end_char="3902" id="token-34-8" morph="none" pos="word" start_char="3901">or</TOKEN>
<TOKEN end_char="3906" id="token-34-9" morph="none" pos="word" start_char="3904">DNA</TOKEN>
<TOKEN end_char="3909" id="token-34-10" morph="none" pos="word" start_char="3908">in</TOKEN>
<TOKEN end_char="3913" id="token-34-11" morph="none" pos="word" start_char="3911">the</TOKEN>
<TOKEN end_char="3920" id="token-34-12" morph="none" pos="word" start_char="3915">sample</TOKEN>
<TOKEN end_char="3925" id="token-34-13" morph="none" pos="word" start_char="3922">that</TOKEN>
<TOKEN end_char="3935" id="token-34-14" morph="none" pos="word" start_char="3927">resembles</TOKEN>
<TOKEN end_char="3939" id="token-34-15" morph="none" pos="word" start_char="3937">the</TOKEN>
<TOKEN end_char="3944" id="token-34-16" morph="none" pos="word" start_char="3941">test</TOKEN>
<TOKEN end_char="3951" id="token-34-17" morph="none" pos="word" start_char="3946">target</TOKEN>
<TOKEN end_char="3956" id="token-34-18" morph="none" pos="word" start_char="3953">site</TOKEN>
<TOKEN end_char="3963" id="token-34-19" morph="none" pos="word" start_char="3958">enough</TOKEN>
<TOKEN end_char="3967" id="token-34-20" morph="none" pos="word" start_char="3965">for</TOKEN>
<TOKEN end_char="3970" id="token-34-21" morph="none" pos="word" start_char="3969">it</TOKEN>
<TOKEN end_char="3973" id="token-34-22" morph="none" pos="word" start_char="3972">to</TOKEN>
<TOKEN end_char="3978" id="token-34-23" morph="none" pos="word" start_char="3975">give</TOKEN>
<TOKEN end_char="3980" id="token-34-24" morph="none" pos="word" start_char="3980">a</TOKEN>
<TOKEN end_char="3989" id="token-34-25" morph="none" pos="word" start_char="3982">positive</TOKEN>
<TOKEN end_char="3996" id="token-34-26" morph="none" pos="word" start_char="3991">result</TOKEN>
<TOKEN end_char="3999" id="token-34-27" morph="none" pos="word" start_char="3998">at</TOKEN>
<TOKEN end_char="4003" id="token-34-28" morph="none" pos="word" start_char="4001">the</TOKEN>
<TOKEN end_char="4008" id="token-34-29" morph="none" pos="word" start_char="4005">39th</TOKEN>
<TOKEN end_char="4014" id="token-34-30" morph="none" pos="word" start_char="4010">cycle</TOKEN>
<TOKEN end_char="4017" id="token-34-31" morph="none" pos="word" start_char="4016">of</TOKEN>
<TOKEN end_char="4031" id="token-34-32" morph="none" pos="word" start_char="4019">amplification</TOKEN>
<TOKEN end_char="4032" id="token-34-33" morph="none" pos="punct" start_char="4032">.</TOKEN>
</SEG>
<SEG end_char="4219" id="segment-35" start_char="4035">
<ORIGINAL_TEXT>Further tests need to be carried out to conclude that the sample contains SARS-CoV-2, and a finding of that magnitude would need to be replicated separately by independent laboratories.</ORIGINAL_TEXT>
<TOKEN end_char="4041" id="token-35-0" morph="none" pos="word" start_char="4035">Further</TOKEN>
<TOKEN end_char="4047" id="token-35-1" morph="none" pos="word" start_char="4043">tests</TOKEN>
<TOKEN end_char="4052" id="token-35-2" morph="none" pos="word" start_char="4049">need</TOKEN>
<TOKEN end_char="4055" id="token-35-3" morph="none" pos="word" start_char="4054">to</TOKEN>
<TOKEN end_char="4058" id="token-35-4" morph="none" pos="word" start_char="4057">be</TOKEN>
<TOKEN end_char="4066" id="token-35-5" morph="none" pos="word" start_char="4060">carried</TOKEN>
<TOKEN end_char="4070" id="token-35-6" morph="none" pos="word" start_char="4068">out</TOKEN>
<TOKEN end_char="4073" id="token-35-7" morph="none" pos="word" start_char="4072">to</TOKEN>
<TOKEN end_char="4082" id="token-35-8" morph="none" pos="word" start_char="4075">conclude</TOKEN>
<TOKEN end_char="4087" id="token-35-9" morph="none" pos="word" start_char="4084">that</TOKEN>
<TOKEN end_char="4091" id="token-35-10" morph="none" pos="word" start_char="4089">the</TOKEN>
<TOKEN end_char="4098" id="token-35-11" morph="none" pos="word" start_char="4093">sample</TOKEN>
<TOKEN end_char="4107" id="token-35-12" morph="none" pos="word" start_char="4100">contains</TOKEN>
<TOKEN end_char="4118" id="token-35-13" morph="none" pos="unknown" start_char="4109">SARS-CoV-2</TOKEN>
<TOKEN end_char="4119" id="token-35-14" morph="none" pos="punct" start_char="4119">,</TOKEN>
<TOKEN end_char="4123" id="token-35-15" morph="none" pos="word" start_char="4121">and</TOKEN>
<TOKEN end_char="4125" id="token-35-16" morph="none" pos="word" start_char="4125">a</TOKEN>
<TOKEN end_char="4133" id="token-35-17" morph="none" pos="word" start_char="4127">finding</TOKEN>
<TOKEN end_char="4136" id="token-35-18" morph="none" pos="word" start_char="4135">of</TOKEN>
<TOKEN end_char="4141" id="token-35-19" morph="none" pos="word" start_char="4138">that</TOKEN>
<TOKEN end_char="4151" id="token-35-20" morph="none" pos="word" start_char="4143">magnitude</TOKEN>
<TOKEN end_char="4157" id="token-35-21" morph="none" pos="word" start_char="4153">would</TOKEN>
<TOKEN end_char="4162" id="token-35-22" morph="none" pos="word" start_char="4159">need</TOKEN>
<TOKEN end_char="4165" id="token-35-23" morph="none" pos="word" start_char="4164">to</TOKEN>
<TOKEN end_char="4168" id="token-35-24" morph="none" pos="word" start_char="4167">be</TOKEN>
<TOKEN end_char="4179" id="token-35-25" morph="none" pos="word" start_char="4170">replicated</TOKEN>
<TOKEN end_char="4190" id="token-35-26" morph="none" pos="word" start_char="4181">separately</TOKEN>
<TOKEN end_char="4193" id="token-35-27" morph="none" pos="word" start_char="4192">by</TOKEN>
<TOKEN end_char="4205" id="token-35-28" morph="none" pos="word" start_char="4195">independent</TOKEN>
<TOKEN end_char="4218" id="token-35-29" morph="none" pos="word" start_char="4207">laboratories</TOKEN>
<TOKEN end_char="4219" id="token-35-30" morph="none" pos="punct" start_char="4219">.</TOKEN>
</SEG>
<SEG end_char="4246" id="segment-36" start_char="4222">
<ORIGINAL_TEXT>Reasons to be circumspect</ORIGINAL_TEXT>
<TOKEN end_char="4228" id="token-36-0" morph="none" pos="word" start_char="4222">Reasons</TOKEN>
<TOKEN end_char="4231" id="token-36-1" morph="none" pos="word" start_char="4230">to</TOKEN>
<TOKEN end_char="4234" id="token-36-2" morph="none" pos="word" start_char="4233">be</TOKEN>
<TOKEN end_char="4246" id="token-36-3" morph="none" pos="word" start_char="4236">circumspect</TOKEN>
</SEG>
<SEG end_char="4347" id="segment-37" start_char="4250">
<ORIGINAL_TEXT>A curious thing about this finding is that it disagrees with epidemiological data about the virus.</ORIGINAL_TEXT>
<TOKEN end_char="4250" id="token-37-0" morph="none" pos="word" start_char="4250">A</TOKEN>
<TOKEN end_char="4258" id="token-37-1" morph="none" pos="word" start_char="4252">curious</TOKEN>
<TOKEN end_char="4264" id="token-37-2" morph="none" pos="word" start_char="4260">thing</TOKEN>
<TOKEN end_char="4270" id="token-37-3" morph="none" pos="word" start_char="4266">about</TOKEN>
<TOKEN end_char="4275" id="token-37-4" morph="none" pos="word" start_char="4272">this</TOKEN>
<TOKEN end_char="4283" id="token-37-5" morph="none" pos="word" start_char="4277">finding</TOKEN>
<TOKEN end_char="4286" id="token-37-6" morph="none" pos="word" start_char="4285">is</TOKEN>
<TOKEN end_char="4291" id="token-37-7" morph="none" pos="word" start_char="4288">that</TOKEN>
<TOKEN end_char="4294" id="token-37-8" morph="none" pos="word" start_char="4293">it</TOKEN>
<TOKEN end_char="4304" id="token-37-9" morph="none" pos="word" start_char="4296">disagrees</TOKEN>
<TOKEN end_char="4309" id="token-37-10" morph="none" pos="word" start_char="4306">with</TOKEN>
<TOKEN end_char="4325" id="token-37-11" morph="none" pos="word" start_char="4311">epidemiological</TOKEN>
<TOKEN end_char="4330" id="token-37-12" morph="none" pos="word" start_char="4327">data</TOKEN>
<TOKEN end_char="4336" id="token-37-13" morph="none" pos="word" start_char="4332">about</TOKEN>
<TOKEN end_char="4340" id="token-37-14" morph="none" pos="word" start_char="4338">the</TOKEN>
<TOKEN end_char="4346" id="token-37-15" morph="none" pos="word" start_char="4342">virus</TOKEN>
<TOKEN end_char="4347" id="token-37-16" morph="none" pos="punct" start_char="4347">.</TOKEN>
</SEG>
<SEG end_char="4492" id="segment-38" start_char="4349">
<ORIGINAL_TEXT>The authors don’t cite reports of a spike in the number of respiratory disease cases in the local population following the date of the sampling.</ORIGINAL_TEXT>
<TOKEN end_char="4351" id="token-38-0" morph="none" pos="word" start_char="4349">The</TOKEN>
<TOKEN end_char="4359" id="token-38-1" morph="none" pos="word" start_char="4353">authors</TOKEN>
<TOKEN end_char="4365" id="token-38-2" morph="none" pos="word" start_char="4361">don’t</TOKEN>
<TOKEN end_char="4370" id="token-38-3" morph="none" pos="word" start_char="4367">cite</TOKEN>
<TOKEN end_char="4378" id="token-38-4" morph="none" pos="word" start_char="4372">reports</TOKEN>
<TOKEN end_char="4381" id="token-38-5" morph="none" pos="word" start_char="4380">of</TOKEN>
<TOKEN end_char="4383" id="token-38-6" morph="none" pos="word" start_char="4383">a</TOKEN>
<TOKEN end_char="4389" id="token-38-7" morph="none" pos="word" start_char="4385">spike</TOKEN>
<TOKEN end_char="4392" id="token-38-8" morph="none" pos="word" start_char="4391">in</TOKEN>
<TOKEN end_char="4396" id="token-38-9" morph="none" pos="word" start_char="4394">the</TOKEN>
<TOKEN end_char="4403" id="token-38-10" morph="none" pos="word" start_char="4398">number</TOKEN>
<TOKEN end_char="4406" id="token-38-11" morph="none" pos="word" start_char="4405">of</TOKEN>
<TOKEN end_char="4418" id="token-38-12" morph="none" pos="word" start_char="4408">respiratory</TOKEN>
<TOKEN end_char="4426" id="token-38-13" morph="none" pos="word" start_char="4420">disease</TOKEN>
<TOKEN end_char="4432" id="token-38-14" morph="none" pos="word" start_char="4428">cases</TOKEN>
<TOKEN end_char="4435" id="token-38-15" morph="none" pos="word" start_char="4434">in</TOKEN>
<TOKEN end_char="4439" id="token-38-16" morph="none" pos="word" start_char="4437">the</TOKEN>
<TOKEN end_char="4445" id="token-38-17" morph="none" pos="word" start_char="4441">local</TOKEN>
<TOKEN end_char="4456" id="token-38-18" morph="none" pos="word" start_char="4447">population</TOKEN>
<TOKEN end_char="4466" id="token-38-19" morph="none" pos="word" start_char="4458">following</TOKEN>
<TOKEN end_char="4470" id="token-38-20" morph="none" pos="word" start_char="4468">the</TOKEN>
<TOKEN end_char="4475" id="token-38-21" morph="none" pos="word" start_char="4472">date</TOKEN>
<TOKEN end_char="4478" id="token-38-22" morph="none" pos="word" start_char="4477">of</TOKEN>
<TOKEN end_char="4482" id="token-38-23" morph="none" pos="word" start_char="4480">the</TOKEN>
<TOKEN end_char="4491" id="token-38-24" morph="none" pos="word" start_char="4484">sampling</TOKEN>
<TOKEN end_char="4492" id="token-38-25" morph="none" pos="punct" start_char="4492">.</TOKEN>
</SEG>
<SEG end_char="4576" id="segment-39" start_char="4495">
<ORIGINAL_TEXT>Also, we know SARS-CoV-2 to be highly transmissible, at least in its current form.</ORIGINAL_TEXT>
<TOKEN end_char="4498" id="token-39-0" morph="none" pos="word" start_char="4495">Also</TOKEN>
<TOKEN end_char="4499" id="token-39-1" morph="none" pos="punct" start_char="4499">,</TOKEN>
<TOKEN end_char="4502" id="token-39-2" morph="none" pos="word" start_char="4501">we</TOKEN>
<TOKEN end_char="4507" id="token-39-3" morph="none" pos="word" start_char="4504">know</TOKEN>
<TOKEN end_char="4518" id="token-39-4" morph="none" pos="unknown" start_char="4509">SARS-CoV-2</TOKEN>
<TOKEN end_char="4521" id="token-39-5" morph="none" pos="word" start_char="4520">to</TOKEN>
<TOKEN end_char="4524" id="token-39-6" morph="none" pos="word" start_char="4523">be</TOKEN>
<TOKEN end_char="4531" id="token-39-7" morph="none" pos="word" start_char="4526">highly</TOKEN>
<TOKEN end_char="4545" id="token-39-8" morph="none" pos="word" start_char="4533">transmissible</TOKEN>
<TOKEN end_char="4546" id="token-39-9" morph="none" pos="punct" start_char="4546">,</TOKEN>
<TOKEN end_char="4549" id="token-39-10" morph="none" pos="word" start_char="4548">at</TOKEN>
<TOKEN end_char="4555" id="token-39-11" morph="none" pos="word" start_char="4551">least</TOKEN>
<TOKEN end_char="4558" id="token-39-12" morph="none" pos="word" start_char="4557">in</TOKEN>
<TOKEN end_char="4562" id="token-39-13" morph="none" pos="word" start_char="4560">its</TOKEN>
<TOKEN end_char="4570" id="token-39-14" morph="none" pos="word" start_char="4564">current</TOKEN>
<TOKEN end_char="4575" id="token-39-15" morph="none" pos="word" start_char="4572">form</TOKEN>
<TOKEN end_char="4576" id="token-39-16" morph="none" pos="punct" start_char="4576">.</TOKEN>
</SEG>
<SEG end_char="4854" id="segment-40" start_char="4578">
<ORIGINAL_TEXT>If this result is a true positive it suggests the virus was present in the population at a high enough incidence to be detected in an 800ml sample of sewage, but then not present at a high enough incidence to be detected for nine months, when no control measures were in place.</ORIGINAL_TEXT>
<TOKEN end_char="4579" id="token-40-0" morph="none" pos="word" start_char="4578">If</TOKEN>
<TOKEN end_char="4584" id="token-40-1" morph="none" pos="word" start_char="4581">this</TOKEN>
<TOKEN end_char="4591" id="token-40-2" morph="none" pos="word" start_char="4586">result</TOKEN>
<TOKEN end_char="4594" id="token-40-3" morph="none" pos="word" start_char="4593">is</TOKEN>
<TOKEN end_char="4596" id="token-40-4" morph="none" pos="word" start_char="4596">a</TOKEN>
<TOKEN end_char="4601" id="token-40-5" morph="none" pos="word" start_char="4598">true</TOKEN>
<TOKEN end_char="4610" id="token-40-6" morph="none" pos="word" start_char="4603">positive</TOKEN>
<TOKEN end_char="4613" id="token-40-7" morph="none" pos="word" start_char="4612">it</TOKEN>
<TOKEN end_char="4622" id="token-40-8" morph="none" pos="word" start_char="4615">suggests</TOKEN>
<TOKEN end_char="4626" id="token-40-9" morph="none" pos="word" start_char="4624">the</TOKEN>
<TOKEN end_char="4632" id="token-40-10" morph="none" pos="word" start_char="4628">virus</TOKEN>
<TOKEN end_char="4636" id="token-40-11" morph="none" pos="word" start_char="4634">was</TOKEN>
<TOKEN end_char="4644" id="token-40-12" morph="none" pos="word" start_char="4638">present</TOKEN>
<TOKEN end_char="4647" id="token-40-13" morph="none" pos="word" start_char="4646">in</TOKEN>
<TOKEN end_char="4651" id="token-40-14" morph="none" pos="word" start_char="4649">the</TOKEN>
<TOKEN end_char="4662" id="token-40-15" morph="none" pos="word" start_char="4653">population</TOKEN>
<TOKEN end_char="4665" id="token-40-16" morph="none" pos="word" start_char="4664">at</TOKEN>
<TOKEN end_char="4667" id="token-40-17" morph="none" pos="word" start_char="4667">a</TOKEN>
<TOKEN end_char="4672" id="token-40-18" morph="none" pos="word" start_char="4669">high</TOKEN>
<TOKEN end_char="4679" id="token-40-19" morph="none" pos="word" start_char="4674">enough</TOKEN>
<TOKEN end_char="4689" id="token-40-20" morph="none" pos="word" start_char="4681">incidence</TOKEN>
<TOKEN end_char="4692" id="token-40-21" morph="none" pos="word" start_char="4691">to</TOKEN>
<TOKEN end_char="4695" id="token-40-22" morph="none" pos="word" start_char="4694">be</TOKEN>
<TOKEN end_char="4704" id="token-40-23" morph="none" pos="word" start_char="4697">detected</TOKEN>
<TOKEN end_char="4707" id="token-40-24" morph="none" pos="word" start_char="4706">in</TOKEN>
<TOKEN end_char="4710" id="token-40-25" morph="none" pos="word" start_char="4709">an</TOKEN>
<TOKEN end_char="4716" id="token-40-26" morph="none" pos="word" start_char="4712">800ml</TOKEN>
<TOKEN end_char="4723" id="token-40-27" morph="none" pos="word" start_char="4718">sample</TOKEN>
<TOKEN end_char="4726" id="token-40-28" morph="none" pos="word" start_char="4725">of</TOKEN>
<TOKEN end_char="4733" id="token-40-29" morph="none" pos="word" start_char="4728">sewage</TOKEN>
<TOKEN end_char="4734" id="token-40-30" morph="none" pos="punct" start_char="4734">,</TOKEN>
<TOKEN end_char="4738" id="token-40-31" morph="none" pos="word" start_char="4736">but</TOKEN>
<TOKEN end_char="4743" id="token-40-32" morph="none" pos="word" start_char="4740">then</TOKEN>
<TOKEN end_char="4747" id="token-40-33" morph="none" pos="word" start_char="4745">not</TOKEN>
<TOKEN end_char="4755" id="token-40-34" morph="none" pos="word" start_char="4749">present</TOKEN>
<TOKEN end_char="4758" id="token-40-35" morph="none" pos="word" start_char="4757">at</TOKEN>
<TOKEN end_char="4760" id="token-40-36" morph="none" pos="word" start_char="4760">a</TOKEN>
<TOKEN end_char="4765" id="token-40-37" morph="none" pos="word" start_char="4762">high</TOKEN>
<TOKEN end_char="4772" id="token-40-38" morph="none" pos="word" start_char="4767">enough</TOKEN>
<TOKEN end_char="4782" id="token-40-39" morph="none" pos="word" start_char="4774">incidence</TOKEN>
<TOKEN end_char="4785" id="token-40-40" morph="none" pos="word" start_char="4784">to</TOKEN>
<TOKEN end_char="4788" id="token-40-41" morph="none" pos="word" start_char="4787">be</TOKEN>
<TOKEN end_char="4797" id="token-40-42" morph="none" pos="word" start_char="4790">detected</TOKEN>
<TOKEN end_char="4801" id="token-40-43" morph="none" pos="word" start_char="4799">for</TOKEN>
<TOKEN end_char="4806" id="token-40-44" morph="none" pos="word" start_char="4803">nine</TOKEN>
<TOKEN end_char="4813" id="token-40-45" morph="none" pos="word" start_char="4808">months</TOKEN>
<TOKEN end_char="4814" id="token-40-46" morph="none" pos="punct" start_char="4814">,</TOKEN>
<TOKEN end_char="4819" id="token-40-47" morph="none" pos="word" start_char="4816">when</TOKEN>
<TOKEN end_char="4822" id="token-40-48" morph="none" pos="word" start_char="4821">no</TOKEN>
<TOKEN end_char="4830" id="token-40-49" morph="none" pos="word" start_char="4824">control</TOKEN>
<TOKEN end_char="4839" id="token-40-50" morph="none" pos="word" start_char="4832">measures</TOKEN>
<TOKEN end_char="4844" id="token-40-51" morph="none" pos="word" start_char="4841">were</TOKEN>
<TOKEN end_char="4847" id="token-40-52" morph="none" pos="word" start_char="4846">in</TOKEN>
<TOKEN end_char="4853" id="token-40-53" morph="none" pos="word" start_char="4849">place</TOKEN>
<TOKEN end_char="4854" id="token-40-54" morph="none" pos="punct" start_char="4854">.</TOKEN>
</SEG>
<SEG end_char="4945" id="segment-41" start_char="4857">
<ORIGINAL_TEXT>So, until further studies are carried out, it is best not to draw definitive conclusions.</ORIGINAL_TEXT>
<TOKEN end_char="4858" id="token-41-0" morph="none" pos="word" start_char="4857">So</TOKEN>
<TOKEN end_char="4859" id="token-41-1" morph="none" pos="punct" start_char="4859">,</TOKEN>
<TOKEN end_char="4865" id="token-41-2" morph="none" pos="word" start_char="4861">until</TOKEN>
<TOKEN end_char="4873" id="token-41-3" morph="none" pos="word" start_char="4867">further</TOKEN>
<TOKEN end_char="4881" id="token-41-4" morph="none" pos="word" start_char="4875">studies</TOKEN>
<TOKEN end_char="4885" id="token-41-5" morph="none" pos="word" start_char="4883">are</TOKEN>
<TOKEN end_char="4893" id="token-41-6" morph="none" pos="word" start_char="4887">carried</TOKEN>
<TOKEN end_char="4897" id="token-41-7" morph="none" pos="word" start_char="4895">out</TOKEN>
<TOKEN end_char="4898" id="token-41-8" morph="none" pos="punct" start_char="4898">,</TOKEN>
<TOKEN end_char="4901" id="token-41-9" morph="none" pos="word" start_char="4900">it</TOKEN>
<TOKEN end_char="4904" id="token-41-10" morph="none" pos="word" start_char="4903">is</TOKEN>
<TOKEN end_char="4909" id="token-41-11" morph="none" pos="word" start_char="4906">best</TOKEN>
<TOKEN end_char="4913" id="token-41-12" morph="none" pos="word" start_char="4911">not</TOKEN>
<TOKEN end_char="4916" id="token-41-13" morph="none" pos="word" start_char="4915">to</TOKEN>
<TOKEN end_char="4921" id="token-41-14" morph="none" pos="word" start_char="4918">draw</TOKEN>
<TOKEN end_char="4932" id="token-41-15" morph="none" pos="word" start_char="4923">definitive</TOKEN>
<TOKEN end_char="4944" id="token-41-16" morph="none" pos="word" start_char="4934">conclusions</TOKEN>
<TOKEN end_char="4945" id="token-41-17" morph="none" pos="punct" start_char="4945">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>