<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CABF" lang="spa" raw_text_char_length="2915" raw_text_md5="d83a4af873ec9e1add6aefdd51973a37" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="46" id="segment-0" start_char="1">
<ORIGINAL_TEXT>When was the first case of Covid in the world?</ORIGINAL_TEXT>
<TOKEN end_char="4" id="token-0-0" morph="none" pos="word" start_char="1">When</TOKEN>
<TOKEN end_char="8" id="token-0-1" morph="none" pos="word" start_char="6">was</TOKEN>
<TOKEN end_char="12" id="token-0-2" morph="none" pos="word" start_char="10">the</TOKEN>
<TOKEN end_char="18" id="token-0-3" morph="none" pos="word" start_char="14">first</TOKEN>
<TOKEN end_char="23" id="token-0-4" morph="none" pos="word" start_char="20">case</TOKEN>
<TOKEN end_char="26" id="token-0-5" morph="none" pos="word" start_char="25">of</TOKEN>
<TOKEN end_char="32" id="token-0-6" morph="none" pos="word" start_char="28">Covid</TOKEN>
<TOKEN end_char="35" id="token-0-7" morph="none" pos="word" start_char="34">in</TOKEN>
<TOKEN end_char="39" id="token-0-8" morph="none" pos="word" start_char="37">the</TOKEN>
<TOKEN end_char="45" id="token-0-9" morph="none" pos="word" start_char="41">world</TOKEN>
<TOKEN end_char="46" id="token-0-10" morph="none" pos="punct" start_char="46">?</TOKEN>
</SEG>
<SEG end_char="127" id="segment-1" start_char="51">
<ORIGINAL_TEXT>Coronavirus may have been around longer than we think (Picture: Getty Images)</ORIGINAL_TEXT>
<TOKEN end_char="61" id="token-1-0" morph="none" pos="word" start_char="51">Coronavirus</TOKEN>
<TOKEN end_char="65" id="token-1-1" morph="none" pos="word" start_char="63">may</TOKEN>
<TOKEN end_char="70" id="token-1-2" morph="none" pos="word" start_char="67">have</TOKEN>
<TOKEN end_char="75" id="token-1-3" morph="none" pos="word" start_char="72">been</TOKEN>
<TOKEN end_char="82" id="token-1-4" morph="none" pos="word" start_char="77">around</TOKEN>
<TOKEN end_char="89" id="token-1-5" morph="none" pos="word" start_char="84">longer</TOKEN>
<TOKEN end_char="94" id="token-1-6" morph="none" pos="word" start_char="91">than</TOKEN>
<TOKEN end_char="97" id="token-1-7" morph="none" pos="word" start_char="96">we</TOKEN>
<TOKEN end_char="103" id="token-1-8" morph="none" pos="word" start_char="99">think</TOKEN>
<TOKEN end_char="105" id="token-1-9" morph="none" pos="punct" start_char="105">(</TOKEN>
<TOKEN end_char="112" id="token-1-10" morph="none" pos="word" start_char="106">Picture</TOKEN>
<TOKEN end_char="113" id="token-1-11" morph="none" pos="punct" start_char="113">:</TOKEN>
<TOKEN end_char="119" id="token-1-12" morph="none" pos="word" start_char="115">Getty</TOKEN>
<TOKEN end_char="126" id="token-1-13" morph="none" pos="word" start_char="121">Images</TOKEN>
<TOKEN end_char="127" id="token-1-14" morph="none" pos="punct" start_char="127">)</TOKEN>
</SEG>
<SEG end_char="277" id="segment-2" start_char="131">
<ORIGINAL_TEXT>Coronavirus has become such a big part of life in 2020 it’s hard to imagine that just 12 months ago nobody had any idea what Covid-19 actually was.</ORIGINAL_TEXT>
<TOKEN end_char="141" id="token-2-0" morph="none" pos="word" start_char="131">Coronavirus</TOKEN>
<TOKEN end_char="145" id="token-2-1" morph="none" pos="word" start_char="143">has</TOKEN>
<TOKEN end_char="152" id="token-2-2" morph="none" pos="word" start_char="147">become</TOKEN>
<TOKEN end_char="157" id="token-2-3" morph="none" pos="word" start_char="154">such</TOKEN>
<TOKEN end_char="159" id="token-2-4" morph="none" pos="word" start_char="159">a</TOKEN>
<TOKEN end_char="163" id="token-2-5" morph="none" pos="word" start_char="161">big</TOKEN>
<TOKEN end_char="168" id="token-2-6" morph="none" pos="word" start_char="165">part</TOKEN>
<TOKEN end_char="171" id="token-2-7" morph="none" pos="word" start_char="170">of</TOKEN>
<TOKEN end_char="176" id="token-2-8" morph="none" pos="word" start_char="173">life</TOKEN>
<TOKEN end_char="179" id="token-2-9" morph="none" pos="word" start_char="178">in</TOKEN>
<TOKEN end_char="184" id="token-2-10" morph="none" pos="word" start_char="181">2020</TOKEN>
<TOKEN end_char="189" id="token-2-11" morph="none" pos="word" start_char="186">it’s</TOKEN>
<TOKEN end_char="194" id="token-2-12" morph="none" pos="word" start_char="191">hard</TOKEN>
<TOKEN end_char="197" id="token-2-13" morph="none" pos="word" start_char="196">to</TOKEN>
<TOKEN end_char="205" id="token-2-14" morph="none" pos="word" start_char="199">imagine</TOKEN>
<TOKEN end_char="210" id="token-2-15" morph="none" pos="word" start_char="207">that</TOKEN>
<TOKEN end_char="215" id="token-2-16" morph="none" pos="word" start_char="212">just</TOKEN>
<TOKEN end_char="218" id="token-2-17" morph="none" pos="word" start_char="217">12</TOKEN>
<TOKEN end_char="225" id="token-2-18" morph="none" pos="word" start_char="220">months</TOKEN>
<TOKEN end_char="229" id="token-2-19" morph="none" pos="word" start_char="227">ago</TOKEN>
<TOKEN end_char="236" id="token-2-20" morph="none" pos="word" start_char="231">nobody</TOKEN>
<TOKEN end_char="240" id="token-2-21" morph="none" pos="word" start_char="238">had</TOKEN>
<TOKEN end_char="244" id="token-2-22" morph="none" pos="word" start_char="242">any</TOKEN>
<TOKEN end_char="249" id="token-2-23" morph="none" pos="word" start_char="246">idea</TOKEN>
<TOKEN end_char="254" id="token-2-24" morph="none" pos="word" start_char="251">what</TOKEN>
<TOKEN end_char="263" id="token-2-25" morph="none" pos="unknown" start_char="256">Covid-19</TOKEN>
<TOKEN end_char="272" id="token-2-26" morph="none" pos="word" start_char="265">actually</TOKEN>
<TOKEN end_char="276" id="token-2-27" morph="none" pos="word" start_char="274">was</TOKEN>
<TOKEN end_char="277" id="token-2-28" morph="none" pos="punct" start_char="277">.</TOKEN>
</SEG>
<SEG end_char="419" id="segment-3" start_char="280">
<ORIGINAL_TEXT>And yet it’s already been a year since what is now thought to be the first documented case – coming a month earlier than originally thought.</ORIGINAL_TEXT>
<TOKEN end_char="282" id="token-3-0" morph="none" pos="word" start_char="280">And</TOKEN>
<TOKEN end_char="286" id="token-3-1" morph="none" pos="word" start_char="284">yet</TOKEN>
<TOKEN end_char="291" id="token-3-2" morph="none" pos="word" start_char="288">it’s</TOKEN>
<TOKEN end_char="299" id="token-3-3" morph="none" pos="word" start_char="293">already</TOKEN>
<TOKEN end_char="304" id="token-3-4" morph="none" pos="word" start_char="301">been</TOKEN>
<TOKEN end_char="306" id="token-3-5" morph="none" pos="word" start_char="306">a</TOKEN>
<TOKEN end_char="311" id="token-3-6" morph="none" pos="word" start_char="308">year</TOKEN>
<TOKEN end_char="317" id="token-3-7" morph="none" pos="word" start_char="313">since</TOKEN>
<TOKEN end_char="322" id="token-3-8" morph="none" pos="word" start_char="319">what</TOKEN>
<TOKEN end_char="325" id="token-3-9" morph="none" pos="word" start_char="324">is</TOKEN>
<TOKEN end_char="329" id="token-3-10" morph="none" pos="word" start_char="327">now</TOKEN>
<TOKEN end_char="337" id="token-3-11" morph="none" pos="word" start_char="331">thought</TOKEN>
<TOKEN end_char="340" id="token-3-12" morph="none" pos="word" start_char="339">to</TOKEN>
<TOKEN end_char="343" id="token-3-13" morph="none" pos="word" start_char="342">be</TOKEN>
<TOKEN end_char="347" id="token-3-14" morph="none" pos="word" start_char="345">the</TOKEN>
<TOKEN end_char="353" id="token-3-15" morph="none" pos="word" start_char="349">first</TOKEN>
<TOKEN end_char="364" id="token-3-16" morph="none" pos="word" start_char="355">documented</TOKEN>
<TOKEN end_char="369" id="token-3-17" morph="none" pos="word" start_char="366">case</TOKEN>
<TOKEN end_char="371" id="token-3-18" morph="none" pos="punct" start_char="371">–</TOKEN>
<TOKEN end_char="378" id="token-3-19" morph="none" pos="word" start_char="373">coming</TOKEN>
<TOKEN end_char="380" id="token-3-20" morph="none" pos="word" start_char="380">a</TOKEN>
<TOKEN end_char="386" id="token-3-21" morph="none" pos="word" start_char="382">month</TOKEN>
<TOKEN end_char="394" id="token-3-22" morph="none" pos="word" start_char="388">earlier</TOKEN>
<TOKEN end_char="399" id="token-3-23" morph="none" pos="word" start_char="396">than</TOKEN>
<TOKEN end_char="410" id="token-3-24" morph="none" pos="word" start_char="401">originally</TOKEN>
<TOKEN end_char="418" id="token-3-25" morph="none" pos="word" start_char="412">thought</TOKEN>
<TOKEN end_char="419" id="token-3-26" morph="none" pos="punct" start_char="419">.</TOKEN>
</SEG>
<SEG end_char="564" id="segment-4" start_char="422">
<ORIGINAL_TEXT>So just when and where was the first case of Covid-19 in the world thought to have been – and how long was it before cases first came to light?</ORIGINAL_TEXT>
<TOKEN end_char="423" id="token-4-0" morph="none" pos="word" start_char="422">So</TOKEN>
<TOKEN end_char="428" id="token-4-1" morph="none" pos="word" start_char="425">just</TOKEN>
<TOKEN end_char="433" id="token-4-2" morph="none" pos="word" start_char="430">when</TOKEN>
<TOKEN end_char="437" id="token-4-3" morph="none" pos="word" start_char="435">and</TOKEN>
<TOKEN end_char="443" id="token-4-4" morph="none" pos="word" start_char="439">where</TOKEN>
<TOKEN end_char="447" id="token-4-5" morph="none" pos="word" start_char="445">was</TOKEN>
<TOKEN end_char="451" id="token-4-6" morph="none" pos="word" start_char="449">the</TOKEN>
<TOKEN end_char="457" id="token-4-7" morph="none" pos="word" start_char="453">first</TOKEN>
<TOKEN end_char="462" id="token-4-8" morph="none" pos="word" start_char="459">case</TOKEN>
<TOKEN end_char="465" id="token-4-9" morph="none" pos="word" start_char="464">of</TOKEN>
<TOKEN end_char="474" id="token-4-10" morph="none" pos="unknown" start_char="467">Covid-19</TOKEN>
<TOKEN end_char="477" id="token-4-11" morph="none" pos="word" start_char="476">in</TOKEN>
<TOKEN end_char="481" id="token-4-12" morph="none" pos="word" start_char="479">the</TOKEN>
<TOKEN end_char="487" id="token-4-13" morph="none" pos="word" start_char="483">world</TOKEN>
<TOKEN end_char="495" id="token-4-14" morph="none" pos="word" start_char="489">thought</TOKEN>
<TOKEN end_char="498" id="token-4-15" morph="none" pos="word" start_char="497">to</TOKEN>
<TOKEN end_char="503" id="token-4-16" morph="none" pos="word" start_char="500">have</TOKEN>
<TOKEN end_char="508" id="token-4-17" morph="none" pos="word" start_char="505">been</TOKEN>
<TOKEN end_char="510" id="token-4-18" morph="none" pos="punct" start_char="510">–</TOKEN>
<TOKEN end_char="514" id="token-4-19" morph="none" pos="word" start_char="512">and</TOKEN>
<TOKEN end_char="518" id="token-4-20" morph="none" pos="word" start_char="516">how</TOKEN>
<TOKEN end_char="523" id="token-4-21" morph="none" pos="word" start_char="520">long</TOKEN>
<TOKEN end_char="527" id="token-4-22" morph="none" pos="word" start_char="525">was</TOKEN>
<TOKEN end_char="530" id="token-4-23" morph="none" pos="word" start_char="529">it</TOKEN>
<TOKEN end_char="537" id="token-4-24" morph="none" pos="word" start_char="532">before</TOKEN>
<TOKEN end_char="543" id="token-4-25" morph="none" pos="word" start_char="539">cases</TOKEN>
<TOKEN end_char="549" id="token-4-26" morph="none" pos="word" start_char="545">first</TOKEN>
<TOKEN end_char="554" id="token-4-27" morph="none" pos="word" start_char="551">came</TOKEN>
<TOKEN end_char="557" id="token-4-28" morph="none" pos="word" start_char="556">to</TOKEN>
<TOKEN end_char="563" id="token-4-29" morph="none" pos="word" start_char="559">light</TOKEN>
<TOKEN end_char="564" id="token-4-30" morph="none" pos="punct" start_char="564">?</TOKEN>
</SEG>
<SEG end_char="595" id="segment-5" start_char="567">
<ORIGINAL_TEXT>Here’s what you need to know…</ORIGINAL_TEXT>
<TOKEN end_char="572" id="token-5-0" morph="none" pos="word" start_char="567">Here’s</TOKEN>
<TOKEN end_char="577" id="token-5-1" morph="none" pos="word" start_char="574">what</TOKEN>
<TOKEN end_char="581" id="token-5-2" morph="none" pos="word" start_char="579">you</TOKEN>
<TOKEN end_char="586" id="token-5-3" morph="none" pos="word" start_char="583">need</TOKEN>
<TOKEN end_char="589" id="token-5-4" morph="none" pos="word" start_char="588">to</TOKEN>
<TOKEN end_char="594" id="token-5-5" morph="none" pos="word" start_char="591">know</TOKEN>
<TOKEN end_char="595" id="token-5-6" morph="none" pos="punct" start_char="595">…</TOKEN>
</SEG>
<SEG end_char="644" id="segment-6" start_char="599">
<ORIGINAL_TEXT>When was the first case of Covid in the world?</ORIGINAL_TEXT>
<TOKEN end_char="602" id="token-6-0" morph="none" pos="word" start_char="599">When</TOKEN>
<TOKEN end_char="606" id="token-6-1" morph="none" pos="word" start_char="604">was</TOKEN>
<TOKEN end_char="610" id="token-6-2" morph="none" pos="word" start_char="608">the</TOKEN>
<TOKEN end_char="616" id="token-6-3" morph="none" pos="word" start_char="612">first</TOKEN>
<TOKEN end_char="621" id="token-6-4" morph="none" pos="word" start_char="618">case</TOKEN>
<TOKEN end_char="624" id="token-6-5" morph="none" pos="word" start_char="623">of</TOKEN>
<TOKEN end_char="630" id="token-6-6" morph="none" pos="word" start_char="626">Covid</TOKEN>
<TOKEN end_char="633" id="token-6-7" morph="none" pos="word" start_char="632">in</TOKEN>
<TOKEN end_char="637" id="token-6-8" morph="none" pos="word" start_char="635">the</TOKEN>
<TOKEN end_char="643" id="token-6-9" morph="none" pos="word" start_char="639">world</TOKEN>
<TOKEN end_char="644" id="token-6-10" morph="none" pos="punct" start_char="644">?</TOKEN>
</SEG>
<SEG end_char="748" id="segment-7" start_char="648">
<ORIGINAL_TEXT>It’s been reported that the first known case of Covid in the world was detected on November 17, 2019.</ORIGINAL_TEXT>
<TOKEN end_char="651" id="token-7-0" morph="none" pos="word" start_char="648">It’s</TOKEN>
<TOKEN end_char="656" id="token-7-1" morph="none" pos="word" start_char="653">been</TOKEN>
<TOKEN end_char="665" id="token-7-2" morph="none" pos="word" start_char="658">reported</TOKEN>
<TOKEN end_char="670" id="token-7-3" morph="none" pos="word" start_char="667">that</TOKEN>
<TOKEN end_char="674" id="token-7-4" morph="none" pos="word" start_char="672">the</TOKEN>
<TOKEN end_char="680" id="token-7-5" morph="none" pos="word" start_char="676">first</TOKEN>
<TOKEN end_char="686" id="token-7-6" morph="none" pos="word" start_char="682">known</TOKEN>
<TOKEN end_char="691" id="token-7-7" morph="none" pos="word" start_char="688">case</TOKEN>
<TOKEN end_char="694" id="token-7-8" morph="none" pos="word" start_char="693">of</TOKEN>
<TOKEN end_char="700" id="token-7-9" morph="none" pos="word" start_char="696">Covid</TOKEN>
<TOKEN end_char="703" id="token-7-10" morph="none" pos="word" start_char="702">in</TOKEN>
<TOKEN end_char="707" id="token-7-11" morph="none" pos="word" start_char="705">the</TOKEN>
<TOKEN end_char="713" id="token-7-12" morph="none" pos="word" start_char="709">world</TOKEN>
<TOKEN end_char="717" id="token-7-13" morph="none" pos="word" start_char="715">was</TOKEN>
<TOKEN end_char="726" id="token-7-14" morph="none" pos="word" start_char="719">detected</TOKEN>
<TOKEN end_char="729" id="token-7-15" morph="none" pos="word" start_char="728">on</TOKEN>
<TOKEN end_char="738" id="token-7-16" morph="none" pos="word" start_char="731">November</TOKEN>
<TOKEN end_char="741" id="token-7-17" morph="none" pos="word" start_char="740">17</TOKEN>
<TOKEN end_char="742" id="token-7-18" morph="none" pos="punct" start_char="742">,</TOKEN>
<TOKEN end_char="747" id="token-7-19" morph="none" pos="word" start_char="744">2019</TOKEN>
<TOKEN end_char="748" id="token-7-20" morph="none" pos="punct" start_char="748">.</TOKEN>
</SEG>
<SEG end_char="924" id="segment-8" start_char="751">
<ORIGINAL_TEXT>According to the report from the South China Morning Post, a 55-year-old from Hubei Province in China may have been the first person to contract the potentially lethal virus.</ORIGINAL_TEXT>
<TOKEN end_char="759" id="token-8-0" morph="none" pos="word" start_char="751">According</TOKEN>
<TOKEN end_char="762" id="token-8-1" morph="none" pos="word" start_char="761">to</TOKEN>
<TOKEN end_char="766" id="token-8-2" morph="none" pos="word" start_char="764">the</TOKEN>
<TOKEN end_char="773" id="token-8-3" morph="none" pos="word" start_char="768">report</TOKEN>
<TOKEN end_char="778" id="token-8-4" morph="none" pos="word" start_char="775">from</TOKEN>
<TOKEN end_char="782" id="token-8-5" morph="none" pos="word" start_char="780">the</TOKEN>
<TOKEN end_char="788" id="token-8-6" morph="none" pos="word" start_char="784">South</TOKEN>
<TOKEN end_char="794" id="token-8-7" morph="none" pos="word" start_char="790">China</TOKEN>
<TOKEN end_char="802" id="token-8-8" morph="none" pos="word" start_char="796">Morning</TOKEN>
<TOKEN end_char="807" id="token-8-9" morph="none" pos="word" start_char="804">Post</TOKEN>
<TOKEN end_char="808" id="token-8-10" morph="none" pos="punct" start_char="808">,</TOKEN>
<TOKEN end_char="810" id="token-8-11" morph="none" pos="word" start_char="810">a</TOKEN>
<TOKEN end_char="822" id="token-8-12" morph="none" pos="unknown" start_char="812">55-year-old</TOKEN>
<TOKEN end_char="827" id="token-8-13" morph="none" pos="word" start_char="824">from</TOKEN>
<TOKEN end_char="833" id="token-8-14" morph="none" pos="word" start_char="829">Hubei</TOKEN>
<TOKEN end_char="842" id="token-8-15" morph="none" pos="word" start_char="835">Province</TOKEN>
<TOKEN end_char="845" id="token-8-16" morph="none" pos="word" start_char="844">in</TOKEN>
<TOKEN end_char="851" id="token-8-17" morph="none" pos="word" start_char="847">China</TOKEN>
<TOKEN end_char="855" id="token-8-18" morph="none" pos="word" start_char="853">may</TOKEN>
<TOKEN end_char="860" id="token-8-19" morph="none" pos="word" start_char="857">have</TOKEN>
<TOKEN end_char="865" id="token-8-20" morph="none" pos="word" start_char="862">been</TOKEN>
<TOKEN end_char="869" id="token-8-21" morph="none" pos="word" start_char="867">the</TOKEN>
<TOKEN end_char="875" id="token-8-22" morph="none" pos="word" start_char="871">first</TOKEN>
<TOKEN end_char="882" id="token-8-23" morph="none" pos="word" start_char="877">person</TOKEN>
<TOKEN end_char="885" id="token-8-24" morph="none" pos="word" start_char="884">to</TOKEN>
<TOKEN end_char="894" id="token-8-25" morph="none" pos="word" start_char="887">contract</TOKEN>
<TOKEN end_char="898" id="token-8-26" morph="none" pos="word" start_char="896">the</TOKEN>
<TOKEN end_char="910" id="token-8-27" morph="none" pos="word" start_char="900">potentially</TOKEN>
<TOKEN end_char="917" id="token-8-28" morph="none" pos="word" start_char="912">lethal</TOKEN>
<TOKEN end_char="923" id="token-8-29" morph="none" pos="word" start_char="919">virus</TOKEN>
<TOKEN end_char="924" id="token-8-30" morph="none" pos="punct" start_char="924">.</TOKEN>
</SEG>
<SEG end_char="1013" id="segment-9" start_char="927">
<ORIGINAL_TEXT>However it was a month before reports began emerging of cases being confirmed in Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="933" id="token-9-0" morph="none" pos="word" start_char="927">However</TOKEN>
<TOKEN end_char="936" id="token-9-1" morph="none" pos="word" start_char="935">it</TOKEN>
<TOKEN end_char="940" id="token-9-2" morph="none" pos="word" start_char="938">was</TOKEN>
<TOKEN end_char="942" id="token-9-3" morph="none" pos="word" start_char="942">a</TOKEN>
<TOKEN end_char="948" id="token-9-4" morph="none" pos="word" start_char="944">month</TOKEN>
<TOKEN end_char="955" id="token-9-5" morph="none" pos="word" start_char="950">before</TOKEN>
<TOKEN end_char="963" id="token-9-6" morph="none" pos="word" start_char="957">reports</TOKEN>
<TOKEN end_char="969" id="token-9-7" morph="none" pos="word" start_char="965">began</TOKEN>
<TOKEN end_char="978" id="token-9-8" morph="none" pos="word" start_char="971">emerging</TOKEN>
<TOKEN end_char="981" id="token-9-9" morph="none" pos="word" start_char="980">of</TOKEN>
<TOKEN end_char="987" id="token-9-10" morph="none" pos="word" start_char="983">cases</TOKEN>
<TOKEN end_char="993" id="token-9-11" morph="none" pos="word" start_char="989">being</TOKEN>
<TOKEN end_char="1003" id="token-9-12" morph="none" pos="word" start_char="995">confirmed</TOKEN>
<TOKEN end_char="1006" id="token-9-13" morph="none" pos="word" start_char="1005">in</TOKEN>
<TOKEN end_char="1012" id="token-9-14" morph="none" pos="word" start_char="1008">Wuhan</TOKEN>
<TOKEN end_char="1013" id="token-9-15" morph="none" pos="punct" start_char="1013">.</TOKEN>
</SEG>
<SEG end_char="1126" id="segment-10" start_char="1017">
<ORIGINAL_TEXT>To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video</ORIGINAL_TEXT>
<TOKEN end_char="1018" id="token-10-0" morph="none" pos="word" start_char="1017">To</TOKEN>
<TOKEN end_char="1023" id="token-10-1" morph="none" pos="word" start_char="1020">view</TOKEN>
<TOKEN end_char="1028" id="token-10-2" morph="none" pos="word" start_char="1025">this</TOKEN>
<TOKEN end_char="1034" id="token-10-3" morph="none" pos="word" start_char="1030">video</TOKEN>
<TOKEN end_char="1041" id="token-10-4" morph="none" pos="word" start_char="1036">please</TOKEN>
<TOKEN end_char="1048" id="token-10-5" morph="none" pos="word" start_char="1043">enable</TOKEN>
<TOKEN end_char="1059" id="token-10-6" morph="none" pos="word" start_char="1050">JavaScript</TOKEN>
<TOKEN end_char="1060" id="token-10-7" morph="none" pos="punct" start_char="1060">,</TOKEN>
<TOKEN end_char="1064" id="token-10-8" morph="none" pos="word" start_char="1062">and</TOKEN>
<TOKEN end_char="1073" id="token-10-9" morph="none" pos="word" start_char="1066">consider</TOKEN>
<TOKEN end_char="1083" id="token-10-10" morph="none" pos="word" start_char="1075">upgrading</TOKEN>
<TOKEN end_char="1086" id="token-10-11" morph="none" pos="word" start_char="1085">to</TOKEN>
<TOKEN end_char="1088" id="token-10-12" morph="none" pos="word" start_char="1088">a</TOKEN>
<TOKEN end_char="1092" id="token-10-13" morph="none" pos="word" start_char="1090">web</TOKEN>
<TOKEN end_char="1100" id="token-10-14" morph="none" pos="word" start_char="1094">browser</TOKEN>
<TOKEN end_char="1105" id="token-10-15" morph="none" pos="word" start_char="1102">that</TOKEN>
<TOKEN end_char="1114" id="token-10-16" morph="none" pos="word" start_char="1107">supports</TOKEN>
<TOKEN end_char="1120" id="token-10-17" morph="none" pos="word" start_char="1116">HTML5</TOKEN>
<TOKEN end_char="1126" id="token-10-18" morph="none" pos="word" start_char="1122">video</TOKEN>
</SEG>
<SEG end_char="1370" id="segment-11" start_char="1130">
<ORIGINAL_TEXT>A report back in March stated that between November 17 and the start of December last year, around one to five new cases were reported every day – with 27 having been reported by December 15 – and the number reaching 60 just five days later.</ORIGINAL_TEXT>
<TOKEN end_char="1130" id="token-11-0" morph="none" pos="word" start_char="1130">A</TOKEN>
<TOKEN end_char="1137" id="token-11-1" morph="none" pos="word" start_char="1132">report</TOKEN>
<TOKEN end_char="1142" id="token-11-2" morph="none" pos="word" start_char="1139">back</TOKEN>
<TOKEN end_char="1145" id="token-11-3" morph="none" pos="word" start_char="1144">in</TOKEN>
<TOKEN end_char="1151" id="token-11-4" morph="none" pos="word" start_char="1147">March</TOKEN>
<TOKEN end_char="1158" id="token-11-5" morph="none" pos="word" start_char="1153">stated</TOKEN>
<TOKEN end_char="1163" id="token-11-6" morph="none" pos="word" start_char="1160">that</TOKEN>
<TOKEN end_char="1171" id="token-11-7" morph="none" pos="word" start_char="1165">between</TOKEN>
<TOKEN end_char="1180" id="token-11-8" morph="none" pos="word" start_char="1173">November</TOKEN>
<TOKEN end_char="1183" id="token-11-9" morph="none" pos="word" start_char="1182">17</TOKEN>
<TOKEN end_char="1187" id="token-11-10" morph="none" pos="word" start_char="1185">and</TOKEN>
<TOKEN end_char="1191" id="token-11-11" morph="none" pos="word" start_char="1189">the</TOKEN>
<TOKEN end_char="1197" id="token-11-12" morph="none" pos="word" start_char="1193">start</TOKEN>
<TOKEN end_char="1200" id="token-11-13" morph="none" pos="word" start_char="1199">of</TOKEN>
<TOKEN end_char="1209" id="token-11-14" morph="none" pos="word" start_char="1202">December</TOKEN>
<TOKEN end_char="1214" id="token-11-15" morph="none" pos="word" start_char="1211">last</TOKEN>
<TOKEN end_char="1219" id="token-11-16" morph="none" pos="word" start_char="1216">year</TOKEN>
<TOKEN end_char="1220" id="token-11-17" morph="none" pos="punct" start_char="1220">,</TOKEN>
<TOKEN end_char="1227" id="token-11-18" morph="none" pos="word" start_char="1222">around</TOKEN>
<TOKEN end_char="1231" id="token-11-19" morph="none" pos="word" start_char="1229">one</TOKEN>
<TOKEN end_char="1234" id="token-11-20" morph="none" pos="word" start_char="1233">to</TOKEN>
<TOKEN end_char="1239" id="token-11-21" morph="none" pos="word" start_char="1236">five</TOKEN>
<TOKEN end_char="1243" id="token-11-22" morph="none" pos="word" start_char="1241">new</TOKEN>
<TOKEN end_char="1249" id="token-11-23" morph="none" pos="word" start_char="1245">cases</TOKEN>
<TOKEN end_char="1254" id="token-11-24" morph="none" pos="word" start_char="1251">were</TOKEN>
<TOKEN end_char="1263" id="token-11-25" morph="none" pos="word" start_char="1256">reported</TOKEN>
<TOKEN end_char="1269" id="token-11-26" morph="none" pos="word" start_char="1265">every</TOKEN>
<TOKEN end_char="1273" id="token-11-27" morph="none" pos="word" start_char="1271">day</TOKEN>
<TOKEN end_char="1275" id="token-11-28" morph="none" pos="punct" start_char="1275">–</TOKEN>
<TOKEN end_char="1280" id="token-11-29" morph="none" pos="word" start_char="1277">with</TOKEN>
<TOKEN end_char="1283" id="token-11-30" morph="none" pos="word" start_char="1282">27</TOKEN>
<TOKEN end_char="1290" id="token-11-31" morph="none" pos="word" start_char="1285">having</TOKEN>
<TOKEN end_char="1295" id="token-11-32" morph="none" pos="word" start_char="1292">been</TOKEN>
<TOKEN end_char="1304" id="token-11-33" morph="none" pos="word" start_char="1297">reported</TOKEN>
<TOKEN end_char="1307" id="token-11-34" morph="none" pos="word" start_char="1306">by</TOKEN>
<TOKEN end_char="1316" id="token-11-35" morph="none" pos="word" start_char="1309">December</TOKEN>
<TOKEN end_char="1319" id="token-11-36" morph="none" pos="word" start_char="1318">15</TOKEN>
<TOKEN end_char="1321" id="token-11-37" morph="none" pos="punct" start_char="1321">–</TOKEN>
<TOKEN end_char="1325" id="token-11-38" morph="none" pos="word" start_char="1323">and</TOKEN>
<TOKEN end_char="1329" id="token-11-39" morph="none" pos="word" start_char="1327">the</TOKEN>
<TOKEN end_char="1336" id="token-11-40" morph="none" pos="word" start_char="1331">number</TOKEN>
<TOKEN end_char="1345" id="token-11-41" morph="none" pos="word" start_char="1338">reaching</TOKEN>
<TOKEN end_char="1348" id="token-11-42" morph="none" pos="word" start_char="1347">60</TOKEN>
<TOKEN end_char="1353" id="token-11-43" morph="none" pos="word" start_char="1350">just</TOKEN>
<TOKEN end_char="1358" id="token-11-44" morph="none" pos="word" start_char="1355">five</TOKEN>
<TOKEN end_char="1363" id="token-11-45" morph="none" pos="word" start_char="1360">days</TOKEN>
<TOKEN end_char="1369" id="token-11-46" morph="none" pos="word" start_char="1365">later</TOKEN>
<TOKEN end_char="1370" id="token-11-47" morph="none" pos="punct" start_char="1370">.</TOKEN>
</SEG>
<SEG end_char="1550" id="segment-12" start_char="1373">
<ORIGINAL_TEXT>However, it’s now been suggested that Covid-19 could have been around a lot earlier than that, amid reports of the virus possibly circulating in Italy as early as last September.</ORIGINAL_TEXT>
<TOKEN end_char="1379" id="token-12-0" morph="none" pos="word" start_char="1373">However</TOKEN>
<TOKEN end_char="1380" id="token-12-1" morph="none" pos="punct" start_char="1380">,</TOKEN>
<TOKEN end_char="1385" id="token-12-2" morph="none" pos="word" start_char="1382">it’s</TOKEN>
<TOKEN end_char="1389" id="token-12-3" morph="none" pos="word" start_char="1387">now</TOKEN>
<TOKEN end_char="1394" id="token-12-4" morph="none" pos="word" start_char="1391">been</TOKEN>
<TOKEN end_char="1404" id="token-12-5" morph="none" pos="word" start_char="1396">suggested</TOKEN>
<TOKEN end_char="1409" id="token-12-6" morph="none" pos="word" start_char="1406">that</TOKEN>
<TOKEN end_char="1418" id="token-12-7" morph="none" pos="unknown" start_char="1411">Covid-19</TOKEN>
<TOKEN end_char="1424" id="token-12-8" morph="none" pos="word" start_char="1420">could</TOKEN>
<TOKEN end_char="1429" id="token-12-9" morph="none" pos="word" start_char="1426">have</TOKEN>
<TOKEN end_char="1434" id="token-12-10" morph="none" pos="word" start_char="1431">been</TOKEN>
<TOKEN end_char="1441" id="token-12-11" morph="none" pos="word" start_char="1436">around</TOKEN>
<TOKEN end_char="1443" id="token-12-12" morph="none" pos="word" start_char="1443">a</TOKEN>
<TOKEN end_char="1447" id="token-12-13" morph="none" pos="word" start_char="1445">lot</TOKEN>
<TOKEN end_char="1455" id="token-12-14" morph="none" pos="word" start_char="1449">earlier</TOKEN>
<TOKEN end_char="1460" id="token-12-15" morph="none" pos="word" start_char="1457">than</TOKEN>
<TOKEN end_char="1465" id="token-12-16" morph="none" pos="word" start_char="1462">that</TOKEN>
<TOKEN end_char="1466" id="token-12-17" morph="none" pos="punct" start_char="1466">,</TOKEN>
<TOKEN end_char="1471" id="token-12-18" morph="none" pos="word" start_char="1468">amid</TOKEN>
<TOKEN end_char="1479" id="token-12-19" morph="none" pos="word" start_char="1473">reports</TOKEN>
<TOKEN end_char="1482" id="token-12-20" morph="none" pos="word" start_char="1481">of</TOKEN>
<TOKEN end_char="1486" id="token-12-21" morph="none" pos="word" start_char="1484">the</TOKEN>
<TOKEN end_char="1492" id="token-12-22" morph="none" pos="word" start_char="1488">virus</TOKEN>
<TOKEN end_char="1501" id="token-12-23" morph="none" pos="word" start_char="1494">possibly</TOKEN>
<TOKEN end_char="1513" id="token-12-24" morph="none" pos="word" start_char="1503">circulating</TOKEN>
<TOKEN end_char="1516" id="token-12-25" morph="none" pos="word" start_char="1515">in</TOKEN>
<TOKEN end_char="1522" id="token-12-26" morph="none" pos="word" start_char="1518">Italy</TOKEN>
<TOKEN end_char="1525" id="token-12-27" morph="none" pos="word" start_char="1524">as</TOKEN>
<TOKEN end_char="1531" id="token-12-28" morph="none" pos="word" start_char="1527">early</TOKEN>
<TOKEN end_char="1534" id="token-12-29" morph="none" pos="word" start_char="1533">as</TOKEN>
<TOKEN end_char="1539" id="token-12-30" morph="none" pos="word" start_char="1536">last</TOKEN>
<TOKEN end_char="1549" id="token-12-31" morph="none" pos="word" start_char="1541">September</TOKEN>
<TOKEN end_char="1550" id="token-12-32" morph="none" pos="punct" start_char="1550">.</TOKEN>
</SEG>
<SEG end_char="1768" id="segment-13" start_char="1553">
<ORIGINAL_TEXT>Scientists there have said they have evidence of Covid being around at that time after checking the blood samples of patients taking part in a cancer survey – and finding that some contained antibodies for the virus.</ORIGINAL_TEXT>
<TOKEN end_char="1562" id="token-13-0" morph="none" pos="word" start_char="1553">Scientists</TOKEN>
<TOKEN end_char="1568" id="token-13-1" morph="none" pos="word" start_char="1564">there</TOKEN>
<TOKEN end_char="1573" id="token-13-2" morph="none" pos="word" start_char="1570">have</TOKEN>
<TOKEN end_char="1578" id="token-13-3" morph="none" pos="word" start_char="1575">said</TOKEN>
<TOKEN end_char="1583" id="token-13-4" morph="none" pos="word" start_char="1580">they</TOKEN>
<TOKEN end_char="1588" id="token-13-5" morph="none" pos="word" start_char="1585">have</TOKEN>
<TOKEN end_char="1597" id="token-13-6" morph="none" pos="word" start_char="1590">evidence</TOKEN>
<TOKEN end_char="1600" id="token-13-7" morph="none" pos="word" start_char="1599">of</TOKEN>
<TOKEN end_char="1606" id="token-13-8" morph="none" pos="word" start_char="1602">Covid</TOKEN>
<TOKEN end_char="1612" id="token-13-9" morph="none" pos="word" start_char="1608">being</TOKEN>
<TOKEN end_char="1619" id="token-13-10" morph="none" pos="word" start_char="1614">around</TOKEN>
<TOKEN end_char="1622" id="token-13-11" morph="none" pos="word" start_char="1621">at</TOKEN>
<TOKEN end_char="1627" id="token-13-12" morph="none" pos="word" start_char="1624">that</TOKEN>
<TOKEN end_char="1632" id="token-13-13" morph="none" pos="word" start_char="1629">time</TOKEN>
<TOKEN end_char="1638" id="token-13-14" morph="none" pos="word" start_char="1634">after</TOKEN>
<TOKEN end_char="1647" id="token-13-15" morph="none" pos="word" start_char="1640">checking</TOKEN>
<TOKEN end_char="1651" id="token-13-16" morph="none" pos="word" start_char="1649">the</TOKEN>
<TOKEN end_char="1657" id="token-13-17" morph="none" pos="word" start_char="1653">blood</TOKEN>
<TOKEN end_char="1665" id="token-13-18" morph="none" pos="word" start_char="1659">samples</TOKEN>
<TOKEN end_char="1668" id="token-13-19" morph="none" pos="word" start_char="1667">of</TOKEN>
<TOKEN end_char="1677" id="token-13-20" morph="none" pos="word" start_char="1670">patients</TOKEN>
<TOKEN end_char="1684" id="token-13-21" morph="none" pos="word" start_char="1679">taking</TOKEN>
<TOKEN end_char="1689" id="token-13-22" morph="none" pos="word" start_char="1686">part</TOKEN>
<TOKEN end_char="1692" id="token-13-23" morph="none" pos="word" start_char="1691">in</TOKEN>
<TOKEN end_char="1694" id="token-13-24" morph="none" pos="word" start_char="1694">a</TOKEN>
<TOKEN end_char="1701" id="token-13-25" morph="none" pos="word" start_char="1696">cancer</TOKEN>
<TOKEN end_char="1708" id="token-13-26" morph="none" pos="word" start_char="1703">survey</TOKEN>
<TOKEN end_char="1710" id="token-13-27" morph="none" pos="punct" start_char="1710">–</TOKEN>
<TOKEN end_char="1714" id="token-13-28" morph="none" pos="word" start_char="1712">and</TOKEN>
<TOKEN end_char="1722" id="token-13-29" morph="none" pos="word" start_char="1716">finding</TOKEN>
<TOKEN end_char="1727" id="token-13-30" morph="none" pos="word" start_char="1724">that</TOKEN>
<TOKEN end_char="1732" id="token-13-31" morph="none" pos="word" start_char="1729">some</TOKEN>
<TOKEN end_char="1742" id="token-13-32" morph="none" pos="word" start_char="1734">contained</TOKEN>
<TOKEN end_char="1753" id="token-13-33" morph="none" pos="word" start_char="1744">antibodies</TOKEN>
<TOKEN end_char="1757" id="token-13-34" morph="none" pos="word" start_char="1755">for</TOKEN>
<TOKEN end_char="1761" id="token-13-35" morph="none" pos="word" start_char="1759">the</TOKEN>
<TOKEN end_char="1767" id="token-13-36" morph="none" pos="word" start_char="1763">virus</TOKEN>
<TOKEN end_char="1768" id="token-13-37" morph="none" pos="punct" start_char="1768">.</TOKEN>
</SEG>
<SEG end_char="1817" id="segment-14" start_char="1771">
<ORIGINAL_TEXT>When was the UK’s first recorded case of Covid?</ORIGINAL_TEXT>
<TOKEN end_char="1774" id="token-14-0" morph="none" pos="word" start_char="1771">When</TOKEN>
<TOKEN end_char="1778" id="token-14-1" morph="none" pos="word" start_char="1776">was</TOKEN>
<TOKEN end_char="1782" id="token-14-2" morph="none" pos="word" start_char="1780">the</TOKEN>
<TOKEN end_char="1787" id="token-14-3" morph="none" pos="word" start_char="1784">UK’s</TOKEN>
<TOKEN end_char="1793" id="token-14-4" morph="none" pos="word" start_char="1789">first</TOKEN>
<TOKEN end_char="1802" id="token-14-5" morph="none" pos="word" start_char="1795">recorded</TOKEN>
<TOKEN end_char="1807" id="token-14-6" morph="none" pos="word" start_char="1804">case</TOKEN>
<TOKEN end_char="1810" id="token-14-7" morph="none" pos="word" start_char="1809">of</TOKEN>
<TOKEN end_char="1816" id="token-14-8" morph="none" pos="word" start_char="1812">Covid</TOKEN>
<TOKEN end_char="1817" id="token-14-9" morph="none" pos="punct" start_char="1817">?</TOKEN>
</SEG>
<SEG end_char="1963" id="segment-15" start_char="1821">
<ORIGINAL_TEXT>The UK recorded its first confirmed cases of coronavirus on January 29, when two Chinese nationals fell ill at the StayCity Aparthotel in York.</ORIGINAL_TEXT>
<TOKEN end_char="1823" id="token-15-0" morph="none" pos="word" start_char="1821">The</TOKEN>
<TOKEN end_char="1826" id="token-15-1" morph="none" pos="word" start_char="1825">UK</TOKEN>
<TOKEN end_char="1835" id="token-15-2" morph="none" pos="word" start_char="1828">recorded</TOKEN>
<TOKEN end_char="1839" id="token-15-3" morph="none" pos="word" start_char="1837">its</TOKEN>
<TOKEN end_char="1845" id="token-15-4" morph="none" pos="word" start_char="1841">first</TOKEN>
<TOKEN end_char="1855" id="token-15-5" morph="none" pos="word" start_char="1847">confirmed</TOKEN>
<TOKEN end_char="1861" id="token-15-6" morph="none" pos="word" start_char="1857">cases</TOKEN>
<TOKEN end_char="1864" id="token-15-7" morph="none" pos="word" start_char="1863">of</TOKEN>
<TOKEN end_char="1876" id="token-15-8" morph="none" pos="word" start_char="1866">coronavirus</TOKEN>
<TOKEN end_char="1879" id="token-15-9" morph="none" pos="word" start_char="1878">on</TOKEN>
<TOKEN end_char="1887" id="token-15-10" morph="none" pos="word" start_char="1881">January</TOKEN>
<TOKEN end_char="1890" id="token-15-11" morph="none" pos="word" start_char="1889">29</TOKEN>
<TOKEN end_char="1891" id="token-15-12" morph="none" pos="punct" start_char="1891">,</TOKEN>
<TOKEN end_char="1896" id="token-15-13" morph="none" pos="word" start_char="1893">when</TOKEN>
<TOKEN end_char="1900" id="token-15-14" morph="none" pos="word" start_char="1898">two</TOKEN>
<TOKEN end_char="1908" id="token-15-15" morph="none" pos="word" start_char="1902">Chinese</TOKEN>
<TOKEN end_char="1918" id="token-15-16" morph="none" pos="word" start_char="1910">nationals</TOKEN>
<TOKEN end_char="1923" id="token-15-17" morph="none" pos="word" start_char="1920">fell</TOKEN>
<TOKEN end_char="1927" id="token-15-18" morph="none" pos="word" start_char="1925">ill</TOKEN>
<TOKEN end_char="1930" id="token-15-19" morph="none" pos="word" start_char="1929">at</TOKEN>
<TOKEN end_char="1934" id="token-15-20" morph="none" pos="word" start_char="1932">the</TOKEN>
<TOKEN end_char="1943" id="token-15-21" morph="none" pos="word" start_char="1936">StayCity</TOKEN>
<TOKEN end_char="1954" id="token-15-22" morph="none" pos="word" start_char="1945">Aparthotel</TOKEN>
<TOKEN end_char="1957" id="token-15-23" morph="none" pos="word" start_char="1956">in</TOKEN>
<TOKEN end_char="1962" id="token-15-24" morph="none" pos="word" start_char="1959">York</TOKEN>
<TOKEN end_char="1963" id="token-15-25" morph="none" pos="punct" start_char="1963">.</TOKEN>
</SEG>
<SEG end_char="2042" id="segment-16" start_char="1967">
<ORIGINAL_TEXT>The virus has impacted every aspect of life globally (Picture: Getty Images)</ORIGINAL_TEXT>
<TOKEN end_char="1969" id="token-16-0" morph="none" pos="word" start_char="1967">The</TOKEN>
<TOKEN end_char="1975" id="token-16-1" morph="none" pos="word" start_char="1971">virus</TOKEN>
<TOKEN end_char="1979" id="token-16-2" morph="none" pos="word" start_char="1977">has</TOKEN>
<TOKEN end_char="1988" id="token-16-3" morph="none" pos="word" start_char="1981">impacted</TOKEN>
<TOKEN end_char="1994" id="token-16-4" morph="none" pos="word" start_char="1990">every</TOKEN>
<TOKEN end_char="2001" id="token-16-5" morph="none" pos="word" start_char="1996">aspect</TOKEN>
<TOKEN end_char="2004" id="token-16-6" morph="none" pos="word" start_char="2003">of</TOKEN>
<TOKEN end_char="2009" id="token-16-7" morph="none" pos="word" start_char="2006">life</TOKEN>
<TOKEN end_char="2018" id="token-16-8" morph="none" pos="word" start_char="2011">globally</TOKEN>
<TOKEN end_char="2020" id="token-16-9" morph="none" pos="punct" start_char="2020">(</TOKEN>
<TOKEN end_char="2027" id="token-16-10" morph="none" pos="word" start_char="2021">Picture</TOKEN>
<TOKEN end_char="2028" id="token-16-11" morph="none" pos="punct" start_char="2028">:</TOKEN>
<TOKEN end_char="2034" id="token-16-12" morph="none" pos="word" start_char="2030">Getty</TOKEN>
<TOKEN end_char="2041" id="token-16-13" morph="none" pos="word" start_char="2036">Images</TOKEN>
<TOKEN end_char="2042" id="token-16-14" morph="none" pos="punct" start_char="2042">)</TOKEN>
</SEG>
<SEG end_char="2153" id="segment-17" start_char="2046">
<ORIGINAL_TEXT>On February 6 a British businessman in Brighton was diagnosed with the virus after catching it in Singapore.</ORIGINAL_TEXT>
<TOKEN end_char="2047" id="token-17-0" morph="none" pos="word" start_char="2046">On</TOKEN>
<TOKEN end_char="2056" id="token-17-1" morph="none" pos="word" start_char="2049">February</TOKEN>
<TOKEN end_char="2058" id="token-17-2" morph="none" pos="word" start_char="2058">6</TOKEN>
<TOKEN end_char="2060" id="token-17-3" morph="none" pos="word" start_char="2060">a</TOKEN>
<TOKEN end_char="2068" id="token-17-4" morph="none" pos="word" start_char="2062">British</TOKEN>
<TOKEN end_char="2080" id="token-17-5" morph="none" pos="word" start_char="2070">businessman</TOKEN>
<TOKEN end_char="2083" id="token-17-6" morph="none" pos="word" start_char="2082">in</TOKEN>
<TOKEN end_char="2092" id="token-17-7" morph="none" pos="word" start_char="2085">Brighton</TOKEN>
<TOKEN end_char="2096" id="token-17-8" morph="none" pos="word" start_char="2094">was</TOKEN>
<TOKEN end_char="2106" id="token-17-9" morph="none" pos="word" start_char="2098">diagnosed</TOKEN>
<TOKEN end_char="2111" id="token-17-10" morph="none" pos="word" start_char="2108">with</TOKEN>
<TOKEN end_char="2115" id="token-17-11" morph="none" pos="word" start_char="2113">the</TOKEN>
<TOKEN end_char="2121" id="token-17-12" morph="none" pos="word" start_char="2117">virus</TOKEN>
<TOKEN end_char="2127" id="token-17-13" morph="none" pos="word" start_char="2123">after</TOKEN>
<TOKEN end_char="2136" id="token-17-14" morph="none" pos="word" start_char="2129">catching</TOKEN>
<TOKEN end_char="2139" id="token-17-15" morph="none" pos="word" start_char="2138">it</TOKEN>
<TOKEN end_char="2142" id="token-17-16" morph="none" pos="word" start_char="2141">in</TOKEN>
<TOKEN end_char="2152" id="token-17-17" morph="none" pos="word" start_char="2144">Singapore</TOKEN>
<TOKEN end_char="2153" id="token-17-18" morph="none" pos="punct" start_char="2153">.</TOKEN>
</SEG>
<SEG end_char="2251" id="segment-18" start_char="2156">
<ORIGINAL_TEXT>The so-called ‘super spreader’ was later linked to 11 other cases, five of which were in the UK.</ORIGINAL_TEXT>
<TOKEN end_char="2158" id="token-18-0" morph="none" pos="word" start_char="2156">The</TOKEN>
<TOKEN end_char="2168" id="token-18-1" morph="none" pos="unknown" start_char="2160">so-called</TOKEN>
<TOKEN end_char="2170" id="token-18-2" morph="none" pos="punct" start_char="2170">‘</TOKEN>
<TOKEN end_char="2175" id="token-18-3" morph="none" pos="word" start_char="2171">super</TOKEN>
<TOKEN end_char="2184" id="token-18-4" morph="none" pos="word" start_char="2177">spreader</TOKEN>
<TOKEN end_char="2185" id="token-18-5" morph="none" pos="punct" start_char="2185">’</TOKEN>
<TOKEN end_char="2189" id="token-18-6" morph="none" pos="word" start_char="2187">was</TOKEN>
<TOKEN end_char="2195" id="token-18-7" morph="none" pos="word" start_char="2191">later</TOKEN>
<TOKEN end_char="2202" id="token-18-8" morph="none" pos="word" start_char="2197">linked</TOKEN>
<TOKEN end_char="2205" id="token-18-9" morph="none" pos="word" start_char="2204">to</TOKEN>
<TOKEN end_char="2208" id="token-18-10" morph="none" pos="word" start_char="2207">11</TOKEN>
<TOKEN end_char="2214" id="token-18-11" morph="none" pos="word" start_char="2210">other</TOKEN>
<TOKEN end_char="2220" id="token-18-12" morph="none" pos="word" start_char="2216">cases</TOKEN>
<TOKEN end_char="2221" id="token-18-13" morph="none" pos="punct" start_char="2221">,</TOKEN>
<TOKEN end_char="2226" id="token-18-14" morph="none" pos="word" start_char="2223">five</TOKEN>
<TOKEN end_char="2229" id="token-18-15" morph="none" pos="word" start_char="2228">of</TOKEN>
<TOKEN end_char="2235" id="token-18-16" morph="none" pos="word" start_char="2231">which</TOKEN>
<TOKEN end_char="2240" id="token-18-17" morph="none" pos="word" start_char="2237">were</TOKEN>
<TOKEN end_char="2243" id="token-18-18" morph="none" pos="word" start_char="2242">in</TOKEN>
<TOKEN end_char="2247" id="token-18-19" morph="none" pos="word" start_char="2245">the</TOKEN>
<TOKEN end_char="2250" id="token-18-20" morph="none" pos="word" start_char="2249">UK</TOKEN>
<TOKEN end_char="2251" id="token-18-21" morph="none" pos="punct" start_char="2251">.</TOKEN>
</SEG>
<SEG end_char="2405" id="segment-19" start_char="2254">
<ORIGINAL_TEXT>Later that month, on February 28, the first person to catch coronavirus in the UK was diagnosed, a man who lived in Surrey, but who had not been abroad.</ORIGINAL_TEXT>
<TOKEN end_char="2258" id="token-19-0" morph="none" pos="word" start_char="2254">Later</TOKEN>
<TOKEN end_char="2263" id="token-19-1" morph="none" pos="word" start_char="2260">that</TOKEN>
<TOKEN end_char="2269" id="token-19-2" morph="none" pos="word" start_char="2265">month</TOKEN>
<TOKEN end_char="2270" id="token-19-3" morph="none" pos="punct" start_char="2270">,</TOKEN>
<TOKEN end_char="2273" id="token-19-4" morph="none" pos="word" start_char="2272">on</TOKEN>
<TOKEN end_char="2282" id="token-19-5" morph="none" pos="word" start_char="2275">February</TOKEN>
<TOKEN end_char="2285" id="token-19-6" morph="none" pos="word" start_char="2284">28</TOKEN>
<TOKEN end_char="2286" id="token-19-7" morph="none" pos="punct" start_char="2286">,</TOKEN>
<TOKEN end_char="2290" id="token-19-8" morph="none" pos="word" start_char="2288">the</TOKEN>
<TOKEN end_char="2296" id="token-19-9" morph="none" pos="word" start_char="2292">first</TOKEN>
<TOKEN end_char="2303" id="token-19-10" morph="none" pos="word" start_char="2298">person</TOKEN>
<TOKEN end_char="2306" id="token-19-11" morph="none" pos="word" start_char="2305">to</TOKEN>
<TOKEN end_char="2312" id="token-19-12" morph="none" pos="word" start_char="2308">catch</TOKEN>
<TOKEN end_char="2324" id="token-19-13" morph="none" pos="word" start_char="2314">coronavirus</TOKEN>
<TOKEN end_char="2327" id="token-19-14" morph="none" pos="word" start_char="2326">in</TOKEN>
<TOKEN end_char="2331" id="token-19-15" morph="none" pos="word" start_char="2329">the</TOKEN>
<TOKEN end_char="2334" id="token-19-16" morph="none" pos="word" start_char="2333">UK</TOKEN>
<TOKEN end_char="2338" id="token-19-17" morph="none" pos="word" start_char="2336">was</TOKEN>
<TOKEN end_char="2348" id="token-19-18" morph="none" pos="word" start_char="2340">diagnosed</TOKEN>
<TOKEN end_char="2349" id="token-19-19" morph="none" pos="punct" start_char="2349">,</TOKEN>
<TOKEN end_char="2351" id="token-19-20" morph="none" pos="word" start_char="2351">a</TOKEN>
<TOKEN end_char="2355" id="token-19-21" morph="none" pos="word" start_char="2353">man</TOKEN>
<TOKEN end_char="2359" id="token-19-22" morph="none" pos="word" start_char="2357">who</TOKEN>
<TOKEN end_char="2365" id="token-19-23" morph="none" pos="word" start_char="2361">lived</TOKEN>
<TOKEN end_char="2368" id="token-19-24" morph="none" pos="word" start_char="2367">in</TOKEN>
<TOKEN end_char="2375" id="token-19-25" morph="none" pos="word" start_char="2370">Surrey</TOKEN>
<TOKEN end_char="2376" id="token-19-26" morph="none" pos="punct" start_char="2376">,</TOKEN>
<TOKEN end_char="2380" id="token-19-27" morph="none" pos="word" start_char="2378">but</TOKEN>
<TOKEN end_char="2384" id="token-19-28" morph="none" pos="word" start_char="2382">who</TOKEN>
<TOKEN end_char="2388" id="token-19-29" morph="none" pos="word" start_char="2386">had</TOKEN>
<TOKEN end_char="2392" id="token-19-30" morph="none" pos="word" start_char="2390">not</TOKEN>
<TOKEN end_char="2397" id="token-19-31" morph="none" pos="word" start_char="2394">been</TOKEN>
<TOKEN end_char="2404" id="token-19-32" morph="none" pos="word" start_char="2399">abroad</TOKEN>
<TOKEN end_char="2405" id="token-19-33" morph="none" pos="punct" start_char="2405">.</TOKEN>
</SEG>
<SEG end_char="2636" id="segment-20" start_char="2408">
<ORIGINAL_TEXT>However, it’s since been claimed that the UK’s first case could also have occurred earlier, after a 66-year-old man revealed he was hospitalised with a mystery illness after returning from a family holiday in Italy 14 months ago.</ORIGINAL_TEXT>
<TOKEN end_char="2414" id="token-20-0" morph="none" pos="word" start_char="2408">However</TOKEN>
<TOKEN end_char="2415" id="token-20-1" morph="none" pos="punct" start_char="2415">,</TOKEN>
<TOKEN end_char="2420" id="token-20-2" morph="none" pos="word" start_char="2417">it’s</TOKEN>
<TOKEN end_char="2426" id="token-20-3" morph="none" pos="word" start_char="2422">since</TOKEN>
<TOKEN end_char="2431" id="token-20-4" morph="none" pos="word" start_char="2428">been</TOKEN>
<TOKEN end_char="2439" id="token-20-5" morph="none" pos="word" start_char="2433">claimed</TOKEN>
<TOKEN end_char="2444" id="token-20-6" morph="none" pos="word" start_char="2441">that</TOKEN>
<TOKEN end_char="2448" id="token-20-7" morph="none" pos="word" start_char="2446">the</TOKEN>
<TOKEN end_char="2453" id="token-20-8" morph="none" pos="word" start_char="2450">UK’s</TOKEN>
<TOKEN end_char="2459" id="token-20-9" morph="none" pos="word" start_char="2455">first</TOKEN>
<TOKEN end_char="2464" id="token-20-10" morph="none" pos="word" start_char="2461">case</TOKEN>
<TOKEN end_char="2470" id="token-20-11" morph="none" pos="word" start_char="2466">could</TOKEN>
<TOKEN end_char="2475" id="token-20-12" morph="none" pos="word" start_char="2472">also</TOKEN>
<TOKEN end_char="2480" id="token-20-13" morph="none" pos="word" start_char="2477">have</TOKEN>
<TOKEN end_char="2489" id="token-20-14" morph="none" pos="word" start_char="2482">occurred</TOKEN>
<TOKEN end_char="2497" id="token-20-15" morph="none" pos="word" start_char="2491">earlier</TOKEN>
<TOKEN end_char="2498" id="token-20-16" morph="none" pos="punct" start_char="2498">,</TOKEN>
<TOKEN end_char="2504" id="token-20-17" morph="none" pos="word" start_char="2500">after</TOKEN>
<TOKEN end_char="2506" id="token-20-18" morph="none" pos="word" start_char="2506">a</TOKEN>
<TOKEN end_char="2518" id="token-20-19" morph="none" pos="unknown" start_char="2508">66-year-old</TOKEN>
<TOKEN end_char="2522" id="token-20-20" morph="none" pos="word" start_char="2520">man</TOKEN>
<TOKEN end_char="2531" id="token-20-21" morph="none" pos="word" start_char="2524">revealed</TOKEN>
<TOKEN end_char="2534" id="token-20-22" morph="none" pos="word" start_char="2533">he</TOKEN>
<TOKEN end_char="2538" id="token-20-23" morph="none" pos="word" start_char="2536">was</TOKEN>
<TOKEN end_char="2551" id="token-20-24" morph="none" pos="word" start_char="2540">hospitalised</TOKEN>
<TOKEN end_char="2556" id="token-20-25" morph="none" pos="word" start_char="2553">with</TOKEN>
<TOKEN end_char="2558" id="token-20-26" morph="none" pos="word" start_char="2558">a</TOKEN>
<TOKEN end_char="2566" id="token-20-27" morph="none" pos="word" start_char="2560">mystery</TOKEN>
<TOKEN end_char="2574" id="token-20-28" morph="none" pos="word" start_char="2568">illness</TOKEN>
<TOKEN end_char="2580" id="token-20-29" morph="none" pos="word" start_char="2576">after</TOKEN>
<TOKEN end_char="2590" id="token-20-30" morph="none" pos="word" start_char="2582">returning</TOKEN>
<TOKEN end_char="2595" id="token-20-31" morph="none" pos="word" start_char="2592">from</TOKEN>
<TOKEN end_char="2597" id="token-20-32" morph="none" pos="word" start_char="2597">a</TOKEN>
<TOKEN end_char="2604" id="token-20-33" morph="none" pos="word" start_char="2599">family</TOKEN>
<TOKEN end_char="2612" id="token-20-34" morph="none" pos="word" start_char="2606">holiday</TOKEN>
<TOKEN end_char="2615" id="token-20-35" morph="none" pos="word" start_char="2614">in</TOKEN>
<TOKEN end_char="2621" id="token-20-36" morph="none" pos="word" start_char="2617">Italy</TOKEN>
<TOKEN end_char="2624" id="token-20-37" morph="none" pos="word" start_char="2623">14</TOKEN>
<TOKEN end_char="2631" id="token-20-38" morph="none" pos="word" start_char="2626">months</TOKEN>
<TOKEN end_char="2635" id="token-20-39" morph="none" pos="word" start_char="2633">ago</TOKEN>
<TOKEN end_char="2636" id="token-20-40" morph="none" pos="punct" start_char="2636">.</TOKEN>
</SEG>
<SEG end_char="2792" id="segment-21" start_char="2639">
<ORIGINAL_TEXT>Brian Stoodley said that he suffered many of Covid’s most common symptoms during the illness, which left him bed-bound and gasping for oxygen in hospital.</ORIGINAL_TEXT>
<TOKEN end_char="2643" id="token-21-0" morph="none" pos="word" start_char="2639">Brian</TOKEN>
<TOKEN end_char="2652" id="token-21-1" morph="none" pos="word" start_char="2645">Stoodley</TOKEN>
<TOKEN end_char="2657" id="token-21-2" morph="none" pos="word" start_char="2654">said</TOKEN>
<TOKEN end_char="2662" id="token-21-3" morph="none" pos="word" start_char="2659">that</TOKEN>
<TOKEN end_char="2665" id="token-21-4" morph="none" pos="word" start_char="2664">he</TOKEN>
<TOKEN end_char="2674" id="token-21-5" morph="none" pos="word" start_char="2667">suffered</TOKEN>
<TOKEN end_char="2679" id="token-21-6" morph="none" pos="word" start_char="2676">many</TOKEN>
<TOKEN end_char="2682" id="token-21-7" morph="none" pos="word" start_char="2681">of</TOKEN>
<TOKEN end_char="2690" id="token-21-8" morph="none" pos="word" start_char="2684">Covid’s</TOKEN>
<TOKEN end_char="2695" id="token-21-9" morph="none" pos="word" start_char="2692">most</TOKEN>
<TOKEN end_char="2702" id="token-21-10" morph="none" pos="word" start_char="2697">common</TOKEN>
<TOKEN end_char="2711" id="token-21-11" morph="none" pos="word" start_char="2704">symptoms</TOKEN>
<TOKEN end_char="2718" id="token-21-12" morph="none" pos="word" start_char="2713">during</TOKEN>
<TOKEN end_char="2722" id="token-21-13" morph="none" pos="word" start_char="2720">the</TOKEN>
<TOKEN end_char="2730" id="token-21-14" morph="none" pos="word" start_char="2724">illness</TOKEN>
<TOKEN end_char="2731" id="token-21-15" morph="none" pos="punct" start_char="2731">,</TOKEN>
<TOKEN end_char="2737" id="token-21-16" morph="none" pos="word" start_char="2733">which</TOKEN>
<TOKEN end_char="2742" id="token-21-17" morph="none" pos="word" start_char="2739">left</TOKEN>
<TOKEN end_char="2746" id="token-21-18" morph="none" pos="word" start_char="2744">him</TOKEN>
<TOKEN end_char="2756" id="token-21-19" morph="none" pos="unknown" start_char="2748">bed-bound</TOKEN>
<TOKEN end_char="2760" id="token-21-20" morph="none" pos="word" start_char="2758">and</TOKEN>
<TOKEN end_char="2768" id="token-21-21" morph="none" pos="word" start_char="2762">gasping</TOKEN>
<TOKEN end_char="2772" id="token-21-22" morph="none" pos="word" start_char="2770">for</TOKEN>
<TOKEN end_char="2779" id="token-21-23" morph="none" pos="word" start_char="2774">oxygen</TOKEN>
<TOKEN end_char="2782" id="token-21-24" morph="none" pos="word" start_char="2781">in</TOKEN>
<TOKEN end_char="2791" id="token-21-25" morph="none" pos="word" start_char="2784">hospital</TOKEN>
<TOKEN end_char="2792" id="token-21-26" morph="none" pos="punct" start_char="2792">.</TOKEN>
</SEG>
<SEG end_char="2870" id="segment-22" start_char="2795">
<ORIGINAL_TEXT>Follow Metro across our social channels, on Facebook, Twitter and Instagram.</ORIGINAL_TEXT>
<TOKEN end_char="2800" id="token-22-0" morph="none" pos="word" start_char="2795">Follow</TOKEN>
<TOKEN end_char="2806" id="token-22-1" morph="none" pos="word" start_char="2802">Metro</TOKEN>
<TOKEN end_char="2813" id="token-22-2" morph="none" pos="word" start_char="2808">across</TOKEN>
<TOKEN end_char="2817" id="token-22-3" morph="none" pos="word" start_char="2815">our</TOKEN>
<TOKEN end_char="2824" id="token-22-4" morph="none" pos="word" start_char="2819">social</TOKEN>
<TOKEN end_char="2833" id="token-22-5" morph="none" pos="word" start_char="2826">channels</TOKEN>
<TOKEN end_char="2834" id="token-22-6" morph="none" pos="punct" start_char="2834">,</TOKEN>
<TOKEN end_char="2837" id="token-22-7" morph="none" pos="word" start_char="2836">on</TOKEN>
<TOKEN end_char="2846" id="token-22-8" morph="none" pos="word" start_char="2839">Facebook</TOKEN>
<TOKEN end_char="2847" id="token-22-9" morph="none" pos="punct" start_char="2847">,</TOKEN>
<TOKEN end_char="2855" id="token-22-10" morph="none" pos="word" start_char="2849">Twitter</TOKEN>
<TOKEN end_char="2859" id="token-22-11" morph="none" pos="word" start_char="2857">and</TOKEN>
<TOKEN end_char="2869" id="token-22-12" morph="none" pos="word" start_char="2861">Instagram</TOKEN>
<TOKEN end_char="2870" id="token-22-13" morph="none" pos="punct" start_char="2870">.</TOKEN>
</SEG>
<SEG end_char="2911" id="segment-23" start_char="2873">
<ORIGINAL_TEXT>Share your views in the comments below.</ORIGINAL_TEXT>
<TOKEN end_char="2877" id="token-23-0" morph="none" pos="word" start_char="2873">Share</TOKEN>
<TOKEN end_char="2882" id="token-23-1" morph="none" pos="word" start_char="2879">your</TOKEN>
<TOKEN end_char="2888" id="token-23-2" morph="none" pos="word" start_char="2884">views</TOKEN>
<TOKEN end_char="2891" id="token-23-3" morph="none" pos="word" start_char="2890">in</TOKEN>
<TOKEN end_char="2895" id="token-23-4" morph="none" pos="word" start_char="2893">the</TOKEN>
<TOKEN end_char="2904" id="token-23-5" morph="none" pos="word" start_char="2897">comments</TOKEN>
<TOKEN end_char="2910" id="token-23-6" morph="none" pos="word" start_char="2906">below</TOKEN>
<TOKEN end_char="2911" id="token-23-7" morph="none" pos="punct" start_char="2911">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>