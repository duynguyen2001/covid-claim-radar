<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CAC3" lang="spa" raw_text_char_length="954" raw_text_md5="35314d2c297e38121965a7fa9769231b" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="112" id="segment-0" start_char="1">
<ORIGINAL_TEXT>False: A RAI TV show from 2015 warned about Chinese research to create a new coronavirus from other virus parts.</ORIGINAL_TEXT>
<TOKEN end_char="5" id="token-0-0" morph="none" pos="word" start_char="1">False</TOKEN>
<TOKEN end_char="6" id="token-0-1" morph="none" pos="punct" start_char="6">:</TOKEN>
<TOKEN end_char="8" id="token-0-2" morph="none" pos="word" start_char="8">A</TOKEN>
<TOKEN end_char="12" id="token-0-3" morph="none" pos="word" start_char="10">RAI</TOKEN>
<TOKEN end_char="15" id="token-0-4" morph="none" pos="word" start_char="14">TV</TOKEN>
<TOKEN end_char="20" id="token-0-5" morph="none" pos="word" start_char="17">show</TOKEN>
<TOKEN end_char="25" id="token-0-6" morph="none" pos="word" start_char="22">from</TOKEN>
<TOKEN end_char="30" id="token-0-7" morph="none" pos="word" start_char="27">2015</TOKEN>
<TOKEN end_char="37" id="token-0-8" morph="none" pos="word" start_char="32">warned</TOKEN>
<TOKEN end_char="43" id="token-0-9" morph="none" pos="word" start_char="39">about</TOKEN>
<TOKEN end_char="51" id="token-0-10" morph="none" pos="word" start_char="45">Chinese</TOKEN>
<TOKEN end_char="60" id="token-0-11" morph="none" pos="word" start_char="53">research</TOKEN>
<TOKEN end_char="63" id="token-0-12" morph="none" pos="word" start_char="62">to</TOKEN>
<TOKEN end_char="70" id="token-0-13" morph="none" pos="word" start_char="65">create</TOKEN>
<TOKEN end_char="72" id="token-0-14" morph="none" pos="word" start_char="72">a</TOKEN>
<TOKEN end_char="76" id="token-0-15" morph="none" pos="word" start_char="74">new</TOKEN>
<TOKEN end_char="88" id="token-0-16" morph="none" pos="word" start_char="78">coronavirus</TOKEN>
<TOKEN end_char="93" id="token-0-17" morph="none" pos="word" start_char="90">from</TOKEN>
<TOKEN end_char="99" id="token-0-18" morph="none" pos="word" start_char="95">other</TOKEN>
<TOKEN end_char="105" id="token-0-19" morph="none" pos="word" start_char="101">virus</TOKEN>
<TOKEN end_char="111" id="token-0-20" morph="none" pos="word" start_char="107">parts</TOKEN>
<TOKEN end_char="112" id="token-0-21" morph="none" pos="punct" start_char="112">.</TOKEN>
</SEG>
<SEG end_char="140" id="segment-1" start_char="114">
<ORIGINAL_TEXT>It all took place in Wuhan.</ORIGINAL_TEXT>
<TOKEN end_char="115" id="token-1-0" morph="none" pos="word" start_char="114">It</TOKEN>
<TOKEN end_char="119" id="token-1-1" morph="none" pos="word" start_char="117">all</TOKEN>
<TOKEN end_char="124" id="token-1-2" morph="none" pos="word" start_char="121">took</TOKEN>
<TOKEN end_char="130" id="token-1-3" morph="none" pos="word" start_char="126">place</TOKEN>
<TOKEN end_char="133" id="token-1-4" morph="none" pos="word" start_char="132">in</TOKEN>
<TOKEN end_char="139" id="token-1-5" morph="none" pos="word" start_char="135">Wuhan</TOKEN>
<TOKEN end_char="140" id="token-1-6" morph="none" pos="punct" start_char="140">.</TOKEN>
</SEG>
<SEG end_char="274" id="segment-2" start_char="144">
<ORIGINAL_TEXT>Explanation: A RAI TV show did run a segment on a research, that resulted in an article published by Nature, involving coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="154" id="token-2-0" morph="none" pos="word" start_char="144">Explanation</TOKEN>
<TOKEN end_char="155" id="token-2-1" morph="none" pos="punct" start_char="155">:</TOKEN>
<TOKEN end_char="157" id="token-2-2" morph="none" pos="word" start_char="157">A</TOKEN>
<TOKEN end_char="161" id="token-2-3" morph="none" pos="word" start_char="159">RAI</TOKEN>
<TOKEN end_char="164" id="token-2-4" morph="none" pos="word" start_char="163">TV</TOKEN>
<TOKEN end_char="169" id="token-2-5" morph="none" pos="word" start_char="166">show</TOKEN>
<TOKEN end_char="173" id="token-2-6" morph="none" pos="word" start_char="171">did</TOKEN>
<TOKEN end_char="177" id="token-2-7" morph="none" pos="word" start_char="175">run</TOKEN>
<TOKEN end_char="179" id="token-2-8" morph="none" pos="word" start_char="179">a</TOKEN>
<TOKEN end_char="187" id="token-2-9" morph="none" pos="word" start_char="181">segment</TOKEN>
<TOKEN end_char="190" id="token-2-10" morph="none" pos="word" start_char="189">on</TOKEN>
<TOKEN end_char="192" id="token-2-11" morph="none" pos="word" start_char="192">a</TOKEN>
<TOKEN end_char="201" id="token-2-12" morph="none" pos="word" start_char="194">research</TOKEN>
<TOKEN end_char="202" id="token-2-13" morph="none" pos="punct" start_char="202">,</TOKEN>
<TOKEN end_char="207" id="token-2-14" morph="none" pos="word" start_char="204">that</TOKEN>
<TOKEN end_char="216" id="token-2-15" morph="none" pos="word" start_char="209">resulted</TOKEN>
<TOKEN end_char="219" id="token-2-16" morph="none" pos="word" start_char="218">in</TOKEN>
<TOKEN end_char="222" id="token-2-17" morph="none" pos="word" start_char="221">an</TOKEN>
<TOKEN end_char="230" id="token-2-18" morph="none" pos="word" start_char="224">article</TOKEN>
<TOKEN end_char="240" id="token-2-19" morph="none" pos="word" start_char="232">published</TOKEN>
<TOKEN end_char="243" id="token-2-20" morph="none" pos="word" start_char="242">by</TOKEN>
<TOKEN end_char="250" id="token-2-21" morph="none" pos="word" start_char="245">Nature</TOKEN>
<TOKEN end_char="251" id="token-2-22" morph="none" pos="punct" start_char="251">,</TOKEN>
<TOKEN end_char="261" id="token-2-23" morph="none" pos="word" start_char="253">involving</TOKEN>
<TOKEN end_char="273" id="token-2-24" morph="none" pos="word" start_char="263">coronavirus</TOKEN>
<TOKEN end_char="274" id="token-2-25" morph="none" pos="punct" start_char="274">.</TOKEN>
</SEG>
<SEG end_char="347" id="segment-3" start_char="276">
<ORIGINAL_TEXT>Both RAI and Nature denied any links between COVID-19 and this research.</ORIGINAL_TEXT>
<TOKEN end_char="279" id="token-3-0" morph="none" pos="word" start_char="276">Both</TOKEN>
<TOKEN end_char="283" id="token-3-1" morph="none" pos="word" start_char="281">RAI</TOKEN>
<TOKEN end_char="287" id="token-3-2" morph="none" pos="word" start_char="285">and</TOKEN>
<TOKEN end_char="294" id="token-3-3" morph="none" pos="word" start_char="289">Nature</TOKEN>
<TOKEN end_char="301" id="token-3-4" morph="none" pos="word" start_char="296">denied</TOKEN>
<TOKEN end_char="305" id="token-3-5" morph="none" pos="word" start_char="303">any</TOKEN>
<TOKEN end_char="311" id="token-3-6" morph="none" pos="word" start_char="307">links</TOKEN>
<TOKEN end_char="319" id="token-3-7" morph="none" pos="word" start_char="313">between</TOKEN>
<TOKEN end_char="328" id="token-3-8" morph="none" pos="unknown" start_char="321">COVID-19</TOKEN>
<TOKEN end_char="332" id="token-3-9" morph="none" pos="word" start_char="330">and</TOKEN>
<TOKEN end_char="337" id="token-3-10" morph="none" pos="word" start_char="334">this</TOKEN>
<TOKEN end_char="346" id="token-3-11" morph="none" pos="word" start_char="339">research</TOKEN>
<TOKEN end_char="347" id="token-3-12" morph="none" pos="punct" start_char="347">.</TOKEN>
</SEG>
<SEG end_char="467" id="segment-4" start_char="349">
<ORIGINAL_TEXT>Also, although two people involved in the research were Chinese, it was done in a lab in North Carolina, United States.</ORIGINAL_TEXT>
<TOKEN end_char="352" id="token-4-0" morph="none" pos="word" start_char="349">Also</TOKEN>
<TOKEN end_char="353" id="token-4-1" morph="none" pos="punct" start_char="353">,</TOKEN>
<TOKEN end_char="362" id="token-4-2" morph="none" pos="word" start_char="355">although</TOKEN>
<TOKEN end_char="366" id="token-4-3" morph="none" pos="word" start_char="364">two</TOKEN>
<TOKEN end_char="373" id="token-4-4" morph="none" pos="word" start_char="368">people</TOKEN>
<TOKEN end_char="382" id="token-4-5" morph="none" pos="word" start_char="375">involved</TOKEN>
<TOKEN end_char="385" id="token-4-6" morph="none" pos="word" start_char="384">in</TOKEN>
<TOKEN end_char="389" id="token-4-7" morph="none" pos="word" start_char="387">the</TOKEN>
<TOKEN end_char="398" id="token-4-8" morph="none" pos="word" start_char="391">research</TOKEN>
<TOKEN end_char="403" id="token-4-9" morph="none" pos="word" start_char="400">were</TOKEN>
<TOKEN end_char="411" id="token-4-10" morph="none" pos="word" start_char="405">Chinese</TOKEN>
<TOKEN end_char="412" id="token-4-11" morph="none" pos="punct" start_char="412">,</TOKEN>
<TOKEN end_char="415" id="token-4-12" morph="none" pos="word" start_char="414">it</TOKEN>
<TOKEN end_char="419" id="token-4-13" morph="none" pos="word" start_char="417">was</TOKEN>
<TOKEN end_char="424" id="token-4-14" morph="none" pos="word" start_char="421">done</TOKEN>
<TOKEN end_char="427" id="token-4-15" morph="none" pos="word" start_char="426">in</TOKEN>
<TOKEN end_char="429" id="token-4-16" morph="none" pos="word" start_char="429">a</TOKEN>
<TOKEN end_char="433" id="token-4-17" morph="none" pos="word" start_char="431">lab</TOKEN>
<TOKEN end_char="436" id="token-4-18" morph="none" pos="word" start_char="435">in</TOKEN>
<TOKEN end_char="442" id="token-4-19" morph="none" pos="word" start_char="438">North</TOKEN>
<TOKEN end_char="451" id="token-4-20" morph="none" pos="word" start_char="444">Carolina</TOKEN>
<TOKEN end_char="452" id="token-4-21" morph="none" pos="punct" start_char="452">,</TOKEN>
<TOKEN end_char="459" id="token-4-22" morph="none" pos="word" start_char="454">United</TOKEN>
<TOKEN end_char="466" id="token-4-23" morph="none" pos="word" start_char="461">States</TOKEN>
<TOKEN end_char="467" id="token-4-24" morph="none" pos="punct" start_char="467">.</TOKEN>
</SEG>
<SEG end_char="536" id="segment-5" start_char="469">
<ORIGINAL_TEXT>All references to Wuhan were inserted after the video was published.</ORIGINAL_TEXT>
<TOKEN end_char="471" id="token-5-0" morph="none" pos="word" start_char="469">All</TOKEN>
<TOKEN end_char="482" id="token-5-1" morph="none" pos="word" start_char="473">references</TOKEN>
<TOKEN end_char="485" id="token-5-2" morph="none" pos="word" start_char="484">to</TOKEN>
<TOKEN end_char="491" id="token-5-3" morph="none" pos="word" start_char="487">Wuhan</TOKEN>
<TOKEN end_char="496" id="token-5-4" morph="none" pos="word" start_char="493">were</TOKEN>
<TOKEN end_char="505" id="token-5-5" morph="none" pos="word" start_char="498">inserted</TOKEN>
<TOKEN end_char="511" id="token-5-6" morph="none" pos="word" start_char="507">after</TOKEN>
<TOKEN end_char="515" id="token-5-7" morph="none" pos="word" start_char="513">the</TOKEN>
<TOKEN end_char="521" id="token-5-8" morph="none" pos="word" start_char="517">video</TOKEN>
<TOKEN end_char="525" id="token-5-9" morph="none" pos="word" start_char="523">was</TOKEN>
<TOKEN end_char="535" id="token-5-10" morph="none" pos="word" start_char="527">published</TOKEN>
<TOKEN end_char="536" id="token-5-11" morph="none" pos="punct" start_char="536">.</TOKEN>
</SEG>
<SEG end_char="574" id="segment-6" start_char="539">
<ORIGINAL_TEXT>Read the Full Article (Agência Lupa)</ORIGINAL_TEXT>
<TOKEN end_char="542" id="token-6-0" morph="none" pos="word" start_char="539">Read</TOKEN>
<TOKEN end_char="546" id="token-6-1" morph="none" pos="word" start_char="544">the</TOKEN>
<TOKEN end_char="551" id="token-6-2" morph="none" pos="word" start_char="548">Full</TOKEN>
<TOKEN end_char="559" id="token-6-3" morph="none" pos="word" start_char="553">Article</TOKEN>
<TOKEN end_char="561" id="token-6-4" morph="none" pos="punct" start_char="561">(</TOKEN>
<TOKEN end_char="568" id="token-6-5" morph="none" pos="word" start_char="562">Agência</TOKEN>
<TOKEN end_char="573" id="token-6-6" morph="none" pos="word" start_char="570">Lupa</TOKEN>
<TOKEN end_char="574" id="token-6-7" morph="none" pos="punct" start_char="574">)</TOKEN>
</SEG>
<SEG end_char="618" id="segment-7" start_char="577">
<ORIGINAL_TEXT>This false claim originated from: Facebook</ORIGINAL_TEXT>
<TOKEN end_char="580" id="token-7-0" morph="none" pos="word" start_char="577">This</TOKEN>
<TOKEN end_char="586" id="token-7-1" morph="none" pos="word" start_char="582">false</TOKEN>
<TOKEN end_char="592" id="token-7-2" morph="none" pos="word" start_char="588">claim</TOKEN>
<TOKEN end_char="603" id="token-7-3" morph="none" pos="word" start_char="594">originated</TOKEN>
<TOKEN end_char="608" id="token-7-4" morph="none" pos="word" start_char="605">from</TOKEN>
<TOKEN end_char="609" id="token-7-5" morph="none" pos="punct" start_char="609">:</TOKEN>
<TOKEN end_char="618" id="token-7-6" morph="none" pos="word" start_char="611">Facebook</TOKEN>
</SEG>
<SEG end_char="726" id="segment-8" start_char="621">
<ORIGINAL_TEXT>The #CoronavirusFacts database records fact-checks published since the beginning of the COVID-19 outbreak.</ORIGINAL_TEXT>
<TOKEN end_char="623" id="token-8-0" morph="none" pos="word" start_char="621">The</TOKEN>
<TOKEN end_char="641" id="token-8-1" morph="none" pos="tag" start_char="625">#CoronavirusFacts</TOKEN>
<TOKEN end_char="650" id="token-8-2" morph="none" pos="word" start_char="643">database</TOKEN>
<TOKEN end_char="658" id="token-8-3" morph="none" pos="word" start_char="652">records</TOKEN>
<TOKEN end_char="670" id="token-8-4" morph="none" pos="unknown" start_char="660">fact-checks</TOKEN>
<TOKEN end_char="680" id="token-8-5" morph="none" pos="word" start_char="672">published</TOKEN>
<TOKEN end_char="686" id="token-8-6" morph="none" pos="word" start_char="682">since</TOKEN>
<TOKEN end_char="690" id="token-8-7" morph="none" pos="word" start_char="688">the</TOKEN>
<TOKEN end_char="700" id="token-8-8" morph="none" pos="word" start_char="692">beginning</TOKEN>
<TOKEN end_char="703" id="token-8-9" morph="none" pos="word" start_char="702">of</TOKEN>
<TOKEN end_char="707" id="token-8-10" morph="none" pos="word" start_char="705">the</TOKEN>
<TOKEN end_char="716" id="token-8-11" morph="none" pos="unknown" start_char="709">COVID-19</TOKEN>
<TOKEN end_char="725" id="token-8-12" morph="none" pos="word" start_char="718">outbreak</TOKEN>
<TOKEN end_char="726" id="token-8-13" morph="none" pos="punct" start_char="726">.</TOKEN>
</SEG>
<SEG end_char="854" id="segment-9" start_char="728">
<ORIGINAL_TEXT>The pandemic and its consequences are constantly evolving and data that was accurate weeks or even days ago might have changed.</ORIGINAL_TEXT>
<TOKEN end_char="730" id="token-9-0" morph="none" pos="word" start_char="728">The</TOKEN>
<TOKEN end_char="739" id="token-9-1" morph="none" pos="word" start_char="732">pandemic</TOKEN>
<TOKEN end_char="743" id="token-9-2" morph="none" pos="word" start_char="741">and</TOKEN>
<TOKEN end_char="747" id="token-9-3" morph="none" pos="word" start_char="745">its</TOKEN>
<TOKEN end_char="760" id="token-9-4" morph="none" pos="word" start_char="749">consequences</TOKEN>
<TOKEN end_char="764" id="token-9-5" morph="none" pos="word" start_char="762">are</TOKEN>
<TOKEN end_char="775" id="token-9-6" morph="none" pos="word" start_char="766">constantly</TOKEN>
<TOKEN end_char="784" id="token-9-7" morph="none" pos="word" start_char="777">evolving</TOKEN>
<TOKEN end_char="788" id="token-9-8" morph="none" pos="word" start_char="786">and</TOKEN>
<TOKEN end_char="793" id="token-9-9" morph="none" pos="word" start_char="790">data</TOKEN>
<TOKEN end_char="798" id="token-9-10" morph="none" pos="word" start_char="795">that</TOKEN>
<TOKEN end_char="802" id="token-9-11" morph="none" pos="word" start_char="800">was</TOKEN>
<TOKEN end_char="811" id="token-9-12" morph="none" pos="word" start_char="804">accurate</TOKEN>
<TOKEN end_char="817" id="token-9-13" morph="none" pos="word" start_char="813">weeks</TOKEN>
<TOKEN end_char="820" id="token-9-14" morph="none" pos="word" start_char="819">or</TOKEN>
<TOKEN end_char="825" id="token-9-15" morph="none" pos="word" start_char="822">even</TOKEN>
<TOKEN end_char="830" id="token-9-16" morph="none" pos="word" start_char="827">days</TOKEN>
<TOKEN end_char="834" id="token-9-17" morph="none" pos="word" start_char="832">ago</TOKEN>
<TOKEN end_char="840" id="token-9-18" morph="none" pos="word" start_char="836">might</TOKEN>
<TOKEN end_char="845" id="token-9-19" morph="none" pos="word" start_char="842">have</TOKEN>
<TOKEN end_char="853" id="token-9-20" morph="none" pos="word" start_char="847">changed</TOKEN>
<TOKEN end_char="854" id="token-9-21" morph="none" pos="punct" start_char="854">.</TOKEN>
</SEG>
<SEG end_char="950" id="segment-10" start_char="856">
<ORIGINAL_TEXT>Remember to check the date when the fact-check you are reading was published before sharing it.</ORIGINAL_TEXT>
<TOKEN end_char="863" id="token-10-0" morph="none" pos="word" start_char="856">Remember</TOKEN>
<TOKEN end_char="866" id="token-10-1" morph="none" pos="word" start_char="865">to</TOKEN>
<TOKEN end_char="872" id="token-10-2" morph="none" pos="word" start_char="868">check</TOKEN>
<TOKEN end_char="876" id="token-10-3" morph="none" pos="word" start_char="874">the</TOKEN>
<TOKEN end_char="881" id="token-10-4" morph="none" pos="word" start_char="878">date</TOKEN>
<TOKEN end_char="886" id="token-10-5" morph="none" pos="word" start_char="883">when</TOKEN>
<TOKEN end_char="890" id="token-10-6" morph="none" pos="word" start_char="888">the</TOKEN>
<TOKEN end_char="901" id="token-10-7" morph="none" pos="unknown" start_char="892">fact-check</TOKEN>
<TOKEN end_char="905" id="token-10-8" morph="none" pos="word" start_char="903">you</TOKEN>
<TOKEN end_char="909" id="token-10-9" morph="none" pos="word" start_char="907">are</TOKEN>
<TOKEN end_char="917" id="token-10-10" morph="none" pos="word" start_char="911">reading</TOKEN>
<TOKEN end_char="921" id="token-10-11" morph="none" pos="word" start_char="919">was</TOKEN>
<TOKEN end_char="931" id="token-10-12" morph="none" pos="word" start_char="923">published</TOKEN>
<TOKEN end_char="938" id="token-10-13" morph="none" pos="word" start_char="933">before</TOKEN>
<TOKEN end_char="946" id="token-10-14" morph="none" pos="word" start_char="940">sharing</TOKEN>
<TOKEN end_char="949" id="token-10-15" morph="none" pos="word" start_char="948">it</TOKEN>
<TOKEN end_char="950" id="token-10-16" morph="none" pos="punct" start_char="950">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>